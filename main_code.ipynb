{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import preprocessing\n",
    "import numpy as np\n",
    "from math import sqrt\n",
    "from transformers import LlamaConfig, LlamaModel, LlamaTokenizer, GPT2Config, GPT2Model, GPT2Tokenizer, BertConfig, BertModel, BertTokenizer\n",
    "import transformers\n",
    "import accelerate\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor\n",
    "from torch.nn.utils import weight_norm\n",
    "import math\n",
    "from torch.utils.data import TensorDataset, DataLoader, random_split, Subset\n",
    "import torch\n",
    "from torchmetrics.regression import R2Score\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from transformers import AutoModel, AutoTokenizer, AutoConfig\n",
    "from transformers import BitsAndBytesConfig\n",
    "from sklearn.model_selection import KFold, StratifiedKFold, cross_val_score\n",
    "import random\n",
    "from sklearn import model_selection\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, LabelEncoder\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import time\n",
    "import itertools\n",
    "import ramanspy as rs\n",
    "from ramanspy import preprocessing\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from scipy.signal import savgol_filter\n",
    "from sklearn.metrics import accuracy_score, f1_score"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Parameter     | Meaning | \n",
    "|:---------|:---:|\n",
    "| pred_len   | How many futuresteps the model should predict./Number of classes if classficiation  |    \n",
    "| seq_len   | How many past time-steps are given the model as input.   |     \n",
    "| d_ff     | The size of the vectors extracted from the LLM’s hidden states before the final head. This is the “feature‐for‐forecast” dimension.  |      \n",
    "| patch_len     | How many consecutive time‐steps make up one patch when we slice your series.  |   \n",
    "| stride     | How far you shift your sliding window between patches.   |   \n",
    "| llm_layers     | How many Transformer layers we want to use from the pretrained LLM.  |   \n",
    "| d_model     | The hidden‐dimensionality of your patch embeddings (the output of the Conv1d). In Transformer terms, this is the “model width.”  |   \n",
    "| n_heads     | The number of attention heads in your ReprogrammingLayer.   |   \n",
    "| enc_in     | Number of parallel input channels (i.e. number of series or variables).   |   \n",
    "| d_llm     | The hidden‐state size of the LLM.  |   \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Details:\n",
    "    def __init__(self, pred_len,seq_len,d_ff,stride,llm_layers, description, dropout, \n",
    "                 d_model, n_heads,enc_in, patch_len, d_llm,llm_model, lr, task = 'regression'):\n",
    "        self.task = task\n",
    "        self.pred_len = pred_len #Time steps to predict; 1 in our case\n",
    "        self.seq_len = seq_len # length of historical input sequence\n",
    "        self.d_ff = d_ff #Features extracted from the LLM hidden states\n",
    "        self.patch_len = patch_len\n",
    "        self.stride = stride\n",
    "        self.llm_layers = llm_layers\n",
    "        self.content = description\n",
    "        self.dropout = dropout\n",
    "        self.d_model = d_model\n",
    "        self.enc_in = enc_in #number of variables ie number of parallel time series; 1 in our case becasue univariate time series\n",
    "        self.n_heads = n_heads\n",
    "        self.d_llm = d_llm\n",
    "        self.llm_model = llm_model\n",
    "        self.lr = lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformers.logging.set_verbosity_error()\n",
    "\n",
    "'''\n",
    "class ClassificationHead(nn.Module):\n",
    "    def __init__(self, nf, num_classes, head_dropout=0):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(nf, num_classes)\n",
    "        self.dropout = nn.Dropout(head_dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.mean(x, dim=1)        \n",
    "        x = torch.mean(x, dim=-1)\n",
    "        x = self.dropout(x)\n",
    "        x = self.linear(x)\n",
    "        return x\n",
    "'''\n",
    "class FlattenHead(nn.Module):\n",
    "    def __init__(self, n_vars, nf, target_window, head_dropout=0):\n",
    "        super().__init__()\n",
    "        self.n_vars = n_vars\n",
    "        self.flatten = nn.Flatten(start_dim=-2)\n",
    "        self.linear = nn.Linear(nf, target_window)\n",
    "        self.dropout = nn.Dropout(head_dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        x = self.linear(x)\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "    \n",
    "class Model(nn.Module):\n",
    "    #configs is the details of the task\n",
    "    def __init__(self, configs, patch_len=16, stride=8):\n",
    "        super(Model, self).__init__()\n",
    "        self.task = configs.task\n",
    "        self.pred_len = configs.pred_len\n",
    "        self.seq_len = configs.seq_len\n",
    "        self.d_ff = configs.d_ff\n",
    "        self.top_k = 5\n",
    "        self.d_llm = configs.d_llm\n",
    "        self.patch_len = configs.patch_len\n",
    "        self.stride = configs.stride\n",
    "\n",
    "        if configs.llm_model == 'LLAMA':\n",
    "            self.llama_config = LlamaConfig.from_pretrained('huggyllama/llama-7b')\n",
    "            self.llama_config.num_hidden_layers = configs.llm_layers\n",
    "            self.llama_config.output_attentions = True\n",
    "            self.llama_config.output_hidden_states = True\n",
    "\n",
    "            self.llm_model = LlamaModel.from_pretrained(\n",
    "                        'huggyllama/llama-7b',\n",
    "                        trust_remote_code=True,\n",
    "                        local_files_only=True,\n",
    "                        config=self.llama_config,\n",
    "                    )\n",
    "            \n",
    "            self.tokenizer = LlamaTokenizer.from_pretrained(\n",
    "                        'huggyllama/llama-7b',\n",
    "                        trust_remote_code=True,\n",
    "                        local_files_only=True\n",
    "                    )\n",
    "        elif configs.llm_model == 'GPT2':\n",
    "\n",
    "            self.gpt2_config = GPT2Config.from_pretrained('openai-community/gpt2')\n",
    "            self.gpt2_config.num_hidden_layers = configs.llm_layers\n",
    "            self.gpt2_config.output_attentions = True\n",
    "            self.gpt2_config.output_hidden_states = True\n",
    "\n",
    "            self.llm_model = GPT2Model.from_pretrained(\n",
    "                        'openai-community/gpt2',\n",
    "                        trust_remote_code=True,\n",
    "                        #local_files_only=True,\n",
    "                        config=self.gpt2_config,\n",
    "                    )\n",
    "            self.tokenizer = GPT2Tokenizer.from_pretrained(\n",
    "                        'openai-community/gpt2',\n",
    "                        trust_remote_code=True,\n",
    "                        #local_files_only=True\n",
    "                    )\n",
    "            \n",
    "        elif configs.llm_model == 'BERT':\n",
    "\n",
    "            self.bert_config = BertConfig.from_pretrained('google-bert/bert-base-uncased')\n",
    "\n",
    "            self.bert_config.num_hidden_layers = configs.llm_layers\n",
    "            self.bert_config.output_attentions = True\n",
    "            self.bert_config.output_hidden_states = True\n",
    "\n",
    "            self.llm_model = BertModel.from_pretrained(\n",
    "                    'google-bert/bert-base-uncased',\n",
    "                    trust_remote_code=True,\n",
    "                    local_files_only=True,\n",
    "                    config=self.bert_config,\n",
    "                )\n",
    "    \n",
    "            self.tokenizer = BertTokenizer.from_pretrained(\n",
    "                    'google-bert/bert-base-uncased',\n",
    "                    trust_remote_code=True,\n",
    "                    local_files_only=True\n",
    "                )\n",
    "\n",
    "        else:\n",
    "            bnb_config = BitsAndBytesConfig(\n",
    "                load_in_8bit=True,  \n",
    "                bnb_4bit_use_double_quant=True,\n",
    "                bnb_4bit_compute_dtype=torch.float16,\n",
    "                bnb_4bit_quant_type=\"nf4\",\n",
    "            )\n",
    "\n",
    "            self.llama_config = AutoConfig.from_pretrained(\"TheBloke/LLaMA-7B-GGML\")\n",
    "            self.llama_config.num_hidden_layers = configs.llm_layers\n",
    "            self.llama_config.output_attentions = True\n",
    "            self.llama_config.output_hidden_states = True\n",
    "\n",
    "            self.llm_model = AutoModel.from_pretrained(\n",
    "                \"TheBloke/LLaMA-7B-GGML\",\n",
    "                config=self.llama_config,\n",
    "                device_map=\"auto\",\n",
    "                quantization_config=bnb_config,\n",
    "                trust_remote_code=True,\n",
    "                from_tf=True\n",
    "            )\n",
    "\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(\n",
    "                \"TheBloke/LLaMA-7B-GGML\",\n",
    "                trust_remote_code=True\n",
    "            )\n",
    "\n",
    "        \n",
    "        if self.tokenizer.eos_token:\n",
    "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "        else:\n",
    "            pad_token = '[PAD]'\n",
    "            self.tokenizer.add_special_tokens({'pad_token': pad_token})\n",
    "            self.tokenizer.pad_token = pad_token\n",
    "\n",
    "        #Freeze parameters of LLM:\n",
    "        for param in self.llm_model.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        self.description = configs.content\n",
    "\n",
    "        self.dropout = nn.Dropout(configs.dropout)\n",
    "\n",
    "        self.patch_embedding = PatchEmbedding(\n",
    "            configs.d_model, self.patch_len, self.stride, configs.dropout)\n",
    "\n",
    "        self.word_embeddings = self.llm_model.get_input_embeddings().weight\n",
    "        self.vocab_size = self.word_embeddings.shape[0]\n",
    "        self.num_tokens = 1000\n",
    "        self.mapping_layer = nn.Linear(self.vocab_size, self.num_tokens)\n",
    "\n",
    "        self.reprogramming_layer = ReprogrammingLayer(configs.d_model, configs.n_heads, self.d_ff, self.d_llm)\n",
    "\n",
    "\n",
    "        self.patch_nums = int((configs.seq_len - self.patch_len) / self.stride + 2)\n",
    "        self.head_nf = self.d_ff * self.patch_nums\n",
    "\n",
    "        #if configs.task == 'classification':\n",
    "        #    self.output_projection = ClassificationHead(self.d_ff, configs.pred_len, head_dropout=configs.dropout)   \n",
    "        #else:\n",
    "        self.output_projection = FlattenHead(configs.enc_in, self.head_nf, self.pred_len, head_dropout=configs.dropout)\n",
    "\n",
    "        self.normalize_layers = Normalize(configs.enc_in, affine=False)\n",
    "\n",
    "    def forward(self, x_enc, x_mark_enc, x_dec, x_mark_dec, mask=None):\n",
    "        if self.task == 'classification':\n",
    "            f = self.classify(x_enc, x_mark_enc, x_dec, x_mark_dec)\n",
    "        else:\n",
    "            dec_out = self.forecast(x_enc, x_mark_enc, x_dec, x_mark_dec)\n",
    "            f = dec_out[:, -self.pred_len:, :]\n",
    "        return f\n",
    "\n",
    "    def forecast(self, x_enc, x_mark_enc, x_dec, x_mark_dec):\n",
    "        x_enc = self.normalize_layers(x_enc, 'norm')\n",
    "        B, T, N = x_enc.size()\n",
    "\n",
    "        x_enc = x_enc.permute(0, 2, 1).contiguous().reshape(B * N, T, 1)\n",
    "\n",
    "        min_values = torch.min(x_enc, dim=1)[0]        \n",
    "        max_values = torch.max(x_enc, dim=1)[0]        \n",
    "        medians = torch.median(x_enc, dim=1).values \n",
    " \n",
    "        lags = self.calcute_lags(x_enc)          \n",
    "         \n",
    "        trends = x_enc.diff(dim=1).sum(dim=1)      \n",
    "\n",
    "        prompt = []\n",
    "\n",
    "        for b in range(x_enc.shape[0]):  \n",
    "            min_values_str    = str(min_values[b].tolist()[0])\n",
    "            max_values_str    = str(max_values[b].tolist()[0])\n",
    "            median_values_str = str(medians[b].tolist()[0])\n",
    "            lags_values_str   = str(lags[b].tolist())\n",
    "            prompt_ = (\n",
    "                f\"<|start_prompt|>Dataset description: {self.description} \"\n",
    "                f\"Task description: forecast the next {str(self.pred_len)} steps \"\n",
    "                #f\"Task description: forecast the next step\"\n",
    "                f\"given the previous {str(self.seq_len)} steps information; \"\n",
    "                \"Input statistics: \"\n",
    "                f\"min value {min_values_str}, \"\n",
    "                f\"max value {max_values_str}, \"\n",
    "                f\"median value {median_values_str}, \"\n",
    "                f\"the trend of input is {'upward' if trends[b] > 0 else 'downward'}, \"\n",
    "                f\"top 5 lags are: {lags_values_str}<|<end_prompt>|>\"\n",
    "            )\n",
    "            prompt.append(prompt_)\n",
    "            \n",
    "        x_enc = x_enc.reshape(B, N, T).permute(0, 2, 1).contiguous()\n",
    "\n",
    "        prompt = self.tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True, max_length=2048).input_ids\n",
    "        prompt_embeddings = self.llm_model.get_input_embeddings()(prompt.to(x_enc.device))  # (batch, prompt_token, dim)\n",
    "\n",
    "        source_embeddings = self.mapping_layer(self.word_embeddings.permute(1, 0)).permute(1, 0)\n",
    "\n",
    "        x_enc = x_enc.permute(0, 2, 1).contiguous()\n",
    "        enc_out, n_vars = self.patch_embedding(x_enc) #x_enc.to(torch.bfloat16)\n",
    "        enc_out = self.reprogramming_layer(enc_out, source_embeddings, source_embeddings)\n",
    "        llama_enc_out = torch.cat([prompt_embeddings, enc_out], dim=1)\n",
    "        dec_out = self.llm_model(inputs_embeds=llama_enc_out).last_hidden_state\n",
    "        dec_out = dec_out[:, :, :self.d_ff]\n",
    "\n",
    "        dec_out = torch.reshape(\n",
    "            dec_out, (-1, n_vars, dec_out.shape[-2], dec_out.shape[-1]))\n",
    "        dec_out = dec_out.permute(0, 1, 3, 2).contiguous()\n",
    "\n",
    "        dec_out = self.output_projection(dec_out[:, :, :, -self.patch_nums:])\n",
    "        dec_out = dec_out.permute(0, 2, 1).contiguous()\n",
    "\n",
    "        dec_out = self.normalize_layers(dec_out, 'denorm')\n",
    "\n",
    "        return dec_out\n",
    "\n",
    "    def calcute_lags(self, x_enc):\n",
    "        q_fft = torch.fft.rfft(x_enc.permute(0, 2, 1).contiguous(), dim=-1)\n",
    "        k_fft = torch.fft.rfft(x_enc.permute(0, 2, 1).contiguous(), dim=-1)\n",
    "        res = q_fft * torch.conj(k_fft)\n",
    "        corr = torch.fft.irfft(res, dim=-1)\n",
    "        mean_value = torch.mean(corr, dim=1)\n",
    "        _, lags = torch.topk(mean_value, self.top_k, dim=-1)\n",
    "        return lags\n",
    "    \n",
    "    def classify(self, x_enc, x_mark_enc, x_dec, x_mark_dec):\n",
    "        x_enc = self.normalize_layers(x_enc, 'norm')\n",
    "        B, T, N = x_enc.size()\n",
    "        x_enc = x_enc.permute(0, 2, 1).contiguous().reshape(B * N, T, 1)\n",
    "  \n",
    "        min_values = torch.min(x_enc, dim=1)[0]        \n",
    "        max_values = torch.max(x_enc, dim=1)[0]        \n",
    "        medians = torch.median(x_enc, dim=1).values \n",
    " \n",
    "        lags = self.calcute_lags(x_enc)          \n",
    "         \n",
    "        trends = x_enc.diff(dim=1).sum(dim=1)      \n",
    "\n",
    "        prompt = []\n",
    "\n",
    "        for b in range(x_enc.shape[0]):  \n",
    "            min_values_str    = str(min_values[b].tolist()[0])\n",
    "            max_values_str    = str(max_values[b].tolist()[0])\n",
    "            median_values_str = str(medians[b].tolist()[0])\n",
    "            lags_values_str   = str(lags[b].tolist())\n",
    "            prompt_ = (\n",
    "                f\"<|start_prompt|>Dataset description: {self.description} \"\n",
    "                f\"Task description: classify the input sequence in one of the {str(self.pred_len)} classes \"\n",
    "                f\"given the previous {str(self.seq_len)} steps information; \"\n",
    "                \"Input statistics: \"\n",
    "                f\"min value {min_values_str}, \"\n",
    "                f\"max value {max_values_str}, \"\n",
    "                f\"median value {median_values_str}, \"\n",
    "                f\"the trend of input is {'upward' if trends[b] > 0 else 'downward'}, \"\n",
    "                f\"top 5 lags are: {lags_values_str}<|<end_prompt>|>\"\n",
    "            )\n",
    "            prompt.append(prompt_)\n",
    "            \n",
    "        x_enc = x_enc.reshape(B, N, T).permute(0, 2, 1).contiguous()\n",
    "\n",
    "        prompt = self.tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True, max_length=2048).input_ids\n",
    "        prompt_embeddings = self.llm_model.get_input_embeddings()(prompt.to(x_enc.device))  # (batch, prompt_token, dim)\n",
    "\n",
    "        source_embeddings = self.mapping_layer(self.word_embeddings.permute(1, 0)).permute(1, 0)\n",
    "\n",
    "        x_enc = x_enc.permute(0, 2, 1).contiguous()\n",
    "        enc_out, n_vars = self.patch_embedding(x_enc) #x_enc.to(torch.bfloat16)\n",
    "\n",
    "        enc_out = self.reprogramming_layer(enc_out, source_embeddings, source_embeddings)\n",
    "\n",
    "        llama_enc_out = torch.cat([prompt_embeddings, enc_out], dim=1)\n",
    "        dec_out = self.llm_model(inputs_embeds=llama_enc_out).last_hidden_state\n",
    "\n",
    "        dec_out = dec_out[:, :, :self.d_ff]\n",
    "        \n",
    "        dec_out = torch.reshape(dec_out, (-1, n_vars, dec_out.shape[-2], dec_out.shape[-1]))\n",
    "        dec_out = dec_out.permute(0, 1, 3, 2).contiguous()\n",
    "\n",
    "        #dec_out = self.output_projection(dec_out)\n",
    "        dec_out = self.output_projection(dec_out[:, :, :, -self.patch_nums:]).squeeze(1)\n",
    "        return dec_out  \n",
    "    \n",
    "    \n",
    "class ReprogrammingLayer(nn.Module):\n",
    "    \n",
    "    def __init__(self, d_model, n_heads, d_keys=None, d_llm=None, attention_dropout=0.1):\n",
    "        super(ReprogrammingLayer, self).__init__()\n",
    "\n",
    "        d_keys = d_keys or (d_model // n_heads)\n",
    "\n",
    "        self.query_projection = nn.Linear(d_model, d_keys * n_heads)\n",
    "        self.key_projection = nn.Linear(d_llm, d_keys * n_heads)\n",
    "        self.value_projection = nn.Linear(d_llm, d_keys * n_heads)\n",
    "        self.out_projection = nn.Linear(d_keys * n_heads, d_llm)\n",
    "        self.n_heads = n_heads\n",
    "        self.dropout = nn.Dropout(attention_dropout)\n",
    "    \n",
    "    def forward(self, target_embedding, source_embedding, value_embedding):\n",
    "        B, L, _ = target_embedding.shape\n",
    "        S, _ = source_embedding.shape\n",
    "        H = self.n_heads\n",
    "\n",
    "        target_embedding = self.query_projection(target_embedding).view(B, L, H, -1)\n",
    "        source_embedding = self.key_projection(source_embedding).view(S, H, -1)\n",
    "        value_embedding = self.value_projection(value_embedding).view(S, H, -1)\n",
    "\n",
    "        out = self.reprogramming(target_embedding, source_embedding, value_embedding)\n",
    "\n",
    "        out = out.reshape(B, L, -1)\n",
    "\n",
    "        return self.out_projection(out)\n",
    "\n",
    "    def reprogramming(self, target_embedding, source_embedding, value_embedding):\n",
    "        B, L, H, E = target_embedding.shape\n",
    "\n",
    "        scale = 1. / sqrt(E)\n",
    "\n",
    "        scores = torch.einsum(\"blhe,she->bhls\", target_embedding, source_embedding)\n",
    "\n",
    "        A = self.dropout(torch.softmax(scale * scores, dim=-1))\n",
    "        reprogramming_embedding = torch.einsum(\"bhls,she->blhe\", A, value_embedding)\n",
    "\n",
    "        return reprogramming_embedding\n",
    "    \n",
    "class TokenEmbedding(nn.Module):\n",
    "    def __init__(self, c_in, d_model):\n",
    "        super(TokenEmbedding, self).__init__()\n",
    "        padding = 1 if torch.__version__ >= '1.5.0' else 2\n",
    "        self.tokenConv = nn.Conv1d(in_channels=c_in, out_channels=d_model,\n",
    "                                   kernel_size=3, padding=padding, padding_mode='circular', bias=False)\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv1d):\n",
    "                nn.init.kaiming_normal_(\n",
    "                    m.weight, mode='fan_in', nonlinearity='leaky_relu')\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.tokenConv(x.permute(0, 2, 1)).transpose(1, 2)\n",
    "        return x\n",
    "\n",
    "\n",
    "class ReplicationPad1d(nn.Module):\n",
    "    def __init__(self, padding) -> None:\n",
    "        super(ReplicationPad1d, self).__init__()\n",
    "        self.padding = padding\n",
    "\n",
    "    def forward(self, input: Tensor) -> Tensor:\n",
    "        replicate_padding = input[:, :, -1].unsqueeze(-1).repeat(1, 1, self.padding[-1])\n",
    "        output = torch.cat([input, replicate_padding], dim=-1)\n",
    "        return output\n",
    "\n",
    "\n",
    "class PatchEmbedding(nn.Module):\n",
    "    def __init__(self, d_model, patch_len, stride, dropout):\n",
    "        super(PatchEmbedding, self).__init__()\n",
    "        # Patching\n",
    "        self.patch_len = patch_len\n",
    "        self.stride = stride\n",
    "        self.padding_patch_layer = ReplicationPad1d((0, stride))\n",
    "\n",
    "        # Backbone, Input encoding: projection of feature vectors onto a d-dim vector space\n",
    "        self.value_embedding = TokenEmbedding(patch_len, d_model)\n",
    "\n",
    "        # Positional embedding\n",
    "        # self.position_embedding = PositionalEmbedding(d_model)\n",
    "\n",
    "        # Residual dropout\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # do patching\n",
    "        n_vars = x.shape[1]\n",
    "        x = self.padding_patch_layer(x)\n",
    "        x = x.unfold(dimension=-1, size=self.patch_len, step=self.stride)\n",
    "        x = torch.reshape(x, (x.shape[0] * x.shape[1], x.shape[2], x.shape[3]))\n",
    "        # Input encoding\n",
    "        x = self.value_embedding(x)\n",
    "        return self.dropout(x), n_vars\n",
    "\n",
    "class Normalize(nn.Module):\n",
    "    def __init__(self, num_features: int, eps=1e-5, affine=False, subtract_last=False, non_norm=False):\n",
    "        \"\"\"\n",
    "        :param num_features: the number of features or channels\n",
    "        :param eps: a value added for numerical stability\n",
    "        :param affine: if True, RevIN has learnable affine parameters\n",
    "        \"\"\"\n",
    "        super(Normalize, self).__init__()\n",
    "        self.num_features = num_features\n",
    "        self.eps = eps\n",
    "        self.affine = affine\n",
    "        self.subtract_last = subtract_last\n",
    "        self.non_norm = non_norm\n",
    "        if self.affine:\n",
    "            self._init_params()\n",
    "\n",
    "    def forward(self, x, mode: str):\n",
    "        if mode == 'norm':\n",
    "            self._get_statistics(x)\n",
    "            x = self._normalize(x)\n",
    "        elif mode == 'denorm':\n",
    "            x = self._denormalize(x)\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "        return x\n",
    "\n",
    "    def _init_params(self):\n",
    "        # initialize RevIN params: (C,)\n",
    "        self.affine_weight = nn.Parameter(torch.ones(self.num_features))\n",
    "        self.affine_bias = nn.Parameter(torch.zeros(self.num_features))\n",
    "\n",
    "    def _get_statistics(self, x):\n",
    "        dim2reduce = tuple(range(1, x.ndim - 1))\n",
    "        if self.subtract_last:\n",
    "            self.last = x[:, -1, :].unsqueeze(1)\n",
    "        else:\n",
    "            self.mean = torch.mean(x, dim=dim2reduce, keepdim=True).detach()\n",
    "        self.stdev = torch.sqrt(torch.var(x, dim=dim2reduce, keepdim=True, unbiased=False) + self.eps).detach()\n",
    "\n",
    "    def _normalize(self, x):\n",
    "        if self.non_norm:\n",
    "            return x\n",
    "        if self.subtract_last:\n",
    "            x = x - self.last\n",
    "        else:\n",
    "            x = x - self.mean\n",
    "        x = x / self.stdev\n",
    "        if self.affine:\n",
    "            x = x * self.affine_weight\n",
    "            x = x + self.affine_bias\n",
    "        return x\n",
    "\n",
    "    def _denormalize(self, x):\n",
    "        if self.non_norm:\n",
    "            return x\n",
    "        if self.affine:\n",
    "            x = x - self.affine_bias\n",
    "            x = x / (self.affine_weight + self.eps * self.eps)\n",
    "        x = x * self.stdev\n",
    "        if self.subtract_last:\n",
    "            x = x + self.last\n",
    "        else:\n",
    "            x = x + self.mean\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def supervised_data(data, in_length, out_length):\n",
    "    X = pd.DataFrame()\n",
    "    Y = pd.DataFrame()\n",
    "    n = data.shape[0]\n",
    "    for col in data.columns:\n",
    "        x = []\n",
    "        y = []\n",
    "        for i in range(n - in_length - out_length + 1):\n",
    "            x2 = data.loc[i:i + in_length - 1,col]\n",
    "            y2 = data.loc[i + in_length:i + in_length + out_length -1,col]\n",
    "            x.extend(x2)\n",
    "            y.extend(y2)\n",
    "        X[col] = x\n",
    "        Y[col] = y\n",
    "    return X, Y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pipeline to train raman datasets only:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(configs, train, validation, epochs, batch_size, updates, earlystop, patience = 10):\n",
    "\n",
    "    #Scale spectra\n",
    "\n",
    "    train.columns = train.columns.astype(str)\n",
    "    scaler = StandardScaler()\n",
    "    normalized = scaler.fit_transform(train.iloc[:,1:].T).T\n",
    "    X_train_n = pd.DataFrame(normalized, columns = train.iloc[:,1:].columns)\n",
    "\n",
    "    validation.columns = validation.columns.astype(str)\n",
    "    normalized = scaler.fit_transform(validation.iloc[:,1:].T).T\n",
    "    X_val_n = pd.DataFrame(normalized, columns = validation.iloc[:,1:].columns)\n",
    "\n",
    "    #Scale wavelengths\n",
    "    stand = StandardScaler()\n",
    "    scaled = stand.fit_transform(X_train_n)\n",
    "    X_train_n = pd.DataFrame(scaled, columns = X_train_n.columns)\n",
    "    scaled = stand.transform(X_val_n)\n",
    "    X_val_n = pd.DataFrame(scaled, columns = X_val_n.columns)\n",
    "    \n",
    "    #Prepare tensor dataset\n",
    "\n",
    "    pred_len = configs.pred_len\n",
    "    seq_len = configs.seq_len\n",
    "    n_vars = 1\n",
    "    N_train =  X_train_n.shape[0]\n",
    "    N_val = X_val_n.shape[0]\n",
    "\n",
    "    X = torch.from_numpy(X_train_n.iloc[:,:].to_numpy().astype('float32')).unsqueeze(-1)\n",
    "    y = torch.from_numpy(train.iloc[:,0].to_numpy().astype('float32')).unsqueeze(1).unsqueeze(1)\n",
    "                \n",
    "    X2 = torch.from_numpy(X_val_n.iloc[:,:].to_numpy().astype('float32')).unsqueeze(-1)\n",
    "    y2 = torch.from_numpy(validation.iloc[:,0].to_numpy().astype('float32')).unsqueeze(1).unsqueeze(1)\n",
    "                \n",
    "    train_ds = TensorDataset(X, y)\n",
    "    val_ds = TensorDataset(X2, y2)\n",
    "\n",
    "    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "    val_loader  = DataLoader(val_ds,  batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    #Train model \n",
    "  \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = Model(configs).to(device)\n",
    "    #opt = torch.optim.AdamW(model.parameters(), lr=configs.lr, weight_decay=1e-2)\n",
    "    opt = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=configs.lr)\n",
    "    criterion = torch.nn.MSELoss()\n",
    "\n",
    "    num_epochs = epochs\n",
    "\n",
    "    #Early stop\n",
    "\n",
    "    best_val_rmse = float('inf')\n",
    "\n",
    "    counter = 0\n",
    "\n",
    "    for epoch in range(1, num_epochs+1):\n",
    "        model.train()\n",
    "        sum_mse = 0.0\n",
    "        r2_metric = R2Score()\n",
    "        for x_enc, y_true in train_loader:\n",
    "            x_enc, y_true = x_enc.float().to(device), y_true.float().to(device)\n",
    "            opt.zero_grad()\n",
    "            y_pred = model(x_enc, None, None, None)  \n",
    "            loss = criterion(y_pred, y_true)\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            sum_mse += loss.item()*x_enc.size(0)\n",
    "            #r2_metric.update(y_pred.squeeze(), y_true.squeeze())\n",
    "            r2_metric.update(y_pred.reshape(-1), y_true.reshape(-1))\n",
    "\n",
    "            del y_pred, loss  # Free up memory\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        train_mse = sum_mse / N_train\n",
    "        train_rmse = sqrt(train_mse)\n",
    "        train_r2 = r2_metric.compute()\n",
    "\n",
    "\n",
    "        model.eval()\n",
    "        sum_mse = 0.0\n",
    "        r2_metric2 = R2Score()\n",
    "        with torch.no_grad():\n",
    "            for x_enc, y_true in val_loader:\n",
    "                x_enc, y_true = x_enc.to(device), y_true.to(device)\n",
    "                y_pred = model(x_enc, None, None, None)\n",
    "                sum_mse += criterion(y_pred, y_true).item() * x_enc.size(0)\n",
    "                #r2_metric2.update(y_pred.squeeze(), y_true.squeeze())\n",
    "                r2_metric2.update(y_pred.reshape(-1), y_true.reshape(-1))\n",
    "\n",
    "                del y_pred  # Free up memory\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "        val_mse = sum_mse / N_val\n",
    "        val_rmse = sqrt(val_mse)\n",
    "        val_r2 = r2_metric2.compute()\n",
    "\n",
    "        #print(f\"Epoch {epoch:2d} — train RMSE {train_rmse:.6f} - val RMSE {val_rmse:.6f} \")\n",
    "        if updates:\n",
    "            print(f\"Epoch {epoch:2d} — train RMSE {train_rmse:.6f} - train R2 {train_r2:.6f} — val RMSE {val_rmse:.6f} - val R2 {val_r2:.6f}\")\n",
    "            \n",
    "        if earlystop:\n",
    "            if val_rmse <= best_val_rmse:\n",
    "                best_val_rmse = val_rmse\n",
    "                counter = 0\n",
    "                torch.save(model.state_dict(), 'best_model.pt')  # Save best model\n",
    "            else:\n",
    "                counter += 1\n",
    "                if counter >= patience:\n",
    "                    print(f\"Early stopping triggered; Best val RMSE : {best_val_rmse:.6f}\")\n",
    "                    break\n",
    "    return model, train_rmse, train_r2, val_rmse, val_r2, stand"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train classification model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_classification_model(configs, train, validation, epochs, batch_size, updates, earlystop, patience = 10):\n",
    "\n",
    "    #Scale spectra\n",
    "\n",
    "    train.columns = train.columns.astype(str)\n",
    "    scaler = StandardScaler()\n",
    "    normalized = scaler.fit_transform(train.iloc[:,1:].T).T\n",
    "    X_train_n = pd.DataFrame(normalized, columns = train.iloc[:,1:].columns)\n",
    "\n",
    "    validation.columns = validation.columns.astype(str)\n",
    "    normalized = scaler.fit_transform(validation.iloc[:,1:].T).T\n",
    "    X_val_n = pd.DataFrame(normalized, columns = validation.iloc[:,1:].columns)\n",
    "\n",
    "    #Scale wavelengths\n",
    "    stand = StandardScaler()\n",
    "    scaled = stand.fit_transform(X_train_n)\n",
    "    X_train_n = pd.DataFrame(scaled, columns = X_train_n.columns)\n",
    "    scaled = stand.transform(X_val_n)\n",
    "    X_val_n = pd.DataFrame(scaled, columns = X_val_n.columns)\n",
    "    \n",
    "    #Prepare tensor dataset\n",
    "\n",
    "    pred_len = configs.pred_len\n",
    "    seq_len = configs.seq_len\n",
    "    n_vars = 1\n",
    "    N_train =  X_train_n.shape[0]\n",
    "    N_val = X_val_n.shape[0]\n",
    "\n",
    "    X = torch.from_numpy(X_train_n.iloc[:,:].to_numpy().astype('float32')).unsqueeze(-1)\n",
    "    y = torch.from_numpy(train.iloc[:,0].to_numpy().astype('int64')).unsqueeze(1).squeeze()\n",
    "                \n",
    "    X2 = torch.from_numpy(X_val_n.iloc[:,:].to_numpy().astype('float32')).unsqueeze(-1)\n",
    "    y2 = torch.from_numpy(validation.iloc[:,0].to_numpy().astype('int64')).unsqueeze(1).squeeze()\n",
    "                \n",
    "    train_ds = TensorDataset(X, y)\n",
    "    val_ds = TensorDataset(X2, y2)\n",
    "\n",
    "    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "    val_loader  = DataLoader(val_ds,  batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    #Train model \n",
    "  \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = Model(configs).to(device)\n",
    "    #opt = torch.optim.AdamW(model.parameters(), lr=configs.lr, weight_decay=1e-2)\n",
    "    opt = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=configs.lr)\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    num_epochs = epochs\n",
    "\n",
    "    #Early stop\n",
    "\n",
    "    best_val_acc = 0.0\n",
    "\n",
    "    counter = 0\n",
    "\n",
    "    for epoch in range(1, num_epochs+1):\n",
    "        model.train()\n",
    "        y_pred_list = []\n",
    "        y_label_list = []\n",
    "        for x_enc, y_true in train_loader:\n",
    "            x_enc, y_true = x_enc.float().to(device), y_true.long().to(device)\n",
    "            opt.zero_grad()\n",
    "            y_pred = model(x_enc, None, None, None)  \n",
    "            loss = criterion(y_pred, y_true)\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            \n",
    "            y_pred_list.extend(torch.argmax(y_pred, dim=1).detach().cpu().tolist())\n",
    "            y_label_list.extend(y_true.detach().cpu().tolist())\n",
    "\n",
    "            del y_pred, loss  # Free up memory\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        train_acc = accuracy_score(y_label_list, y_pred_list)\n",
    "        train_f1 = f1_score(y_label_list, y_pred_list, average='weighted')\n",
    "\n",
    "\n",
    "        model.eval()\n",
    "        val_pred_list = []\n",
    "        val_label_list = []\n",
    "        with torch.no_grad():\n",
    "            for x_enc, y_true in val_loader:\n",
    "                x_enc, y_true = x_enc.to(device), y_true.to(device)\n",
    "                y_pred = model(x_enc, None, None, None)\n",
    "                \n",
    "                val_pred_list.extend(torch.argmax(y_pred, dim=1).cpu().tolist())\n",
    "                val_label_list.extend(y_true.cpu().tolist())\n",
    "\n",
    "                del y_pred  # Free up memory\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "        val_acc = accuracy_score(val_label_list, val_pred_list)\n",
    "        val_f1 = f1_score(val_label_list, val_pred_list, average='weighted')\n",
    "\n",
    "\n",
    "        #print(f\"Epoch {epoch:2d} — train RMSE {train_rmse:.6f} - val RMSE {val_rmse:.6f} \")\n",
    "        if updates:\n",
    "            print(f\"Epoch {epoch:2d} — train Accuracy : {train_acc:.6f} - train F1 {train_f1:.6f} — val Accuracy {val_acc:.6f} - val F1 {val_f1:.6f}\")\n",
    "            \n",
    "        if earlystop:\n",
    "            if val_acc >= best_val_acc:\n",
    "                best_val_acc = val_acc\n",
    "                counter = 0\n",
    "                torch.save(model.state_dict(), 'best_class_model.pth')  # Save best model\n",
    "            else:\n",
    "                counter += 1\n",
    "                if counter >= patience:\n",
    "                    print(f\"Early stopping triggered; Best val Accuracy : {best_val_acc:.6f}\")\n",
    "                    break\n",
    "    return model, train_acc, train_f1, val_acc, val_f1, stand"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weather Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather = pd.read_csv('weather.csv')\n",
    "weather['date'] = pd.to_datetime(weather['date'], dayfirst= False)\n",
    "weather\n",
    "del weather['date']\n",
    "\n",
    "#weather = weather.iloc[:1000,:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42157"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_N = round(weather.shape[0]*0.8)\n",
    "train_N"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train-Validation Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = weather.iloc[:train_N,:]\n",
    "test = weather.iloc[train_N:,:].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "train = scaler.fit_transform(train)\n",
    "test = scaler.transform(test)\n",
    "train = pd.DataFrame(train, columns = weather.columns)\n",
    "test = pd.DataFrame(test, columns = weather.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(42157, 21)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task details:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_len = 96\n",
    "seq_len = 512\n",
    "n_vars = len(train.columns)\n",
    "configs = Details(\n",
    "    pred_len = pred_len,      \n",
    "    seq_len  = seq_len,\n",
    "    d_ff = 32,\n",
    "    patch_len= 16,\n",
    "    stride = 8, \n",
    "    llm_layers = 32,\n",
    "    description = \"Weather is recorded every 10 minutes for the 2020 whole year, which contains 21 meteorological indicators, such as air temperature, humidity, etc.\" ,\n",
    "    dropout = 0.1,\n",
    "    n_heads = 8,\n",
    "    d_model = 16,\n",
    "    enc_in = n_vars,\n",
    "    d_llm = 4096,\n",
    "    llm_model = 'LLAMA',\n",
    "    lr = 1e-3\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, Y_train = supervised_data(train, seq_len, pred_len)\n",
    "X_test, Y_test = supervised_data(test, seq_len, pred_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(21273600, 21)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_train = int(X_train.shape[0]/seq_len)\n",
    "X_train_ = torch.from_numpy(X_train.to_numpy().astype('float32')).view(N_train, seq_len, n_vars)\n",
    "Y_train_ = torch.from_numpy(Y_train.to_numpy().astype('float32')).view(N_train, pred_len, n_vars)\n",
    "\n",
    "N_test = int(X_test.shape[0]/seq_len)\n",
    "X_test_ = torch.from_numpy(X_test.to_numpy().astype('float32')).view(N_test, seq_len, n_vars)\n",
    "Y_test_ = torch.from_numpy(Y_test.to_numpy().astype('float32')).view(N_test, pred_len, n_vars)\n",
    "       \n",
    "train_ds = TensorDataset(X_train_, Y_train_)\n",
    "test_ds = TensorDataset(X_test_, Y_test_)\n",
    "\n",
    "batch_size   = 16\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=False)#Or should it be true¿?\n",
    "test_loader  = DataLoader(test_ds,  batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01eb8f9bd2c64146a6ee6d006c3dce97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 670.00 MiB. GPU 0 has a total capacity of 23.68 GiB of which 164.69 MiB is free. Including non-PyTorch memory, this process has 23.32 GiB memory in use. Of the allocated memory 20.88 GiB is allocated by PyTorch, and 1.18 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 16\u001b[0m\n\u001b[1;32m     14\u001b[0m x_enc, y_true \u001b[38;5;241m=\u001b[39m x_enc\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39mto(device), y_true\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     15\u001b[0m opt\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 16\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_enc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m  \n\u001b[1;32m     17\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(y_pred, y_true)\n\u001b[1;32m     18\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[3], line 168\u001b[0m, in \u001b[0;36mModel.forward\u001b[0;34m(self, x_enc, x_mark_enc, x_dec, x_mark_dec, mask)\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x_enc, x_mark_enc, x_dec, x_mark_dec, mask\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m--> 168\u001b[0m     dec_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforecast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_enc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_mark_enc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_dec\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_mark_dec\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    169\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m dec_out[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpred_len:, :]\n",
      "Cell \u001b[0;32mIn[3], line 218\u001b[0m, in \u001b[0;36mModel.forecast\u001b[0;34m(self, x_enc, x_mark_enc, x_dec, x_mark_dec)\u001b[0m\n\u001b[1;32m    216\u001b[0m enc_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreprogramming_layer(enc_out, source_embeddings, source_embeddings)\n\u001b[1;32m    217\u001b[0m llama_enc_out \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([prompt_embeddings, enc_out], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m--> 218\u001b[0m dec_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mllm_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mllama_enc_out\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mlast_hidden_state\n\u001b[1;32m    219\u001b[0m \u001b[38;5;66;03m#print(dec_out)\u001b[39;00m\n\u001b[1;32m    220\u001b[0m dec_out \u001b[38;5;241m=\u001b[39m dec_out[:, :, :\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39md_ff]\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/utils/generic.py:969\u001b[0m, in \u001b[0;36mcan_return_tuple.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    966\u001b[0m     set_attribute_for_modules(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_is_top_level_module\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    968\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 969\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    970\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_requested_to_return_tuple \u001b[38;5;129;01mor\u001b[39;00m (is_configured_to_return_tuple \u001b[38;5;129;01mand\u001b[39;00m is_top_level_module):\n\u001b[1;32m    971\u001b[0m         output \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mto_tuple()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:453\u001b[0m, in \u001b[0;36mLlamaModel.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, cache_position, **flash_attn_kwargs)\u001b[0m\n\u001b[1;32m    450\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states:\n\u001b[1;32m    451\u001b[0m     all_hidden_states \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (hidden_states,)\n\u001b[0;32m--> 453\u001b[0m layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    454\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    455\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    456\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    457\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    458\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    459\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    460\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    461\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    462\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mflash_attn_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    463\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    465\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    467\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/modeling_layers.py:48\u001b[0m, in \u001b[0;36mGradientCheckpointingLayer.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgradient_checkpointing \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining:\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(partial(\u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs), \u001b[38;5;241m*\u001b[39margs)\n\u001b[0;32m---> 48\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:308\u001b[0m, in \u001b[0;36mLlamaDecoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, position_embeddings, **kwargs)\u001b[0m\n\u001b[1;32m    305\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_layernorm(hidden_states)\n\u001b[1;32m    307\u001b[0m \u001b[38;5;66;03m# Self Attention\u001b[39;00m\n\u001b[0;32m--> 308\u001b[0m hidden_states, self_attn_weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    309\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    310\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    311\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    312\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    313\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    314\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    315\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    316\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    317\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    318\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    319\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n\u001b[1;32m    321\u001b[0m \u001b[38;5;66;03m# Fully Connected\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:247\u001b[0m, in \u001b[0;36mLlamaAttention.forward\u001b[0;34m(self, hidden_states, position_embeddings, attention_mask, past_key_value, cache_position, **kwargs)\u001b[0m\n\u001b[1;32m    244\u001b[0m value_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mv_proj(hidden_states)\u001b[38;5;241m.\u001b[39mview(hidden_shape)\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m    246\u001b[0m cos, sin \u001b[38;5;241m=\u001b[39m position_embeddings\n\u001b[0;32m--> 247\u001b[0m query_states, key_states \u001b[38;5;241m=\u001b[39m \u001b[43mapply_rotary_pos_emb\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcos\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msin\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m past_key_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    250\u001b[0m     \u001b[38;5;66;03m# sin and cos are specific to RoPE models; cache_position needed for the static cache\u001b[39;00m\n\u001b[1;32m    251\u001b[0m     cache_kwargs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msin\u001b[39m\u001b[38;5;124m\"\u001b[39m: sin, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcos\u001b[39m\u001b[38;5;124m\"\u001b[39m: cos, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcache_position\u001b[39m\u001b[38;5;124m\"\u001b[39m: cache_position}\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:146\u001b[0m, in \u001b[0;36mapply_rotary_pos_emb\u001b[0;34m(q, k, cos, sin, position_ids, unsqueeze_dim)\u001b[0m\n\u001b[1;32m    144\u001b[0m sin \u001b[38;5;241m=\u001b[39m sin\u001b[38;5;241m.\u001b[39munsqueeze(unsqueeze_dim)\n\u001b[1;32m    145\u001b[0m q_embed \u001b[38;5;241m=\u001b[39m (q \u001b[38;5;241m*\u001b[39m cos) \u001b[38;5;241m+\u001b[39m (rotate_half(q) \u001b[38;5;241m*\u001b[39m sin)\n\u001b[0;32m--> 146\u001b[0m k_embed \u001b[38;5;241m=\u001b[39m (k \u001b[38;5;241m*\u001b[39m cos) \u001b[38;5;241m+\u001b[39m (\u001b[43mrotate_half\u001b[49m\u001b[43m(\u001b[49m\u001b[43mk\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43msin\u001b[49m)\n\u001b[1;32m    147\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m q_embed, k_embed\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 670.00 MiB. GPU 0 has a total capacity of 23.68 GiB of which 164.69 MiB is free. Including non-PyTorch memory, this process has 23.32 GiB memory in use. Of the allocated memory 20.88 GiB is allocated by PyTorch, and 1.18 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = Model(configs).to(device)\n",
    "opt = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=configs.lr)\n",
    "criterion = torch.nn.MSELoss()\n",
    "\n",
    "num_epochs = 100\n",
    "\n",
    "for epoch in range(1, num_epochs+1):\n",
    "    model.train()\n",
    "    sum_mse = 0.0\n",
    "    r2_metric = R2Score()\n",
    "    for x_enc, y_true in train_loader:\n",
    "        x_enc, y_true = x_enc.float().to(device), y_true.float().to(device)\n",
    "        opt.zero_grad()\n",
    "        y_pred = model(x_enc, None, None, None)  \n",
    "        loss = criterion(y_pred, y_true)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        sum_mse += loss.item()*x_enc.size(0)\n",
    "        r2_metric.update(y_pred.reshape(-1, n_vars), y_true.reshape(-1, n_vars))\n",
    "        del y_pred, loss  # Free up memory\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    train_mse = sum_mse / N_train\n",
    "    train_rmse = sqrt(train_mse)\n",
    "    train_r2 = r2_metric.compute()\n",
    "\n",
    "\n",
    "    model.eval()\n",
    "    sum_mse = 0.0\n",
    "    r2_metric2 = R2Score()\n",
    "    with torch.no_grad():\n",
    "        for x_enc, y_true in test_loader:\n",
    "            x_enc, y_true = x_enc.to(device), y_true.to(device)\n",
    "            y_pred = model(x_enc, None, None, None)\n",
    "            sum_mse += criterion(y_pred, y_true).item() * x_enc.size(0)\n",
    "            r2_metric2.update(y_pred.reshape(-1, n_vars), y_true.reshape(-1, n_vars))\n",
    "            del y_pred  # Free up memory\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "    val_mse = sum_mse / N_test\n",
    "    val_rmse = sqrt(val_mse)\n",
    "    val_r2 = r2_metric2.compute()\n",
    "\n",
    "\n",
    "    print(f\"Epoch {epoch:2d} — train MSE {train_mse:.6f} - train R2 {train_r2:.6f} — val MSE {val_mse:.6f} - val R2 {val_r2:.6f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Don't run this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  1 — train RMSE 3.204922 - train R2 0.399476 — val RMSE 1.529246 - val R2 0.512537\n",
      "Epoch  2 — train RMSE 2.215803 - train R2 0.716460 — val RMSE 1.060983 - val R2 0.767389\n",
      "Epoch  3 — train RMSE 1.815567 - train R2 0.791040 — val RMSE 1.022648 - val R2 0.787486\n",
      "Epoch  4 — train RMSE 1.765228 - train R2 0.801854 — val RMSE 1.020342 - val R2 0.798367\n",
      "Epoch  5 — train RMSE 1.768999 - train R2 0.802592 — val RMSE 1.010477 - val R2 0.806963\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 5\n",
    "\n",
    "for epoch in range(1, num_epochs+1):\n",
    "    model.train()\n",
    "    sum_mse = 0.0\n",
    "    r2_metric = R2Score()\n",
    "    for x_enc, y_true in train_loader:\n",
    "        x_enc, y_true = x_enc.float().to(device), y_true.float().to(device)\n",
    "        opt.zero_grad()\n",
    "        y_pred = model(x_enc, None, None, None)  \n",
    "        loss = criterion(y_pred, y_true)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        sum_mse += loss.item()*x_enc.size(0)\n",
    "        r2_metric.update(y_pred.reshape(-1, n_vars), y_true.reshape(-1, n_vars))\n",
    "        del y_pred, loss  # Free up memory\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    train_mse = sum_mse / N_train\n",
    "    train_rmse = sqrt(train_mse)\n",
    "    train_r2 = r2_metric.compute()\n",
    "\n",
    "\n",
    "    model.eval()\n",
    "    sum_mse = 0.0\n",
    "    r2_metric2 = R2Score()\n",
    "    with torch.no_grad():\n",
    "        for x_enc, y_true in test_loader:\n",
    "            x_enc, y_true = x_enc.to(device), y_true.to(device)\n",
    "            y_pred = model(x_enc, None, None, None)\n",
    "            sum_mse += criterion(y_pred, y_true).item() * x_enc.size(0)\n",
    "            r2_metric2.update(y_pred.reshape(-1, n_vars), y_true.reshape(-1, n_vars))\n",
    "            del y_pred  # Free up memory\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "    val_mse = sum_mse / N_test\n",
    "    val_rmse = sqrt(val_mse)\n",
    "    val_r2 = r2_metric2.compute()\n",
    "\n",
    "\n",
    "    print(f\"Epoch {epoch:2d} — train RMSE {train_rmse:.6f} - train R2 {train_r2:.6f} — val RMSE {val_rmse:.6f} - val R2 {val_r2:.6f}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "scaled data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dont run this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 244.00 MiB. GPU 0 has a total capacity of 23.68 GiB of which 251.81 MiB is free. Including non-PyTorch memory, this process has 23.22 GiB memory in use. Of the allocated memory 20.84 GiB is allocated by PyTorch, and 1.11 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m x_enc, y_true \u001b[38;5;241m=\u001b[39m x_enc\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39mto(device), y_true\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m      9\u001b[0m opt\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 10\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_enc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m  \n\u001b[1;32m     11\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(y_pred, y_true)\n\u001b[1;32m     12\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[9], line 127\u001b[0m, in \u001b[0;36mModel.forward\u001b[0;34m(self, x_enc, x_mark_enc, x_dec, x_mark_dec, mask)\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x_enc, x_mark_enc, x_dec, x_mark_dec, mask\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m--> 127\u001b[0m     dec_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforecast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_enc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_mark_enc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_dec\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_mark_dec\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    128\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m dec_out[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpred_len:, :]\n",
      "Cell \u001b[0;32mIn[9], line 177\u001b[0m, in \u001b[0;36mModel.forecast\u001b[0;34m(self, x_enc, x_mark_enc, x_dec, x_mark_dec)\u001b[0m\n\u001b[1;32m    175\u001b[0m enc_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreprogramming_layer(enc_out, source_embeddings, source_embeddings)\n\u001b[1;32m    176\u001b[0m llama_enc_out \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([prompt_embeddings, enc_out], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m--> 177\u001b[0m dec_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mllm_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mllama_enc_out\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mlast_hidden_state\n\u001b[1;32m    178\u001b[0m \u001b[38;5;66;03m#print(dec_out)\u001b[39;00m\n\u001b[1;32m    179\u001b[0m dec_out \u001b[38;5;241m=\u001b[39m dec_out[:, :, :\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39md_ff]\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/utils/generic.py:969\u001b[0m, in \u001b[0;36mcan_return_tuple.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    966\u001b[0m     set_attribute_for_modules(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_is_top_level_module\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    968\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 969\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    970\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_requested_to_return_tuple \u001b[38;5;129;01mor\u001b[39;00m (is_configured_to_return_tuple \u001b[38;5;129;01mand\u001b[39;00m is_top_level_module):\n\u001b[1;32m    971\u001b[0m         output \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mto_tuple()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:453\u001b[0m, in \u001b[0;36mLlamaModel.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, cache_position, **flash_attn_kwargs)\u001b[0m\n\u001b[1;32m    450\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states:\n\u001b[1;32m    451\u001b[0m     all_hidden_states \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (hidden_states,)\n\u001b[0;32m--> 453\u001b[0m layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    454\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    455\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    456\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    457\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    458\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    459\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    460\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    461\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    462\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mflash_attn_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    463\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    465\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    467\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/modeling_layers.py:48\u001b[0m, in \u001b[0;36mGradientCheckpointingLayer.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgradient_checkpointing \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining:\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(partial(\u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs), \u001b[38;5;241m*\u001b[39margs)\n\u001b[0;32m---> 48\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:324\u001b[0m, in \u001b[0;36mLlamaDecoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, position_embeddings, **kwargs)\u001b[0m\n\u001b[1;32m    322\u001b[0m residual \u001b[38;5;241m=\u001b[39m hidden_states\n\u001b[1;32m    323\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpost_attention_layernorm(hidden_states)\n\u001b[0;32m--> 324\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmlp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    325\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n\u001b[1;32m    327\u001b[0m outputs \u001b[38;5;241m=\u001b[39m (hidden_states,)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:162\u001b[0m, in \u001b[0;36mLlamaMLP.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m--> 162\u001b[0m     down_proj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdown_proj\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mact_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgate_proj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mup_proj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    163\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m down_proj\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/linear.py:125\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 244.00 MiB. GPU 0 has a total capacity of 23.68 GiB of which 251.81 MiB is free. Including non-PyTorch memory, this process has 23.22 GiB memory in use. Of the allocated memory 20.84 GiB is allocated by PyTorch, and 1.11 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "num_epochs = 5\n",
    "\n",
    "for epoch in range(1, num_epochs+1):\n",
    "    model.train()\n",
    "    sum_mse = 0.0\n",
    "    r2_metric = R2Score()\n",
    "    for x_enc, y_true in train_loader:\n",
    "        x_enc, y_true = x_enc.float().to(device), y_true.float().to(device)\n",
    "        opt.zero_grad()\n",
    "        y_pred = model(x_enc, None, None, None)  \n",
    "        loss = criterion(y_pred, y_true)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        sum_mse += loss.item()*x_enc.size(0)\n",
    "        r2_metric.update(y_pred.reshape(-1, n_vars), y_true.reshape(-1, n_vars))\n",
    "        del y_pred, loss  # Free up memory\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    train_mse = sum_mse / N_train\n",
    "    train_rmse = sqrt(train_mse)\n",
    "    train_r2 = r2_metric.compute()\n",
    "\n",
    "\n",
    "    model.eval()\n",
    "    sum_mse = 0.0\n",
    "    r2_metric2 = R2Score()\n",
    "    with torch.no_grad():\n",
    "        for x_enc, y_true in test_loader:\n",
    "            x_enc, y_true = x_enc.to(device), y_true.to(device)\n",
    "            y_pred = model(x_enc, None, None, None)\n",
    "            sum_mse += criterion(y_pred, y_true).item() * x_enc.size(0)\n",
    "            r2_metric2.update(y_pred.reshape(-1, n_vars), y_true.reshape(-1, n_vars))\n",
    "            del y_pred  # Free up memory\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "    val_mse = sum_mse / N_test\n",
    "    val_rmse = sqrt(val_mse)\n",
    "    val_r2 = r2_metric2.compute()\n",
    "\n",
    "\n",
    "    print(f\"Epoch {epoch:2d} — train RMSE {train_rmse:.6f} - train R2 {train_r2:.6f} — val RMSE {val_rmse:.6f} - val R2 {val_r2:.6f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Illness dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>% WEIGHTED ILI</th>\n",
       "      <th>%UNWEIGHTED ILI</th>\n",
       "      <th>AGE 0-4</th>\n",
       "      <th>AGE 5-24</th>\n",
       "      <th>ILITOTAL</th>\n",
       "      <th>NUM. OF PROVIDERS</th>\n",
       "      <th>OT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.222620</td>\n",
       "      <td>1.166680</td>\n",
       "      <td>582</td>\n",
       "      <td>805</td>\n",
       "      <td>2060</td>\n",
       "      <td>754</td>\n",
       "      <td>176569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.333440</td>\n",
       "      <td>1.216500</td>\n",
       "      <td>683</td>\n",
       "      <td>872</td>\n",
       "      <td>2267</td>\n",
       "      <td>785</td>\n",
       "      <td>186355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.319290</td>\n",
       "      <td>1.130570</td>\n",
       "      <td>642</td>\n",
       "      <td>878</td>\n",
       "      <td>2176</td>\n",
       "      <td>831</td>\n",
       "      <td>192469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.494840</td>\n",
       "      <td>1.252460</td>\n",
       "      <td>728</td>\n",
       "      <td>1045</td>\n",
       "      <td>2599</td>\n",
       "      <td>863</td>\n",
       "      <td>207512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.471950</td>\n",
       "      <td>1.302370</td>\n",
       "      <td>823</td>\n",
       "      <td>1189</td>\n",
       "      <td>2907</td>\n",
       "      <td>909</td>\n",
       "      <td>223208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>961</th>\n",
       "      <td>0.839059</td>\n",
       "      <td>0.846722</td>\n",
       "      <td>2756</td>\n",
       "      <td>3528</td>\n",
       "      <td>12913</td>\n",
       "      <td>3258</td>\n",
       "      <td>1525058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>962</th>\n",
       "      <td>0.895958</td>\n",
       "      <td>0.908885</td>\n",
       "      <td>3203</td>\n",
       "      <td>3778</td>\n",
       "      <td>13979</td>\n",
       "      <td>3254</td>\n",
       "      <td>1538038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>963</th>\n",
       "      <td>0.910926</td>\n",
       "      <td>0.941625</td>\n",
       "      <td>3478</td>\n",
       "      <td>3796</td>\n",
       "      <td>14389</td>\n",
       "      <td>3177</td>\n",
       "      <td>1528103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>964</th>\n",
       "      <td>0.946945</td>\n",
       "      <td>0.972185</td>\n",
       "      <td>3734</td>\n",
       "      <td>3818</td>\n",
       "      <td>14999</td>\n",
       "      <td>3066</td>\n",
       "      <td>1542813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>965</th>\n",
       "      <td>0.963716</td>\n",
       "      <td>1.013760</td>\n",
       "      <td>3955</td>\n",
       "      <td>3843</td>\n",
       "      <td>15307</td>\n",
       "      <td>3027</td>\n",
       "      <td>1509928</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>966 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     % WEIGHTED ILI  %UNWEIGHTED ILI  AGE 0-4  AGE 5-24  ILITOTAL  \\\n",
       "0          1.222620         1.166680      582       805      2060   \n",
       "1          1.333440         1.216500      683       872      2267   \n",
       "2          1.319290         1.130570      642       878      2176   \n",
       "3          1.494840         1.252460      728      1045      2599   \n",
       "4          1.471950         1.302370      823      1189      2907   \n",
       "..              ...              ...      ...       ...       ...   \n",
       "961        0.839059         0.846722     2756      3528     12913   \n",
       "962        0.895958         0.908885     3203      3778     13979   \n",
       "963        0.910926         0.941625     3478      3796     14389   \n",
       "964        0.946945         0.972185     3734      3818     14999   \n",
       "965        0.963716         1.013760     3955      3843     15307   \n",
       "\n",
       "     NUM. OF PROVIDERS       OT  \n",
       "0                  754   176569  \n",
       "1                  785   186355  \n",
       "2                  831   192469  \n",
       "3                  863   207512  \n",
       "4                  909   223208  \n",
       "..                 ...      ...  \n",
       "961               3258  1525058  \n",
       "962               3254  1538038  \n",
       "963               3177  1528103  \n",
       "964               3066  1542813  \n",
       "965               3027  1509928  \n",
       "\n",
       "[966 rows x 7 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ill = pd.read_csv('national_illness.csv')\n",
    "del ill['date']\n",
    "\n",
    "ill"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train-test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_N = round(ill.shape[0]*0.8)\n",
    "train = ill.iloc[:train_N,:]\n",
    "test = ill.iloc[train_N:,:].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "train = scaler.fit_transform(train)\n",
    "test = scaler.transform(test)\n",
    "train = pd.DataFrame(train, columns = ill.columns)\n",
    "test = pd.DataFrame(test, columns = ill.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_len = 24\n",
    "seq_len = 96\n",
    "n_vars = len(train.columns)\n",
    "configs = Details(\n",
    "    pred_len = pred_len,      \n",
    "    seq_len  = seq_len,\n",
    "    d_ff = 32,\n",
    "    patch_len= 16, \n",
    "    stride = 8, \n",
    "    llm_layers = 32,\n",
    "    description = \"weekly U.S. influenza-like illness (ILI) surveillance data, \" \\\n",
    "    \"including weighted and unweighted ILI percentages, age-specific case counts, total cases, \" \\\n",
    "    \"number of reporting providers, and outpatient visits.\",\n",
    "    dropout = 0.1,\n",
    "    n_heads = 8,\n",
    "    d_model = 16,\n",
    "    enc_in = n_vars,\n",
    "    d_llm = 4096,\n",
    "    llm_model = 'LLAMA',\n",
    "    lr = 1e-2\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, Y_train = supervised_data(train, seq_len, pred_len)\n",
    "X_test, Y_test = supervised_data(test, seq_len, pred_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_train = int(X_train.shape[0]/seq_len)\n",
    "X_train_ = torch.from_numpy(X_train.to_numpy().astype('float32')).view(N_train, seq_len, n_vars)\n",
    "Y_train_ = torch.from_numpy(Y_train.to_numpy().astype('float32')).view(N_train, pred_len, n_vars)\n",
    "\n",
    "N_test = int(X_test.shape[0]/seq_len)\n",
    "X_test_ = torch.from_numpy(X_test.to_numpy().astype('float32')).view(N_test, seq_len, n_vars)\n",
    "Y_test_ = torch.from_numpy(Y_test.to_numpy().astype('float32')).view(N_test, pred_len, n_vars)\n",
    "       \n",
    "train_ds = TensorDataset(X_train_, Y_train_)\n",
    "test_ds = TensorDataset(X_test_, Y_test_)\n",
    "\n",
    "batch_size   = 16\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=False)#Or should it be true\n",
    "test_loader  = DataLoader(test_ds,  batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "784e6b96854c4de29480662573f3e418",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = Model(configs).to(device)\n",
    "opt = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=configs.lr)\n",
    "criterion = torch.nn.MSELoss()\n",
    "\n",
    "num_epochs = 50\n",
    "\n",
    "for epoch in range(1, num_epochs+1):\n",
    "    model.train()\n",
    "    sum_mse = 0.0\n",
    "    r2_metric = R2Score()\n",
    "    for x_enc, y_true in train_loader:\n",
    "        x_enc, y_true = x_enc.float().to(device), y_true.float().to(device)\n",
    "        opt.zero_grad()\n",
    "        y_pred = model(x_enc, None, None, None)  \n",
    "        loss = criterion(y_pred, y_true)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        sum_mse += loss.item()*x_enc.size(0)\n",
    "        r2_metric.update(y_pred.reshape(-1, n_vars), y_true.reshape(-1, n_vars))\n",
    "        del y_pred, loss  # Free up memory\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    train_mse = sum_mse / N_train\n",
    "    train_rmse = sqrt(train_mse)\n",
    "    train_r2 = r2_metric.compute()\n",
    "\n",
    "\n",
    "    model.eval()\n",
    "    sum_mse = 0.0\n",
    "    r2_metric2 = R2Score()\n",
    "    with torch.no_grad():\n",
    "        for x_enc, y_true in test_loader:\n",
    "            x_enc, y_true = x_enc.to(device), y_true.to(device)\n",
    "            y_pred = model(x_enc, None, None, None)\n",
    "            sum_mse += criterion(y_pred, y_true).item() * x_enc.size(0)\n",
    "            r2_metric2.update(y_pred.reshape(-1, n_vars), y_true.reshape(-1, n_vars))\n",
    "            del y_pred  # Free up memory\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "    val_mse = sum_mse / N_test\n",
    "    val_rmse = sqrt(val_mse)\n",
    "    val_r2 = r2_metric2.compute()\n",
    "\n",
    "\n",
    "    print(f\"Epoch {epoch:2d} — train MSE {train_mse:.6f} - train R2 {train_r2:.6f} — val MSE {val_mse:.6f} - val R2 {val_r2:.6f}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lactate Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Concentration</th>\n",
       "      <th>1014.08</th>\n",
       "      <th>1015.85</th>\n",
       "      <th>1017.62</th>\n",
       "      <th>1019.4</th>\n",
       "      <th>1021.19</th>\n",
       "      <th>1022.98</th>\n",
       "      <th>1024.77</th>\n",
       "      <th>1026.58</th>\n",
       "      <th>1028.39</th>\n",
       "      <th>...</th>\n",
       "      <th>2481.26</th>\n",
       "      <th>2491.87</th>\n",
       "      <th>2502.56</th>\n",
       "      <th>2513.35</th>\n",
       "      <th>2524.23</th>\n",
       "      <th>2535.2</th>\n",
       "      <th>2546.27</th>\n",
       "      <th>2557.44</th>\n",
       "      <th>2568.71</th>\n",
       "      <th>2580.07</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.005231</td>\n",
       "      <td>0.038095</td>\n",
       "      <td>0.082302</td>\n",
       "      <td>0.082237</td>\n",
       "      <td>0.076576</td>\n",
       "      <td>0.106940</td>\n",
       "      <td>0.095478</td>\n",
       "      <td>0.089268</td>\n",
       "      <td>0.086707</td>\n",
       "      <td>...</td>\n",
       "      <td>0.026194</td>\n",
       "      <td>0.021901</td>\n",
       "      <td>0.019588</td>\n",
       "      <td>0.029467</td>\n",
       "      <td>0.030502</td>\n",
       "      <td>0.012769</td>\n",
       "      <td>0.000612</td>\n",
       "      <td>-0.015370</td>\n",
       "      <td>-0.047555</td>\n",
       "      <td>-0.068557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.081353</td>\n",
       "      <td>-0.131399</td>\n",
       "      <td>-0.160719</td>\n",
       "      <td>-0.190526</td>\n",
       "      <td>-0.224999</td>\n",
       "      <td>-0.239949</td>\n",
       "      <td>-0.231437</td>\n",
       "      <td>-0.181605</td>\n",
       "      <td>-0.190708</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012446</td>\n",
       "      <td>0.048297</td>\n",
       "      <td>0.064985</td>\n",
       "      <td>0.043647</td>\n",
       "      <td>0.008629</td>\n",
       "      <td>-0.035054</td>\n",
       "      <td>-0.086570</td>\n",
       "      <td>-0.152690</td>\n",
       "      <td>-0.234232</td>\n",
       "      <td>-0.326223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.143674</td>\n",
       "      <td>-0.116576</td>\n",
       "      <td>-0.123858</td>\n",
       "      <td>-0.097288</td>\n",
       "      <td>-0.035242</td>\n",
       "      <td>0.023129</td>\n",
       "      <td>0.056915</td>\n",
       "      <td>0.098068</td>\n",
       "      <td>0.075592</td>\n",
       "      <td>...</td>\n",
       "      <td>0.011739</td>\n",
       "      <td>-0.003521</td>\n",
       "      <td>-0.014063</td>\n",
       "      <td>-0.008997</td>\n",
       "      <td>0.002582</td>\n",
       "      <td>0.017829</td>\n",
       "      <td>0.048742</td>\n",
       "      <td>0.084157</td>\n",
       "      <td>0.115354</td>\n",
       "      <td>0.151298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.096424</td>\n",
       "      <td>0.010348</td>\n",
       "      <td>0.037800</td>\n",
       "      <td>0.007231</td>\n",
       "      <td>-0.021712</td>\n",
       "      <td>-0.037742</td>\n",
       "      <td>-0.082596</td>\n",
       "      <td>-0.089286</td>\n",
       "      <td>-0.070054</td>\n",
       "      <td>...</td>\n",
       "      <td>0.091157</td>\n",
       "      <td>0.068864</td>\n",
       "      <td>0.056702</td>\n",
       "      <td>0.034800</td>\n",
       "      <td>0.020370</td>\n",
       "      <td>0.016746</td>\n",
       "      <td>0.001978</td>\n",
       "      <td>0.007347</td>\n",
       "      <td>0.025432</td>\n",
       "      <td>0.049032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.013245</td>\n",
       "      <td>0.088878</td>\n",
       "      <td>0.055112</td>\n",
       "      <td>0.057282</td>\n",
       "      <td>0.044607</td>\n",
       "      <td>0.047680</td>\n",
       "      <td>0.033936</td>\n",
       "      <td>0.050192</td>\n",
       "      <td>0.005173</td>\n",
       "      <td>...</td>\n",
       "      <td>0.074989</td>\n",
       "      <td>0.022504</td>\n",
       "      <td>-0.014653</td>\n",
       "      <td>-0.054191</td>\n",
       "      <td>-0.089806</td>\n",
       "      <td>-0.118453</td>\n",
       "      <td>-0.134508</td>\n",
       "      <td>-0.148832</td>\n",
       "      <td>-0.151126</td>\n",
       "      <td>-0.144606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>351</th>\n",
       "      <td>1300.0</td>\n",
       "      <td>0.013892</td>\n",
       "      <td>-0.023256</td>\n",
       "      <td>-0.031826</td>\n",
       "      <td>-0.053169</td>\n",
       "      <td>-0.084421</td>\n",
       "      <td>-0.100099</td>\n",
       "      <td>-0.078321</td>\n",
       "      <td>-0.081930</td>\n",
       "      <td>-0.054463</td>\n",
       "      <td>...</td>\n",
       "      <td>0.149447</td>\n",
       "      <td>0.165580</td>\n",
       "      <td>0.169533</td>\n",
       "      <td>0.151979</td>\n",
       "      <td>0.127341</td>\n",
       "      <td>0.088787</td>\n",
       "      <td>0.039029</td>\n",
       "      <td>-0.012803</td>\n",
       "      <td>-0.064349</td>\n",
       "      <td>-0.110492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>352</th>\n",
       "      <td>1300.0</td>\n",
       "      <td>-0.300830</td>\n",
       "      <td>-0.334294</td>\n",
       "      <td>-0.300479</td>\n",
       "      <td>-0.285832</td>\n",
       "      <td>-0.234678</td>\n",
       "      <td>-0.182459</td>\n",
       "      <td>-0.151797</td>\n",
       "      <td>-0.119183</td>\n",
       "      <td>-0.052906</td>\n",
       "      <td>...</td>\n",
       "      <td>0.063646</td>\n",
       "      <td>0.027490</td>\n",
       "      <td>0.003725</td>\n",
       "      <td>-0.002946</td>\n",
       "      <td>0.010705</td>\n",
       "      <td>0.029308</td>\n",
       "      <td>0.049670</td>\n",
       "      <td>0.079636</td>\n",
       "      <td>0.110156</td>\n",
       "      <td>0.135482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>353</th>\n",
       "      <td>1300.0</td>\n",
       "      <td>-0.199545</td>\n",
       "      <td>-0.220742</td>\n",
       "      <td>-0.222265</td>\n",
       "      <td>-0.198332</td>\n",
       "      <td>-0.153330</td>\n",
       "      <td>-0.132572</td>\n",
       "      <td>-0.199724</td>\n",
       "      <td>-0.168876</td>\n",
       "      <td>-0.109947</td>\n",
       "      <td>...</td>\n",
       "      <td>0.096368</td>\n",
       "      <td>0.073663</td>\n",
       "      <td>0.063638</td>\n",
       "      <td>0.061432</td>\n",
       "      <td>0.066310</td>\n",
       "      <td>0.063311</td>\n",
       "      <td>0.060929</td>\n",
       "      <td>0.077316</td>\n",
       "      <td>0.088265</td>\n",
       "      <td>0.111251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>354</th>\n",
       "      <td>1300.0</td>\n",
       "      <td>-0.107494</td>\n",
       "      <td>-0.126795</td>\n",
       "      <td>-0.182586</td>\n",
       "      <td>-0.215886</td>\n",
       "      <td>-0.246338</td>\n",
       "      <td>-0.249917</td>\n",
       "      <td>-0.254625</td>\n",
       "      <td>-0.288691</td>\n",
       "      <td>-0.253613</td>\n",
       "      <td>...</td>\n",
       "      <td>0.123685</td>\n",
       "      <td>0.132682</td>\n",
       "      <td>0.137421</td>\n",
       "      <td>0.141735</td>\n",
       "      <td>0.143292</td>\n",
       "      <td>0.140572</td>\n",
       "      <td>0.137399</td>\n",
       "      <td>0.136859</td>\n",
       "      <td>0.126693</td>\n",
       "      <td>0.115020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>355</th>\n",
       "      <td>1300.0</td>\n",
       "      <td>0.027243</td>\n",
       "      <td>0.027325</td>\n",
       "      <td>0.045799</td>\n",
       "      <td>-0.001712</td>\n",
       "      <td>0.018226</td>\n",
       "      <td>-0.049751</td>\n",
       "      <td>-0.056777</td>\n",
       "      <td>-0.107068</td>\n",
       "      <td>-0.135225</td>\n",
       "      <td>...</td>\n",
       "      <td>0.084787</td>\n",
       "      <td>0.070592</td>\n",
       "      <td>0.061971</td>\n",
       "      <td>0.066389</td>\n",
       "      <td>0.073253</td>\n",
       "      <td>0.083245</td>\n",
       "      <td>0.103166</td>\n",
       "      <td>0.116690</td>\n",
       "      <td>0.136836</td>\n",
       "      <td>0.146329</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>356 rows × 351 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Concentration   1014.08   1015.85   1017.62    1019.4   1021.19  \\\n",
       "0              0.0 -0.005231  0.038095  0.082302  0.082237  0.076576   \n",
       "1              0.0 -0.081353 -0.131399 -0.160719 -0.190526 -0.224999   \n",
       "2              0.0 -0.143674 -0.116576 -0.123858 -0.097288 -0.035242   \n",
       "3              0.0  0.096424  0.010348  0.037800  0.007231 -0.021712   \n",
       "4              0.0  0.013245  0.088878  0.055112  0.057282  0.044607   \n",
       "..             ...       ...       ...       ...       ...       ...   \n",
       "351         1300.0  0.013892 -0.023256 -0.031826 -0.053169 -0.084421   \n",
       "352         1300.0 -0.300830 -0.334294 -0.300479 -0.285832 -0.234678   \n",
       "353         1300.0 -0.199545 -0.220742 -0.222265 -0.198332 -0.153330   \n",
       "354         1300.0 -0.107494 -0.126795 -0.182586 -0.215886 -0.246338   \n",
       "355         1300.0  0.027243  0.027325  0.045799 -0.001712  0.018226   \n",
       "\n",
       "      1022.98   1024.77   1026.58   1028.39  ...   2481.26   2491.87  \\\n",
       "0    0.106940  0.095478  0.089268  0.086707  ...  0.026194  0.021901   \n",
       "1   -0.239949 -0.231437 -0.181605 -0.190708  ...  0.012446  0.048297   \n",
       "2    0.023129  0.056915  0.098068  0.075592  ...  0.011739 -0.003521   \n",
       "3   -0.037742 -0.082596 -0.089286 -0.070054  ...  0.091157  0.068864   \n",
       "4    0.047680  0.033936  0.050192  0.005173  ...  0.074989  0.022504   \n",
       "..        ...       ...       ...       ...  ...       ...       ...   \n",
       "351 -0.100099 -0.078321 -0.081930 -0.054463  ...  0.149447  0.165580   \n",
       "352 -0.182459 -0.151797 -0.119183 -0.052906  ...  0.063646  0.027490   \n",
       "353 -0.132572 -0.199724 -0.168876 -0.109947  ...  0.096368  0.073663   \n",
       "354 -0.249917 -0.254625 -0.288691 -0.253613  ...  0.123685  0.132682   \n",
       "355 -0.049751 -0.056777 -0.107068 -0.135225  ...  0.084787  0.070592   \n",
       "\n",
       "      2502.56   2513.35   2524.23    2535.2   2546.27   2557.44   2568.71  \\\n",
       "0    0.019588  0.029467  0.030502  0.012769  0.000612 -0.015370 -0.047555   \n",
       "1    0.064985  0.043647  0.008629 -0.035054 -0.086570 -0.152690 -0.234232   \n",
       "2   -0.014063 -0.008997  0.002582  0.017829  0.048742  0.084157  0.115354   \n",
       "3    0.056702  0.034800  0.020370  0.016746  0.001978  0.007347  0.025432   \n",
       "4   -0.014653 -0.054191 -0.089806 -0.118453 -0.134508 -0.148832 -0.151126   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "351  0.169533  0.151979  0.127341  0.088787  0.039029 -0.012803 -0.064349   \n",
       "352  0.003725 -0.002946  0.010705  0.029308  0.049670  0.079636  0.110156   \n",
       "353  0.063638  0.061432  0.066310  0.063311  0.060929  0.077316  0.088265   \n",
       "354  0.137421  0.141735  0.143292  0.140572  0.137399  0.136859  0.126693   \n",
       "355  0.061971  0.066389  0.073253  0.083245  0.103166  0.116690  0.136836   \n",
       "\n",
       "      2580.07  \n",
       "0   -0.068557  \n",
       "1   -0.326223  \n",
       "2    0.151298  \n",
       "3    0.049032  \n",
       "4   -0.144606  \n",
       "..        ...  \n",
       "351 -0.110492  \n",
       "352  0.135482  \n",
       "353  0.111251  \n",
       "354  0.115020  \n",
       "355  0.146329  \n",
       "\n",
       "[356 rows x 351 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_excel(\"DataFrame_PathLen.xlsx\", index_col=0)\n",
    "start_columns = data.columns[:2]\n",
    "end_columns = data.columns[-8:]\n",
    "data = data.drop(columns=end_columns)\n",
    "reversed_columns = list(data.columns[2:][::-1])\n",
    "data = pd.concat([data[start_columns], data[reversed_columns]], axis=1)\n",
    "data= data[data['PathLength'] == 5].reset_index(drop=True)\n",
    "data= data.drop(columns='PathLength')\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = model_selection.train_test_split(data, test_size=0.1, random_state=1, stratify= data['Concentration'])\n",
    "\n",
    "train, validation = model_selection.train_test_split(train, test_size=0.1, random_state=1, stratify= train['Concentration'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(cv=5,\n",
       "             estimator=Pipeline(steps=[(&#x27;Scaler&#x27;, StandardScaler()),\n",
       "                                       (&#x27;mod&#x27;, RandomForestRegressor())]),\n",
       "             param_grid={&#x27;mod__max_depth&#x27;: [None, 10, 20, 30],\n",
       "                         &#x27;mod__min_samples_leaf&#x27;: [1, 2, 4],\n",
       "                         &#x27;mod__min_samples_split&#x27;: [2, 5, 10],\n",
       "                         &#x27;mod__n_estimators&#x27;: [100, 200, 500]},\n",
       "             scoring=&#x27;neg_mean_squared_error&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GridSearchCV</label><div class=\"sk-toggleable__content\"><pre>GridSearchCV(cv=5,\n",
       "             estimator=Pipeline(steps=[(&#x27;Scaler&#x27;, StandardScaler()),\n",
       "                                       (&#x27;mod&#x27;, RandomForestRegressor())]),\n",
       "             param_grid={&#x27;mod__max_depth&#x27;: [None, 10, 20, 30],\n",
       "                         &#x27;mod__min_samples_leaf&#x27;: [1, 2, 4],\n",
       "                         &#x27;mod__min_samples_split&#x27;: [2, 5, 10],\n",
       "                         &#x27;mod__n_estimators&#x27;: [100, 200, 500]},\n",
       "             scoring=&#x27;neg_mean_squared_error&#x27;)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;Scaler&#x27;, StandardScaler()), (&#x27;mod&#x27;, RandomForestRegressor())])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">StandardScaler</label><div class=\"sk-toggleable__content\"><pre>StandardScaler()</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestRegressor</label><div class=\"sk-toggleable__content\"><pre>RandomForestRegressor()</pre></div></div></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "GridSearchCV(cv=5,\n",
       "             estimator=Pipeline(steps=[('Scaler', StandardScaler()),\n",
       "                                       ('mod', RandomForestRegressor())]),\n",
       "             param_grid={'mod__max_depth': [None, 10, 20, 30],\n",
       "                         'mod__min_samples_leaf': [1, 2, 4],\n",
       "                         'mod__min_samples_split': [2, 5, 10],\n",
       "                         'mod__n_estimators': [100, 200, 500]},\n",
       "             scoring='neg_mean_squared_error')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf = Pipeline([\n",
    "    (\"Scaler\", StandardScaler()),\n",
    "    (\"mod\", RandomForestRegressor())])\n",
    "param_rf = {\n",
    "    'mod__n_estimators':     [100, 200, 500],\n",
    "    'mod__max_depth':        [None, 10, 20, 30],\n",
    "    'mod__min_samples_split':[2, 5, 10],\n",
    "    'mod__min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "best_rf = GridSearchCV(rf, param_rf, scoring=\"neg_mean_squared_error\", cv = 5)\n",
    "\n",
    "X_train = train.iloc[:,1:]\n",
    "y_train = train.iloc[:,0]\n",
    "\n",
    "best_rf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results — train RMSE 10.760898 - train R2 0.999637 — val RMSE 15.615498 - val R2 0.999227\n"
     ]
    }
   ],
   "source": [
    "rf.set_params(**best_rf.best_params_)\n",
    "rf.fit(X_train,y_train)\n",
    "\n",
    "train_r2 = rf.score(X_train, y_train)\n",
    "\n",
    "X_test = validation.iloc[:,1:]\n",
    "y_test = validation.iloc[:,0]\n",
    "val_r2 = rf.score(X_test, y_test)\n",
    "train_rmse = np.sqrt(mean_squared_error(y_train, rf.predict(X_train)))\n",
    "val_rmse = np.sqrt(mean_squared_error(y_test, rf.predict(X_test)))\n",
    "print(f\"Results — train RMSE {train_rmse:.6f} - train R2 {train_r2:.6f} — val RMSE {val_rmse:.6f} - val R2 {val_r2:.6f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mod__max_depth': 10,\n",
       " 'mod__min_samples_leaf': 2,\n",
       " 'mod__min_samples_split': 2,\n",
       " 'mod__n_estimators': 100}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_rf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-38.83729025  -3.81567821 -17.84841414 -23.98758142 -10.25251147]\n",
      "-18.94829509680605\n"
     ]
    }
   ],
   "source": [
    "train, test = model_selection.train_test_split(data, test_size=0.1, random_state=1, stratify= data['Concentration'])\n",
    "\n",
    "X_train = train.iloc[:,1:]\n",
    "y_train = train.iloc[:,0]\n",
    "\n",
    "cv_scores = model_selection.cross_val_score(rf.set_params(**best_rf.best_params_), X_train, y_train, cv = 5, scoring= 'neg_root_mean_squared_error')\n",
    "print(cv_scores)\n",
    "print(cv_scores.mean())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Without log output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "configs = Details(\n",
    "    pred_len = 1,      \n",
    "    seq_len  = 350,\n",
    "    d_ff = 32,\n",
    "    patch_len= 16, \n",
    "    stride = 8, \n",
    "    llm_layers = 4,\n",
    "    description = \"This dataset contains near-infrared (NIR) spectroscopy measurements of lactate concentration in a \" \\\n",
    "                  \"phosphate-buffered saline (PBS) solution. Each spectrum includes absorbance values across \" \\\n",
    "                  \"350 wavelengths. \" ,\n",
    "    dropout = 0.1,\n",
    "    n_heads = 2,\n",
    "    d_model = 16,\n",
    "    enc_in = 1,\n",
    "    d_llm = 4096,\n",
    "    llm_model = 'LLAMA',\n",
    "    lr = 0.0001\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f8d0610fd38427db7b89ecd45b5cbe1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  1 — train RMSE 694.820907 - train R2 -0.512214 — val RMSE 677.650168 - val R2 -0.456397\n",
      "Epoch  2 — train RMSE 674.128013 - train R2 -0.423483 — val RMSE 649.996659 - val R2 -0.339958\n",
      "Epoch  3 — train RMSE 653.317432 - train R2 -0.336953 — val RMSE 625.962241 - val R2 -0.242697\n",
      "Epoch  4 — train RMSE 630.227815 - train R2 -0.244121 — val RMSE 612.132141 - val R2 -0.188390\n",
      "Epoch  5 — train RMSE 612.597808 - train R2 -0.175489 — val RMSE 592.192200 - val R2 -0.112229\n",
      "Epoch  6 — train RMSE 591.924406 - train R2 -0.097489 — val RMSE 568.059519 - val R2 -0.023426\n",
      "Epoch  7 — train RMSE 582.272246 - train R2 -0.061988 — val RMSE 543.591324 - val R2 0.062840\n",
      "Epoch  8 — train RMSE 546.298650 - train R2 0.065181 — val RMSE 531.960334 - val R2 0.102515\n",
      "Epoch  9 — train RMSE 543.913451 - train R2 0.073326 — val RMSE 489.160533 - val R2 0.241123\n",
      "Epoch 10 — train RMSE 508.329423 - train R2 0.190610 — val RMSE 468.475753 - val R2 0.303946\n",
      "Epoch 11 — train RMSE 472.085978 - train R2 0.301913 — val RMSE 433.240542 - val R2 0.404712\n",
      "Epoch 12 — train RMSE 447.706452 - train R2 0.372152 — val RMSE 408.380074 - val R2 0.471070\n",
      "Epoch 13 — train RMSE 426.462237 - train R2 0.430323 — val RMSE 383.646316 - val R2 0.533200\n",
      "Epoch 14 — train RMSE 406.032724 - train R2 0.483596 — val RMSE 361.648512 - val R2 0.585197\n",
      "Epoch 15 — train RMSE 375.504098 - train R2 0.558331 — val RMSE 337.859762 - val R2 0.637972\n",
      "Epoch 16 — train RMSE 345.735845 - train R2 0.625582 — val RMSE 311.967287 - val R2 0.691335\n",
      "Epoch 17 — train RMSE 347.290911 - train R2 0.622207 — val RMSE 285.695777 - val R2 0.741133\n",
      "Epoch 18 — train RMSE 351.012925 - train R2 0.614065 — val RMSE 270.023422 - val R2 0.768755\n",
      "Epoch 19 — train RMSE 368.919164 - train R2 0.573686 — val RMSE 264.422617 - val R2 0.778249\n",
      "Epoch 20 — train RMSE 320.358042 - train R2 0.678531 — val RMSE 247.567202 - val R2 0.805618\n",
      "Epoch 21 — train RMSE 284.827116 - train R2 0.745885 — val RMSE 215.891511 - val R2 0.852178\n",
      "Epoch 22 — train RMSE 277.912198 - train R2 0.758074 — val RMSE 210.838201 - val R2 0.859017\n",
      "Epoch 23 — train RMSE 279.296736 - train R2 0.755657 — val RMSE 191.865118 - val R2 0.883249\n",
      "Epoch 24 — train RMSE 255.840267 - train R2 0.794976 — val RMSE 247.942902 - val R2 0.805028\n",
      "Epoch 25 — train RMSE 279.705868 - train R2 0.754941 — val RMSE 166.481668 - val R2 0.912097\n",
      "Epoch 26 — train RMSE 282.740110 - train R2 0.749595 — val RMSE 160.903476 - val R2 0.917889\n",
      "Epoch 27 — train RMSE 283.390759 - train R2 0.748442 — val RMSE 155.732734 - val R2 0.923082\n",
      "Epoch 28 — train RMSE 232.724258 - train R2 0.830351 — val RMSE 142.150871 - val R2 0.935913\n",
      "Epoch 29 — train RMSE 258.687731 - train R2 0.790386 — val RMSE 150.907811 - val R2 0.927774\n",
      "Epoch 30 — train RMSE 213.483000 - train R2 0.857244 — val RMSE 129.188881 - val R2 0.947068\n",
      "Epoch 31 — train RMSE 262.106664 - train R2 0.784809 — val RMSE 128.933521 - val R2 0.947277\n",
      "Epoch 32 — train RMSE 248.385641 - train R2 0.806749 — val RMSE 121.678025 - val R2 0.953044\n",
      "Epoch 33 — train RMSE 279.756637 - train R2 0.754852 — val RMSE 129.981719 - val R2 0.946416\n",
      "Epoch 34 — train RMSE 220.953554 - train R2 0.847078 — val RMSE 110.667899 - val R2 0.961157\n",
      "Epoch 35 — train RMSE 231.268426 - train R2 0.832467 — val RMSE 121.421104 - val R2 0.953242\n",
      "Epoch 36 — train RMSE 277.099817 - train R2 0.759486 — val RMSE 108.150740 - val R2 0.962904\n",
      "Epoch 37 — train RMSE 254.936988 - train R2 0.796421 — val RMSE 106.675581 - val R2 0.963909\n",
      "Epoch 38 — train RMSE 233.037323 - train R2 0.829894 — val RMSE 115.288734 - val R2 0.957846\n",
      "Epoch 39 — train RMSE 171.911948 - train R2 0.907428 — val RMSE 107.258049 - val R2 0.963514\n",
      "Epoch 40 — train RMSE 327.354288 - train R2 0.664337 — val RMSE 129.141261 - val R2 0.947107\n",
      "Epoch 41 — train RMSE 188.002071 - train R2 0.889288 — val RMSE 94.410263 - val R2 0.971731\n",
      "Epoch 42 — train RMSE 201.277471 - train R2 0.873101 — val RMSE 98.864581 - val R2 0.969001\n",
      "Epoch 43 — train RMSE 199.250225 - train R2 0.875644 — val RMSE 91.035810 - val R2 0.973716\n",
      "Epoch 44 — train RMSE 171.085826 - train R2 0.908316 — val RMSE 99.599765 - val R2 0.968538\n",
      "Epoch 45 — train RMSE 246.157918 - train R2 0.810200 — val RMSE 127.991633 - val R2 0.948044\n",
      "Epoch 46 — train RMSE 203.847517 - train R2 0.869840 — val RMSE 96.847236 - val R2 0.970253\n",
      "Epoch 47 — train RMSE 251.313449 - train R2 0.802167 — val RMSE 93.766425 - val R2 0.972115\n",
      "Epoch 48 — train RMSE 196.785470 - train R2 0.878702 — val RMSE 87.343111 - val R2 0.975805\n",
      "Epoch 49 — train RMSE 213.091974 - train R2 0.857767 — val RMSE 92.541078 - val R2 0.972839\n",
      "Epoch 50 — train RMSE 197.565829 - train R2 0.877738 — val RMSE 81.823562 - val R2 0.978766\n",
      "Epoch 51 — train RMSE 212.266323 - train R2 0.858867 — val RMSE 86.994856 - val R2 0.975998\n",
      "Epoch 52 — train RMSE 211.957528 - train R2 0.859277 — val RMSE 80.608617 - val R2 0.979392\n",
      "Epoch 53 — train RMSE 247.231372 - train R2 0.808541 — val RMSE 93.964470 - val R2 0.971998\n",
      "Epoch 54 — train RMSE 198.103110 - train R2 0.877072 — val RMSE 87.117795 - val R2 0.975930\n",
      "Epoch 55 — train RMSE 237.194198 - train R2 0.823772 — val RMSE 83.180086 - val R2 0.978056\n",
      "Epoch 56 — train RMSE 236.292416 - train R2 0.825109 — val RMSE 81.234574 - val R2 0.979071\n",
      "Epoch 57 — train RMSE 292.085521 - train R2 0.732769 — val RMSE 86.800790 - val R2 0.976104\n",
      "Epoch 58 — train RMSE 237.459722 - train R2 0.823377 — val RMSE 79.563073 - val R2 0.979923\n",
      "Epoch 59 — train RMSE 197.245741 - train R2 0.878134 — val RMSE 88.144593 - val R2 0.975359\n",
      "Epoch 60 — train RMSE 271.359692 - train R2 0.769348 — val RMSE 77.709712 - val R2 0.980848\n",
      "Epoch 61 — train RMSE 235.804196 - train R2 0.825831 — val RMSE 82.559130 - val R2 0.978383\n",
      "Epoch 62 — train RMSE 320.800265 - train R2 0.677643 — val RMSE 82.318550 - val R2 0.978509\n",
      "Epoch 63 — train RMSE 337.752461 - train R2 0.642674 — val RMSE 80.023648 - val R2 0.979690\n",
      "Epoch 64 — train RMSE 248.228386 - train R2 0.806994 — val RMSE 77.943733 - val R2 0.980732\n",
      "Epoch 65 — train RMSE 248.352488 - train R2 0.806801 — val RMSE 81.676181 - val R2 0.978843\n",
      "Epoch 66 — train RMSE 195.430511 - train R2 0.880367 — val RMSE 75.328694 - val R2 0.982003\n",
      "Epoch 67 — train RMSE 247.605397 - train R2 0.807962 — val RMSE 78.439256 - val R2 0.980486\n",
      "Epoch 68 — train RMSE 280.687680 - train R2 0.753218 — val RMSE 74.861901 - val R2 0.982226\n",
      "Epoch 69 — train RMSE 301.387042 - train R2 0.715477 — val RMSE 83.189298 - val R2 0.978052\n",
      "Epoch 70 — train RMSE 224.290372 - train R2 0.842424 — val RMSE 80.009370 - val R2 0.979697\n",
      "Epoch 71 — train RMSE 247.794264 - train R2 0.807669 — val RMSE 89.725728 - val R2 0.974467\n",
      "Epoch 72 — train RMSE 281.180480 - train R2 0.752350 — val RMSE 90.169249 - val R2 0.974214\n",
      "Epoch 73 — train RMSE 247.967518 - train R2 0.807400 — val RMSE 71.722882 - val R2 0.983685\n",
      "Epoch 74 — train RMSE 208.759321 - train R2 0.863492 — val RMSE 75.122205 - val R2 0.982102\n",
      "Epoch 75 — train RMSE 259.390589 - train R2 0.789246 — val RMSE 73.549878 - val R2 0.982843\n",
      "Epoch 76 — train RMSE 209.547761 - train R2 0.862459 — val RMSE 80.246401 - val R2 0.979577\n",
      "Epoch 77 — train RMSE 235.028218 - train R2 0.826976 — val RMSE 70.119760 - val R2 0.984406\n",
      "Epoch 78 — train RMSE 247.724178 - train R2 0.807777 — val RMSE 82.877461 - val R2 0.978216\n",
      "Epoch 79 — train RMSE 292.022507 - train R2 0.732884 — val RMSE 77.811542 - val R2 0.980798\n",
      "Epoch 80 — train RMSE 208.274765 - train R2 0.864125 — val RMSE 74.321726 - val R2 0.982481\n",
      "Epoch 81 — train RMSE 258.874457 - train R2 0.790084 — val RMSE 84.896897 - val R2 0.977141\n",
      "Epoch 82 — train RMSE 275.366928 - train R2 0.762485 — val RMSE 119.253785 - val R2 0.954896\n",
      "Epoch 83 — train RMSE 219.591389 - train R2 0.848958 — val RMSE 83.462564 - val R2 0.977907\n",
      "Epoch 84 — train RMSE 237.771221 - train R2 0.822913 — val RMSE 87.647358 - val R2 0.975636\n",
      "Epoch 85 — train RMSE 272.324129 - train R2 0.767705 — val RMSE 70.420780 - val R2 0.984272\n",
      "Epoch 86 — train RMSE 179.967922 - train R2 0.898549 — val RMSE 74.475224 - val R2 0.982409\n",
      "Epoch 87 — train RMSE 258.975159 - train R2 0.789921 — val RMSE 70.821952 - val R2 0.984092\n",
      "Epoch 88 — train RMSE 235.504689 - train R2 0.826273 — val RMSE 69.594887 - val R2 0.984639\n",
      "Epoch 89 — train RMSE 207.962143 - train R2 0.864532 — val RMSE 70.883553 - val R2 0.984065\n",
      "Epoch 90 — train RMSE 291.820006 - train R2 0.733254 — val RMSE 69.200585 - val R2 0.984812\n",
      "Epoch 91 — train RMSE 234.994530 - train R2 0.827025 — val RMSE 69.859593 - val R2 0.984522\n",
      "Epoch 92 — train RMSE 222.163038 - train R2 0.845399 — val RMSE 75.844656 - val R2 0.981756\n",
      "Epoch 93 — train RMSE 280.214395 - train R2 0.754049 — val RMSE 72.991144 - val R2 0.983103\n",
      "Epoch 94 — train RMSE 235.028371 - train R2 0.826975 — val RMSE 74.621111 - val R2 0.982340\n",
      "Epoch 95 — train RMSE 222.824385 - train R2 0.844478 — val RMSE 73.600704 - val R2 0.982820\n",
      "Epoch 96 — train RMSE 247.278969 - train R2 0.808468 — val RMSE 75.197663 - val R2 0.982066\n",
      "Epoch 97 — train RMSE 194.594144 - train R2 0.881388 — val RMSE 72.616596 - val R2 0.983276\n",
      "Epoch 98 — train RMSE 222.193157 - train R2 0.845358 — val RMSE 68.117450 - val R2 0.985284\n",
      "Epoch 99 — train RMSE 258.936432 - train R2 0.789983 — val RMSE 71.371712 - val R2 0.983844\n",
      "Epoch 100 — train RMSE 290.648426 - train R2 0.735392 — val RMSE 68.359442 - val R2 0.985179\n"
     ]
    }
   ],
   "source": [
    "train, test = model_selection.train_test_split(data, test_size=0.1, random_state=1, stratify= data['Concentration'])\n",
    "\n",
    "train, validation = model_selection.train_test_split(train, test_size=0.1, random_state=1, stratify= train['Concentration'])\n",
    "\n",
    "model ,train_rmse, train_r2, val_rmse, val_r2, stand  = train_model(configs, train, validation, 100, 8, True, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rerunning the same code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba76caddb0324302b54a8cbda1b8e26f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  1 — train RMSE 696.124659 - train R2 -0.517894 — val RMSE 679.468870 - val R2 -0.464225\n",
      "Epoch  2 — train RMSE 675.346560 - train R2 -0.428634 — val RMSE 651.499910 - val R2 -0.346163\n",
      "Epoch  3 — train RMSE 645.182508 - train R2 -0.303866 — val RMSE 623.508451 - val R2 -0.232973\n",
      "Epoch  4 — train RMSE 629.891544 - train R2 -0.242794 — val RMSE 608.221697 - val R2 -0.173256\n",
      "Epoch  5 — train RMSE 608.361596 - train R2 -0.159287 — val RMSE 591.180546 - val R2 -0.108432\n",
      "Epoch  6 — train RMSE 596.637818 - train R2 -0.115036 — val RMSE 570.462032 - val R2 -0.032101\n",
      "Epoch  7 — train RMSE 555.515346 - train R2 0.033371 — val RMSE 538.980222 - val R2 0.078672\n",
      "Epoch  8 — train RMSE 554.297498 - train R2 0.037605 — val RMSE 516.045404 - val R2 0.155412\n",
      "Epoch  9 — train RMSE 517.079241 - train R2 0.162506 — val RMSE 500.807063 - val R2 0.204556\n",
      "Epoch 10 — train RMSE 499.155911 - train R2 0.219559 — val RMSE 450.243684 - val R2 0.357069\n",
      "Epoch 11 — train RMSE 465.169139 - train R2 0.322219 — val RMSE 422.551573 - val R2 0.433724\n",
      "Epoch 12 — train RMSE 462.885406 - train R2 0.328858 — val RMSE 400.930363 - val R2 0.490192\n",
      "Epoch 13 — train RMSE 415.441372 - train R2 0.459386 — val RMSE 380.874451 - val R2 0.539921\n",
      "Epoch 14 — train RMSE 403.303186 - train R2 0.490516 — val RMSE 347.642060 - val R2 0.616705\n",
      "Epoch 15 — train RMSE 386.235084 - train R2 0.532727 — val RMSE 325.522357 - val R2 0.663929\n",
      "Epoch 16 — train RMSE 406.918607 - train R2 0.481340 — val RMSE 316.825794 - val R2 0.681646\n",
      "Epoch 17 — train RMSE 333.781066 - train R2 0.651028 — val RMSE 313.499461 - val R2 0.688296\n",
      "Epoch 18 — train RMSE 307.982257 - train R2 0.702889 — val RMSE 278.197034 - val R2 0.754544\n",
      "Epoch 19 — train RMSE 326.857308 - train R2 0.665355 — val RMSE 255.335338 - val R2 0.793229\n",
      "Epoch 20 — train RMSE 297.137090 - train R2 0.723445 — val RMSE 236.336308 - val R2 0.822855\n",
      "Epoch 21 — train RMSE 245.828120 - train R2 0.810709 — val RMSE 218.231231 - val R2 0.848956\n",
      "Epoch 22 — train RMSE 292.805765 - train R2 0.731449 — val RMSE 204.733169 - val R2 0.867063\n",
      "Epoch 23 — train RMSE 288.224257 - train R2 0.739787 — val RMSE 207.929848 - val R2 0.862880\n",
      "Epoch 24 — train RMSE 322.021954 - train R2 0.675183 — val RMSE 186.489627 - val R2 0.889699\n",
      "Epoch 25 — train RMSE 230.103097 - train R2 0.834151 — val RMSE 176.348522 - val R2 0.901369\n",
      "Epoch 26 — train RMSE 240.640766 - train R2 0.818613 — val RMSE 168.967192 - val R2 0.909453\n",
      "Epoch 27 — train RMSE 256.676272 - train R2 0.793634 — val RMSE 152.128757 - val R2 0.926601\n",
      "Epoch 28 — train RMSE 202.915338 - train R2 0.871027 — val RMSE 143.436020 - val R2 0.934749\n",
      "Epoch 29 — train RMSE 302.516294 - train R2 0.713341 — val RMSE 377.560462 - val R2 0.547892\n",
      "Epoch 30 — train RMSE 372.359593 - train R2 0.565697 — val RMSE 185.529077 - val R2 0.890833\n",
      "Epoch 31 — train RMSE 233.054123 - train R2 0.829870 — val RMSE 142.320855 - val R2 0.935760\n",
      "Epoch 32 — train RMSE 225.171952 - train R2 0.841183 — val RMSE 136.930983 - val R2 0.940534\n",
      "Epoch 33 — train RMSE 271.239629 - train R2 0.769552 — val RMSE 124.393833 - val R2 0.950924\n",
      "Epoch 34 — train RMSE 192.860172 - train R2 0.883493 — val RMSE 120.659337 - val R2 0.953827\n",
      "Epoch 35 — train RMSE 275.913456 - train R2 0.761541 — val RMSE 113.195448 - val R2 0.959363\n",
      "Epoch 36 — train RMSE 222.020177 - train R2 0.845598 — val RMSE 114.390565 - val R2 0.958500\n",
      "Epoch 37 — train RMSE 265.380019 - train R2 0.779401 — val RMSE 106.275605 - val R2 0.964179\n",
      "Epoch 38 — train RMSE 201.748417 - train R2 0.872507 — val RMSE 105.820191 - val R2 0.964485\n",
      "Epoch 39 — train RMSE 169.620195 - train R2 0.909880 — val RMSE 136.628123 - val R2 0.940796\n",
      "Epoch 40 — train RMSE 253.034284 - train R2 0.799448 — val RMSE 98.634564 - val R2 0.969145\n",
      "Epoch 41 — train RMSE 274.271283 - train R2 0.764371 — val RMSE 96.357928 - val R2 0.970553\n",
      "Epoch 42 — train RMSE 261.818476 - train R2 0.785282 — val RMSE 97.328523 - val R2 0.969957\n",
      "Epoch 43 — train RMSE 250.252718 - train R2 0.803833 — val RMSE 104.788715 - val R2 0.965174\n",
      "Epoch 44 — train RMSE 249.939872 - train R2 0.804324 — val RMSE 93.304386 - val R2 0.972390\n",
      "Epoch 45 — train RMSE 197.283676 - train R2 0.878087 — val RMSE 92.243473 - val R2 0.973014\n",
      "Epoch 46 — train RMSE 182.253701 - train R2 0.895955 — val RMSE 93.291158 - val R2 0.972397\n",
      "Epoch 47 — train RMSE 197.761774 - train R2 0.877495 — val RMSE 92.734787 - val R2 0.972726\n",
      "Epoch 48 — train RMSE 181.385826 - train R2 0.896944 — val RMSE 90.847519 - val R2 0.973825\n",
      "Epoch 49 — train RMSE 239.067216 - train R2 0.820978 — val RMSE 89.304650 - val R2 0.974706\n",
      "Epoch 50 — train RMSE 223.801110 - train R2 0.843111 — val RMSE 86.965072 - val R2 0.976014\n",
      "Epoch 51 — train RMSE 260.730585 - train R2 0.787063 — val RMSE 91.009646 - val R2 0.973731\n",
      "Epoch 52 — train RMSE 223.613912 - train R2 0.843374 — val RMSE 83.522278 - val R2 0.977875\n",
      "Epoch 53 — train RMSE 195.168551 - train R2 0.880687 — val RMSE 84.675001 - val R2 0.977261\n",
      "Epoch 54 — train RMSE 271.343755 - train R2 0.769375 — val RMSE 90.115805 - val R2 0.974244\n",
      "Epoch 55 — train RMSE 195.873700 - train R2 0.879823 — val RMSE 89.971502 - val R2 0.974327\n",
      "Epoch 56 — train RMSE 195.421763 - train R2 0.880377 — val RMSE 82.678770 - val R2 0.978320\n",
      "Epoch 57 — train RMSE 282.154471 - train R2 0.750632 — val RMSE 105.250844 - val R2 0.964867\n",
      "Epoch 58 — train RMSE 224.004821 - train R2 0.842825 — val RMSE 80.216223 - val R2 0.979592\n",
      "Epoch 59 — train RMSE 224.914747 - train R2 0.841546 — val RMSE 79.662430 - val R2 0.979873\n",
      "Epoch 60 — train RMSE 224.389315 - train R2 0.842285 — val RMSE 81.903807 - val R2 0.978725\n",
      "Epoch 61 — train RMSE 270.319128 - train R2 0.771113 — val RMSE 92.973201 - val R2 0.972585\n",
      "Epoch 62 — train RMSE 225.190917 - train R2 0.841157 — val RMSE 89.704538 - val R2 0.974479\n",
      "Epoch 63 — train RMSE 354.757096 - train R2 0.605788 — val RMSE 247.436760 - val R2 0.805823\n",
      "Epoch 64 — train RMSE 253.624630 - train R2 0.798512 — val RMSE 149.552670 - val R2 0.929066\n",
      "Epoch 65 — train RMSE 285.431463 - train R2 0.744806 — val RMSE 97.553938 - val R2 0.969817\n",
      "Epoch 66 — train RMSE 282.866313 - train R2 0.749372 — val RMSE 97.251818 - val R2 0.970004\n",
      "Epoch 67 — train RMSE 165.445442 - train R2 0.914261 — val RMSE 83.941793 - val R2 0.977653\n",
      "Epoch 68 — train RMSE 236.611546 - train R2 0.824636 — val RMSE 87.968909 - val R2 0.975457\n",
      "Epoch 69 — train RMSE 235.712164 - train R2 0.825967 — val RMSE 79.460456 - val R2 0.979975\n",
      "Epoch 70 — train RMSE 236.001895 - train R2 0.825539 — val RMSE 86.854032 - val R2 0.976075\n",
      "Epoch 71 — train RMSE 236.612255 - train R2 0.824635 — val RMSE 84.129421 - val R2 0.977553\n",
      "Epoch 72 — train RMSE 124.937372 - train R2 0.951106 — val RMSE 138.335376 - val R2 0.939307\n",
      "Epoch 73 — train RMSE 226.922768 - train R2 0.838704 — val RMSE 82.293673 - val R2 0.978522\n",
      "Epoch 74 — train RMSE 195.843085 - train R2 0.879861 — val RMSE 83.193441 - val R2 0.978049\n",
      "Epoch 75 — train RMSE 223.155992 - train R2 0.844014 — val RMSE 76.452878 - val R2 0.981462\n",
      "Epoch 76 — train RMSE 210.059474 - train R2 0.861786 — val RMSE 75.206458 - val R2 0.982062\n",
      "Epoch 77 — train RMSE 248.229627 - train R2 0.806992 — val RMSE 86.227072 - val R2 0.976419\n",
      "Epoch 78 — train RMSE 209.389886 - train R2 0.862666 — val RMSE 74.718826 - val R2 0.982294\n",
      "Epoch 79 — train RMSE 248.445407 - train R2 0.806656 — val RMSE 83.697152 - val R2 0.977783\n",
      "Epoch 80 — train RMSE 260.142835 - train R2 0.788022 — val RMSE 85.448872 - val R2 0.976843\n",
      "Epoch 81 — train RMSE 248.305697 - train R2 0.806874 — val RMSE 75.149443 - val R2 0.982089\n",
      "Epoch 82 — train RMSE 259.694420 - train R2 0.788752 — val RMSE 90.496851 - val R2 0.974026\n",
      "Epoch 83 — train RMSE 143.219341 - train R2 0.935750 — val RMSE 75.776043 - val R2 0.981789\n",
      "Epoch 84 — train RMSE 235.528368 - train R2 0.826238 — val RMSE 82.149203 - val R2 0.978597\n",
      "Epoch 85 — train RMSE 222.168556 - train R2 0.845392 — val RMSE 75.935482 - val R2 0.981712\n",
      "Epoch 86 — train RMSE 222.596135 - train R2 0.844796 — val RMSE 73.182823 - val R2 0.983014\n",
      "Epoch 87 — train RMSE 222.719884 - train R2 0.844624 — val RMSE 77.885365 - val R2 0.980761\n",
      "Epoch 88 — train RMSE 234.654560 - train R2 0.827525 — val RMSE 77.829285 - val R2 0.980789\n",
      "Epoch 89 — train RMSE 259.397178 - train R2 0.789235 — val RMSE 87.618899 - val R2 0.975652\n",
      "Epoch 90 — train RMSE 247.580151 - train R2 0.808001 — val RMSE 75.837794 - val R2 0.981759\n",
      "Epoch 91 — train RMSE 161.908926 - train R2 0.917888 — val RMSE 87.509053 - val R2 0.975713\n",
      "Epoch 92 — train RMSE 259.492592 - train R2 0.789080 — val RMSE 79.546638 - val R2 0.979932\n",
      "Epoch 93 — train RMSE 280.696287 - train R2 0.753202 — val RMSE 70.948731 - val R2 0.984035\n",
      "Epoch 94 — train RMSE 246.977423 - train R2 0.808935 — val RMSE 77.066885 - val R2 0.981163\n",
      "Epoch 95 — train RMSE 269.547130 - train R2 0.772419 — val RMSE 80.045684 - val R2 0.979679\n",
      "Epoch 96 — train RMSE 259.117121 - train R2 0.789690 — val RMSE 72.744025 - val R2 0.983217\n",
      "Epoch 97 — train RMSE 246.349749 - train R2 0.809905 — val RMSE 79.168269 - val R2 0.980122\n",
      "Epoch 98 — train RMSE 193.735546 - train R2 0.882433 — val RMSE 74.371101 - val R2 0.982458\n",
      "Epoch 99 — train RMSE 162.762843 - train R2 0.917019 — val RMSE 77.580596 - val R2 0.980911\n",
      "Epoch 100 — train RMSE 246.467292 - train R2 0.809723 — val RMSE 67.966078 - val R2 0.985349\n"
     ]
    }
   ],
   "source": [
    "train, test = model_selection.train_test_split(data, test_size=0.1, random_state=1, stratify= data['Concentration'])\n",
    "\n",
    "train, validation = model_selection.train_test_split(train, test_size=0.1, random_state=1, stratify= train['Concentration'])\n",
    "\n",
    "model ,train_rmse, train_r2, val_rmse, val_r2, stand  = train_model(configs, train, validation, 100, 8, True, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Without scaling wavelengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e750f57fde84ac99a7dc7b5c2863f9a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  1 — train RMSE 694.208917 - train R2 -0.509552 — val RMSE 675.265492 - val R2 -0.446165\n",
      "Epoch  2 — train RMSE 670.290813 - train R2 -0.407324 — val RMSE 640.213551 - val R2 -0.299926\n",
      "Epoch  3 — train RMSE 628.414102 - train R2 -0.236970 — val RMSE 599.859124 - val R2 -0.141215\n",
      "Epoch  4 — train RMSE 596.448701 - train R2 -0.114330 — val RMSE 571.862397 - val R2 -0.037175\n",
      "Epoch  5 — train RMSE 569.351068 - train R2 -0.015378 — val RMSE 546.070544 - val R2 0.054272\n",
      "Epoch  6 — train RMSE 570.973701 - train R2 -0.021174 — val RMSE 537.594112 - val R2 0.083404\n",
      "Epoch  7 — train RMSE 556.701754 - train R2 0.029238 — val RMSE 531.818644 - val R2 0.102993\n",
      "Epoch  8 — train RMSE 549.215033 - train R2 0.055173 — val RMSE 522.090663 - val R2 0.135509\n",
      "Epoch  9 — train RMSE 535.466669 - train R2 0.101884 — val RMSE 517.123599 - val R2 0.151880\n",
      "Epoch 10 — train RMSE 525.758087 - train R2 0.134156 — val RMSE 505.628797 - val R2 0.189165\n",
      "Epoch 11 — train RMSE 513.830947 - train R2 0.172995 — val RMSE 512.573505 - val R2 0.166739\n",
      "Epoch 12 — train RMSE 520.265615 - train R2 0.152152 — val RMSE 498.676921 - val R2 0.211308\n",
      "Epoch 13 — train RMSE 516.402287 - train R2 0.164697 — val RMSE 493.930960 - val R2 0.226249\n",
      "Epoch 14 — train RMSE 510.858951 - train R2 0.182535 — val RMSE 436.626123 - val R2 0.395372\n",
      "Epoch 15 — train RMSE 528.421332 - train R2 0.125362 — val RMSE 513.972352 - val R2 0.162185\n",
      "Epoch 16 — train RMSE 525.417867 - train R2 0.135277 — val RMSE 499.506581 - val R2 0.208682\n",
      "Epoch 17 — train RMSE 520.724289 - train R2 0.150656 — val RMSE 502.910063 - val R2 0.197861\n",
      "Epoch 18 — train RMSE 526.791707 - train R2 0.130749 — val RMSE 508.797905 - val R2 0.178969\n",
      "Epoch 19 — train RMSE 541.573784 - train R2 0.081281 — val RMSE 493.465480 - val R2 0.227707\n",
      "Epoch 20 — train RMSE 521.723872 - train R2 0.147393 — val RMSE 504.348191 - val R2 0.193267\n",
      "Epoch 21 — train RMSE 505.064764 - train R2 0.200972 — val RMSE 487.255680 - val R2 0.247021\n",
      "Epoch 22 — train RMSE 490.911693 - train R2 0.245126 — val RMSE 450.366405 - val R2 0.356719\n",
      "Epoch 23 — train RMSE 472.390342 - train R2 0.301012 — val RMSE 436.924993 - val R2 0.394544\n",
      "Epoch 24 — train RMSE 470.804678 - train R2 0.305697 — val RMSE 437.127542 - val R2 0.393983\n",
      "Epoch 25 — train RMSE 464.404595 - train R2 0.324445 — val RMSE 443.624370 - val R2 0.375835\n",
      "Epoch 26 — train RMSE 450.729521 - train R2 0.363645 — val RMSE 407.955057 - val R2 0.472171\n",
      "Epoch 27 — train RMSE 457.848095 - train R2 0.343386 — val RMSE 414.574646 - val R2 0.454902\n",
      "Epoch 28 — train RMSE 423.688434 - train R2 0.437709 — val RMSE 442.531002 - val R2 0.378908\n",
      "Epoch 29 — train RMSE 377.778029 - train R2 0.552966 — val RMSE 348.076091 - val R2 0.615747\n",
      "Epoch 30 — train RMSE 402.241738 - train R2 0.493194 — val RMSE 307.047546 - val R2 0.700994\n",
      "Epoch 31 — train RMSE 402.028331 - train R2 0.493731 — val RMSE 410.577937 - val R2 0.465362\n",
      "Epoch 32 — train RMSE 410.358537 - train R2 0.472534 — val RMSE 510.046195 - val R2 0.174936\n",
      "Epoch 33 — train RMSE 390.641095 - train R2 0.522005 — val RMSE 403.194132 - val R2 0.484419\n",
      "Epoch 34 — train RMSE 326.369090 - train R2 0.666354 — val RMSE 377.854949 - val R2 0.547187\n",
      "Epoch 35 — train RMSE 407.763878 - train R2 0.479183 — val RMSE 288.777758 - val R2 0.735518\n",
      "Epoch 36 — train RMSE 382.533164 - train R2 0.541641 — val RMSE 467.755226 - val R2 0.306085\n",
      "Epoch 37 — train RMSE 374.495742 - train R2 0.560700 — val RMSE 503.766195 - val R2 0.195128\n",
      "Epoch 38 — train RMSE 309.848446 - train R2 0.699277 — val RMSE 278.100087 - val R2 0.754715\n",
      "Epoch 39 — train RMSE 351.957837 - train R2 0.611985 — val RMSE 508.767913 - val R2 0.179066\n",
      "Epoch 40 — train RMSE 380.313509 - train R2 0.546945 — val RMSE 490.996476 - val R2 0.235415\n",
      "Epoch 41 — train RMSE 309.668581 - train R2 0.699626 — val RMSE 332.048967 - val R2 0.650318\n",
      "Epoch 42 — train RMSE 330.784158 - train R2 0.657266 — val RMSE 517.798414 - val R2 0.149665\n",
      "Epoch 43 — train RMSE 387.468966 - train R2 0.529736 — val RMSE 455.678497 - val R2 0.341454\n",
      "Epoch 44 — train RMSE 370.354767 - train R2 0.570361 — val RMSE 466.287917 - val R2 0.310432\n",
      "Epoch 45 — train RMSE 317.997776 - train R2 0.683251 — val RMSE 544.267060 - val R2 0.060508\n",
      "Epoch 46 — train RMSE 338.371671 - train R2 0.641363 — val RMSE 283.502359 - val R2 0.745093\n",
      "Epoch 47 — train RMSE 313.723132 - train R2 0.691709 — val RMSE 452.661888 - val R2 0.350145\n",
      "Epoch 48 — train RMSE 373.401977 - train R2 0.563262 — val RMSE 553.962646 - val R2 0.026738\n",
      "Epoch 49 — train RMSE 347.332323 - train R2 0.622117 — val RMSE 528.292658 - val R2 0.114848\n",
      "Epoch 50 — train RMSE 258.757454 - train R2 0.790274 — val RMSE 301.336217 - val R2 0.712014\n",
      "Epoch 51 — train RMSE 308.204069 - train R2 0.702461 — val RMSE 348.139182 - val R2 0.615608\n",
      "Epoch 52 — train RMSE 310.557427 - train R2 0.697900 — val RMSE 216.885182 - val R2 0.850814\n",
      "Epoch 53 — train RMSE 326.055197 - train R2 0.666996 — val RMSE 531.539480 - val R2 0.103934\n",
      "Epoch 54 — train RMSE 317.724294 - train R2 0.683795 — val RMSE 254.009656 - val R2 0.795370\n",
      "Epoch 55 — train RMSE 300.076243 - train R2 0.717947 — val RMSE 557.570765 - val R2 0.014018\n",
      "Epoch 56 — train RMSE 325.332600 - train R2 0.668470 — val RMSE 485.455128 - val R2 0.252576\n",
      "Epoch 57 — train RMSE 307.777572 - train R2 0.703284 — val RMSE 536.048114 - val R2 0.088669\n",
      "Epoch 58 — train RMSE 301.070355 - train R2 0.716075 — val RMSE 518.287784 - val R2 0.148057\n",
      "Epoch 59 — train RMSE 356.508856 - train R2 0.601885 — val RMSE 457.374989 - val R2 0.336542\n",
      "Epoch 60 — train RMSE 297.016329 - train R2 0.723670 — val RMSE 388.193029 - val R2 0.522070\n",
      "Epoch 61 — train RMSE 301.947661 - train R2 0.714418 — val RMSE 391.937777 - val R2 0.512805\n",
      "Epoch 62 — train RMSE 290.915341 - train R2 0.734905 — val RMSE 470.349841 - val R2 0.298366\n",
      "Epoch 63 — train RMSE 274.866233 - train R2 0.763348 — val RMSE 504.346830 - val R2 0.193272\n",
      "Epoch 64 — train RMSE 377.503333 - train R2 0.553615 — val RMSE 150.743025 - val R2 0.927932\n",
      "Epoch 65 — train RMSE 357.149135 - train R2 0.600454 — val RMSE 399.142737 - val R2 0.494728\n",
      "Epoch 66 — train RMSE 241.950709 - train R2 0.816633 — val RMSE 494.956491 - val R2 0.223032\n",
      "Epoch 67 — train RMSE 235.849882 - train R2 0.825764 — val RMSE 331.602481 - val R2 0.651258\n",
      "Epoch 68 — train RMSE 285.362514 - train R2 0.744929 — val RMSE 533.164539 - val R2 0.098447\n",
      "Epoch 69 — train RMSE 280.249955 - train R2 0.753987 — val RMSE 202.920744 - val R2 0.869406\n",
      "Epoch 70 — train RMSE 240.359966 - train R2 0.819036 — val RMSE 494.653090 - val R2 0.223985\n",
      "Epoch 71 — train RMSE 212.806712 - train R2 0.858147 — val RMSE 380.499571 - val R2 0.540826\n",
      "Epoch 72 — train RMSE 306.789925 - train R2 0.705185 — val RMSE 535.212359 - val R2 0.091508\n",
      "Epoch 73 — train RMSE 325.717546 - train R2 0.667685 — val RMSE 299.172570 - val R2 0.716135\n",
      "Epoch 74 — train RMSE 259.148078 - train R2 0.789640 — val RMSE 394.513870 - val R2 0.506379\n",
      "Epoch 75 — train RMSE 279.572075 - train R2 0.755175 — val RMSE 292.904770 - val R2 0.727904\n",
      "Epoch 76 — train RMSE 285.739821 - train R2 0.744254 — val RMSE 129.587843 - val R2 0.946740\n",
      "Epoch 77 — train RMSE 252.296911 - train R2 0.800616 — val RMSE 168.568768 - val R2 0.909880\n",
      "Epoch 78 — train RMSE 228.294538 - train R2 0.836748 — val RMSE 197.451894 - val R2 0.876351\n",
      "Epoch 79 — train RMSE 250.329660 - train R2 0.803713 — val RMSE 468.794527 - val R2 0.302998\n",
      "Epoch 80 — train RMSE 231.458085 - train R2 0.832192 — val RMSE 242.673606 - val R2 0.813227\n",
      "Epoch 81 — train RMSE 274.343485 - train R2 0.764247 — val RMSE 231.289096 - val R2 0.830340\n",
      "Epoch 82 — train RMSE 237.733081 - train R2 0.822970 — val RMSE 549.720431 - val R2 0.041587\n",
      "Epoch 83 — train RMSE 285.316022 - train R2 0.745012 — val RMSE 400.941426 - val R2 0.490164\n",
      "Epoch 84 — train RMSE 242.857559 - train R2 0.815256 — val RMSE 180.480855 - val R2 0.896693\n",
      "Epoch 85 — train RMSE 269.450322 - train R2 0.772582 — val RMSE 401.388636 - val R2 0.489026\n",
      "Epoch 86 — train RMSE 256.234618 - train R2 0.794343 — val RMSE 206.592175 - val R2 0.864638\n",
      "Epoch 87 — train RMSE 215.997335 - train R2 0.853862 — val RMSE 317.963415 - val R2 0.679356\n",
      "Epoch 88 — train RMSE 205.331043 - train R2 0.867938 — val RMSE 422.214714 - val R2 0.434626\n",
      "Epoch 89 — train RMSE 256.117321 - train R2 0.794531 — val RMSE 309.434323 - val R2 0.696327\n",
      "Epoch 90 — train RMSE 279.052205 - train R2 0.756085 — val RMSE 272.808034 - val R2 0.763961\n",
      "Epoch 91 — train RMSE 289.986360 - train R2 0.736596 — val RMSE 236.930906 - val R2 0.821962\n",
      "Epoch 92 — train RMSE 280.966560 - train R2 0.752727 — val RMSE 445.947573 - val R2 0.369280\n",
      "Epoch 93 — train RMSE 240.459742 - train R2 0.818886 — val RMSE 384.906726 - val R2 0.530128\n",
      "Epoch 94 — train RMSE 217.947492 - train R2 0.851211 — val RMSE 471.853638 - val R2 0.293872\n",
      "Epoch 95 — train RMSE 249.594424 - train R2 0.804864 — val RMSE 367.594759 - val R2 0.571444\n",
      "Epoch 96 — train RMSE 288.305920 - train R2 0.739640 — val RMSE 120.024382 - val R2 0.954311\n",
      "Epoch 97 — train RMSE 263.317841 - train R2 0.782816 — val RMSE 326.565275 - val R2 0.661773\n",
      "Epoch 98 — train RMSE 314.843495 - train R2 0.689503 — val RMSE 538.877842 - val R2 0.079022\n",
      "Epoch 99 — train RMSE 241.187309 - train R2 0.817788 — val RMSE 498.806779 - val R2 0.210897\n",
      "Epoch 100 — train RMSE 202.484428 - train R2 0.871575 — val RMSE 572.324906 - val R2 -0.038853\n"
     ]
    }
   ],
   "source": [
    "train, test = model_selection.train_test_split(data, test_size=0.1, random_state=1, stratify= data['Concentration'])\n",
    "\n",
    "train, validation = model_selection.train_test_split(train, test_size=0.1, random_state=1, stratify= train['Concentration'])\n",
    "\n",
    "model ,train_rmse, train_r2, val_rmse, val_r2, stand  = train_model(configs, train, validation, 100, 8, True, False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AdamW optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6dc944a401744ce8bdd0e9d3044616b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  1 — train RMSE 694.929570 - train R2 -0.512687 — val RMSE 677.987152 - val R2 -0.457846\n",
      "Epoch  2 — train RMSE 675.280576 - train R2 -0.428355 — val RMSE 651.173052 - val R2 -0.344812\n",
      "Epoch  3 — train RMSE 646.699582 - train R2 -0.310004 — val RMSE 624.521973 - val R2 -0.236984\n",
      "Epoch  4 — train RMSE 630.123120 - train R2 -0.243708 — val RMSE 609.046283 - val R2 -0.176439\n",
      "Epoch  5 — train RMSE 607.373771 - train R2 -0.155526 — val RMSE 589.370122 - val R2 -0.101654\n",
      "Epoch  6 — train RMSE 593.205450 - train R2 -0.102244 — val RMSE 563.827060 - val R2 -0.008232\n",
      "Epoch  7 — train RMSE 580.267099 - train R2 -0.054686 — val RMSE 545.140628 - val R2 0.057490\n",
      "Epoch  8 — train RMSE 545.788180 - train R2 0.066927 — val RMSE 517.426342 - val R2 0.150886\n",
      "Epoch  9 — train RMSE 526.253585 - train R2 0.132523 — val RMSE 489.986125 - val R2 0.238559\n",
      "Epoch 10 — train RMSE 510.303284 - train R2 0.184312 — val RMSE 473.523214 - val R2 0.288866\n",
      "Epoch 11 — train RMSE 506.561824 - train R2 0.196229 — val RMSE 448.528601 - val R2 0.361958\n",
      "Epoch 12 — train RMSE 460.946997 - train R2 0.334467 — val RMSE 430.610763 - val R2 0.411917\n",
      "Epoch 13 — train RMSE 442.279709 - train R2 0.387281 — val RMSE 410.676253 - val R2 0.465106\n",
      "Epoch 14 — train RMSE 419.336344 - train R2 0.449202 — val RMSE 377.903267 - val R2 0.547071\n",
      "Epoch 15 — train RMSE 432.708902 - train R2 0.413512 — val RMSE 355.684985 - val R2 0.598764\n",
      "Epoch 16 — train RMSE 401.091903 - train R2 0.496087 — val RMSE 343.886811 - val R2 0.624941\n",
      "Epoch 17 — train RMSE 379.258141 - train R2 0.549456 — val RMSE 321.803068 - val R2 0.671565\n",
      "Epoch 18 — train RMSE 348.826862 - train R2 0.618858 — val RMSE 300.449978 - val R2 0.713705\n",
      "Epoch 19 — train RMSE 353.695285 - train R2 0.608145 — val RMSE 284.897689 - val R2 0.742577\n",
      "Epoch 20 — train RMSE 375.120027 - train R2 0.559234 — val RMSE 267.233593 - val R2 0.773509\n",
      "Epoch 21 — train RMSE 283.918053 - train R2 0.747505 — val RMSE 249.672315 - val R2 0.802299\n",
      "Epoch 22 — train RMSE 320.599690 - train R2 0.678046 — val RMSE 257.967098 - val R2 0.788944\n",
      "Epoch 23 — train RMSE 276.481650 - train R2 0.760558 — val RMSE 215.180501 - val R2 0.853150\n",
      "Epoch 24 — train RMSE 302.528113 - train R2 0.713319 — val RMSE 197.682493 - val R2 0.876062\n",
      "Epoch 25 — train RMSE 251.699158 - train R2 0.801559 — val RMSE 190.889269 - val R2 0.884434\n",
      "Epoch 26 — train RMSE 327.099464 - train R2 0.664859 — val RMSE 181.477904 - val R2 0.895548\n",
      "Epoch 27 — train RMSE 268.471053 - train R2 0.774232 — val RMSE 165.149382 - val R2 0.913499\n",
      "Epoch 28 — train RMSE 169.325640 - train R2 0.910192 — val RMSE 158.523363 - val R2 0.920301\n",
      "Epoch 29 — train RMSE 249.201996 - train R2 0.805477 — val RMSE 176.656007 - val R2 0.901025\n",
      "Epoch 30 — train RMSE 225.041876 - train R2 0.841367 — val RMSE 158.690715 - val R2 0.920132\n",
      "Epoch 31 — train RMSE 230.735095 - train R2 0.833239 — val RMSE 139.686696 - val R2 0.938116\n",
      "Epoch 32 — train RMSE 280.877507 - train R2 0.752884 — val RMSE 123.584861 - val R2 0.951561\n",
      "Epoch 33 — train RMSE 259.338044 - train R2 0.789331 — val RMSE 119.270466 - val R2 0.954884\n",
      "Epoch 34 — train RMSE 220.139938 - train R2 0.848202 — val RMSE 123.872819 - val R2 0.951335\n",
      "Epoch 35 — train RMSE 191.421156 - train R2 0.885225 — val RMSE 123.930381 - val R2 0.951289\n",
      "Epoch 36 — train RMSE 281.280114 - train R2 0.752175 — val RMSE 157.835653 - val R2 0.920991\n",
      "Epoch 37 — train RMSE 201.565983 - train R2 0.872737 — val RMSE 110.228386 - val R2 0.961465\n",
      "Epoch 38 — train RMSE 252.663822 - train R2 0.800035 — val RMSE 104.281426 - val R2 0.965511\n",
      "Epoch 39 — train RMSE 240.644273 - train R2 0.818608 — val RMSE 120.536482 - val R2 0.953921\n",
      "Epoch 40 — train RMSE 253.926091 - train R2 0.798032 — val RMSE 103.963881 - val R2 0.965721\n",
      "Epoch 41 — train RMSE 192.649151 - train R2 0.883748 — val RMSE 93.858819 - val R2 0.972061\n",
      "Epoch 42 — train RMSE 274.845344 - train R2 0.763384 — val RMSE 94.333268 - val R2 0.971777\n",
      "Epoch 43 — train RMSE 213.104629 - train R2 0.857750 — val RMSE 93.523189 - val R2 0.972260\n",
      "Epoch 44 — train RMSE 212.303090 - train R2 0.858818 — val RMSE 87.619864 - val R2 0.975651\n",
      "Epoch 45 — train RMSE 262.681468 - train R2 0.783864 — val RMSE 99.617797 - val R2 0.968527\n",
      "Epoch 46 — train RMSE 237.805209 - train R2 0.822863 — val RMSE 90.749354 - val R2 0.973881\n",
      "Epoch 47 — train RMSE 212.221544 - train R2 0.858926 — val RMSE 99.167303 - val R2 0.968811\n",
      "Epoch 48 — train RMSE 211.563092 - train R2 0.859800 — val RMSE 89.882686 - val R2 0.974378\n",
      "Epoch 49 — train RMSE 182.713580 - train R2 0.895429 — val RMSE 89.781753 - val R2 0.974435\n",
      "Epoch 50 — train RMSE 224.495197 - train R2 0.842137 — val RMSE 88.655858 - val R2 0.975072\n",
      "Epoch 51 — train RMSE 226.217238 - train R2 0.839705 — val RMSE 94.875952 - val R2 0.971452\n",
      "Epoch 52 — train RMSE 211.327408 - train R2 0.860112 — val RMSE 100.444464 - val R2 0.968002\n",
      "Epoch 53 — train RMSE 238.375767 - train R2 0.822012 — val RMSE 93.872449 - val R2 0.972052\n",
      "Epoch 54 — train RMSE 238.620649 - train R2 0.821646 — val RMSE 94.498271 - val R2 0.971678\n",
      "Epoch 55 — train RMSE 248.741137 - train R2 0.806196 — val RMSE 91.235209 - val R2 0.973601\n",
      "Epoch 56 — train RMSE 180.883264 - train R2 0.897514 — val RMSE 99.172417 - val R2 0.968807\n",
      "Epoch 57 — train RMSE 261.212908 - train R2 0.786274 — val RMSE 99.493774 - val R2 0.968605\n",
      "Epoch 58 — train RMSE 224.140285 - train R2 0.842635 — val RMSE 118.071844 - val R2 0.955786\n",
      "Epoch 59 — train RMSE 211.027991 - train R2 0.860509 — val RMSE 94.406190 - val R2 0.971734\n",
      "Epoch 60 — train RMSE 166.568872 - train R2 0.913093 — val RMSE 123.336510 - val R2 0.951755\n",
      "Epoch 61 — train RMSE 213.004409 - train R2 0.857883 — val RMSE 95.699369 - val R2 0.970954\n",
      "Epoch 62 — train RMSE 211.042054 - train R2 0.860490 — val RMSE 87.378855 - val R2 0.975785\n",
      "Epoch 63 — train RMSE 210.461008 - train R2 0.861257 — val RMSE 85.929652 - val R2 0.976582\n",
      "Epoch 64 — train RMSE 236.237554 - train R2 0.825190 — val RMSE 93.717222 - val R2 0.972145\n",
      "Epoch 65 — train RMSE 212.711693 - train R2 0.858274 — val RMSE 96.318345 - val R2 0.970577\n",
      "Epoch 66 — train RMSE 302.213100 - train R2 0.713916 — val RMSE 105.194401 - val R2 0.964904\n",
      "Epoch 67 — train RMSE 107.265725 - train R2 0.963960 — val RMSE 102.915595 - val R2 0.966408\n",
      "Epoch 68 — train RMSE 182.416821 - train R2 0.895769 — val RMSE 84.372488 - val R2 0.977423\n",
      "Epoch 69 — train RMSE 248.519421 - train R2 0.806541 — val RMSE 92.881442 - val R2 0.972639\n",
      "Epoch 70 — train RMSE 237.895645 - train R2 0.822728 — val RMSE 91.039919 - val R2 0.973714\n",
      "Epoch 71 — train RMSE 196.351302 - train R2 0.879237 — val RMSE 97.727004 - val R2 0.969710\n",
      "Epoch 72 — train RMSE 222.198797 - train R2 0.845350 — val RMSE 92.762478 - val R2 0.972709\n",
      "Epoch 73 — train RMSE 224.087343 - train R2 0.842710 — val RMSE 89.832611 - val R2 0.974406\n",
      "Epoch 74 — train RMSE 208.807244 - train R2 0.863429 — val RMSE 85.587275 - val R2 0.976768\n",
      "Epoch 75 — train RMSE 210.276655 - train R2 0.861500 — val RMSE 87.016466 - val R2 0.975986\n",
      "Epoch 76 — train RMSE 179.903225 - train R2 0.898622 — val RMSE 92.228608 - val R2 0.973023\n",
      "Epoch 77 — train RMSE 268.835341 - train R2 0.773619 — val RMSE 144.548230 - val R2 0.933733\n",
      "Epoch 78 — train RMSE 251.555751 - train R2 0.801785 — val RMSE 81.708407 - val R2 0.978826\n",
      "Epoch 79 — train RMSE 182.787965 - train R2 0.895344 — val RMSE 83.552695 - val R2 0.977859\n",
      "Epoch 80 — train RMSE 248.632418 - train R2 0.806365 — val RMSE 87.275933 - val R2 0.975842\n",
      "Epoch 81 — train RMSE 208.510334 - train R2 0.863817 — val RMSE 75.916925 - val R2 0.981721\n",
      "Epoch 82 — train RMSE 210.050194 - train R2 0.861798 — val RMSE 97.985828 - val R2 0.969549\n",
      "Epoch 83 — train RMSE 178.423653 - train R2 0.900282 — val RMSE 75.332059 - val R2 0.982002\n",
      "Epoch 84 — train RMSE 258.877683 - train R2 0.790079 — val RMSE 69.686108 - val R2 0.984599\n",
      "Epoch 85 — train RMSE 270.000153 - train R2 0.771653 — val RMSE 81.133733 - val R2 0.979123\n",
      "Epoch 86 — train RMSE 223.712385 - train R2 0.843235 — val RMSE 79.291237 - val R2 0.980060\n",
      "Epoch 87 — train RMSE 209.575538 - train R2 0.862422 — val RMSE 81.934497 - val R2 0.978709\n",
      "Epoch 88 — train RMSE 140.956399 - train R2 0.937765 — val RMSE 89.381834 - val R2 0.974662\n",
      "Epoch 89 — train RMSE 208.777628 - train R2 0.863468 — val RMSE 78.485862 - val R2 0.980463\n",
      "Epoch 90 — train RMSE 222.305792 - train R2 0.845201 — val RMSE 80.372543 - val R2 0.979513\n",
      "Epoch 91 — train RMSE 246.544644 - train R2 0.809604 — val RMSE 81.854566 - val R2 0.978750\n",
      "Epoch 92 — train RMSE 235.283588 - train R2 0.826599 — val RMSE 80.390041 - val R2 0.979504\n",
      "Epoch 93 — train RMSE 222.075557 - train R2 0.845521 — val RMSE 82.470308 - val R2 0.978429\n",
      "Epoch 94 — train RMSE 246.441731 - train R2 0.809763 — val RMSE 78.288640 - val R2 0.980561\n",
      "Epoch 95 — train RMSE 222.319508 - train R2 0.845182 — val RMSE 85.184019 - val R2 0.976986\n",
      "Epoch 96 — train RMSE 194.173740 - train R2 0.881900 — val RMSE 89.080030 - val R2 0.974833\n",
      "Epoch 97 — train RMSE 258.383394 - train R2 0.790879 — val RMSE 81.197393 - val R2 0.979090\n",
      "Epoch 98 — train RMSE 221.956110 - train R2 0.845687 — val RMSE 81.389631 - val R2 0.978991\n",
      "Epoch 99 — train RMSE 258.327089 - train R2 0.790971 — val RMSE 78.650182 - val R2 0.980381\n",
      "Epoch 100 — train RMSE 221.707411 - train R2 0.846033 — val RMSE 75.853656 - val R2 0.981752\n"
     ]
    }
   ],
   "source": [
    "train, test = model_selection.train_test_split(data, test_size=0.1, random_state=1, stratify= data['Concentration'])\n",
    "\n",
    "train, validation = model_selection.train_test_split(train, test_size=0.1, random_state=1, stratify= train['Concentration'])\n",
    "\n",
    "model ,train_rmse, train_r2, val_rmse, val_r2, stand  = train_model(configs, train, validation, 100, 8, True, False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Early stoppage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa68dc999fb747ebb19f04831d6c3f50",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  1 — train RMSE 696.442279 - train R2 -0.519280 — val RMSE 680.736659 - val R2 -0.469694\n",
      "Epoch  2 — train RMSE 680.189913 - train R2 -0.449199 — val RMSE 654.011342 - val R2 -0.356561\n",
      "Epoch  3 — train RMSE 646.097220 - train R2 -0.307565 — val RMSE 625.543689 - val R2 -0.241035\n",
      "Epoch  4 — train RMSE 624.219410 - train R2 -0.220512 — val RMSE 606.575306 - val R2 -0.166912\n",
      "Epoch  5 — train RMSE 615.523687 - train R2 -0.186744 — val RMSE 592.219719 - val R2 -0.112332\n",
      "Epoch  6 — train RMSE 598.146050 - train R2 -0.120681 — val RMSE 570.224647 - val R2 -0.031242\n",
      "Epoch  7 — train RMSE 576.905917 - train R2 -0.042504 — val RMSE 544.326609 - val R2 0.060303\n",
      "Epoch  8 — train RMSE 534.136505 - train R2 0.106341 — val RMSE 517.906283 - val R2 0.149310\n",
      "Epoch  9 — train RMSE 530.565854 - train R2 0.118249 — val RMSE 487.323478 - val R2 0.246812\n",
      "Epoch 10 — train RMSE 484.830560 - train R2 0.263712 — val RMSE 465.582427 - val R2 0.312517\n",
      "Epoch 11 — train RMSE 474.501903 - train R2 0.294750 — val RMSE 429.742601 - val R2 0.414286\n",
      "Epoch 12 — train RMSE 438.312818 - train R2 0.398223 — val RMSE 408.248737 - val R2 0.471411\n",
      "Epoch 13 — train RMSE 418.940081 - train R2 0.450242 — val RMSE 382.121616 - val R2 0.536903\n",
      "Epoch 14 — train RMSE 399.491854 - train R2 0.500100 — val RMSE 357.948959 - val R2 0.593640\n",
      "Epoch 15 — train RMSE 356.304851 - train R2 0.602341 — val RMSE 337.060550 - val R2 0.639683\n",
      "Epoch 16 — train RMSE 389.616957 - train R2 0.524508 — val RMSE 312.608647 - val R2 0.690065\n",
      "Epoch 17 — train RMSE 334.597526 - train R2 0.649318 — val RMSE 298.151515 - val R2 0.718069\n",
      "Epoch 18 — train RMSE 360.002826 - train R2 0.594044 — val RMSE 277.164310 - val R2 0.756363\n",
      "Epoch 19 — train RMSE 297.885877 - train R2 0.722050 — val RMSE 260.310307 - val R2 0.785093\n",
      "Epoch 20 — train RMSE 307.635552 - train R2 0.703557 — val RMSE 243.037625 - val R2 0.812666\n",
      "Epoch 21 — train RMSE 288.684150 - train R2 0.738956 — val RMSE 225.968005 - val R2 0.838057\n",
      "Epoch 22 — train RMSE 287.804952 - train R2 0.740544 — val RMSE 211.370655 - val R2 0.858304\n",
      "Epoch 23 — train RMSE 270.323693 - train R2 0.771105 — val RMSE 200.459492 - val R2 0.872555\n",
      "Epoch 24 — train RMSE 301.814676 - train R2 0.714670 — val RMSE 187.853074 - val R2 0.888081\n",
      "Epoch 25 — train RMSE 223.731093 - train R2 0.843209 — val RMSE 177.703455 - val R2 0.899848\n",
      "Epoch 26 — train RMSE 216.165180 - train R2 0.853634 — val RMSE 191.277469 - val R2 0.883963\n",
      "Epoch 27 — train RMSE 231.038114 - train R2 0.832801 — val RMSE 168.139159 - val R2 0.910338\n",
      "Epoch 28 — train RMSE 242.629568 - train R2 0.815602 — val RMSE 146.019915 - val R2 0.932377\n",
      "Epoch 29 — train RMSE 294.304050 - train R2 0.728694 — val RMSE 145.827034 - val R2 0.932556\n",
      "Epoch 30 — train RMSE 228.795950 - train R2 0.836030 — val RMSE 136.607315 - val R2 0.940814\n",
      "Epoch 31 — train RMSE 247.460297 - train R2 0.808187 — val RMSE 135.040749 - val R2 0.942164\n",
      "Epoch 32 — train RMSE 220.368220 - train R2 0.847887 — val RMSE 121.814147 - val R2 0.952939\n",
      "Epoch 33 — train RMSE 187.813490 - train R2 0.889510 — val RMSE 117.004061 - val R2 0.956582\n",
      "Epoch 34 — train RMSE 201.090362 - train R2 0.873337 — val RMSE 110.652797 - val R2 0.961168\n",
      "Epoch 35 — train RMSE 276.495002 - train R2 0.760535 — val RMSE 111.387565 - val R2 0.960650\n",
      "Epoch 36 — train RMSE 263.647993 - train R2 0.782271 — val RMSE 121.055774 - val R2 0.953523\n",
      "Epoch 37 — train RMSE 263.419470 - train R2 0.782648 — val RMSE 105.458357 - val R2 0.964728\n",
      "Epoch 38 — train RMSE 226.991319 - train R2 0.838606 — val RMSE 98.362242 - val R2 0.969315\n",
      "Epoch 39 — train RMSE 212.927858 - train R2 0.857986 — val RMSE 96.043579 - val R2 0.970745\n",
      "Epoch 40 — train RMSE 226.570878 - train R2 0.839204 — val RMSE 99.077888 - val R2 0.968867\n",
      "Epoch 41 — train RMSE 181.266641 - train R2 0.897079 — val RMSE 95.778517 - val R2 0.970906\n",
      "Epoch 42 — train RMSE 251.204065 - train R2 0.802339 — val RMSE 96.331645 - val R2 0.970569\n",
      "Epoch 43 — train RMSE 272.115630 - train R2 0.768061 — val RMSE 88.553890 - val R2 0.975130\n",
      "Epoch 44 — train RMSE 272.014937 - train R2 0.768232 — val RMSE 84.442670 - val R2 0.977385\n",
      "Epoch 45 — train RMSE 224.216041 - train R2 0.842529 — val RMSE 92.783133 - val R2 0.972697\n",
      "Epoch 46 — train RMSE 126.694911 - train R2 0.949721 — val RMSE 95.034684 - val R2 0.971356\n",
      "Epoch 47 — train RMSE 231.168232 - train R2 0.832612 — val RMSE 170.591749 - val R2 0.907704\n",
      "Epoch 48 — train RMSE 236.614046 - train R2 0.824633 — val RMSE 96.679628 - val R2 0.970356\n",
      "Epoch 49 — train RMSE 227.859802 - train R2 0.837369 — val RMSE 111.198902 - val R2 0.960783\n",
      "Early stopping triggered\n"
     ]
    }
   ],
   "source": [
    "train, test = model_selection.train_test_split(data, test_size=0.1, random_state=1, stratify= data['Concentration'])\n",
    "\n",
    "train, validation = model_selection.train_test_split(train, test_size=0.1, random_state=1, stratify= train['Concentration'])\n",
    "\n",
    "model ,train_rmse, train_r2, val_rmse, val_r2, stand  = train_model(configs, train, validation, 100, 8, True, True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "New config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "configs = Details(\n",
    "    pred_len = 1,      \n",
    "    seq_len  = 350,\n",
    "    d_ff = 32,\n",
    "    patch_len= 16, \n",
    "    stride = 8, \n",
    "    llm_layers = 16, #increased\n",
    "    description = \"This dataset contains near-infrared (NIR) spectroscopy measurements of lactate concentration in a \" \\\n",
    "                  \"phosphate-buffered saline (PBS) solution. Each spectrum includes absorbance values across \" \\\n",
    "                  \"350 wavelengths. \" ,\n",
    "    dropout = 0.1,\n",
    "    n_heads = 4, #increased\n",
    "    d_model = 16,\n",
    "    enc_in = 1,\n",
    "    d_llm = 4096,\n",
    "    llm_model = 'LLAMA',\n",
    "    lr = 0.001\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ded01195c72042c2a034ea70c2933740",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 84.00 MiB. GPU 0 has a total capacity of 23.68 GiB of which 34.50 MiB is free. Including non-PyTorch memory, this process has 23.41 GiB memory in use. Of the allocated memory 21.21 GiB is allocated by PyTorch, and 956.18 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m train, test \u001b[38;5;241m=\u001b[39m model_selection\u001b[38;5;241m.\u001b[39mtrain_test_split(data, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, stratify\u001b[38;5;241m=\u001b[39m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mConcentration\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m      3\u001b[0m train, validation \u001b[38;5;241m=\u001b[39m model_selection\u001b[38;5;241m.\u001b[39mtrain_test_split(train, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, stratify\u001b[38;5;241m=\u001b[39m train[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mConcentration\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m----> 5\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model_earlystop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfigs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[5], line 64\u001b[0m, in \u001b[0;36mtrain_model_earlystop\u001b[0;34m(configs, train, validation, epochs, batch_size, updates, patience)\u001b[0m\n\u001b[1;32m     62\u001b[0m x_enc, y_true \u001b[38;5;241m=\u001b[39m x_enc\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39mto(device), y_true\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     63\u001b[0m opt\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 64\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_enc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m  \n\u001b[1;32m     65\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(y_pred, y_true)\n\u001b[1;32m     66\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[2], line 128\u001b[0m, in \u001b[0;36mModel.forward\u001b[0;34m(self, x_enc, x_mark_enc, x_dec, x_mark_dec, mask)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x_enc, x_mark_enc, x_dec, x_mark_dec, mask\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m--> 128\u001b[0m     dec_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforecast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_enc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_mark_enc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_dec\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_mark_dec\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    129\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m dec_out[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpred_len:, :]\n",
      "Cell \u001b[0;32mIn[2], line 178\u001b[0m, in \u001b[0;36mModel.forecast\u001b[0;34m(self, x_enc, x_mark_enc, x_dec, x_mark_dec)\u001b[0m\n\u001b[1;32m    176\u001b[0m enc_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreprogramming_layer(enc_out, source_embeddings, source_embeddings)\n\u001b[1;32m    177\u001b[0m llama_enc_out \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([prompt_embeddings, enc_out], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m--> 178\u001b[0m dec_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mllm_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mllama_enc_out\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mlast_hidden_state\n\u001b[1;32m    179\u001b[0m \u001b[38;5;66;03m#print(dec_out)\u001b[39;00m\n\u001b[1;32m    180\u001b[0m dec_out \u001b[38;5;241m=\u001b[39m dec_out[:, :, :\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39md_ff]\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/utils/generic.py:969\u001b[0m, in \u001b[0;36mcan_return_tuple.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    966\u001b[0m     set_attribute_for_modules(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_is_top_level_module\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    968\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 969\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    970\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_requested_to_return_tuple \u001b[38;5;129;01mor\u001b[39;00m (is_configured_to_return_tuple \u001b[38;5;129;01mand\u001b[39;00m is_top_level_module):\n\u001b[1;32m    971\u001b[0m         output \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mto_tuple()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:453\u001b[0m, in \u001b[0;36mLlamaModel.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, cache_position, **flash_attn_kwargs)\u001b[0m\n\u001b[1;32m    450\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states:\n\u001b[1;32m    451\u001b[0m     all_hidden_states \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (hidden_states,)\n\u001b[0;32m--> 453\u001b[0m layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    454\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    455\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    456\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    457\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    458\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    459\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    460\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    461\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    462\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mflash_attn_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    463\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    465\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    467\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/modeling_layers.py:48\u001b[0m, in \u001b[0;36mGradientCheckpointingLayer.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgradient_checkpointing \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining:\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(partial(\u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs), \u001b[38;5;241m*\u001b[39margs)\n\u001b[0;32m---> 48\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:324\u001b[0m, in \u001b[0;36mLlamaDecoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, position_embeddings, **kwargs)\u001b[0m\n\u001b[1;32m    322\u001b[0m residual \u001b[38;5;241m=\u001b[39m hidden_states\n\u001b[1;32m    323\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpost_attention_layernorm(hidden_states)\n\u001b[0;32m--> 324\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmlp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    325\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n\u001b[1;32m    327\u001b[0m outputs \u001b[38;5;241m=\u001b[39m (hidden_states,)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:162\u001b[0m, in \u001b[0;36mLlamaMLP.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m--> 162\u001b[0m     down_proj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdown_proj(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mact_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgate_proj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mup_proj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    163\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m down_proj\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 84.00 MiB. GPU 0 has a total capacity of 23.68 GiB of which 34.50 MiB is free. Including non-PyTorch memory, this process has 23.41 GiB memory in use. Of the allocated memory 21.21 GiB is allocated by PyTorch, and 956.18 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "train, test = model_selection.train_test_split(data, test_size=0.1, random_state=1, stratify= data['Concentration'])\n",
    "\n",
    "train, validation = model_selection.train_test_split(train, test_size=0.1, random_state=1, stratify= train['Concentration'])\n",
    "\n",
    "model = train_model(configs, train, validation, 100, 8, True, True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Epoch  1 — train RMSE 647.933455 - train R2 -0.315008 — val RMSE 590.783431 - val R2 -0.106943\n",
    "Epoch  2 — train RMSE 554.334365 - train R2 0.037477 — val RMSE 519.252565 - val R2 0.144882\n",
    "Epoch  3 — train RMSE 526.764650 - train R2 0.130838 — val RMSE 427.802442 - val R2 0.419563\n",
    "Epoch  4 — train RMSE 405.918747 - train R2 0.483886 — val RMSE 242.420577 - val R2 0.813616\n",
    "Epoch  5 — train RMSE 259.992053 - train R2 0.788267 — val RMSE 111.821413 - val R2 0.960343\n",
    "Epoch  6 — train RMSE 225.142789 - train R2 0.841224 — val RMSE 78.034123 - val R2 0.980687\n",
    "Epoch  7 — train RMSE 225.829603 - train R2 0.840254 — val RMSE 87.608671 - val R2 0.975658\n",
    "Epoch  8 — train RMSE 239.649822 - train R2 0.820104 — val RMSE 103.283298 - val R2 0.966168\n",
    "Epoch  9 — train RMSE 218.162494 - train R2 0.850917 — val RMSE 96.721934 - val R2 0.970330\n",
    "Epoch 10 — train RMSE 262.254871 - train R2 0.784566 — val RMSE 204.681310 - val R2 0.867131\n",
    "Epoch 11 — train RMSE 284.733322 - train R2 0.746052 — val RMSE 217.162258 - val R2 0.850432\n",
    "Epoch 12 — train RMSE 273.583936 - train R2 0.765551 — val RMSE 111.706636 - val R2 0.960424\n",
    "Epoch 13 — train RMSE 244.976271 - train R2 0.812018 — val RMSE 132.690155 - val R2 0.944160\n",
    "Epoch 14 — train RMSE 232.132526 - train R2 0.831213 — val RMSE 81.462859 - val R2 0.978953\n",
    "Epoch 15 — train RMSE 186.777658 - train R2 0.890726 — val RMSE 99.534035 - val R2 0.968580\n",
    "Epoch 16 — train RMSE 273.329657 - train R2 0.765986 — val RMSE 74.748253 - val R2 0.982280\n",
    "Epoch 17 — train RMSE 189.396052 - train R2 0.887641 — val RMSE 95.888920 - val R2 0.970839\n",
    "Epoch 18 — train RMSE 274.031313 - train R2 0.764783 — val RMSE 128.773256 - val R2 0.947408\n",
    "Epoch 19 — train RMSE 212.659864 - train R2 0.858343 — val RMSE 59.524765 - val R2 0.988763\n",
    "Epoch 20 — train RMSE 259.917481 - train R2 0.788389 — val RMSE 98.310219 - val R2 0.969347\n",
    "Epoch 21 — train RMSE 222.779508 - train R2 0.844540 — val RMSE 67.700090 - val R2 0.985464\n",
    "Epoch 22 — train RMSE 222.768471 - train R2 0.844556 — val RMSE 65.380988 - val R2 0.986443\n",
    "Epoch 23 — train RMSE 236.554575 - train R2 0.824721 — val RMSE 57.695442 - val R2 0.989443\n",
    "Epoch 24 — train RMSE 225.247918 - train R2 0.841076 — val RMSE 82.387769 - val R2 0.978472\n",
    "Epoch 25 — train RMSE 236.934763 - train R2 0.824157 — val RMSE 67.392820 - val R2 0.985596\n",
    "Epoch 26 — train RMSE 248.071697 - train R2 0.807238 — val RMSE 78.450618 - val R2 0.980481\n",
    "Epoch 27 — train RMSE 210.518964 - train R2 0.861181 — val RMSE 57.891505 - val R2 0.989371\n",
    "Epoch 28 — train RMSE 195.953701 - train R2 0.879725 — val RMSE 89.896251 - val R2 0.974370\n",
    "Epoch 29 — train RMSE 195.274108 - train R2 0.880558 — val RMSE 50.932884 - val R2 0.991773\n",
    "Epoch 30 — train RMSE 222.186458 - train R2 0.845367 — val RMSE 62.777909 - val R2 0.987501\n",
    "Epoch 31 — train RMSE 177.366306 - train R2 0.901461 — val RMSE 58.224725 - val R2 0.989248\n",
    "Epoch 32 — train RMSE 191.663961 - train R2 0.884934 — val RMSE 51.307647 - val R2 0.991651\n",
    "Epoch 33 — train RMSE 140.003503 - train R2 0.938603 — val RMSE 65.591855 - val R2 0.986355\n",
    "Epoch 34 — train RMSE 207.488315 - train R2 0.865149 — val RMSE 58.179779 - val R2 0.989265\n",
    "Epoch 35 — train RMSE 192.416205 - train R2 0.884029 — val RMSE 49.677407 - val R2 0.992173\n",
    "Epoch 36 — train RMSE 246.397685 - train R2 0.809831 — val RMSE 83.766312 - val R2 0.977746\n",
    "Epoch 37 — train RMSE 195.741398 - train R2 0.879986 — val RMSE 64.266039 - val R2 0.986901\n",
    "Epoch 38 — train RMSE 252.020815 - train R2 0.801052 — val RMSE 62.718037 - val R2 0.987525\n",
    "Epoch 39 — train RMSE 260.489098 - train R2 0.787457 — val RMSE 87.761534 - val R2 0.975573\n",
    "Epoch 40 — train RMSE 270.706953 - train R2 0.770456 — val RMSE 73.990626 - val R2 0.982637\n",
    "Epoch 41 — train RMSE 193.077308 - train R2 0.883230 — val RMSE 62.386516 - val R2 0.987656\n",
    "Epoch 42 — train RMSE 222.709220 - train R2 0.844638 — val RMSE 56.211224 - val R2 0.989979\n",
    "Epoch 43 — train RMSE 193.706473 - train R2 0.882468 — val RMSE 68.752004 - val R2 0.985009\n",
    "Epoch 44 — train RMSE 160.370031 - train R2 0.919441 — val RMSE 54.623470 - val R2 0.990537\n",
    "Epoch 45 — train RMSE 233.484939 - train R2 0.829240 — val RMSE 71.820866 - val R2 0.983640\n",
    "Epoch 46 — train RMSE 208.150687 - train R2 0.864286 — val RMSE 66.932094 - val R2 0.985792\n",
    "Epoch 47 — train RMSE 221.307029 - train R2 0.846588 — val RMSE 70.514370 - val R2 0.984230\n",
    "Epoch 48 — train RMSE 193.526990 - train R2 0.882686 — val RMSE 66.622031 - val R2 0.985923\n",
    "Epoch 49 — train RMSE 177.295699 - train R2 0.901539 — val RMSE 73.677913 - val R2 0.982784\n",
    "Epoch 50 — train RMSE 194.113478 - train R2 0.881974 — val RMSE 76.524885 - val R2 0.981427\n",
    "Epoch 51 — train RMSE 234.503027 - train R2 0.827748 — val RMSE 67.803062 - val R2 0.985420\n",
    "Epoch 52 — train RMSE 246.821589 - train R2 0.809176 — val RMSE 68.505124 - val R2 0.985116\n",
    "Epoch 53 — train RMSE 206.986450 - train R2 0.865800 — val RMSE 56.618211 - val R2 0.989833\n",
    "Epoch 54 — train RMSE 257.492518 - train R2 0.792319 — val RMSE 66.517137 - val R2 0.985967\n",
    "Epoch 55 — train RMSE 220.718738 - train R2 0.847403 — val RMSE 67.874026 - val R2 0.985389\n",
    "Epoch 56 — train RMSE 221.465702 - train R2 0.846368 — val RMSE 61.709236 - val R2 0.987923\n",
    "Epoch 57 — train RMSE 233.576072 - train R2 0.829107 — val RMSE 66.964073 - val R2 0.985778\n",
    "Epoch 58 — train RMSE 257.642484 - train R2 0.792077 — val RMSE 58.672171 - val R2 0.989082\n",
    "Epoch 59 — train RMSE 177.827767 - train R2 0.900947 — val RMSE 70.909587 - val R2 0.984053\n",
    "Epoch 60 — train RMSE 176.563579 - train R2 0.902351 — val RMSE 50.410932 - val R2 0.991940\n",
    "Epoch 61 — train RMSE 205.035570 - train R2 0.868318 — val RMSE 62.451023 - val R2 0.987631\n",
    "Epoch 62 — train RMSE 232.925949 - train R2 0.830057 — val RMSE 56.260436 - val R2 0.989961\n",
    "Epoch 63 — train RMSE 257.318634 - train R2 0.792599 — val RMSE 51.317448 - val R2 0.991648\n",
    "Epoch 64 — train RMSE 246.144955 - train R2 0.810220 — val RMSE 57.359755 - val R2 0.989565\n",
    "Epoch 65 — train RMSE 245.645796 - train R2 0.810989 — val RMSE 56.545341 - val R2 0.989859\n",
    "Epoch 66 — train RMSE 246.225224 - train R2 0.810097 — val RMSE 68.738537 - val R2 0.985015\n",
    "Epoch 67 — train RMSE 258.143453 - train R2 0.791268 — val RMSE 74.796053 - val R2 0.982257\n",
    "Epoch 68 — train RMSE 257.183617 - train R2 0.792817 — val RMSE 58.010287 - val R2 0.989327\n",
    "Epoch 69 — train RMSE 176.315585 - train R2 0.902625 — val RMSE 53.946784 - val R2 0.990770\n",
    "Epoch 70 — train RMSE 268.406021 - train R2 0.774341 — val RMSE 47.917516 - val R2 0.992718\n",
    "Epoch 71 — train RMSE 158.687798 - train R2 0.921122 — val RMSE 54.538018 - val R2 0.990567\n",
    "Epoch 72 — train RMSE 207.091296 - train R2 0.865664 — val RMSE 62.547627 - val R2 0.987592\n",
    "Epoch 73 — train RMSE 207.313206 - train R2 0.865376 — val RMSE 63.649774 - val R2 0.987151\n",
    "Epoch 74 — train RMSE 233.869369 - train R2 0.828678 — val RMSE 48.378893 - val R2 0.992577\n",
    "Epoch 75 — train RMSE 207.293031 - train R2 0.865403 — val RMSE 49.218000 - val R2 0.992317\n",
    "Epoch 76 — train RMSE 221.727798 - train R2 0.846005 — val RMSE 64.252048 - val R2 0.986907\n",
    "Epoch 77 — train RMSE 175.968764 - train R2 0.903007 — val RMSE 54.083346 - val R2 0.990723\n",
    "Epoch 78 — train RMSE 206.299465 - train R2 0.866690 — val RMSE 62.655637 - val R2 0.987549\n",
    "Epoch 79 — train RMSE 220.535845 - train R2 0.847656 — val RMSE 52.221089 - val R2 0.991351\n",
    "Epoch 80 — train RMSE 232.751398 - train R2 0.830312 — val RMSE 48.730239 - val R2 0.992469\n",
    "Epoch 81 — train RMSE 191.753990 - train R2 0.884825 — val RMSE 48.250687 - val R2 0.992616\n",
    "Epoch 82 — train RMSE 234.538251 - train R2 0.827696 — val RMSE 51.232930 - val R2 0.991675\n",
    "Epoch 83 — train RMSE 267.974160 - train R2 0.775067 — val RMSE 63.712996 - val R2 0.987126\n",
    "Epoch 84 — train RMSE 289.398406 - train R2 0.737663 — val RMSE 55.629509 - val R2 0.990185\n",
    "Epoch 85 — train RMSE 279.228745 - train R2 0.755776 — val RMSE 43.946445 - val R2 0.993875\n",
    "Epoch 86 — train RMSE 245.837264 - train R2 0.810695 — val RMSE 58.499524 - val R2 0.989146\n",
    "Epoch 87 — train RMSE 244.719813 - train R2 0.812412 — val RMSE 63.712965 - val R2 0.987126\n",
    "Epoch 88 — train RMSE 219.288635 - train R2 0.849374 — val RMSE 60.452224 - val R2 0.988410\n",
    "Epoch 89 — train RMSE 256.419968 - train R2 0.794046 — val RMSE 44.369312 - val R2 0.993756\n",
    "Epoch 90 — train RMSE 191.022641 - train R2 0.885702 — val RMSE 56.342505 - val R2 0.989932\n",
    "Epoch 91 — train RMSE 220.675689 - train R2 0.847463 — val RMSE 66.351615 - val R2 0.986037\n",
    "Epoch 92 — train RMSE 278.484414 - train R2 0.757077 — val RMSE 48.916995 - val R2 0.992411\n",
    "Epoch 93 — train RMSE 259.067124 - train R2 0.789771 — val RMSE 70.129189 - val R2 0.984402\n",
    "Epoch 94 — train RMSE 269.145029 - train R2 0.773097 — val RMSE 59.313521 - val R2 0.988842\n",
    "Epoch 95 — train RMSE 257.527233 - train R2 0.792263 — val RMSE 68.189781 - val R2 0.985253\n",
    "Epoch 96 — train RMSE 233.824828 - train R2 0.828743 — val RMSE 47.819657 - val R2 0.992748\n",
    "Epoch 97 — train RMSE 280.761518 - train R2 0.753088 — val RMSE 45.924158 - val R2 0.993311\n",
    "Epoch 98 — train RMSE 177.906563 - train R2 0.900859 — val RMSE 52.388500 - val R2 0.991296\n",
    "Epoch 99 — train RMSE 322.090981 - train R2 0.675044 — val RMSE 333.826504 - val R2 0.646564\n",
    "Epoch 100 — train RMSE 279.152773 - train R2 0.755909 — val RMSE 166.940327 - val R2 0.911612"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Different config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "configs = Details(\n",
    "    pred_len = 1,      \n",
    "    seq_len  = 350,\n",
    "    d_ff = 32,\n",
    "    patch_len= 16, \n",
    "    stride = 8, \n",
    "    llm_layers = 8, #increased\n",
    "    description = \"This dataset contains near-infrared (NIR) spectroscopy measurements of lactate concentration in a \" \\\n",
    "                  \"phosphate-buffered saline (PBS) solution. Each spectrum includes absorbance values across \" \\\n",
    "                  \"350 wavelengths. \" ,\n",
    "    dropout = 0.1,\n",
    "    n_heads = 2,\n",
    "    d_model = 16,\n",
    "    enc_in = 1,\n",
    "    d_llm = 4096,\n",
    "    llm_model = 'LLAMA',\n",
    "    lr = 0.0001\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "206bb7dec0eb4bd38642904ad4326359",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  1 — train RMSE 698.529635 - train R2 -0.528401 — val RMSE 686.871303 - val R2 -0.496303\n",
      "Epoch  2 — train RMSE 692.385459 - train R2 -0.501631 — val RMSE 678.774838 - val R2 -0.461236\n",
      "Epoch  3 — train RMSE 682.929578 - train R2 -0.460896 — val RMSE 668.664212 - val R2 -0.418029\n",
      "Epoch  4 — train RMSE 670.880472 - train R2 -0.409801 — val RMSE 655.785812 - val R2 -0.363932\n",
      "Epoch  5 — train RMSE 655.931472 - train R2 -0.347673 — val RMSE 642.891199 - val R2 -0.310822\n",
      "Epoch  6 — train RMSE 646.518512 - train R2 -0.309271 — val RMSE 631.273131 - val R2 -0.263873\n",
      "Epoch  7 — train RMSE 638.474735 - train R2 -0.276894 — val RMSE 621.695929 - val R2 -0.225815\n",
      "Epoch  8 — train RMSE 632.362440 - train R2 -0.252563 — val RMSE 614.541979 - val R2 -0.197766\n",
      "Epoch  9 — train RMSE 621.447047 - train R2 -0.209695 — val RMSE 605.453872 - val R2 -0.162602\n",
      "Epoch 10 — train RMSE 617.888078 - train R2 -0.195879 — val RMSE 599.424086 - val R2 -0.139560\n",
      "Epoch 11 — train RMSE 603.538415 - train R2 -0.140978 — val RMSE 591.765381 - val R2 -0.110626\n",
      "Epoch 12 — train RMSE 593.550360 - train R2 -0.103526 — val RMSE 578.312064 - val R2 -0.060702\n",
      "Epoch 13 — train RMSE 581.439491 - train R2 -0.058953 — val RMSE 567.380466 - val R2 -0.020981\n",
      "Epoch 14 — train RMSE 576.377474 - train R2 -0.040594 — val RMSE 551.760945 - val R2 0.034459\n",
      "Epoch 15 — train RMSE 562.727059 - train R2 0.008111 — val RMSE 537.352015 - val R2 0.084230\n",
      "Epoch 16 — train RMSE 543.482707 - train R2 0.074793 — val RMSE 526.034479 - val R2 0.122399\n",
      "Epoch 17 — train RMSE 529.639604 - train R2 0.121325 — val RMSE 507.551891 - val R2 0.182986\n",
      "Epoch 18 — train RMSE 520.125726 - train R2 0.152608 — val RMSE 493.015950 - val R2 0.229113\n",
      "Epoch 19 — train RMSE 488.394966 - train R2 0.252846 — val RMSE 479.139137 - val R2 0.271898\n",
      "Epoch 20 — train RMSE 489.126964 - train R2 0.250605 — val RMSE 467.320428 - val R2 0.307375\n",
      "Epoch 21 — train RMSE 479.597848 - train R2 0.279520 — val RMSE 452.755211 - val R2 0.349877\n",
      "Epoch 22 — train RMSE 489.773250 - train R2 0.248623 — val RMSE 446.827725 - val R2 0.366788\n",
      "Epoch 23 — train RMSE 464.061376 - train R2 0.325443 — val RMSE 430.266206 - val R2 0.412858\n",
      "Epoch 24 — train RMSE 423.592174 - train R2 0.437965 — val RMSE 416.556841 - val R2 0.449677\n",
      "Epoch 25 — train RMSE 442.310763 - train R2 0.387195 — val RMSE 405.785273 - val R2 0.477771\n",
      "Epoch 26 — train RMSE 431.028468 - train R2 0.418058 — val RMSE 397.758740 - val R2 0.498226\n",
      "Epoch 27 — train RMSE 437.354752 - train R2 0.400850 — val RMSE 385.253478 - val R2 0.529281\n",
      "Epoch 28 — train RMSE 440.950678 - train R2 0.390957 — val RMSE 373.627216 - val R2 0.557263\n",
      "Epoch 29 — train RMSE 382.410332 - train R2 0.541935 — val RMSE 371.235790 - val R2 0.562912\n",
      "Epoch 30 — train RMSE 382.677009 - train R2 0.541296 — val RMSE 353.573609 - val R2 0.603513\n",
      "Epoch 31 — train RMSE 407.240852 - train R2 0.480518 — val RMSE 353.266664 - val R2 0.604201\n",
      "Epoch 32 — train RMSE 389.577318 - train R2 0.524605 — val RMSE 334.756867 - val R2 0.644592\n",
      "Epoch 33 — train RMSE 358.273264 - train R2 0.597935 — val RMSE 324.007246 - val R2 0.667051\n",
      "Epoch 34 — train RMSE 349.552901 - train R2 0.617269 — val RMSE 312.817270 - val R2 0.689651\n",
      "Epoch 35 — train RMSE 333.923448 - train R2 0.650730 — val RMSE 302.110137 - val R2 0.710533\n",
      "Epoch 36 — train RMSE 365.889456 - train R2 0.580659 — val RMSE 294.526268 - val R2 0.724883\n",
      "Epoch 37 — train RMSE 328.926524 - train R2 0.661105 — val RMSE 284.158420 - val R2 0.743912\n",
      "Epoch 38 — train RMSE 333.860734 - train R2 0.650861 — val RMSE 282.585703 - val R2 0.746738\n",
      "Epoch 39 — train RMSE 355.921480 - train R2 0.603196 — val RMSE 273.267783 - val R2 0.763165\n",
      "Epoch 40 — train RMSE 320.202632 - train R2 0.678843 — val RMSE 263.589628 - val R2 0.779644\n",
      "Epoch 41 — train RMSE 328.392715 - train R2 0.662204 — val RMSE 251.854342 - val R2 0.798828\n",
      "Epoch 42 — train RMSE 326.036269 - train R2 0.667034 — val RMSE 251.853046 - val R2 0.798830\n",
      "Epoch 43 — train RMSE 312.899385 - train R2 0.693326 — val RMSE 238.605343 - val R2 0.819437\n",
      "Epoch 44 — train RMSE 288.172958 - train R2 0.739880 — val RMSE 233.856167 - val R2 0.826553\n",
      "Epoch 45 — train RMSE 263.166838 - train R2 0.783065 — val RMSE 223.344307 - val R2 0.841796\n",
      "Epoch 46 — train RMSE 287.440632 - train R2 0.741200 — val RMSE 217.309332 - val R2 0.850230\n",
      "Epoch 47 — train RMSE 253.600237 - train R2 0.798550 — val RMSE 214.258414 - val R2 0.854406\n",
      "Epoch 48 — train RMSE 262.110266 - train R2 0.784803 — val RMSE 203.659297 - val R2 0.868454\n",
      "Epoch 49 — train RMSE 277.183031 - train R2 0.759342 — val RMSE 197.414435 - val R2 0.876398\n",
      "Epoch 50 — train RMSE 229.009303 - train R2 0.835724 — val RMSE 193.094971 - val R2 0.881747\n",
      "Epoch 51 — train RMSE 283.744059 - train R2 0.747814 — val RMSE 188.145405 - val R2 0.887732\n",
      "Epoch 52 — train RMSE 267.442151 - train R2 0.775959 — val RMSE 181.255011 - val R2 0.895805\n",
      "Epoch 53 — train RMSE 305.878228 - train R2 0.706935 — val RMSE 177.558045 - val R2 0.900012\n",
      "Epoch 54 — train RMSE 226.813515 - train R2 0.838859 — val RMSE 175.497574 - val R2 0.902319\n",
      "Epoch 55 — train RMSE 237.595898 - train R2 0.823174 — val RMSE 164.891181 - val R2 0.913769\n",
      "Epoch 56 — train RMSE 235.179821 - train R2 0.826752 — val RMSE 160.878457 - val R2 0.917915\n",
      "Epoch 57 — train RMSE 234.616944 - train R2 0.827581 — val RMSE 155.658148 - val R2 0.923156\n",
      "Epoch 58 — train RMSE 275.073296 - train R2 0.762991 — val RMSE 151.482666 - val R2 0.927223\n",
      "Epoch 59 — train RMSE 263.453356 - train R2 0.782592 — val RMSE 148.817668 - val R2 0.929761\n",
      "Epoch 60 — train RMSE 240.090043 - train R2 0.819442 — val RMSE 147.826935 - val R2 0.930693\n",
      "Epoch 61 — train RMSE 213.704742 - train R2 0.856947 — val RMSE 144.082523 - val R2 0.934160\n",
      "Epoch 62 — train RMSE 291.679483 - train R2 0.733511 — val RMSE 137.685479 - val R2 0.939876\n",
      "Epoch 63 — train RMSE 209.404849 - train R2 0.862646 — val RMSE 133.379873 - val R2 0.943578\n",
      "Epoch 64 — train RMSE 221.698163 - train R2 0.846046 — val RMSE 130.708578 - val R2 0.945815\n",
      "Epoch 65 — train RMSE 232.365594 - train R2 0.830874 — val RMSE 127.019587 - val R2 0.948831\n",
      "Epoch 66 — train RMSE 255.768906 - train R2 0.795090 — val RMSE 124.215682 - val R2 0.951065\n",
      "Epoch 67 — train RMSE 190.681615 - train R2 0.886110 — val RMSE 131.155494 - val R2 0.945444\n",
      "Epoch 68 — train RMSE 235.985204 - train R2 0.825564 — val RMSE 140.069504 - val R2 0.937776\n",
      "Epoch 69 — train RMSE 244.443539 - train R2 0.812835 — val RMSE 128.397614 - val R2 0.947714\n",
      "Epoch 70 — train RMSE 215.627159 - train R2 0.854362 — val RMSE 115.942849 - val R2 0.957366\n",
      "Epoch 71 — train RMSE 228.601246 - train R2 0.836309 — val RMSE 113.007384 - val R2 0.959497\n",
      "Epoch 72 — train RMSE 199.313034 - train R2 0.875566 — val RMSE 109.394936 - val R2 0.962045\n",
      "Epoch 73 — train RMSE 238.874390 - train R2 0.821266 — val RMSE 107.347857 - val R2 0.963453\n",
      "Epoch 74 — train RMSE 273.300241 - train R2 0.766037 — val RMSE 111.758821 - val R2 0.960387\n",
      "Epoch 75 — train RMSE 251.941011 - train R2 0.801178 — val RMSE 112.510661 - val R2 0.959853\n",
      "Epoch 76 — train RMSE 213.764384 - train R2 0.856868 — val RMSE 105.079293 - val R2 0.964981\n",
      "Epoch 77 — train RMSE 183.415424 - train R2 0.894625 — val RMSE 102.691347 - val R2 0.966555\n",
      "Epoch 78 — train RMSE 238.156429 - train R2 0.822339 — val RMSE 103.176143 - val R2 0.966238\n",
      "Epoch 79 — train RMSE 181.795658 - train R2 0.896478 — val RMSE 99.219464 - val R2 0.968778\n",
      "Epoch 80 — train RMSE 197.420069 - train R2 0.877918 — val RMSE 103.062862 - val R2 0.966312\n",
      "Epoch 81 — train RMSE 224.754354 - train R2 0.841772 — val RMSE 99.279952 - val R2 0.968740\n",
      "Epoch 82 — train RMSE 249.762903 - train R2 0.804600 — val RMSE 102.319568 - val R2 0.966796\n",
      "Epoch 83 — train RMSE 182.278759 - train R2 0.895927 — val RMSE 97.484351 - val R2 0.969860\n",
      "Epoch 84 — train RMSE 236.555470 - train R2 0.824719 — val RMSE 98.667635 - val R2 0.969124\n",
      "Epoch 85 — train RMSE 211.588294 - train R2 0.859767 — val RMSE 95.421122 - val R2 0.971123\n",
      "Epoch 86 — train RMSE 237.412201 - train R2 0.823448 — val RMSE 96.070574 - val R2 0.970728\n",
      "Epoch 87 — train RMSE 282.317606 - train R2 0.750343 — val RMSE 94.374426 - val R2 0.971753\n",
      "Epoch 88 — train RMSE 210.738318 - train R2 0.860891 — val RMSE 89.992974 - val R2 0.974315\n",
      "Epoch 89 — train RMSE 195.466081 - train R2 0.880323 — val RMSE 96.193845 - val R2 0.970653\n",
      "Epoch 90 — train RMSE 248.514545 - train R2 0.806549 — val RMSE 91.130377 - val R2 0.973661\n",
      "Epoch 91 — train RMSE 224.450177 - train R2 0.842200 — val RMSE 88.771722 - val R2 0.975007\n",
      "Epoch 92 — train RMSE 236.336075 - train R2 0.825045 — val RMSE 88.157898 - val R2 0.975351\n",
      "Epoch 93 — train RMSE 209.416933 - train R2 0.862630 — val RMSE 91.956904 - val R2 0.973181\n",
      "Epoch 94 — train RMSE 162.671551 - train R2 0.917112 — val RMSE 89.297070 - val R2 0.974710\n",
      "Epoch 95 — train RMSE 247.824405 - train R2 0.807622 — val RMSE 88.672077 - val R2 0.975063\n",
      "Epoch 96 — train RMSE 143.542975 - train R2 0.935460 — val RMSE 85.031412 - val R2 0.977069\n",
      "Epoch 97 — train RMSE 281.183405 - train R2 0.752345 — val RMSE 96.141325 - val R2 0.970685\n",
      "Epoch 98 — train RMSE 247.908984 - train R2 0.807491 — val RMSE 83.409006 - val R2 0.977935\n",
      "Epoch 99 — train RMSE 249.343782 - train R2 0.805256 — val RMSE 96.319729 - val R2 0.970576\n",
      "Epoch 100 — train RMSE 248.462567 - train R2 0.806630 — val RMSE 90.773865 - val R2 0.973867\n"
     ]
    }
   ],
   "source": [
    "train, test = model_selection.train_test_split(data, test_size=0.1, random_state=1, stratify= data['Concentration'])\n",
    "\n",
    "train, validation = model_selection.train_test_split(train, test_size=0.1, random_state=1, stratify= train['Concentration'])\n",
    "\n",
    "model = train_model(configs, train, validation, 100, 16, True, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "configs = Details(\n",
    "    pred_len = 1,      \n",
    "    seq_len  = 350,\n",
    "    d_ff = 32,\n",
    "    patch_len= 16, \n",
    "    stride = 8, \n",
    "    llm_layers = 10,#increased\n",
    "    description = \"This dataset contains near-infrared (NIR) spectroscopy measurements of lactate concentration in a \" \\\n",
    "                  \"phosphate-buffered saline (PBS) solution. Each spectrum includes absorbance values across \" \\\n",
    "                  \"350 wavelengths. \" ,\n",
    "    dropout = 0.1,\n",
    "    n_heads = 2,\n",
    "    d_model = 16,\n",
    "    enc_in = 1,\n",
    "    d_llm = 4096,\n",
    "    llm_model = 'LLAMA',\n",
    "    lr = 0.0001\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "350825b8047241a5af6ff95b794bd4e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  1 — train RMSE 696.926833 - train R2 -0.521395 — val RMSE 682.083051 - val R2 -0.475514\n",
      "Epoch  2 — train RMSE 681.566362 - train R2 -0.455070 — val RMSE 663.005962 - val R2 -0.394131\n",
      "Epoch  3 — train RMSE 662.701806 - train R2 -0.375637 — val RMSE 643.593998 - val R2 -0.313690\n",
      "Epoch  4 — train RMSE 642.290885 - train R2 -0.292204 — val RMSE 626.815762 - val R2 -0.246088\n",
      "Epoch  5 — train RMSE 630.708416 - train R2 -0.246019 — val RMSE 613.986805 - val R2 -0.195603\n",
      "Epoch  6 — train RMSE 614.342829 - train R2 -0.182195 — val RMSE 595.250879 - val R2 -0.123748\n",
      "Epoch  7 — train RMSE 597.576276 - train R2 -0.118547 — val RMSE 580.025585 - val R2 -0.066997\n",
      "Epoch  8 — train RMSE 588.930044 - train R2 -0.086413 — val RMSE 569.645723 - val R2 -0.029150\n",
      "Epoch  9 — train RMSE 575.885645 - train R2 -0.038820 — val RMSE 553.499936 - val R2 0.028363\n",
      "Epoch 10 — train RMSE 563.163650 - train R2 0.006572 — val RMSE 550.168064 - val R2 0.040026\n",
      "Epoch 11 — train RMSE 565.061636 - train R2 -0.000136 — val RMSE 538.905916 - val R2 0.078926\n",
      "Epoch 12 — train RMSE 571.866367 - train R2 -0.024369 — val RMSE 528.406988 - val R2 0.114465\n",
      "Epoch 13 — train RMSE 538.039278 - train R2 0.093233 — val RMSE 534.799210 - val R2 0.092910\n",
      "Epoch 14 — train RMSE 546.508327 - train R2 0.064463 — val RMSE 522.387907 - val R2 0.134524\n",
      "Epoch 15 — train RMSE 536.132292 - train R2 0.099650 — val RMSE 510.044430 - val R2 0.174941\n",
      "Epoch 16 — train RMSE 529.572462 - train R2 0.121547 — val RMSE 503.922667 - val R2 0.194628\n",
      "Epoch 17 — train RMSE 502.985325 - train R2 0.207538 — val RMSE 447.664517 - val R2 0.364414\n",
      "Epoch 18 — train RMSE 467.126871 - train R2 0.316502 — val RMSE 427.223074 - val R2 0.421134\n",
      "Epoch 19 — train RMSE 456.791889 - train R2 0.346412 — val RMSE 410.605560 - val R2 0.465290\n",
      "Epoch 20 — train RMSE 445.272774 - train R2 0.378960 — val RMSE 392.480851 - val R2 0.511454\n",
      "Epoch 21 — train RMSE 416.665433 - train R2 0.456196 — val RMSE 364.503102 - val R2 0.578623\n",
      "Epoch 22 — train RMSE 403.953290 - train R2 0.488872 — val RMSE 357.070147 - val R2 0.595633\n",
      "Epoch 23 — train RMSE 373.594728 - train R2 0.562811 — val RMSE 322.428832 - val R2 0.670287\n",
      "Epoch 24 — train RMSE 362.638186 - train R2 0.588078 — val RMSE 322.810131 - val R2 0.669506\n",
      "Epoch 25 — train RMSE 357.276966 - train R2 0.600168 — val RMSE 302.858768 - val R2 0.709096\n",
      "Epoch 26 — train RMSE 401.391949 - train R2 0.495333 — val RMSE 289.411465 - val R2 0.734356\n",
      "Epoch 27 — train RMSE 384.599231 - train R2 0.536676 — val RMSE 340.291109 - val R2 0.632743\n",
      "Epoch 28 — train RMSE 359.175258 - train R2 0.595908 — val RMSE 287.425252 - val R2 0.737990\n",
      "Epoch 29 — train RMSE 366.971018 - train R2 0.578176 — val RMSE 282.603813 - val R2 0.746706\n",
      "Epoch 30 — train RMSE 332.190959 - train R2 0.654345 — val RMSE 262.797071 - val R2 0.780967\n",
      "Epoch 31 — train RMSE 332.088435 - train R2 0.654558 — val RMSE 287.534592 - val R2 0.737790\n",
      "Epoch 32 — train RMSE 312.585826 - train R2 0.693940 — val RMSE 228.501058 - val R2 0.834406\n",
      "Epoch 33 — train RMSE 309.497000 - train R2 0.699959 — val RMSE 207.666507 - val R2 0.863227\n",
      "Epoch 34 — train RMSE 327.825299 - train R2 0.663370 — val RMSE 216.350884 - val R2 0.851548\n",
      "Epoch 35 — train RMSE 267.496015 - train R2 0.775869 — val RMSE 183.275213 - val R2 0.893469\n",
      "Epoch 36 — train RMSE 244.282795 - train R2 0.813081 — val RMSE 165.707108 - val R2 0.912913\n",
      "Epoch 37 — train RMSE 220.867679 - train R2 0.847197 — val RMSE 152.723502 - val R2 0.926026\n",
      "Epoch 38 — train RMSE 172.340856 - train R2 0.906965 — val RMSE 150.043853 - val R2 0.928599\n",
      "Epoch 39 — train RMSE 181.501367 - train R2 0.896812 — val RMSE 134.897544 - val R2 0.942287\n",
      "Epoch 40 — train RMSE 221.181921 - train R2 0.846762 — val RMSE 126.935542 - val R2 0.948898\n",
      "Epoch 41 — train RMSE 245.757358 - train R2 0.810818 — val RMSE 124.043896 - val R2 0.951200\n",
      "Epoch 42 — train RMSE 245.345158 - train R2 0.811452 — val RMSE 118.990642 - val R2 0.955095\n",
      "Epoch 43 — train RMSE 190.504618 - train R2 0.886321 — val RMSE 115.036274 - val R2 0.958030\n",
      "Epoch 44 — train RMSE 232.490976 - train R2 0.830691 — val RMSE 127.270851 - val R2 0.948628\n",
      "Epoch 45 — train RMSE 253.780740 - train R2 0.798263 — val RMSE 109.903101 - val R2 0.961692\n",
      "Epoch 46 — train RMSE 172.748857 - train R2 0.906525 — val RMSE 111.458472 - val R2 0.960600\n",
      "Epoch 47 — train RMSE 254.740158 - train R2 0.796735 — val RMSE 116.948414 - val R2 0.956623\n",
      "Epoch 48 — train RMSE 346.615928 - train R2 0.623674 — val RMSE 257.830232 - val R2 0.789168\n",
      "Epoch 49 — train RMSE 383.195590 - train R2 0.540052 — val RMSE 204.025281 - val R2 0.867981\n",
      "Epoch 50 — train RMSE 297.485065 - train R2 0.722797 — val RMSE 417.431361 - val R2 0.447364\n",
      "Epoch 51 — train RMSE 357.878345 - train R2 0.598821 — val RMSE 193.697522 - val R2 0.881008\n",
      "Epoch 52 — train RMSE 300.801339 - train R2 0.716582 — val RMSE 166.966089 - val R2 0.911585\n",
      "Epoch 53 — train RMSE 197.831162 - train R2 0.877409 — val RMSE 155.930866 - val R2 0.922886\n",
      "Epoch 54 — train RMSE 255.826705 - train R2 0.794997 — val RMSE 134.462358 - val R2 0.942658\n",
      "Epoch 55 — train RMSE 202.492810 - train R2 0.871564 — val RMSE 168.487014 - val R2 0.909967\n",
      "Early stopping triggered; Best val RMSE : 109.903101\n"
     ]
    }
   ],
   "source": [
    "train, test = model_selection.train_test_split(data, test_size=0.1, random_state=1, stratify= data['Concentration'])\n",
    "\n",
    "train, validation = model_selection.train_test_split(train, test_size=0.1, random_state=1, stratify= train['Concentration'])\n",
    "\n",
    "model = train_model(configs, train, validation, 100, 8, True, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a271937be3834ac8a0a7710c470029f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  1 — train RMSE 695.520008 - train R2 -0.515259 — val RMSE 681.391631 - val R2 -0.472524\n",
      "Epoch  2 — train RMSE 681.746886 - train R2 -0.455841 — val RMSE 665.108773 - val R2 -0.402989\n",
      "Epoch  3 — train RMSE 670.796588 - train R2 -0.409449 — val RMSE 650.730449 - val R2 -0.342985\n",
      "Epoch  4 — train RMSE 658.297653 - train R2 -0.357414 — val RMSE 638.381284 - val R2 -0.292496\n",
      "Epoch  5 — train RMSE 642.436627 - train R2 -0.292790 — val RMSE 623.357974 - val R2 -0.232378\n",
      "Epoch  6 — train RMSE 620.918306 - train R2 -0.207637 — val RMSE 605.856786 - val R2 -0.164149\n",
      "Epoch  7 — train RMSE 605.927928 - train R2 -0.150031 — val RMSE 596.618492 - val R2 -0.128918\n",
      "Epoch  8 — train RMSE 592.066726 - train R2 -0.098016 — val RMSE 580.348178 - val R2 -0.068184\n",
      "Epoch  9 — train RMSE 596.830264 - train R2 -0.115756 — val RMSE 569.614158 - val R2 -0.029035\n",
      "Epoch 10 — train RMSE 576.964496 - train R2 -0.042715 — val RMSE 560.564531 - val R2 0.003402\n",
      "Epoch 11 — train RMSE 561.906069 - train R2 0.011003 — val RMSE 556.519822 - val R2 0.017732\n",
      "Epoch 12 — train RMSE 544.365589 - train R2 0.071785 — val RMSE 535.636027 - val R2 0.090069\n",
      "Epoch 13 — train RMSE 564.109810 - train R2 0.003230 — val RMSE 520.091898 - val R2 0.142115\n",
      "Epoch 14 — train RMSE 539.248060 - train R2 0.089154 — val RMSE 498.413847 - val R2 0.212140\n",
      "Epoch 15 — train RMSE 495.517185 - train R2 0.230896 — val RMSE 471.171927 - val R2 0.295911\n",
      "Epoch 16 — train RMSE 475.414360 - train R2 0.292035 — val RMSE 439.276789 - val R2 0.388009\n",
      "Epoch 17 — train RMSE 449.201644 - train R2 0.367952 — val RMSE 423.512756 - val R2 0.431145\n",
      "Epoch 18 — train RMSE 420.776987 - train R2 0.445411 — val RMSE 396.791010 - val R2 0.500664\n",
      "Epoch 19 — train RMSE 414.634928 - train R2 0.461483 — val RMSE 378.924547 - val R2 0.544620\n",
      "Epoch 20 — train RMSE 399.688416 - train R2 0.499608 — val RMSE 364.215483 - val R2 0.579287\n",
      "Epoch 21 — train RMSE 368.675345 - train R2 0.574249 — val RMSE 339.564628 - val R2 0.634309\n",
      "Epoch 22 — train RMSE 312.288504 - train R2 0.694522 — val RMSE 314.956464 - val R2 0.685392\n",
      "Epoch 23 — train RMSE 359.092297 - train R2 0.596094 — val RMSE 298.115789 - val R2 0.718137\n",
      "Epoch 24 — train RMSE 327.722933 - train R2 0.663580 — val RMSE 305.649663 - val R2 0.703710\n",
      "Epoch 25 — train RMSE 312.059846 - train R2 0.694969 — val RMSE 268.167173 - val R2 0.771924\n",
      "Epoch 26 — train RMSE 312.632270 - train R2 0.693849 — val RMSE 264.849034 - val R2 0.777533\n",
      "Epoch 27 — train RMSE 462.380207 - train R2 0.330322 — val RMSE 391.707839 - val R2 0.513376\n",
      "Epoch 28 — train RMSE 349.289491 - train R2 0.617846 — val RMSE 284.062816 - val R2 0.744084\n",
      "Epoch 29 — train RMSE 315.269906 - train R2 0.688662 — val RMSE 242.249502 - val R2 0.813879\n",
      "Epoch 30 — train RMSE 278.995487 - train R2 0.756184 — val RMSE 221.870775 - val R2 0.843876\n",
      "Epoch 31 — train RMSE 251.424246 - train R2 0.801992 — val RMSE 202.555213 - val R2 0.869877\n",
      "Epoch 32 — train RMSE 271.488268 - train R2 0.769129 — val RMSE 201.138028 - val R2 0.871691\n",
      "Epoch 33 — train RMSE 308.062670 - train R2 0.702734 — val RMSE 181.818441 - val R2 0.895156\n",
      "Epoch 34 — train RMSE 264.567735 - train R2 0.780749 — val RMSE 178.635469 - val R2 0.898794\n",
      "Epoch 35 — train RMSE 224.656899 - train R2 0.841909 — val RMSE 168.012438 - val R2 0.910473\n",
      "Epoch 36 — train RMSE 243.075513 - train R2 0.814924 — val RMSE 150.979570 - val R2 0.927706\n",
      "Epoch 37 — train RMSE 250.916837 - train R2 0.802791 — val RMSE 143.228914 - val R2 0.934938\n",
      "Epoch 38 — train RMSE 246.430367 - train R2 0.809780 — val RMSE 133.994083 - val R2 0.943057\n",
      "Epoch 39 — train RMSE 244.808326 - train R2 0.812276 — val RMSE 129.905997 - val R2 0.946479\n",
      "Epoch 40 — train RMSE 232.285517 - train R2 0.830990 — val RMSE 126.707935 - val R2 0.949081\n",
      "Epoch 41 — train RMSE 217.040414 - train R2 0.852447 — val RMSE 121.361193 - val R2 0.953288\n",
      "Epoch 42 — train RMSE 215.814352 - train R2 0.854109 — val RMSE 117.847177 - val R2 0.955954\n",
      "Epoch 43 — train RMSE 201.262077 - train R2 0.873121 — val RMSE 115.160654 - val R2 0.957939\n",
      "Epoch 44 — train RMSE 198.950117 - train R2 0.876019 — val RMSE 108.973995 - val R2 0.962337\n",
      "Epoch 45 — train RMSE 212.454931 - train R2 0.858616 — val RMSE 104.195894 - val R2 0.965567\n",
      "Epoch 46 — train RMSE 312.258331 - train R2 0.694581 — val RMSE 104.952063 - val R2 0.965066\n",
      "Epoch 47 — train RMSE 283.965356 - train R2 0.747420 — val RMSE 101.768007 - val R2 0.967153\n",
      "Epoch 48 — train RMSE 293.100704 - train R2 0.730908 — val RMSE 103.285722 - val R2 0.966166\n",
      "Epoch 49 — train RMSE 283.558868 - train R2 0.748143 — val RMSE 100.433730 - val R2 0.968009\n",
      "Epoch 50 — train RMSE 275.072754 - train R2 0.762992 — val RMSE 101.844068 - val R2 0.967104\n",
      "Epoch 51 — train RMSE 211.649872 - train R2 0.859685 — val RMSE 96.643380 - val R2 0.970378\n",
      "Epoch 52 — train RMSE 211.035542 - train R2 0.860499 — val RMSE 92.110391 - val R2 0.973092\n",
      "Epoch 53 — train RMSE 211.055130 - train R2 0.860473 — val RMSE 103.370285 - val R2 0.966111\n",
      "Epoch 54 — train RMSE 248.778938 - train R2 0.806137 — val RMSE 95.079725 - val R2 0.971329\n",
      "Epoch 55 — train RMSE 195.890757 - train R2 0.879802 — val RMSE 93.453664 - val R2 0.972301\n",
      "Epoch 56 — train RMSE 195.590347 - train R2 0.880171 — val RMSE 90.318642 - val R2 0.974128\n",
      "Epoch 57 — train RMSE 194.953968 - train R2 0.880949 — val RMSE 91.031970 - val R2 0.973718\n",
      "Epoch 58 — train RMSE 222.907225 - train R2 0.844362 — val RMSE 89.926504 - val R2 0.974353\n",
      "Epoch 59 — train RMSE 208.713505 - train R2 0.863551 — val RMSE 88.290042 - val R2 0.975277\n",
      "Epoch 60 — train RMSE 235.596670 - train R2 0.826138 — val RMSE 86.904161 - val R2 0.976048\n",
      "Epoch 61 — train RMSE 194.505350 - train R2 0.881497 — val RMSE 94.399640 - val R2 0.971738\n",
      "Epoch 62 — train RMSE 235.837940 - train R2 0.825781 — val RMSE 101.600732 - val R2 0.967261\n",
      "Epoch 63 — train RMSE 178.999517 - train R2 0.899638 — val RMSE 88.589084 - val R2 0.975110\n",
      "Epoch 64 — train RMSE 235.519561 - train R2 0.826251 — val RMSE 91.341266 - val R2 0.973539\n",
      "Epoch 65 — train RMSE 248.019102 - train R2 0.807319 — val RMSE 89.471496 - val R2 0.974611\n",
      "Epoch 66 — train RMSE 209.868272 - train R2 0.862038 — val RMSE 82.421432 - val R2 0.978455\n",
      "Epoch 67 — train RMSE 234.730079 - train R2 0.827414 — val RMSE 91.224795 - val R2 0.973607\n",
      "Epoch 68 — train RMSE 260.347911 - train R2 0.787687 — val RMSE 79.798044 - val R2 0.979805\n",
      "Epoch 69 — train RMSE 208.353309 - train R2 0.864022 — val RMSE 81.872261 - val R2 0.978741\n",
      "Epoch 70 — train RMSE 258.146839 - train R2 0.791262 — val RMSE 79.533820 - val R2 0.979938\n",
      "Epoch 71 — train RMSE 161.253022 - train R2 0.918551 — val RMSE 102.358053 - val R2 0.966771\n",
      "Epoch 72 — train RMSE 209.983640 - train R2 0.861886 — val RMSE 86.911877 - val R2 0.976043\n",
      "Epoch 73 — train RMSE 162.914690 - train R2 0.916864 — val RMSE 81.987990 - val R2 0.978681\n",
      "Epoch 74 — train RMSE 208.902657 - train R2 0.863304 — val RMSE 76.032211 - val R2 0.981666\n",
      "Epoch 75 — train RMSE 235.307005 - train R2 0.826565 — val RMSE 81.410431 - val R2 0.978980\n",
      "Epoch 76 — train RMSE 221.992114 - train R2 0.845637 — val RMSE 82.874871 - val R2 0.978217\n",
      "Epoch 77 — train RMSE 207.628784 - train R2 0.864966 — val RMSE 81.515758 - val R2 0.978926\n",
      "Epoch 78 — train RMSE 193.276511 - train R2 0.882989 — val RMSE 116.696895 - val R2 0.956810\n",
      "Epoch 79 — train RMSE 259.580728 - train R2 0.788937 — val RMSE 80.087147 - val R2 0.979658\n",
      "Epoch 80 — train RMSE 247.107884 - train R2 0.808733 — val RMSE 81.739749 - val R2 0.978810\n",
      "Epoch 81 — train RMSE 246.540453 - train R2 0.809610 — val RMSE 87.628288 - val R2 0.975647\n",
      "Epoch 82 — train RMSE 194.557425 - train R2 0.881433 — val RMSE 85.893675 - val R2 0.976601\n",
      "Epoch 83 — train RMSE 193.036490 - train R2 0.883280 — val RMSE 69.982358 - val R2 0.984467\n",
      "Epoch 84 — train RMSE 233.379702 - train R2 0.829394 — val RMSE 82.134294 - val R2 0.978605\n",
      "Epoch 85 — train RMSE 279.884272 - train R2 0.754628 — val RMSE 80.200360 - val R2 0.979600\n",
      "Epoch 86 — train RMSE 179.234447 - train R2 0.899374 — val RMSE 78.925936 - val R2 0.980244\n",
      "Epoch 87 — train RMSE 234.825403 - train R2 0.827274 — val RMSE 75.961940 - val R2 0.981700\n",
      "Epoch 88 — train RMSE 258.207489 - train R2 0.791164 — val RMSE 67.440077 - val R2 0.985575\n",
      "Epoch 89 — train RMSE 209.099180 - train R2 0.863047 — val RMSE 83.029934 - val R2 0.978136\n",
      "Epoch 90 — train RMSE 279.935226 - train R2 0.754539 — val RMSE 84.243570 - val R2 0.977492\n",
      "Epoch 91 — train RMSE 299.930218 - train R2 0.718221 — val RMSE 74.511757 - val R2 0.982392\n",
      "Epoch 92 — train RMSE 234.092479 - train R2 0.828351 — val RMSE 96.958831 - val R2 0.970184\n",
      "Epoch 93 — train RMSE 139.767193 - train R2 0.938810 — val RMSE 88.489562 - val R2 0.975166\n",
      "Epoch 94 — train RMSE 246.822539 - train R2 0.809174 — val RMSE 78.910893 - val R2 0.980251\n",
      "Epoch 95 — train RMSE 193.540990 - train R2 0.882669 — val RMSE 81.084000 - val R2 0.979148\n",
      "Epoch 96 — train RMSE 221.270779 - train R2 0.846639 — val RMSE 72.925917 - val R2 0.983133\n",
      "Epoch 97 — train RMSE 207.257508 - train R2 0.865449 — val RMSE 82.550593 - val R2 0.978387\n",
      "Epoch 98 — train RMSE 234.976901 - train R2 0.827051 — val RMSE 67.211503 - val R2 0.985673\n",
      "Epoch 99 — train RMSE 245.529044 - train R2 0.811169 — val RMSE 77.845658 - val R2 0.980781\n",
      "Epoch 100 — train RMSE 207.010465 - train R2 0.865769 — val RMSE 81.957056 - val R2 0.978697\n"
     ]
    }
   ],
   "source": [
    "configs = Details(\n",
    "    pred_len = 1,      \n",
    "    seq_len  = 350,\n",
    "    d_ff = 32,\n",
    "    patch_len= 16, \n",
    "    stride = 8, \n",
    "    llm_layers = 14, #Increased\n",
    "    description = \"This dataset contains near-infrared (NIR) spectroscopy measurements of lactate concentration in a \" \\\n",
    "                  \"phosphate-buffered saline (PBS) solution. Each spectrum includes absorbance values across \" \\\n",
    "                  \"350 wavelengths. \" ,\n",
    "    dropout = 0.1,\n",
    "    n_heads = 4, #Increased\n",
    "    d_model = 16,\n",
    "    enc_in = 1,\n",
    "    d_llm = 4096,\n",
    "    llm_model = 'LLAMA',\n",
    "    lr = 0.0001\n",
    ")\n",
    "\n",
    "train, test = model_selection.train_test_split(data, test_size=0.1, random_state=1, stratify= data['Concentration'])\n",
    "\n",
    "train, validation = model_selection.train_test_split(train, test_size=0.1, random_state=1, stratify= train['Concentration'])\n",
    "\n",
    "model = train_model(configs, train, validation, 100, 8, True, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07c209431ea943228afc841a1836f027",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  1 — train RMSE 648.940741 - train R2 -0.319100 — val RMSE 603.506557 - val R2 -0.155135\n",
      "Epoch  2 — train RMSE 598.371144 - train R2 -0.121525 — val RMSE 522.453272 - val R2 0.134307\n",
      "Epoch  3 — train RMSE 490.634196 - train R2 0.245979 — val RMSE 305.258174 - val R2 0.704469\n",
      "Epoch  4 — train RMSE 362.307965 - train R2 0.588828 — val RMSE 191.363799 - val R2 0.883858\n",
      "Epoch  5 — train RMSE 250.837878 - train R2 0.802915 — val RMSE 205.095075 - val R2 0.866593\n",
      "Epoch  6 — train RMSE 286.282024 - train R2 0.743282 — val RMSE 101.069427 - val R2 0.967603\n",
      "Epoch  7 — train RMSE 274.496863 - train R2 0.763983 — val RMSE 74.170660 - val R2 0.982553\n",
      "Epoch  8 — train RMSE 251.189024 - train R2 0.802363 — val RMSE 74.514280 - val R2 0.982390\n",
      "Epoch  9 — train RMSE 274.315071 - train R2 0.764296 — val RMSE 92.427212 - val R2 0.972906\n",
      "Epoch 10 — train RMSE 251.448012 - train R2 0.801955 — val RMSE 123.983495 - val R2 0.951248\n",
      "Epoch 11 — train RMSE 257.634799 - train R2 0.792089 — val RMSE 84.990294 - val R2 0.977091\n",
      "Epoch 12 — train RMSE 265.806705 - train R2 0.778691 — val RMSE 100.364862 - val R2 0.968053\n",
      "Epoch 13 — train RMSE 266.290663 - train R2 0.777884 — val RMSE 79.738533 - val R2 0.979835\n",
      "Epoch 14 — train RMSE 267.448694 - train R2 0.775948 — val RMSE 86.220257 - val R2 0.976423\n",
      "Epoch 15 — train RMSE 227.269274 - train R2 0.838211 — val RMSE 78.392784 - val R2 0.980510\n",
      "Epoch 16 — train RMSE 214.688775 - train R2 0.855627 — val RMSE 77.986064 - val R2 0.980711\n",
      "Epoch 17 — train RMSE 197.119674 - train R2 0.878290 — val RMSE 82.567613 - val R2 0.978378\n",
      "Early stopping triggered; Best val RMSE : 74.170660\n"
     ]
    }
   ],
   "source": [
    "configs = Details(\n",
    "    pred_len = 1,      \n",
    "    seq_len  = 350,\n",
    "    d_ff = 32,\n",
    "    patch_len= 16, \n",
    "    stride = 8, \n",
    "    llm_layers = 14,\n",
    "    description = \"This dataset contains near-infrared (NIR) spectroscopy measurements of lactate concentration in a \" \\\n",
    "                  \"phosphate-buffered saline (PBS) solution. Each spectrum includes absorbance values across \" \\\n",
    "                  \"350 wavelengths. \" ,\n",
    "    dropout = 0.1,\n",
    "    n_heads = 4,\n",
    "    d_model = 16,\n",
    "    enc_in = 1,\n",
    "    d_llm = 4096,\n",
    "    llm_model = 'LLAMA',\n",
    "    lr = 0.001 #Increased learning rate\n",
    ")\n",
    "\n",
    "train, test = model_selection.train_test_split(data, test_size=0.1, random_state=1, stratify= data['Concentration'])\n",
    "\n",
    "train, validation = model_selection.train_test_split(train, test_size=0.1, random_state=1, stratify= train['Concentration'])\n",
    "\n",
    "model = train_model(configs, train, validation, 100, 8, True, True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Increasing learning rate clearly obtained better results in fewer epochs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "035ea03ed4eb422b8f662c3c4136cb7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  1 — train RMSE 663.442306 - train R2 -0.378713 — val RMSE 603.828049 - val R2 -0.156366\n",
      "Epoch  2 — train RMSE 604.630480 - train R2 -0.145111 — val RMSE 536.161542 - val R2 0.088283\n",
      "Epoch  3 — train RMSE 547.640894 - train R2 0.060581 — val RMSE 494.291262 - val R2 0.225120\n",
      "Epoch  4 — train RMSE 429.893507 - train R2 0.421119 — val RMSE 293.641380 - val R2 0.726534\n",
      "Epoch  5 — train RMSE 358.862853 - train R2 0.596611 — val RMSE 273.390894 - val R2 0.762952\n",
      "Epoch  6 — train RMSE 388.132399 - train R2 0.528125 — val RMSE 169.188288 - val R2 0.909216\n",
      "Epoch  7 — train RMSE 306.860267 - train R2 0.705050 — val RMSE 138.377800 - val R2 0.939270\n",
      "Epoch  8 — train RMSE 446.980617 - train R2 0.374187 — val RMSE 381.827391 - val R2 0.537616\n",
      "Epoch  9 — train RMSE 447.865198 - train R2 0.371707 — val RMSE 309.187620 - val R2 0.696811\n",
      "Epoch 10 — train RMSE 378.523513 - train R2 0.551200 — val RMSE 181.908387 - val R2 0.895052\n",
      "Epoch 11 — train RMSE 329.876814 - train R2 0.659144 — val RMSE 196.847478 - val R2 0.877107\n",
      "Epoch 12 — train RMSE 356.910442 - train R2 0.600988 — val RMSE 126.438350 - val R2 0.949298\n",
      "Epoch 13 — train RMSE 344.093343 - train R2 0.629131 — val RMSE 139.581159 - val R2 0.938209\n",
      "Epoch 14 — train RMSE 365.582841 - train R2 0.581362 — val RMSE 132.010049 - val R2 0.944731\n",
      "Epoch 15 — train RMSE 349.047599 - train R2 0.618375 — val RMSE 130.855658 - val R2 0.945693\n",
      "Epoch 16 — train RMSE 318.871462 - train R2 0.681508 — val RMSE 122.950013 - val R2 0.952057\n",
      "Epoch 17 — train RMSE 346.292849 - train R2 0.624375 — val RMSE 136.488960 - val R2 0.940917\n",
      "Epoch 18 — train RMSE 300.144900 - train R2 0.717818 — val RMSE 133.352220 - val R2 0.943601\n",
      "Epoch 19 — train RMSE 392.532951 - train R2 0.517364 — val RMSE 150.643849 - val R2 0.928027\n",
      "Epoch 20 — train RMSE 345.938358 - train R2 0.625143 — val RMSE 160.136325 - val R2 0.918670\n",
      "Epoch 21 — train RMSE 325.978250 - train R2 0.667153 — val RMSE 126.496932 - val R2 0.949251\n",
      "Epoch 22 — train RMSE 335.173880 - train R2 0.648109 — val RMSE 138.865710 - val R2 0.938841\n",
      "Epoch 23 — train RMSE 342.169963 - train R2 0.633266 — val RMSE 124.135874 - val R2 0.951128\n",
      "Epoch 24 — train RMSE 335.697739 - train R2 0.647009 — val RMSE 135.249715 - val R2 0.941985\n",
      "Epoch 25 — train RMSE 325.444984 - train R2 0.668241 — val RMSE 139.796277 - val R2 0.938019\n",
      "Epoch 26 — train RMSE 333.244598 - train R2 0.652148 — val RMSE 127.419168 - val R2 0.948508\n",
      "Early stopping triggered; Best val RMSE : 122.950013\n"
     ]
    }
   ],
   "source": [
    "configs = Details(\n",
    "    pred_len = 1,      \n",
    "    seq_len  = 350,\n",
    "    d_ff = 32,\n",
    "    patch_len= 16, \n",
    "    stride = 8, \n",
    "    llm_layers = 14,\n",
    "    description = \"This dataset contains near-infrared (NIR) spectroscopy measurements of lactate concentration in a \" \\\n",
    "                  \"phosphate-buffered saline (PBS) solution. Each spectrum includes absorbance values across \" \\\n",
    "                  \"350 wavelengths. \" ,\n",
    "    dropout = 0.2, #Increased dropout\n",
    "    n_heads = 4,\n",
    "    d_model = 16,\n",
    "    enc_in = 1,\n",
    "    d_llm = 4096,\n",
    "    llm_model = 'LLAMA',\n",
    "    lr = 0.001 \n",
    ")\n",
    "\n",
    "train, test = model_selection.train_test_split(data, test_size=0.1, random_state=1, stratify= data['Concentration'])\n",
    "\n",
    "train, validation = model_selection.train_test_split(train, test_size=0.1, random_state=1, stratify= train['Concentration'])\n",
    "\n",
    "model = train_model(configs, train, validation, 100, 8, True, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "915c07b40e764d2ba6c8fecedde531e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  1 — train RMSE 695.241510 - train R2 -0.514045 — val RMSE 681.282941 - val R2 -0.472054\n",
      "Epoch  2 — train RMSE 682.048649 - train R2 -0.457129 — val RMSE 664.908523 - val R2 -0.402144\n",
      "Epoch  3 — train RMSE 669.103661 - train R2 -0.402343 — val RMSE 651.421624 - val R2 -0.345839\n",
      "Epoch  4 — train RMSE 656.024611 - train R2 -0.348056 — val RMSE 637.376624 - val R2 -0.288431\n",
      "Epoch  5 — train RMSE 629.852475 - train R2 -0.242639 — val RMSE 616.851317 - val R2 -0.206785\n",
      "Epoch  6 — train RMSE 627.137422 - train R2 -0.231949 — val RMSE 602.216972 - val R2 -0.150204\n",
      "Epoch  7 — train RMSE 606.554046 - train R2 -0.152408 — val RMSE 587.133383 - val R2 -0.093308\n",
      "Epoch  8 — train RMSE 594.341260 - train R2 -0.106469 — val RMSE 571.486364 - val R2 -0.035811\n",
      "Epoch  9 — train RMSE 635.433086 - train R2 -0.264757 — val RMSE 594.018459 - val R2 -0.119099\n",
      "Epoch 10 — train RMSE 608.526889 - train R2 -0.159917 — val RMSE 566.643992 - val R2 -0.018332\n",
      "Epoch 11 — train RMSE 569.939752 - train R2 -0.017479 — val RMSE 551.769908 - val R2 0.034428\n",
      "Epoch 12 — train RMSE 583.950614 - train R2 -0.068120 — val RMSE 546.610600 - val R2 0.052400\n",
      "Epoch 13 — train RMSE 563.481055 - train R2 0.005451 — val RMSE 547.911204 - val R2 0.047886\n",
      "Epoch 14 — train RMSE 570.018397 - train R2 -0.017760 — val RMSE 534.273305 - val R2 0.094693\n",
      "Epoch 15 — train RMSE 578.071839 - train R2 -0.046722 — val RMSE 523.148797 - val R2 0.132001\n",
      "Epoch 16 — train RMSE 525.583215 - train R2 0.134733 — val RMSE 511.967879 - val R2 0.168707\n",
      "Epoch 17 — train RMSE 508.585610 - train R2 0.189793 — val RMSE 458.226131 - val R2 0.334070\n",
      "Epoch 18 — train RMSE 472.364582 - train R2 0.301089 — val RMSE 445.976242 - val R2 0.369199\n",
      "Epoch 19 — train RMSE 436.962694 - train R2 0.401924 — val RMSE 421.849578 - val R2 0.435604\n",
      "Epoch 20 — train RMSE 442.836532 - train R2 0.385737 — val RMSE 391.207441 - val R2 0.514619\n",
      "Epoch 21 — train RMSE 428.140977 - train R2 0.425829 — val RMSE 375.419999 - val R2 0.553004\n",
      "Epoch 22 — train RMSE 405.990480 - train R2 0.483703 — val RMSE 378.133693 - val R2 0.546519\n",
      "Epoch 23 — train RMSE 364.954510 - train R2 0.582799 — val RMSE 332.350140 - val R2 0.649684\n",
      "Epoch 24 — train RMSE 438.563830 - train R2 0.397533 — val RMSE 318.988832 - val R2 0.677285\n",
      "Epoch 25 — train RMSE 357.359772 - train R2 0.599983 — val RMSE 302.382606 - val R2 0.710010\n",
      "Epoch 26 — train RMSE 376.571554 - train R2 0.555817 — val RMSE 289.318930 - val R2 0.734526\n",
      "Epoch 27 — train RMSE 370.963174 - train R2 0.568949 — val RMSE 280.300251 - val R2 0.750818\n",
      "Epoch 28 — train RMSE 419.455027 - train R2 0.448890 — val RMSE 263.679930 - val R2 0.779493\n",
      "Epoch 29 — train RMSE 409.137773 - train R2 0.475668 — val RMSE 249.893221 - val R2 0.801949\n",
      "Epoch 30 — train RMSE 336.721452 - train R2 0.644852 — val RMSE 239.603923 - val R2 0.817922\n",
      "Epoch 31 — train RMSE 324.859085 - train R2 0.669434 — val RMSE 229.824447 - val R2 0.832482\n",
      "Epoch 32 — train RMSE 345.889103 - train R2 0.625250 — val RMSE 218.975640 - val R2 0.847924\n",
      "Epoch 33 — train RMSE 342.502182 - train R2 0.632553 — val RMSE 211.958011 - val R2 0.857515\n",
      "Epoch 34 — train RMSE 305.822842 - train R2 0.707041 — val RMSE 203.803037 - val R2 0.868268\n",
      "Epoch 35 — train RMSE 357.997068 - train R2 0.598555 — val RMSE 206.222844 - val R2 0.865122\n",
      "Epoch 36 — train RMSE 300.858095 - train R2 0.716475 — val RMSE 189.663482 - val R2 0.885913\n",
      "Epoch 37 — train RMSE 308.623094 - train R2 0.701651 — val RMSE 185.856471 - val R2 0.890447\n",
      "Epoch 38 — train RMSE 316.272350 - train R2 0.686679 — val RMSE 178.595900 - val R2 0.898839\n",
      "Epoch 39 — train RMSE 324.592518 - train R2 0.669977 — val RMSE 171.595911 - val R2 0.906614\n",
      "Epoch 40 — train RMSE 306.025123 - train R2 0.706653 — val RMSE 170.794389 - val R2 0.907484\n",
      "Epoch 41 — train RMSE 299.497892 - train R2 0.719033 — val RMSE 479.351533 - val R2 0.271253\n",
      "Epoch 42 — train RMSE 393.599013 - train R2 0.514739 — val RMSE 208.509409 - val R2 0.862114\n",
      "Epoch 43 — train RMSE 317.932606 - train R2 0.683380 — val RMSE 176.559079 - val R2 0.901134\n",
      "Epoch 44 — train RMSE 400.578904 - train R2 0.497375 — val RMSE 299.573580 - val R2 0.715373\n",
      "Epoch 45 — train RMSE 341.524372 - train R2 0.634648 — val RMSE 582.213132 - val R2 -0.075060\n",
      "Epoch 46 — train RMSE 478.206639 - train R2 0.283694 — val RMSE 588.298075 - val R2 -0.097649\n",
      "Epoch 47 — train RMSE 531.381747 - train R2 0.115535 — val RMSE 368.076557 - val R2 0.570320\n",
      "Epoch 48 — train RMSE 354.938775 - train R2 0.605384 — val RMSE 223.399632 - val R2 0.841717\n",
      "Epoch 49 — train RMSE 262.251656 - train R2 0.784571 — val RMSE 179.858959 - val R2 0.897403\n",
      "Epoch 50 — train RMSE 333.388240 - train R2 0.651849 — val RMSE 165.548544 - val R2 0.913080\n",
      "Epoch 51 — train RMSE 332.764224 - train R2 0.653151 — val RMSE 159.746286 - val R2 0.919066\n",
      "Epoch 52 — train RMSE 395.123166 - train R2 0.510973 — val RMSE 158.474110 - val R2 0.920350\n",
      "Epoch 53 — train RMSE 519.629795 - train R2 0.154223 — val RMSE 519.031512 - val R2 0.145610\n",
      "Epoch 54 — train RMSE 448.077961 - train R2 0.371110 — val RMSE 305.493236 - val R2 0.704013\n",
      "Epoch 55 — train RMSE 372.639491 - train R2 0.565044 — val RMSE 186.511662 - val R2 0.889673\n",
      "Epoch 56 — train RMSE 350.231305 - train R2 0.615782 — val RMSE 164.577865 - val R2 0.914096\n",
      "Epoch 57 — train RMSE 252.361094 - train R2 0.800514 — val RMSE 156.953083 - val R2 0.921872\n",
      "Epoch 58 — train RMSE 294.104972 - train R2 0.729061 — val RMSE 150.963733 - val R2 0.927721\n",
      "Epoch 59 — train RMSE 356.268705 - train R2 0.602421 — val RMSE 153.029324 - val R2 0.925729\n",
      "Epoch 60 — train RMSE 262.931405 - train R2 0.783453 — val RMSE 152.418432 - val R2 0.926321\n",
      "Epoch 61 — train RMSE 338.938454 - train R2 0.640160 — val RMSE 147.619786 - val R2 0.930887\n",
      "Epoch 62 — train RMSE 355.798122 - train R2 0.603471 — val RMSE 144.992942 - val R2 0.933325\n",
      "Epoch 63 — train RMSE 272.869827 - train R2 0.766773 — val RMSE 142.542469 - val R2 0.935560\n",
      "Epoch 64 — train RMSE 329.952210 - train R2 0.658988 — val RMSE 146.672963 - val R2 0.931771\n",
      "Epoch 65 — train RMSE 271.176495 - train R2 0.769659 — val RMSE 141.835111 - val R2 0.936198\n",
      "Epoch 66 — train RMSE 346.971844 - train R2 0.622900 — val RMSE 145.707179 - val R2 0.932667\n",
      "Epoch 67 — train RMSE 292.734263 - train R2 0.731580 — val RMSE 146.569218 - val R2 0.931867\n",
      "Epoch 68 — train RMSE 371.870219 - train R2 0.566838 — val RMSE 141.369617 - val R2 0.936616\n",
      "Epoch 69 — train RMSE 364.030856 - train R2 0.584908 — val RMSE 145.050109 - val R2 0.933272\n",
      "Epoch 70 — train RMSE 329.776546 - train R2 0.659351 — val RMSE 139.456624 - val R2 0.938320\n",
      "Epoch 71 — train RMSE 284.963306 - train R2 0.745642 — val RMSE 149.794100 - val R2 0.928836\n",
      "Epoch 72 — train RMSE 483.478364 - train R2 0.267814 — val RMSE 442.189854 - val R2 0.379865\n",
      "Epoch 73 — train RMSE 450.509082 - train R2 0.364267 — val RMSE 361.581015 - val R2 0.585352\n",
      "Epoch 74 — train RMSE 507.299709 - train R2 0.193886 — val RMSE 358.988909 - val R2 0.591275\n",
      "Epoch 75 — train RMSE 360.467085 - train R2 0.592996 — val RMSE 194.507322 - val R2 0.880011\n",
      "Epoch 76 — train RMSE 289.624264 - train R2 0.737253 — val RMSE 162.468083 - val R2 0.916285\n",
      "Epoch 77 — train RMSE 357.126372 - train R2 0.600505 — val RMSE 150.911583 - val R2 0.927771\n",
      "Epoch 78 — train RMSE 302.543951 - train R2 0.713289 — val RMSE 148.033712 - val R2 0.930499\n",
      "Epoch 79 — train RMSE 293.861288 - train R2 0.729509 — val RMSE 143.613654 - val R2 0.934588\n",
      "Epoch 80 — train RMSE 293.307086 - train R2 0.730529 — val RMSE 147.369373 - val R2 0.931122\n",
      "Early stopping triggered; Best val RMSE : 139.456624\n"
     ]
    }
   ],
   "source": [
    "configs = Details(\n",
    "    pred_len = 1,      \n",
    "    seq_len  = 350,\n",
    "    d_ff = 32,\n",
    "    patch_len= 16, \n",
    "    stride = 8, \n",
    "    llm_layers = 14,\n",
    "    description = \"This dataset contains near-infrared (NIR) spectroscopy measurements of lactate concentration in a \" \\\n",
    "                  \"phosphate-buffered saline (PBS) solution. Each spectrum includes absorbance values across \" \\\n",
    "                  \"350 wavelengths. \" ,\n",
    "    dropout = 0.2, #Increased dropout\n",
    "    n_heads = 4,\n",
    "    d_model = 16,\n",
    "    enc_in = 1,\n",
    "    d_llm = 4096,\n",
    "    llm_model = 'LLAMA',\n",
    "    lr = 0.0001 #Decreased learning rate\n",
    ")\n",
    "\n",
    "train, test = model_selection.train_test_split(data, test_size=0.1, random_state=1, stratify= data['Concentration'])\n",
    "\n",
    "train, validation = model_selection.train_test_split(train, test_size=0.1, random_state=1, stratify= train['Concentration'])\n",
    "\n",
    "model = train_model(configs, train, validation, 100, 8, True, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "606bf1ba182b4c468f9ce39684a13782",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  1 — train RMSE 700.984634 - train R2 -0.539163 — val RMSE 692.355332 - val R2 -0.520291\n",
      "Epoch  2 — train RMSE 700.543003 - train R2 -0.537224 — val RMSE 692.239216 - val R2 -0.519782\n",
      "Epoch  3 — train RMSE 700.280079 - train R2 -0.536070 — val RMSE 691.816190 - val R2 -0.517925\n",
      "Epoch  4 — train RMSE 699.353373 - train R2 -0.532007 — val RMSE 689.836818 - val R2 -0.509251\n",
      "Epoch  5 — train RMSE 697.163112 - train R2 -0.522426 — val RMSE 687.047584 - val R2 -0.497071\n",
      "Epoch  6 — train RMSE 693.969090 - train R2 -0.508509 — val RMSE 684.210323 - val R2 -0.484732\n",
      "Epoch  7 — train RMSE 691.219594 - train R2 -0.496579 — val RMSE 681.355697 - val R2 -0.472369\n",
      "Epoch  8 — train RMSE 687.219660 - train R2 -0.479308 — val RMSE 678.003964 - val R2 -0.457919\n",
      "Epoch  9 — train RMSE 684.720456 - train R2 -0.468568 — val RMSE 674.930425 - val R2 -0.444730\n",
      "Epoch 10 — train RMSE 681.517139 - train R2 -0.454860 — val RMSE 672.168273 - val R2 -0.432930\n",
      "Epoch 11 — train RMSE 680.182300 - train R2 -0.449166 — val RMSE 669.549610 - val R2 -0.421786\n",
      "Epoch 12 — train RMSE 675.639468 - train R2 -0.429873 — val RMSE 666.839344 - val R2 -0.410299\n",
      "Epoch 13 — train RMSE 674.755056 - train R2 -0.426132 — val RMSE 664.392039 - val R2 -0.399966\n",
      "Epoch 14 — train RMSE 671.923670 - train R2 -0.414189 — val RMSE 661.910575 - val R2 -0.389529\n",
      "Epoch 15 — train RMSE 668.009216 - train R2 -0.397759 — val RMSE 659.710081 - val R2 -0.380305\n",
      "Epoch 16 — train RMSE 664.160123 - train R2 -0.381698 — val RMSE 657.585676 - val R2 -0.371430\n",
      "Epoch 17 — train RMSE 665.947736 - train R2 -0.389146 — val RMSE 655.711505 - val R2 -0.363623\n",
      "Epoch 18 — train RMSE 663.339541 - train R2 -0.378286 — val RMSE 653.623249 - val R2 -0.354952\n",
      "Epoch 19 — train RMSE 661.357976 - train R2 -0.370064 — val RMSE 652.924933 - val R2 -0.352058\n",
      "Epoch 20 — train RMSE 661.246753 - train R2 -0.369603 — val RMSE 650.406508 - val R2 -0.341648\n",
      "Epoch 21 — train RMSE 657.143948 - train R2 -0.352660 — val RMSE 647.879968 - val R2 -0.331245\n",
      "Epoch 22 — train RMSE 655.721294 - train R2 -0.346809 — val RMSE 647.369913 - val R2 -0.329149\n",
      "Epoch 23 — train RMSE 655.459287 - train R2 -0.345734 — val RMSE 644.030776 - val R2 -0.315473\n",
      "Epoch 24 — train RMSE 656.124842 - train R2 -0.348467 — val RMSE 643.914439 - val R2 -0.314998\n",
      "Epoch 25 — train RMSE 653.725869 - train R2 -0.338624 — val RMSE 641.383514 - val R2 -0.304681\n",
      "Epoch 26 — train RMSE 647.486231 - train R2 -0.313193 — val RMSE 638.938518 - val R2 -0.294753\n",
      "Epoch 27 — train RMSE 645.617954 - train R2 -0.305626 — val RMSE 639.996332 - val R2 -0.299044\n",
      "Epoch 28 — train RMSE 647.540496 - train R2 -0.313413 — val RMSE 637.182666 - val R2 -0.287647\n",
      "Epoch 29 — train RMSE 644.460759 - train R2 -0.300949 — val RMSE 634.553362 - val R2 -0.277042\n",
      "Epoch 30 — train RMSE 641.968709 - train R2 -0.290908 — val RMSE 632.171135 - val R2 -0.267471\n",
      "Epoch 31 — train RMSE 640.649878 - train R2 -0.285609 — val RMSE 630.604831 - val R2 -0.261198\n",
      "Epoch 32 — train RMSE 636.814469 - train R2 -0.270262 — val RMSE 628.743272 - val R2 -0.253763\n",
      "Epoch 33 — train RMSE 641.416193 - train R2 -0.288686 — val RMSE 627.577429 - val R2 -0.249118\n",
      "Epoch 34 — train RMSE 635.286710 - train R2 -0.264174 — val RMSE 625.547198 - val R2 -0.241049\n",
      "Epoch 35 — train RMSE 635.505503 - train R2 -0.265046 — val RMSE 624.572554 - val R2 -0.237185\n",
      "Epoch 36 — train RMSE 634.291502 - train R2 -0.260216 — val RMSE 621.922467 - val R2 -0.226708\n",
      "Epoch 37 — train RMSE 632.351046 - train R2 -0.252518 — val RMSE 620.120471 - val R2 -0.219610\n",
      "Epoch 38 — train RMSE 628.709799 - train R2 -0.238135 — val RMSE 618.539649 - val R2 -0.213400\n",
      "Epoch 39 — train RMSE 630.703153 - train R2 -0.245999 — val RMSE 617.132153 - val R2 -0.207884\n",
      "Epoch 40 — train RMSE 626.520610 - train R2 -0.229527 — val RMSE 615.169813 - val R2 -0.200214\n",
      "Epoch 41 — train RMSE 620.053247 - train R2 -0.204274 — val RMSE 614.489200 - val R2 -0.197560\n",
      "Epoch 42 — train RMSE 618.976826 - train R2 -0.200097 — val RMSE 612.301754 - val R2 -0.189049\n",
      "Epoch 43 — train RMSE 621.016326 - train R2 -0.208018 — val RMSE 609.781896 - val R2 -0.179282\n",
      "Epoch 44 — train RMSE 619.196829 - train R2 -0.200951 — val RMSE 612.171934 - val R2 -0.188545\n",
      "Epoch 45 — train RMSE 622.803881 - train R2 -0.214983 — val RMSE 605.842653 - val R2 -0.164095\n",
      "Epoch 46 — train RMSE 617.772070 - train R2 -0.195430 — val RMSE 604.054911 - val R2 -0.157235\n",
      "Epoch 47 — train RMSE 609.397061 - train R2 -0.163237 — val RMSE 603.837578 - val R2 -0.156403\n",
      "Epoch 48 — train RMSE 612.541709 - train R2 -0.175273 — val RMSE 600.558041 - val R2 -0.143876\n",
      "Epoch 49 — train RMSE 613.489360 - train R2 -0.178913 — val RMSE 598.622475 - val R2 -0.136514\n",
      "Epoch 50 — train RMSE 609.780270 - train R2 -0.164700 — val RMSE 597.524555 - val R2 -0.132349\n",
      "Epoch 51 — train RMSE 608.561971 - train R2 -0.160051 — val RMSE 594.156545 - val R2 -0.119620\n",
      "Epoch 52 — train RMSE 602.714345 - train R2 -0.137865 — val RMSE 591.634395 - val R2 -0.110135\n",
      "Epoch 53 — train RMSE 598.877078 - train R2 -0.123422 — val RMSE 590.163510 - val R2 -0.104622\n",
      "Epoch 54 — train RMSE 609.032686 - train R2 -0.161847 — val RMSE 588.282823 - val R2 -0.097592\n",
      "Epoch 55 — train RMSE 599.167736 - train R2 -0.124513 — val RMSE 588.188918 - val R2 -0.097242\n",
      "Epoch 56 — train RMSE 594.834814 - train R2 -0.108307 — val RMSE 584.757128 - val R2 -0.084476\n",
      "Epoch 57 — train RMSE 590.942426 - train R2 -0.093850 — val RMSE 585.468542 - val R2 -0.087116\n",
      "Epoch 58 — train RMSE 594.874252 - train R2 -0.108454 — val RMSE 580.987111 - val R2 -0.070537\n",
      "Epoch 59 — train RMSE 588.959991 - train R2 -0.086523 — val RMSE 578.875843 - val R2 -0.062771\n",
      "Epoch 60 — train RMSE 582.975678 - train R2 -0.064556 — val RMSE 576.058699 - val R2 -0.052452\n",
      "Epoch 61 — train RMSE 592.340449 - train R2 -0.099031 — val RMSE 574.637066 - val R2 -0.047264\n",
      "Epoch 62 — train RMSE 585.194677 - train R2 -0.072675 — val RMSE 585.204598 - val R2 -0.086136\n",
      "Epoch 63 — train RMSE 583.684545 - train R2 -0.067146 — val RMSE 571.310986 - val R2 -0.035175\n",
      "Epoch 64 — train RMSE 581.995728 - train R2 -0.060980 — val RMSE 569.718070 - val R2 -0.029411\n",
      "Epoch 65 — train RMSE 572.664593 - train R2 -0.027231 — val RMSE 571.720061 - val R2 -0.036658\n",
      "Epoch 66 — train RMSE 575.139446 - train R2 -0.036129 — val RMSE 578.173694 - val R2 -0.060194\n",
      "Epoch 67 — train RMSE 574.685632 - train R2 -0.034495 — val RMSE 568.018039 - val R2 -0.023277\n",
      "Epoch 68 — train RMSE 572.973931 - train R2 -0.028341 — val RMSE 566.003195 - val R2 -0.016030\n",
      "Epoch 69 — train RMSE 567.779565 - train R2 -0.009781 — val RMSE 559.037063 - val R2 0.008826\n",
      "Epoch 70 — train RMSE 571.543630 - train R2 -0.023214 — val RMSE 556.407801 - val R2 0.018127\n",
      "Epoch 71 — train RMSE 567.468177 - train R2 -0.008673 — val RMSE 555.676727 - val R2 0.020706\n",
      "Epoch 72 — train RMSE 574.062704 - train R2 -0.032253 — val RMSE 553.826088 - val R2 0.027218\n",
      "Epoch 73 — train RMSE 566.436837 - train R2 -0.005010 — val RMSE 553.621070 - val R2 0.027938\n",
      "Epoch 74 — train RMSE 570.100510 - train R2 -0.018053 — val RMSE 550.578155 - val R2 0.038594\n",
      "Epoch 75 — train RMSE 550.520795 - train R2 0.050674 — val RMSE 548.030087 - val R2 0.047472\n",
      "Epoch 76 — train RMSE 559.189760 - train R2 0.020542 — val RMSE 546.688154 - val R2 0.052131\n",
      "Epoch 77 — train RMSE 563.842602 - train R2 0.004174 — val RMSE 546.343878 - val R2 0.053325\n",
      "Epoch 78 — train RMSE 554.387688 - train R2 0.037292 — val RMSE 542.889081 - val R2 0.065260\n",
      "Epoch 79 — train RMSE 543.708451 - train R2 0.074024 — val RMSE 540.982739 - val R2 0.071813\n",
      "Epoch 80 — train RMSE 553.448054 - train R2 0.040553 — val RMSE 537.292049 - val R2 0.084434\n",
      "Epoch 81 — train RMSE 549.408845 - train R2 0.054506 — val RMSE 540.929499 - val R2 0.071995\n",
      "Epoch 82 — train RMSE 549.654886 - train R2 0.053659 — val RMSE 535.152647 - val R2 0.091711\n",
      "Epoch 83 — train RMSE 543.855953 - train R2 0.073521 — val RMSE 532.496992 - val R2 0.100703\n",
      "Epoch 84 — train RMSE 554.199709 - train R2 0.037944 — val RMSE 530.709637 - val R2 0.106730\n",
      "Epoch 85 — train RMSE 554.369541 - train R2 0.037355 — val RMSE 528.349852 - val R2 0.114656\n",
      "Epoch 86 — train RMSE 544.749124 - train R2 0.070476 — val RMSE 528.122359 - val R2 0.115418\n",
      "Epoch 87 — train RMSE 544.386997 - train R2 0.071711 — val RMSE 524.932410 - val R2 0.126072\n",
      "Epoch 88 — train RMSE 524.813315 - train R2 0.137265 — val RMSE 523.676789 - val R2 0.130248\n",
      "Epoch 89 — train RMSE 535.598368 - train R2 0.101442 — val RMSE 521.439714 - val R2 0.137663\n",
      "Epoch 90 — train RMSE 533.671624 - train R2 0.107896 — val RMSE 521.544113 - val R2 0.137318\n",
      "Epoch 91 — train RMSE 536.224444 - train R2 0.099340 — val RMSE 522.438764 - val R2 0.134356\n",
      "Epoch 92 — train RMSE 527.987274 - train R2 0.126799 — val RMSE 516.474229 - val R2 0.154008\n",
      "Epoch 93 — train RMSE 536.213823 - train R2 0.099376 — val RMSE 522.123212 - val R2 0.135401\n",
      "Epoch 94 — train RMSE 541.802295 - train R2 0.080505 — val RMSE 513.051411 - val R2 0.165184\n",
      "Epoch 95 — train RMSE 528.145463 - train R2 0.126276 — val RMSE 510.389389 - val R2 0.173825\n",
      "Epoch 96 — train RMSE 511.877658 - train R2 0.179271 — val RMSE 508.646930 - val R2 0.179456\n",
      "Epoch 97 — train RMSE 511.113614 - train R2 0.181719 — val RMSE 506.632776 - val R2 0.185942\n",
      "Epoch 98 — train RMSE 516.440891 - train R2 0.164573 — val RMSE 504.796912 - val R2 0.191831\n",
      "Epoch 99 — train RMSE 531.655841 - train R2 0.114622 — val RMSE 505.647311 - val R2 0.189106\n",
      "Epoch 100 — train RMSE 516.799891 - train R2 0.163410 — val RMSE 508.709131 - val R2 0.179256\n"
     ]
    }
   ],
   "source": [
    "\n",
    "configs = Details(\n",
    "    pred_len = 1,      \n",
    "    seq_len  = 350,\n",
    "    d_ff = 32,\n",
    "    patch_len= 16, \n",
    "    stride = 8, \n",
    "    llm_layers = 14, \n",
    "    description = \"This dataset contains near-infrared (NIR) spectroscopy measurements of lactate concentration in a \" \\\n",
    "                  \"phosphate-buffered saline (PBS) solution. Each spectrum includes absorbance values across \" \\\n",
    "                  \"350 wavelengths. \" ,\n",
    "    dropout = 0.1, \n",
    "    n_heads = 8,#increased heads\n",
    "    d_model = 16,\n",
    "    enc_in = 1,\n",
    "    d_llm = 4096,\n",
    "    llm_model = 'LLAMA',\n",
    "    lr = 0.00001 #Decreased learning rate\n",
    ")\n",
    "\n",
    "train, test = model_selection.train_test_split(data, test_size=0.1, random_state=1, stratify= data['Concentration'])\n",
    "\n",
    "train, validation = model_selection.train_test_split(train, test_size=0.1, random_state=1, stratify= train['Concentration'])\n",
    "\n",
    "model = train_model(configs, train, validation, 100, 8, True, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "New config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36253cf6e10f4dc18baa2f466954b69e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  1 — train RMSE 695.011203 - train R2 -0.513043 — val RMSE 679.084534 - val R2 -0.462569\n",
      "Epoch  2 — train RMSE 679.398365 - train R2 -0.445828 — val RMSE 661.937810 - val R2 -0.389643\n",
      "Epoch  3 — train RMSE 663.941727 - train R2 -0.380789 — val RMSE 648.913039 - val R2 -0.335494\n",
      "Epoch  4 — train RMSE 648.446455 - train R2 -0.317091 — val RMSE 631.543682 - val R2 -0.264957\n",
      "Epoch  5 — train RMSE 627.364303 - train R2 -0.232841 — val RMSE 612.729657 - val R2 -0.190712\n",
      "Epoch  6 — train RMSE 621.612436 - train R2 -0.210339 — val RMSE 600.627387 - val R2 -0.144140\n",
      "Epoch  7 — train RMSE 607.563974 - train R2 -0.156250 — val RMSE 580.470371 - val R2 -0.068634\n",
      "Epoch  8 — train RMSE 580.058141 - train R2 -0.053927 — val RMSE 569.208627 - val R2 -0.027571\n",
      "Epoch  9 — train RMSE 572.676793 - train R2 -0.027275 — val RMSE 538.092787 - val R2 0.081703\n",
      "Epoch 10 — train RMSE 560.765652 - train R2 0.015013 — val RMSE 520.508364 - val R2 0.140741\n",
      "Epoch 11 — train RMSE 536.769209 - train R2 0.097509 — val RMSE 505.196690 - val R2 0.190550\n",
      "Epoch 12 — train RMSE 513.582097 - train R2 0.173796 — val RMSE 466.304982 - val R2 0.310381\n",
      "Epoch 13 — train RMSE 463.869313 - train R2 0.326002 — val RMSE 442.960715 - val R2 0.377701\n",
      "Epoch 14 — train RMSE 435.005913 - train R2 0.407269 — val RMSE 413.410354 - val R2 0.457960\n",
      "Epoch 15 — train RMSE 408.398307 - train R2 0.477561 — val RMSE 386.880674 - val R2 0.525296\n",
      "Epoch 16 — train RMSE 398.734738 - train R2 0.501993 — val RMSE 375.330328 - val R2 0.553218\n",
      "Epoch 17 — train RMSE 530.138435 - train R2 0.119669 — val RMSE 451.899571 - val R2 0.352332\n",
      "Epoch 18 — train RMSE 456.904426 - train R2 0.346090 — val RMSE 406.367272 - val R2 0.476271\n",
      "Epoch 19 — train RMSE 403.635383 - train R2 0.489676 — val RMSE 348.989512 - val R2 0.613728\n",
      "Epoch 20 — train RMSE 360.779644 - train R2 0.592290 — val RMSE 328.604343 - val R2 0.657536\n",
      "Epoch 21 — train RMSE 349.315333 - train R2 0.617789 — val RMSE 295.385451 - val R2 0.723276\n",
      "Epoch 22 — train RMSE 348.139026 - train R2 0.620359 — val RMSE 277.248172 - val R2 0.756215\n",
      "Epoch 23 — train RMSE 335.100238 - train R2 0.648264 — val RMSE 272.300708 - val R2 0.764838\n",
      "Epoch 24 — train RMSE 282.713521 - train R2 0.749642 — val RMSE 246.850070 - val R2 0.806743\n",
      "Epoch 25 — train RMSE 274.329971 - train R2 0.764270 — val RMSE 226.021769 - val R2 0.837980\n",
      "Epoch 26 — train RMSE 293.076523 - train R2 0.730952 — val RMSE 215.975028 - val R2 0.852063\n",
      "Epoch 27 — train RMSE 267.393177 - train R2 0.776041 — val RMSE 210.685664 - val R2 0.859221\n",
      "Epoch 28 — train RMSE 248.145752 - train R2 0.807123 — val RMSE 191.737988 - val R2 0.883404\n",
      "Epoch 29 — train RMSE 283.153268 - train R2 0.748863 — val RMSE 182.714303 - val R2 0.894120\n",
      "Epoch 30 — train RMSE 262.466681 - train R2 0.784218 — val RMSE 172.534614 - val R2 0.905589\n",
      "Epoch 31 — train RMSE 229.888044 - train R2 0.834461 — val RMSE 160.650044 - val R2 0.918148\n",
      "Epoch 32 — train RMSE 213.670472 - train R2 0.856993 — val RMSE 157.644745 - val R2 0.921182\n",
      "Epoch 33 — train RMSE 214.228950 - train R2 0.856245 — val RMSE 146.471109 - val R2 0.931959\n",
      "Epoch 34 — train RMSE 279.764754 - train R2 0.754838 — val RMSE 141.213803 - val R2 0.936755\n",
      "Epoch 35 — train RMSE 243.301443 - train R2 0.814580 — val RMSE 125.623807 - val R2 0.949949\n",
      "Epoch 36 — train RMSE 202.581283 - train R2 0.871452 — val RMSE 123.589764 - val R2 0.951557\n",
      "Epoch 37 — train RMSE 201.117946 - train R2 0.873302 — val RMSE 120.898302 - val R2 0.953644\n",
      "Epoch 38 — train RMSE 251.028229 - train R2 0.802616 — val RMSE 115.194758 - val R2 0.957914\n",
      "Epoch 39 — train RMSE 261.150514 - train R2 0.786376 — val RMSE 111.373269 - val R2 0.960660\n",
      "Epoch 40 — train RMSE 198.928537 - train R2 0.876046 — val RMSE 128.354146 - val R2 0.947750\n",
      "Epoch 41 — train RMSE 125.938582 - train R2 0.950320 — val RMSE 104.871631 - val R2 0.965119\n",
      "Epoch 42 — train RMSE 237.799950 - train R2 0.822870 — val RMSE 100.197049 - val R2 0.968160\n",
      "Epoch 43 — train RMSE 181.293739 - train R2 0.897048 — val RMSE 98.247325 - val R2 0.969387\n",
      "Epoch 44 — train RMSE 197.040605 - train R2 0.878387 — val RMSE 98.251703 - val R2 0.969384\n",
      "Epoch 45 — train RMSE 249.373480 - train R2 0.805209 — val RMSE 95.386520 - val R2 0.971144\n",
      "Epoch 46 — train RMSE 223.300048 - train R2 0.843813 — val RMSE 98.736520 - val R2 0.969081\n",
      "Epoch 47 — train RMSE 211.552227 - train R2 0.859815 — val RMSE 95.739697 - val R2 0.970930\n",
      "Epoch 48 — train RMSE 212.683901 - train R2 0.858311 — val RMSE 130.131103 - val R2 0.946293\n",
      "Epoch 49 — train RMSE 249.129204 - train R2 0.805591 — val RMSE 98.120358 - val R2 0.969466\n",
      "Epoch 50 — train RMSE 223.206697 - train R2 0.843943 — val RMSE 89.686811 - val R2 0.974489\n",
      "Epoch 51 — train RMSE 258.856747 - train R2 0.790113 — val RMSE 88.268935 - val R2 0.975289\n",
      "Epoch 52 — train RMSE 209.856359 - train R2 0.862053 — val RMSE 87.780639 - val R2 0.975562\n",
      "Epoch 53 — train RMSE 210.431178 - train R2 0.861296 — val RMSE 92.001899 - val R2 0.973155\n",
      "Epoch 54 — train RMSE 248.121493 - train R2 0.807160 — val RMSE 84.721800 - val R2 0.977235\n",
      "Epoch 55 — train RMSE 235.785553 - train R2 0.825859 — val RMSE 95.211471 - val R2 0.971249\n",
      "Epoch 56 — train RMSE 235.410522 - train R2 0.826412 — val RMSE 83.634161 - val R2 0.977816\n",
      "Epoch 57 — train RMSE 223.581130 - train R2 0.843419 — val RMSE 102.222296 - val R2 0.966859\n",
      "Epoch 58 — train RMSE 259.662051 - train R2 0.788805 — val RMSE 79.770263 - val R2 0.979819\n",
      "Epoch 59 — train RMSE 282.114751 - train R2 0.750702 — val RMSE 93.466193 - val R2 0.972294\n",
      "Epoch 60 — train RMSE 229.585058 - train R2 0.834897 — val RMSE 96.441271 - val R2 0.970502\n",
      "Epoch 61 — train RMSE 270.784287 - train R2 0.770325 — val RMSE 106.241062 - val R2 0.964202\n",
      "Epoch 62 — train RMSE 236.254274 - train R2 0.825166 — val RMSE 88.006119 - val R2 0.975436\n",
      "Epoch 63 — train RMSE 194.790088 - train R2 0.881149 — val RMSE 95.067923 - val R2 0.971336\n",
      "Epoch 64 — train RMSE 269.920320 - train R2 0.771788 — val RMSE 78.565874 - val R2 0.980423\n",
      "Epoch 65 — train RMSE 193.304645 - train R2 0.882955 — val RMSE 77.936639 - val R2 0.980736\n",
      "Epoch 66 — train RMSE 258.759776 - train R2 0.790270 — val RMSE 75.098328 - val R2 0.982113\n",
      "Epoch 67 — train RMSE 161.236179 - train R2 0.918568 — val RMSE 74.385902 - val R2 0.982451\n",
      "Epoch 68 — train RMSE 160.311234 - train R2 0.919500 — val RMSE 79.338202 - val R2 0.980037\n",
      "Epoch 69 — train RMSE 269.087351 - train R2 0.773194 — val RMSE 76.642843 - val R2 0.981370\n",
      "Epoch 70 — train RMSE 235.751920 - train R2 0.825908 — val RMSE 85.793079 - val R2 0.976656\n",
      "Epoch 71 — train RMSE 280.994993 - train R2 0.752677 — val RMSE 81.230755 - val R2 0.979073\n",
      "Epoch 72 — train RMSE 300.208377 - train R2 0.717698 — val RMSE 68.982772 - val R2 0.984908\n",
      "Epoch 73 — train RMSE 193.365808 - train R2 0.882881 — val RMSE 80.325426 - val R2 0.979537\n",
      "Epoch 74 — train RMSE 207.837415 - train R2 0.864695 — val RMSE 86.251732 - val R2 0.976406\n",
      "Epoch 75 — train RMSE 246.432810 - train R2 0.809776 — val RMSE 80.545014 - val R2 0.979425\n",
      "Epoch 76 — train RMSE 192.973103 - train R2 0.883356 — val RMSE 84.286082 - val R2 0.977469\n",
      "Epoch 77 — train RMSE 221.751260 - train R2 0.845972 — val RMSE 85.794252 - val R2 0.976655\n",
      "Epoch 78 — train RMSE 221.125763 - train R2 0.846840 — val RMSE 71.475512 - val R2 0.983797\n",
      "Epoch 79 — train RMSE 208.839805 - train R2 0.863386 — val RMSE 84.177164 - val R2 0.977527\n",
      "Epoch 80 — train RMSE 269.163802 - train R2 0.773065 — val RMSE 68.127293 - val R2 0.985280\n",
      "Epoch 81 — train RMSE 221.431054 - train R2 0.846417 — val RMSE 84.872387 - val R2 0.977154\n",
      "Epoch 82 — train RMSE 208.290542 - train R2 0.864104 — val RMSE 78.183185 - val R2 0.980614\n",
      "Epoch 83 — train RMSE 207.738041 - train R2 0.864824 — val RMSE 69.434647 - val R2 0.984710\n",
      "Epoch 84 — train RMSE 192.296150 - train R2 0.884173 — val RMSE 67.171419 - val R2 0.985690\n",
      "Epoch 85 — train RMSE 192.919321 - train R2 0.883421 — val RMSE 72.946417 - val R2 0.983124\n",
      "Epoch 86 — train RMSE 192.192712 - train R2 0.884298 — val RMSE 82.892658 - val R2 0.978208\n",
      "Epoch 87 — train RMSE 192.911791 - train R2 0.883430 — val RMSE 68.474797 - val R2 0.985129\n",
      "Epoch 88 — train RMSE 191.904999 - train R2 0.884644 — val RMSE 67.906807 - val R2 0.985375\n",
      "Epoch 89 — train RMSE 207.139889 - train R2 0.865601 — val RMSE 72.930505 - val R2 0.983131\n",
      "Epoch 90 — train RMSE 191.967386 - train R2 0.884569 — val RMSE 75.852085 - val R2 0.981752\n",
      "Epoch 91 — train RMSE 257.575800 - train R2 0.792185 — val RMSE 66.959352 - val R2 0.985780\n",
      "Epoch 92 — train RMSE 206.499742 - train R2 0.866431 — val RMSE 85.046205 - val R2 0.977061\n",
      "Epoch 93 — train RMSE 192.532448 - train R2 0.883888 — val RMSE 75.196823 - val R2 0.982066\n",
      "Epoch 94 — train RMSE 258.854759 - train R2 0.790116 — val RMSE 104.092304 - val R2 0.965636\n",
      "Epoch 95 — train RMSE 160.162829 - train R2 0.919649 — val RMSE 68.903268 - val R2 0.984943\n",
      "Epoch 96 — train RMSE 257.396481 - train R2 0.792474 — val RMSE 66.688830 - val R2 0.985895\n",
      "Epoch 97 — train RMSE 220.797462 - train R2 0.847294 — val RMSE 71.703999 - val R2 0.983694\n",
      "Epoch 98 — train RMSE 245.660766 - train R2 0.810966 — val RMSE 73.081824 - val R2 0.983061\n",
      "Epoch 99 — train RMSE 221.036589 - train R2 0.846963 — val RMSE 73.464580 - val R2 0.982883\n",
      "Epoch 100 — train RMSE 220.336628 - train R2 0.847931 — val RMSE 67.093746 - val R2 0.985723\n"
     ]
    }
   ],
   "source": [
    "configs = Details(\n",
    "    pred_len = 1,      \n",
    "    seq_len  = 350,\n",
    "    d_ff = 32,\n",
    "    patch_len= 16, \n",
    "    stride = 8, \n",
    "    llm_layers = 14, \n",
    "    description = \"This dataset contains near-infrared (NIR) spectroscopy measurements of lactate concentration in a \" \\\n",
    "                  \"phosphate-buffered saline (PBS) solution. Each spectrum includes absorbance values across \" \\\n",
    "                  \"350 wavelengths. \" ,\n",
    "    dropout = 0.1,\n",
    "    n_heads = 8, #Increased\n",
    "    d_model = 16,\n",
    "    enc_in = 1,\n",
    "    d_llm = 4096,\n",
    "    llm_model = 'LLAMA',\n",
    "    lr = 0.0001\n",
    ")\n",
    "\n",
    "train, test = model_selection.train_test_split(data, test_size=0.1, random_state=1, stratify= data['Concentration'])\n",
    "\n",
    "train, validation = model_selection.train_test_split(train, test_size=0.1, random_state=1, stratify= train['Concentration'])\n",
    "\n",
    "model = train_model(configs, train, validation, 100, 8, True, True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test RMSE 156.189932 — Test R2 0.924961\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "#Normalize test data\n",
    "test.columns = test.columns.astype(str)\n",
    "scaler = StandardScaler()\n",
    "normalized = scaler.fit_transform(test.iloc[:,1:].T).T\n",
    "X_test_n = pd.DataFrame(normalized, columns = test.iloc[:,1:].columns)\n",
    "\n",
    "stand = StandardScaler()\n",
    "scaled = stand.fit_transform(X_test_n.iloc[:,1:])#Data leackage fix!!!!!\n",
    "X_test_n = pd.DataFrame(scaled, columns = X_test_n.iloc[:,1:].columns)\n",
    "\n",
    "X_test = torch.from_numpy(X_test_n.to_numpy().astype('float32')).unsqueeze(-1) \n",
    "y_test = torch.from_numpy(test.iloc[:,0].to_numpy().astype('float32')).unsqueeze(1).unsqueeze(1)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "X_test = X_test.to(device)\n",
    "y_test = y_test.to(device)\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    y_pred = model(X_test, None, None, None)\n",
    "\n",
    "r2_metric = R2Score() \n",
    "criterion = torch.nn.MSELoss()\n",
    "r2_metric.update(y_pred.squeeze(), y_test.squeeze())\n",
    "test_r2 = r2_metric.compute()\n",
    "test_rmse = sqrt(criterion(y_test, y_pred))\n",
    "\n",
    "\n",
    "print(f\"Test RMSE {test_rmse:.6f} — Test R2 {test_r2:.6f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1300.]] [[989.5504]]\n",
      "[[130.]] [[49.47639]]\n",
      "[[13.]] [[-18.178505]]\n",
      "[[13.]] [[-9.151216]]\n",
      "[[13.]] [[13.981222]]\n",
      "[[1300.]] [[1059.87]]\n",
      "[[1300.]] [[1084.0287]]\n",
      "[[130.]] [[84.786255]]\n",
      "[[130.]] [[66.15697]]\n",
      "[[130.]] [[44.047195]]\n",
      "[[130.]] [[51.052605]]\n",
      "[[130.]] [[26.825974]]\n",
      "[[1300.]] [[1147.9308]]\n",
      "[[130.]] [[40.68179]]\n",
      "[[130.]] [[48.303932]]\n",
      "[[0.]] [[239.86089]]\n",
      "[[130.]] [[134.98781]]\n",
      "[[13.]] [[-11.023138]]\n",
      "[[1300.]] [[1170.4755]]\n",
      "[[130.]] [[38.09917]]\n",
      "[[13.]] [[-12.153199]]\n",
      "[[1300.]] [[1070.7787]]\n",
      "[[13.]] [[-3.4081333]]\n",
      "[[1300.]] [[1102.3422]]\n",
      "[[13.]] [[-6.505058]]\n",
      "[[13.]] [[-10.581898]]\n",
      "[[0.]] [[425.99167]]\n",
      "[[0.]] [[170.8798]]\n",
      "[[1300.]] [[1096.2156]]\n",
      "[[1300.]] [[1112.2673]]\n",
      "[[130.]] [[79.177635]]\n",
      "[[1300.]] [[1106.8644]]\n",
      "[[13.]] [[-6.1857886]]\n",
      "[[1300.]] [[1054.8514]]\n",
      "[[0.]] [[181.70709]]\n",
      "[[130.]] [[68.194374]]\n"
     ]
    }
   ],
   "source": [
    "errors = {}\n",
    "y_test_np = y_test.cpu().numpy()\n",
    "y_pred_np = y_pred.cpu().numpy()\n",
    "for i in np.unique(y_test_np):\n",
    "    errors[i] = []\n",
    "\n",
    "for i in range(36):\n",
    "    #print(y_test.cpu().numpy()[i], y_pred.cpu().numpy()[i])\n",
    "    error[y_test_np[i]].append(abs(y_test_np[i] - y_pred_np[i]))\n",
    "\n",
    "for k in error.keys():\n",
    "    print(f'{k} mean error of : {mean(error[k])}')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Corss Val learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "525bb580b8ed479f9de0b673273ef82f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold  1 — train RMSE 306.631782 - train R2 0.708471 — val RMSE 238.800127 - val R2 0.812031\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ee91579ad694aa4b21c8b34a1026b04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold  2 — train RMSE 341.398960 - train R2 0.641077 — val RMSE 304.676218 - val R2 0.682768\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ed0200741844316be7655a8e80eb33f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold  3 — train RMSE 365.527141 - train R2 0.575687 — val RMSE 353.782735 - val R2 0.624702\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6cddf861269d4814b7207317f27ffda8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold  4 — train RMSE 305.243363 - train R2 0.714522 — val RMSE 239.239394 - val R2 0.797419\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b34ec2e209004851bc31c245b56a3c3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold  5 — train RMSE 318.618733 - train R2 0.666564 — val RMSE 337.168615 - val R2 0.684770\n",
      "CrossVal reuslts for leraning rate 0.000100 — train RMSE 327.483996 - train R2 0.661264 — val RMSE 294.733418 - val R2 0.720338\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5bc9b92966b4728861bb7e98a941794",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold  1 — train RMSE 241.495311 - train R2 0.819172 — val RMSE 68.339521 - val R2 0.984606\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "122d791089aa430db8fcb77bc18b535a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold  2 — train RMSE 240.993487 - train R2 0.821150 — val RMSE 83.293858 - val R2 0.976290\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23e037d3926145beaa6fa22f3d9b9a66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold  3 — train RMSE 218.030081 - train R2 0.849034 — val RMSE 83.735146 - val R2 0.978976\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2dc33224dd9c4392a04a84950342217b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold  4 — train RMSE 247.797070 - train R2 0.811864 — val RMSE 56.156056 - val R2 0.988838\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61fde9ac2fe740dc860b2d80e34e03c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold  5 — train RMSE 213.863850 - train R2 0.849774 — val RMSE 79.472172 - val R2 0.982487\n",
      "CrossVal reuslts for leraning rate 0.001000 — train RMSE 232.435960 - train R2 0.830199 — val RMSE 74.199351 - val R2 0.982239\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c83c24fb5e94cc4a78a5c02a8ac0722",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold  1 — train RMSE 348.548996 - train R2 0.623318 — val RMSE 268.966415 - val R2 0.761542\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf69f0b827f940f3b61abdd9e752df9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold  2 — train RMSE 451.933688 - train R2 0.371034 — val RMSE 263.956396 - val R2 0.761898\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd36ddc3b5eb4cc5befe48c7990e7f64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold  3 — train RMSE 333.323498 - train R2 0.647159 — val RMSE 204.939043 - val R2 0.874063\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb26a4ebdf084e5886193c40e4d298a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold  4 — train RMSE 226.540781 - train R2 0.842756 — val RMSE 105.157881 - val R2 0.960860\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9ca2dc375824cfab6f6261828b37564",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold  5 — train RMSE 224.487092 - train R2 0.834479 — val RMSE 128.079144 - val R2 0.954513\n",
      "CrossVal reuslts for leraning rate 0.010000 — train RMSE 316.966811 - train R2 0.663749 — val RMSE 194.219776 - val R2 0.862575\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6c0b6a8e13e4831aea3d485333d15da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold  1 — train RMSE 202.598012 - train R2 0.872732 — val RMSE 133.847599 - val R2 0.940948\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef804415d2134faba47935172d3ac418",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold  2 — train RMSE 332.432053 - train R2 0.659683 — val RMSE 197.117110 - val R2 0.867215\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ea6bb555d2d46e4830e9e9b1e0171f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold  3 — train RMSE 347.879422 - train R2 0.615670 — val RMSE 280.255221 - val R2 0.764489\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10b2084978244580a2ba528f9c2d6213",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold  4 — train RMSE 336.136952 - train R2 0.653812 — val RMSE 78.307207 - val R2 0.978296\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7978b404fc34908b1a9be63b32297f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold  5 — train RMSE 318.522527 - train R2 0.666765 — val RMSE 233.359818 - val R2 0.848997\n",
      "CrossVal reuslts for leraning rate 0.100000 — train RMSE 307.513793 - train R2 0.693732 — val RMSE 184.577391 - val R2 0.879989\n"
     ]
    }
   ],
   "source": [
    "train, test = model_selection.train_test_split(data, test_size=0.1, random_state=1, stratify= data['Concentration'])\n",
    "\n",
    "for lr in [1e-4, 1e-3, 1e-2, 1e-1]:\n",
    "    configs = Details(\n",
    "    pred_len = 1,      \n",
    "    seq_len  = 350,\n",
    "    d_ff = 32,\n",
    "    patch_len= 16, \n",
    "    stride = 8, \n",
    "    llm_layers = 4,\n",
    "    description = \"This dataset contains near-infrared (NIR) spectroscopy measurements of lactate concentration in a \" \\\n",
    "                  \"phosphate-buffered saline (PBS) solution. Each spectrum includes absorbance values across \" \\\n",
    "                  \"350 wavelengths. \" ,\n",
    "    dropout = 0.1,\n",
    "    n_heads = 2,\n",
    "    d_model = 16,\n",
    "    enc_in = 1,\n",
    "    d_llm = 4096,\n",
    "    llm_model = 'LLAMA',\n",
    "    lr = lr\n",
    "    )\n",
    "\n",
    "    k = 5\n",
    "    kf = KFold(n_splits=k, shuffle=True, random_state=42)\n",
    "\n",
    "    train_rmse_list = []\n",
    "    train_r2_list = []\n",
    "    val_rmse_list = []\n",
    "    val_r2_list = []\n",
    "\n",
    "    for f, (train_i, val_i) in enumerate(kf.split(train)):\n",
    "        \n",
    "        #Split data \n",
    "        train_ds = train.iloc[train_i,:]\n",
    "        val_ds = train.iloc[val_i,:]\n",
    "\n",
    "        model, train_rmse, train_r2, val_rmse, val_r2 = train_model(configs, train_ds, val_ds, 20, 8, False,False)\n",
    "\n",
    "        train_rmse_list.append(train_rmse)\n",
    "        train_r2_list.append(train_r2)\n",
    "        val_rmse_list.append(val_rmse)\n",
    "        val_r2_list.append(val_r2)\n",
    "        print(f\"Fold {f +1:2d} — train RMSE {train_rmse:.6f} - train R2 {train_r2:.6f} — val RMSE {val_rmse:.6f} - val R2 {val_r2:.6f}\")\n",
    "    print(f\"CrossVal reuslts for leraning rate {lr:.6f} — train RMSE {sum(train_rmse_list)/k:.6f} - train R2 {sum(train_r2_list)/k:.6f} — val RMSE {sum(val_rmse_list)/k:.6f} - val R2 {sum(val_r2_list)/k:.6f}\")\n",
    "\n",
    "        \n",
    "        "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best lr = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "configs = Details(\n",
    "    pred_len = 1,      \n",
    "    seq_len  = 350,\n",
    "    d_ff = 32,\n",
    "    patch_len= 16, \n",
    "    stride = 8, \n",
    "    llm_layers = 4,\n",
    "    description = \"This dataset contains near-infrared (NIR) spectroscopy measurements of lactate concentration in a \" \\\n",
    "                  \"phosphate-buffered saline (PBS) solution. Each spectrum includes absorbance values across \" \\\n",
    "                  \"350 wavelengths. \" ,\n",
    "    dropout = 0.1,\n",
    "    n_heads = 2,\n",
    "    d_model = 16,\n",
    "    enc_in = 1,\n",
    "    d_llm = 4096,\n",
    "    llm_model = 'LLAMA',\n",
    "    lr = 0.001\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7cdd66162aae454cb61ed6d4842c00ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  1 — train RMSE 654.211866 - train R2 -0.340616 — val RMSE 659.208806 - val R2 -0.378208\n",
      "Epoch  2 — train RMSE 619.817002 - train R2 -0.203357 — val RMSE 579.661411 - val R2 -0.065657\n",
      "Epoch  3 — train RMSE 509.771222 - train R2 0.186012 — val RMSE 456.100007 - val R2 0.340235\n",
      "Epoch  4 — train RMSE 369.568820 - train R2 0.572183 — val RMSE 239.069394 - val R2 0.818734\n",
      "Epoch  5 — train RMSE 377.251939 - train R2 0.554210 — val RMSE 222.791873 - val R2 0.842577\n",
      "Epoch  6 — train RMSE 287.477750 - train R2 0.741133 — val RMSE 231.272010 - val R2 0.830365\n",
      "Epoch  7 — train RMSE 302.177535 - train R2 0.713983 — val RMSE 287.779518 - val R2 0.737343\n",
      "Epoch  8 — train RMSE 259.858258 - train R2 0.788485 — val RMSE 212.239863 - val R2 0.857136\n",
      "Epoch  9 — train RMSE 296.417205 - train R2 0.724784 — val RMSE 208.214701 - val R2 0.862504\n",
      "Epoch 10 — train RMSE 295.031373 - train R2 0.727351 — val RMSE 215.559127 - val R2 0.852633\n",
      "Epoch 11 — train RMSE 262.992899 - train R2 0.783352 — val RMSE 204.135534 - val R2 0.867838\n",
      "Epoch 12 — train RMSE 255.749236 - train R2 0.795122 — val RMSE 179.939452 - val R2 0.897312\n",
      "Epoch 13 — train RMSE 315.848817 - train R2 0.687517 — val RMSE 204.497507 - val R2 0.867369\n",
      "Epoch 14 — train RMSE 219.453283 - train R2 0.849148 — val RMSE 175.513541 - val R2 0.902301\n",
      "Epoch 15 — train RMSE 205.746939 - train R2 0.867403 — val RMSE 151.991331 - val R2 0.926733\n",
      "Epoch 16 — train RMSE 259.816315 - train R2 0.788554 — val RMSE 157.561320 - val R2 0.921265\n",
      "Epoch 17 — train RMSE 330.582515 - train R2 0.657684 — val RMSE 156.570866 - val R2 0.922252\n",
      "Epoch 18 — train RMSE 162.082132 - train R2 0.917712 — val RMSE 150.715700 - val R2 0.927958\n",
      "Epoch 19 — train RMSE 280.758283 - train R2 0.753093 — val RMSE 149.608030 - val R2 0.929013\n",
      "Epoch 20 — train RMSE 233.753850 - train R2 0.828847 — val RMSE 165.599242 - val R2 0.913027\n",
      "Epoch 21 — train RMSE 244.271027 - train R2 0.813099 — val RMSE 150.421438 - val R2 0.928239\n",
      "Epoch 22 — train RMSE 252.508657 - train R2 0.800281 — val RMSE 168.779349 - val R2 0.909654\n",
      "Epoch 23 — train RMSE 275.594646 - train R2 0.762092 — val RMSE 344.712573 - val R2 0.623137\n",
      "Epoch 24 — train RMSE 232.398252 - train R2 0.830826 — val RMSE 122.677617 - val R2 0.952269\n",
      "Epoch 25 — train RMSE 288.827937 - train R2 0.738696 — val RMSE 117.137952 - val R2 0.956482\n",
      "Epoch 26 — train RMSE 276.885678 - train R2 0.759858 — val RMSE 135.669743 - val R2 0.941624\n",
      "Epoch 27 — train RMSE 268.479396 - train R2 0.774218 — val RMSE 132.901803 - val R2 0.943982\n",
      "Epoch 28 — train RMSE 256.489679 - train R2 0.793934 — val RMSE 142.935133 - val R2 0.935204\n",
      "Epoch 29 — train RMSE 328.106800 - train R2 0.662792 — val RMSE 114.365982 - val R2 0.958518\n",
      "Epoch 30 — train RMSE 214.439904 - train R2 0.855961 — val RMSE 124.659856 - val R2 0.950714\n",
      "Epoch 31 — train RMSE 255.710820 - train R2 0.795183 — val RMSE 107.617166 - val R2 0.963269\n",
      "Epoch 32 — train RMSE 212.516937 - train R2 0.858533 — val RMSE 109.709997 - val R2 0.961827\n",
      "Epoch 33 — train RMSE 310.134691 - train R2 0.698722 — val RMSE 209.474779 - val R2 0.860834\n",
      "Epoch 34 — train RMSE 284.196518 - train R2 0.747009 — val RMSE 136.960890 - val R2 0.940508\n",
      "Epoch 35 — train RMSE 185.712481 - train R2 0.891969 — val RMSE 123.822985 - val R2 0.951374\n",
      "Epoch 36 — train RMSE 270.466827 - train R2 0.770863 — val RMSE 122.992219 - val R2 0.952024\n",
      "Epoch 37 — train RMSE 190.082550 - train R2 0.886825 — val RMSE 130.410263 - val R2 0.946062\n",
      "Epoch 38 — train RMSE 279.273455 - train R2 0.755698 — val RMSE 108.064002 - val R2 0.962963\n",
      "Epoch 39 — train RMSE 271.428322 - train R2 0.769231 — val RMSE 99.679095 - val R2 0.968488\n",
      "Epoch 40 — train RMSE 300.176316 - train R2 0.717759 — val RMSE 161.045408 - val R2 0.917744\n",
      "Epoch 41 — train RMSE 173.422358 - train R2 0.905794 — val RMSE 104.022455 - val R2 0.965682\n",
      "Epoch 42 — train RMSE 328.340976 - train R2 0.662310 — val RMSE 99.210974 - val R2 0.968783\n",
      "Epoch 43 — train RMSE 169.401741 - train R2 0.910112 — val RMSE 96.014037 - val R2 0.970763\n",
      "Epoch 44 — train RMSE 252.361583 - train R2 0.800513 — val RMSE 91.169741 - val R2 0.973638\n",
      "Epoch 45 — train RMSE 240.394395 - train R2 0.818984 — val RMSE 98.856919 - val R2 0.969006\n",
      "Epoch 46 — train RMSE 251.513983 - train R2 0.801851 — val RMSE 89.139745 - val R2 0.974799\n",
      "Epoch 47 — train RMSE 315.946505 - train R2 0.687324 — val RMSE 93.244879 - val R2 0.972425\n",
      "Epoch 48 — train RMSE 231.158314 - train R2 0.832627 — val RMSE 91.301493 - val R2 0.973562\n",
      "Epoch 49 — train RMSE 294.389831 - train R2 0.728536 — val RMSE 87.739226 - val R2 0.975585\n",
      "Epoch 50 — train RMSE 237.155853 - train R2 0.823829 — val RMSE 82.155577 - val R2 0.978594\n",
      "Epoch 51 — train RMSE 264.539317 - train R2 0.780796 — val RMSE 94.181915 - val R2 0.971868\n",
      "Epoch 52 — train RMSE 226.799300 - train R2 0.838879 — val RMSE 82.977639 - val R2 0.978163\n",
      "Epoch 53 — train RMSE 285.116073 - train R2 0.745369 — val RMSE 78.564105 - val R2 0.980424\n",
      "Epoch 54 — train RMSE 210.219913 - train R2 0.861575 — val RMSE 82.696864 - val R2 0.978311\n",
      "Epoch 55 — train RMSE 199.459196 - train R2 0.875383 — val RMSE 98.870060 - val R2 0.968997\n",
      "Epoch 56 — train RMSE 210.000098 - train R2 0.861864 — val RMSE 100.610406 - val R2 0.967896\n",
      "Epoch 57 — train RMSE 263.940100 - train R2 0.781788 — val RMSE 86.373416 - val R2 0.976339\n",
      "Epoch 58 — train RMSE 274.183606 - train R2 0.764522 — val RMSE 93.860122 - val R2 0.972060\n",
      "Epoch 59 — train RMSE 270.772124 - train R2 0.770345 — val RMSE 77.910305 - val R2 0.980749\n",
      "Epoch 60 — train RMSE 141.548810 - train R2 0.937240 — val RMSE 81.917890 - val R2 0.978717\n",
      "Epoch 61 — train RMSE 226.666382 - train R2 0.839068 — val RMSE 143.559203 - val R2 0.934637\n",
      "Epoch 62 — train RMSE 252.340160 - train R2 0.800547 — val RMSE 102.216554 - val R2 0.966863\n",
      "Epoch 63 — train RMSE 287.475999 - train R2 0.741137 — val RMSE 87.608721 - val R2 0.975658\n",
      "Epoch 64 — train RMSE 235.630115 - train R2 0.826088 — val RMSE 76.321394 - val R2 0.981526\n",
      "Epoch 65 — train RMSE 286.471854 - train R2 0.742942 — val RMSE 82.168864 - val R2 0.978587\n",
      "Epoch 66 — train RMSE 241.836073 - train R2 0.816807 — val RMSE 83.226152 - val R2 0.978032\n",
      "Epoch 67 — train RMSE 217.201169 - train R2 0.852228 — val RMSE 85.595012 - val R2 0.976764\n",
      "Epoch 68 — train RMSE 296.404536 - train R2 0.724807 — val RMSE 74.011635 - val R2 0.982627\n",
      "Epoch 69 — train RMSE 245.879002 - train R2 0.810630 — val RMSE 105.350749 - val R2 0.964800\n",
      "Epoch 70 — train RMSE 184.764270 - train R2 0.893069 — val RMSE 69.521576 - val R2 0.984671\n",
      "Epoch 71 — train RMSE 217.432525 - train R2 0.851913 — val RMSE 170.625791 - val R2 0.907667\n",
      "Epoch 72 — train RMSE 245.676081 - train R2 0.810943 — val RMSE 86.055172 - val R2 0.976513\n",
      "Epoch 73 — train RMSE 294.394155 - train R2 0.728527 — val RMSE 128.877080 - val R2 0.947323\n",
      "Epoch 74 — train RMSE 257.109719 - train R2 0.792936 — val RMSE 282.864924 - val R2 0.746238\n",
      "Epoch 75 — train RMSE 327.623311 - train R2 0.663785 — val RMSE 309.676455 - val R2 0.695852\n",
      "Epoch 76 — train RMSE 306.093403 - train R2 0.706522 — val RMSE 204.810283 - val R2 0.866963\n",
      "Epoch 77 — train RMSE 315.174292 - train R2 0.688851 — val RMSE 113.205664 - val R2 0.959355\n",
      "Epoch 78 — train RMSE 314.348195 - train R2 0.690480 — val RMSE 148.823951 - val R2 0.929755\n",
      "Epoch 79 — train RMSE 352.599295 - train R2 0.610569 — val RMSE 211.326136 - val R2 0.858364\n",
      "Epoch 80 — train RMSE 325.868351 - train R2 0.667377 — val RMSE 155.471757 - val R2 0.923339\n",
      "Epoch 81 — train RMSE 316.165614 - train R2 0.686890 — val RMSE 134.859737 - val R2 0.942319\n",
      "Epoch 82 — train RMSE 266.103378 - train R2 0.778197 — val RMSE 102.359855 - val R2 0.966770\n",
      "Epoch 83 — train RMSE 242.162802 - train R2 0.816311 — val RMSE 142.843443 - val R2 0.935287\n",
      "Epoch 84 — train RMSE 364.197524 - train R2 0.584528 — val RMSE 119.117233 - val R2 0.954999\n",
      "Epoch 85 — train RMSE 312.012700 - train R2 0.695062 — val RMSE 106.326718 - val R2 0.964145\n",
      "Epoch 86 — train RMSE 253.417576 - train R2 0.798840 — val RMSE 94.396237 - val R2 0.971740\n",
      "Epoch 87 — train RMSE 287.505007 - train R2 0.741084 — val RMSE 110.844308 - val R2 0.961033\n",
      "Epoch 88 — train RMSE 268.608619 - train R2 0.774001 — val RMSE 114.861973 - val R2 0.958157\n",
      "Epoch 89 — train RMSE 310.350222 - train R2 0.698303 — val RMSE 119.229824 - val R2 0.954914\n",
      "Epoch 90 — train RMSE 277.693348 - train R2 0.758455 — val RMSE 166.672382 - val R2 0.911896\n",
      "Epoch 91 — train RMSE 252.540133 - train R2 0.800231 — val RMSE 129.942552 - val R2 0.946449\n",
      "Epoch 92 — train RMSE 287.799886 - train R2 0.740553 — val RMSE 123.225276 - val R2 0.951842\n",
      "Epoch 93 — train RMSE 291.588943 - train R2 0.733676 — val RMSE 109.326693 - val R2 0.962093\n",
      "Epoch 94 — train RMSE 251.138184 - train R2 0.802443 — val RMSE 115.197805 - val R2 0.957912\n",
      "Epoch 95 — train RMSE 327.370865 - train R2 0.664303 — val RMSE 111.058296 - val R2 0.960883\n",
      "Epoch 96 — train RMSE 284.073258 - train R2 0.747228 — val RMSE 109.638330 - val R2 0.961876\n",
      "Epoch 97 — train RMSE 205.316094 - train R2 0.867958 — val RMSE 82.336220 - val R2 0.978499\n",
      "Epoch 98 — train RMSE 298.675424 - train R2 0.720574 — val RMSE 77.096938 - val R2 0.981149\n",
      "Epoch 99 — train RMSE 244.616569 - train R2 0.812570 — val RMSE 79.708657 - val R2 0.979850\n",
      "Epoch 100 — train RMSE 216.440184 - train R2 0.853262 — val RMSE 83.569750 - val R2 0.977850\n"
     ]
    }
   ],
   "source": [
    "train, test = model_selection.train_test_split(data, test_size=0.1, random_state=1, stratify= data['Concentration'])\n",
    "\n",
    "train, validation = model_selection.train_test_split(train, test_size=0.1, random_state=1, stratify= train['Concentration'])\n",
    "\n",
    "model_lr = train_model(configs, train, validation, 100, 8, True, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lr, train_rmse, train_r2, val_rmse, val_r2 = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test RMSE 97.008809 — Test R2 0.969740\n"
     ]
    }
   ],
   "source": [
    "model_lr.eval()\n",
    "\n",
    "#Normalize test data\n",
    "test.columns = test.columns.astype(str)\n",
    "scaler = StandardScaler()\n",
    "normalized = scaler.fit_transform(test.iloc[:,1:].T).T\n",
    "X_test_n = pd.DataFrame(normalized, columns = test.iloc[:,1:].columns)\n",
    "\n",
    "stand = StandardScaler()\n",
    "scaled = stand.fit_transform(X_test_n.iloc[:,1:])#Data leackage fix!!!!!\n",
    "X_test_n = pd.DataFrame(scaled, columns = X_test_n.iloc[:,1:].columns)\n",
    "\n",
    "X_test = torch.from_numpy(X_test_n.to_numpy().astype('float32')).unsqueeze(-1) \n",
    "y_test = torch.from_numpy(test.iloc[:,0].to_numpy().astype('float32')).unsqueeze(1).unsqueeze(1)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "X_test = X_test.to(device)\n",
    "y_test = y_test.to(device)\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    y_pred = model_lr(X_test, None, None, None)\n",
    "\n",
    "r2_metric = R2Score() \n",
    "criterion = torch.nn.MSELoss()\n",
    "r2_metric.update(y_pred.squeeze(), y_test.squeeze())\n",
    "test_r2 = r2_metric.compute()\n",
    "test_rmse = sqrt(criterion(y_test, y_pred))\n",
    "\n",
    "\n",
    "print(f\"Test RMSE {test_rmse:.6f} — Test R2 {test_r2:.6f}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Increase batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "configs = Details(\n",
    "    pred_len = 1,      \n",
    "    seq_len  = 350,\n",
    "    d_ff = 32,\n",
    "    patch_len= 16, \n",
    "    stride = 8, \n",
    "    llm_layers = 4,\n",
    "    description = \"This dataset contains near-infrared (NIR) spectroscopy measurements of lactate concentration in a \" \\\n",
    "                  \"phosphate-buffered saline (PBS) solution. Each spectrum includes absorbance values across \" \\\n",
    "                  \"350 wavelengths. \" ,\n",
    "    dropout = 0.1,\n",
    "    n_heads = 2,\n",
    "    d_model = 16,\n",
    "    enc_in = 1,\n",
    "    d_llm = 4096,\n",
    "    llm_model = 'LLAMA',\n",
    "    lr = 0.001\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68d7926ecb6d49679cf4af6c3675d653",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  1 — train RMSE 671.317481 - train R2 -0.411638 — val RMSE 614.105040 - val R2 -0.196063\n",
      "Epoch  2 — train RMSE 610.635144 - train R2 -0.167969 — val RMSE 605.197411 - val R2 -0.161617\n",
      "Epoch  3 — train RMSE 584.503924 - train R2 -0.070144 — val RMSE 519.387034 - val R2 0.144439\n",
      "Epoch  4 — train RMSE 509.324987 - train R2 0.187436 — val RMSE 412.297526 - val R2 0.460874\n",
      "Epoch  5 — train RMSE 439.017116 - train R2 0.396287 — val RMSE 505.881424 - val R2 0.188355\n",
      "Epoch  6 — train RMSE 344.242235 - train R2 0.628810 — val RMSE 245.188026 - val R2 0.809337\n",
      "Epoch  7 — train RMSE 320.512998 - train R2 0.678220 — val RMSE 200.867297 - val R2 0.872036\n",
      "Epoch  8 — train RMSE 313.414014 - train R2 0.692316 — val RMSE 364.980661 - val R2 0.577518\n",
      "Epoch  9 — train RMSE 346.913743 - train R2 0.623027 — val RMSE 179.003247 - val R2 0.898377\n",
      "Epoch 10 — train RMSE 257.724034 - train R2 0.791945 — val RMSE 170.486789 - val R2 0.907817\n",
      "Epoch 11 — train RMSE 255.480254 - train R2 0.795552 — val RMSE 138.925127 - val R2 0.938789\n",
      "Epoch 12 — train RMSE 226.748679 - train R2 0.838951 — val RMSE 140.497480 - val R2 0.937395\n",
      "Epoch 13 — train RMSE 159.554946 - train R2 0.920258 — val RMSE 120.402738 - val R2 0.954023\n",
      "Epoch 14 — train RMSE 285.490861 - train R2 0.744699 — val RMSE 126.242922 - val R2 0.949454\n",
      "Epoch 15 — train RMSE 240.019449 - train R2 0.819548 — val RMSE 114.992620 - val R2 0.958062\n",
      "Epoch 16 — train RMSE 209.448298 - train R2 0.862589 — val RMSE 102.572733 - val R2 0.966632\n",
      "Epoch 17 — train RMSE 297.681994 - train R2 0.722430 — val RMSE 88.149519 - val R2 0.975356\n",
      "Epoch 18 — train RMSE 276.512095 - train R2 0.760505 — val RMSE 83.497173 - val R2 0.977889\n",
      "Epoch 19 — train RMSE 290.211246 - train R2 0.736187 — val RMSE 99.674976 - val R2 0.968491\n",
      "Epoch 20 — train RMSE 205.732905 - train R2 0.867421 — val RMSE 105.446711 - val R2 0.964736\n",
      "Epoch 21 — train RMSE 264.676695 - train R2 0.780568 — val RMSE 105.476593 - val R2 0.964716\n",
      "Epoch 22 — train RMSE 215.379144 - train R2 0.854697 — val RMSE 91.255950 - val R2 0.973589\n",
      "Epoch 23 — train RMSE 274.718253 - train R2 0.763603 — val RMSE 93.456959 - val R2 0.972299\n",
      "Epoch 24 — train RMSE 263.683123 - train R2 0.782213 — val RMSE 79.843999 - val R2 0.979781\n",
      "Epoch 25 — train RMSE 263.382670 - train R2 0.782709 — val RMSE 93.477210 - val R2 0.972287\n",
      "Epoch 26 — train RMSE 184.268882 - train R2 0.893642 — val RMSE 96.987810 - val R2 0.970167\n",
      "Epoch 27 — train RMSE 273.474877 - train R2 0.765738 — val RMSE 101.417091 - val R2 0.967379\n",
      "Epoch 28 — train RMSE 227.378562 - train R2 0.838055 — val RMSE 80.614452 - val R2 0.979389\n",
      "Epoch 29 — train RMSE 294.872981 - train R2 0.727644 — val RMSE 74.310691 - val R2 0.982487\n",
      "Epoch 30 — train RMSE 213.782753 - train R2 0.856843 — val RMSE 93.897049 - val R2 0.972038\n",
      "Epoch 31 — train RMSE 201.297622 - train R2 0.873076 — val RMSE 102.517720 - val R2 0.966668\n",
      "Epoch 32 — train RMSE 261.713692 - train R2 0.785454 — val RMSE 82.180558 - val R2 0.978581\n",
      "Epoch 33 — train RMSE 214.683823 - train R2 0.855634 — val RMSE 90.995372 - val R2 0.973739\n",
      "Epoch 34 — train RMSE 126.503961 - train R2 0.949873 — val RMSE 80.493951 - val R2 0.979451\n",
      "Epoch 35 — train RMSE 252.834808 - train R2 0.799764 — val RMSE 84.734744 - val R2 0.977228\n",
      "Epoch 36 — train RMSE 199.901213 - train R2 0.874831 — val RMSE 92.251052 - val R2 0.973009\n",
      "Epoch 37 — train RMSE 238.432600 - train R2 0.821927 — val RMSE 66.814329 - val R2 0.985842\n",
      "Epoch 38 — train RMSE 199.558005 - train R2 0.875260 — val RMSE 67.852722 - val R2 0.985398\n",
      "Epoch 39 — train RMSE 214.452820 - train R2 0.855944 — val RMSE 95.462461 - val R2 0.971098\n",
      "Epoch 40 — train RMSE 261.897817 - train R2 0.785152 — val RMSE 91.677646 - val R2 0.973344\n",
      "Epoch 41 — train RMSE 238.107858 - train R2 0.822411 — val RMSE 93.616097 - val R2 0.972205\n",
      "Epoch 42 — train RMSE 226.807038 - train R2 0.838868 — val RMSE 79.985437 - val R2 0.979710\n",
      "Epoch 43 — train RMSE 248.781716 - train R2 0.806133 — val RMSE 67.597743 - val R2 0.985508\n",
      "Epoch 44 — train RMSE 236.606938 - train R2 0.824643 — val RMSE 64.321301 - val R2 0.986879\n",
      "Epoch 45 — train RMSE 196.215994 - train R2 0.879403 — val RMSE 62.595752 - val R2 0.987573\n",
      "Epoch 46 — train RMSE 238.472720 - train R2 0.821867 — val RMSE 86.999923 - val R2 0.975995\n",
      "Epoch 47 — train RMSE 196.217697 - train R2 0.879401 — val RMSE 81.986400 - val R2 0.978682\n",
      "Epoch 48 — train RMSE 180.146014 - train R2 0.898348 — val RMSE 78.156236 - val R2 0.980627\n",
      "Epoch 49 — train RMSE 165.821345 - train R2 0.913871 — val RMSE 86.880455 - val R2 0.976061\n",
      "Epoch 50 — train RMSE 236.624735 - train R2 0.824617 — val RMSE 71.328609 - val R2 0.983864\n",
      "Epoch 51 — train RMSE 248.606996 - train R2 0.806405 — val RMSE 87.833597 - val R2 0.975532\n",
      "Epoch 52 — train RMSE 195.777289 - train R2 0.879942 — val RMSE 71.675358 - val R2 0.983707\n",
      "Epoch 53 — train RMSE 212.845188 - train R2 0.858096 — val RMSE 77.238253 - val R2 0.981079\n",
      "Epoch 54 — train RMSE 252.593709 - train R2 0.800146 — val RMSE 94.581596 - val R2 0.971628\n",
      "Epoch 55 — train RMSE 212.265076 - train R2 0.858868 — val RMSE 84.125763 - val R2 0.977555\n",
      "Epoch 56 — train RMSE 225.031447 - train R2 0.841381 — val RMSE 78.264008 - val R2 0.980574\n",
      "Epoch 57 — train RMSE 213.634112 - train R2 0.857042 — val RMSE 74.231061 - val R2 0.982524\n",
      "Epoch 58 — train RMSE 287.889026 - train R2 0.740392 — val RMSE 202.649279 - val R2 0.869756\n",
      "Epoch 59 — train RMSE 206.133405 - train R2 0.866904 — val RMSE 96.733349 - val R2 0.970323\n",
      "Epoch 60 — train RMSE 229.274286 - train R2 0.835344 — val RMSE 76.050986 - val R2 0.981657\n",
      "Epoch 61 — train RMSE 253.791943 - train R2 0.798245 — val RMSE 93.572039 - val R2 0.972231\n",
      "Epoch 62 — train RMSE 213.930919 - train R2 0.856644 — val RMSE 93.622356 - val R2 0.972201\n",
      "Epoch 63 — train RMSE 253.728595 - train R2 0.798346 — val RMSE 72.071294 - val R2 0.983526\n",
      "Epoch 64 — train RMSE 183.055235 - train R2 0.895038 — val RMSE 95.696251 - val R2 0.970956\n",
      "Epoch 65 — train RMSE 184.593457 - train R2 0.893267 — val RMSE 85.300088 - val R2 0.976924\n",
      "Epoch 66 — train RMSE 253.167167 - train R2 0.799238 — val RMSE 87.023620 - val R2 0.975982\n",
      "Epoch 67 — train RMSE 212.634418 - train R2 0.858377 — val RMSE 75.232037 - val R2 0.982050\n",
      "Epoch 68 — train RMSE 225.124098 - train R2 0.841251 — val RMSE 74.579590 - val R2 0.982360\n",
      "Epoch 69 — train RMSE 252.130700 - train R2 0.800878 — val RMSE 91.555155 - val R2 0.973415\n",
      "Epoch 70 — train RMSE 212.153286 - train R2 0.859017 — val RMSE 78.886382 - val R2 0.980263\n",
      "Epoch 71 — train RMSE 210.270082 - train R2 0.861509 — val RMSE 88.963617 - val R2 0.974899\n",
      "Epoch 72 — train RMSE 226.439008 - train R2 0.839391 — val RMSE 90.463674 - val R2 0.974045\n",
      "Epoch 73 — train RMSE 260.944309 - train R2 0.786714 — val RMSE 81.499486 - val R2 0.978934\n",
      "Epoch 74 — train RMSE 212.136721 - train R2 0.859039 — val RMSE 92.846336 - val R2 0.972660\n",
      "Epoch 75 — train RMSE 283.018151 - train R2 0.749103 — val RMSE 77.559692 - val R2 0.980922\n",
      "Epoch 76 — train RMSE 240.490861 - train R2 0.818839 — val RMSE 71.907821 - val R2 0.983601\n",
      "Epoch 77 — train RMSE 225.845446 - train R2 0.840232 — val RMSE 74.919478 - val R2 0.982198\n",
      "Epoch 78 — train RMSE 224.052971 - train R2 0.842758 — val RMSE 82.930166 - val R2 0.978188\n",
      "Epoch 79 — train RMSE 210.081384 - train R2 0.861757 — val RMSE 75.011843 - val R2 0.982154\n",
      "Epoch 80 — train RMSE 223.666731 - train R2 0.843300 — val RMSE 78.538199 - val R2 0.980437\n",
      "Epoch 81 — train RMSE 179.380586 - train R2 0.899210 — val RMSE 80.234115 - val R2 0.979583\n",
      "Epoch 82 — train RMSE 179.696847 - train R2 0.898854 — val RMSE 76.230477 - val R2 0.981570\n",
      "Epoch 83 — train RMSE 235.592508 - train R2 0.826144 — val RMSE 74.675367 - val R2 0.982314\n",
      "Epoch 84 — train RMSE 210.325446 - train R2 0.861436 — val RMSE 84.390382 - val R2 0.977413\n",
      "Epoch 85 — train RMSE 195.300658 - train R2 0.880526 — val RMSE 61.409888 - val R2 0.988040\n",
      "Epoch 86 — train RMSE 194.134822 - train R2 0.881948 — val RMSE 71.533159 - val R2 0.983771\n",
      "Epoch 87 — train RMSE 210.935382 - train R2 0.860631 — val RMSE 92.109345 - val R2 0.973092\n",
      "Epoch 88 — train RMSE 195.169127 - train R2 0.880686 — val RMSE 80.698691 - val R2 0.979346\n",
      "Epoch 89 — train RMSE 246.915097 - train R2 0.809031 — val RMSE 68.714344 - val R2 0.985025\n",
      "Epoch 90 — train RMSE 259.528710 - train R2 0.789021 — val RMSE 69.353620 - val R2 0.984745\n",
      "Epoch 91 — train RMSE 223.204194 - train R2 0.843947 — val RMSE 70.807392 - val R2 0.984099\n",
      "Epoch 92 — train RMSE 249.134768 - train R2 0.805582 — val RMSE 77.535880 - val R2 0.980933\n",
      "Epoch 93 — train RMSE 259.263286 - train R2 0.789453 — val RMSE 66.802165 - val R2 0.985847\n",
      "Epoch 94 — train RMSE 235.467007 - train R2 0.826329 — val RMSE 76.498007 - val R2 0.981440\n",
      "Epoch 95 — train RMSE 259.796247 - train R2 0.788586 — val RMSE 73.568494 - val R2 0.982835\n",
      "Epoch 96 — train RMSE 195.197398 - train R2 0.880652 — val RMSE 63.581619 - val R2 0.987179\n",
      "Epoch 97 — train RMSE 260.294930 - train R2 0.787774 — val RMSE 74.319629 - val R2 0.982482\n",
      "Epoch 98 — train RMSE 260.508783 - train R2 0.787425 — val RMSE 66.276626 - val R2 0.986069\n",
      "Epoch 99 — train RMSE 247.346528 - train R2 0.808363 — val RMSE 65.668063 - val R2 0.986323\n",
      "Epoch 100 — train RMSE 270.414732 - train R2 0.770951 — val RMSE 73.442840 - val R2 0.982893\n"
     ]
    }
   ],
   "source": [
    "train, test = model_selection.train_test_split(data, test_size=0.1, random_state=1, stratify= data['Concentration'])\n",
    "\n",
    "train, validation = model_selection.train_test_split(train, test_size=0.1, random_state=1, stratify= train['Concentration'])\n",
    "\n",
    "model_16 = train_model(configs, train, validation, 100, 16, True, False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only concentration >= 13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2 = data[data['Concentration']>1.3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = model_selection.train_test_split(data2, test_size=0.1, random_state=1, stratify = data2['Concentration'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c11347d684349e182beac8fab0fa974",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  1 — train RMSE 743.472150 - train R2 -0.652276 — val RMSE 729.533913 - val R2 -0.597559\n",
      "Epoch  2 — train RMSE 725.898850 - train R2 -0.575091 — val RMSE 707.223863 - val R2 -0.501342\n",
      "Epoch  3 — train RMSE 697.267952 - train R2 -0.453292 — val RMSE 679.852310 - val R2 -0.387379\n",
      "Epoch  4 — train RMSE 681.497750 - train R2 -0.388296 — val RMSE 661.738601 - val R2 -0.314434\n",
      "Epoch  5 — train RMSE 660.329915 - train R2 -0.303393 — val RMSE 645.392740 - val R2 -0.250300\n",
      "Epoch  6 — train RMSE 643.070002 - train R2 -0.236146 — val RMSE 626.801253 - val R2 -0.179304\n",
      "Epoch  7 — train RMSE 623.763343 - train R2 -0.163036 — val RMSE 603.159891 - val R2 -0.092021\n",
      "Epoch  8 — train RMSE 597.727648 - train R2 -0.067973 — val RMSE 579.519087 - val R2 -0.008095\n",
      "Epoch  9 — train RMSE 570.979971 - train R2 0.025470 — val RMSE 556.005510 - val R2 0.072051\n",
      "Epoch 10 — train RMSE 552.266893 - train R2 0.088301 — val RMSE 529.651023 - val R2 0.157935\n",
      "Epoch 11 — train RMSE 533.623683 - train R2 0.148816 — val RMSE 510.326742 - val R2 0.218260\n",
      "Epoch 12 — train RMSE 512.536985 - train R2 0.214758 — val RMSE 488.210361 - val R2 0.284549\n",
      "Epoch 13 — train RMSE 488.934653 - train R2 0.285413 — val RMSE 461.177375 - val R2 0.361587\n",
      "Epoch 14 — train RMSE 471.195353 - train R2 0.336325 — val RMSE 464.488786 - val R2 0.352386\n",
      "Epoch 15 — train RMSE 451.468156 - train R2 0.390733 — val RMSE 422.584694 - val R2 0.463965\n",
      "Epoch 16 — train RMSE 432.292368 - train R2 0.441390 — val RMSE 402.523588 - val R2 0.513650\n",
      "Epoch 17 — train RMSE 386.428134 - train R2 0.553634 — val RMSE 377.855420 - val R2 0.571434\n",
      "Epoch 18 — train RMSE 411.409562 - train R2 0.494057 — val RMSE 359.461944 - val R2 0.612143\n",
      "Epoch 19 — train RMSE 375.239656 - train R2 0.579108 — val RMSE 340.656973 - val R2 0.651662\n",
      "Epoch 20 — train RMSE 352.336676 - train R2 0.628919 — val RMSE 327.821958 - val R2 0.677417\n",
      "Epoch 21 — train RMSE 338.778305 - train R2 0.656929 — val RMSE 314.642463 - val R2 0.702833\n",
      "Epoch 22 — train RMSE 323.361498 - train R2 0.687442 — val RMSE 291.836221 - val R2 0.744351\n",
      "Epoch 23 — train RMSE 310.208081 - train R2 0.712353 — val RMSE 275.291425 - val R2 0.772516\n",
      "Epoch 24 — train RMSE 275.767990 - train R2 0.772678 — val RMSE 262.898459 - val R2 0.792536\n",
      "Epoch 25 — train RMSE 350.551032 - train R2 0.632670 — val RMSE 260.924543 - val R2 0.795640\n",
      "Epoch 26 — train RMSE 268.783806 - train R2 0.784047 — val RMSE 235.417184 - val R2 0.833643\n",
      "Epoch 27 — train RMSE 260.014287 - train R2 0.797909 — val RMSE 237.340388 - val R2 0.830913\n",
      "Epoch 28 — train RMSE 276.015534 - train R2 0.772270 — val RMSE 217.566964 - val R2 0.857914\n",
      "Epoch 29 — train RMSE 245.800791 - train R2 0.819399 — val RMSE 208.145611 - val R2 0.869953\n",
      "Epoch 30 — train RMSE 262.068436 - train R2 0.794703 — val RMSE 204.902309 - val R2 0.873974\n",
      "Epoch 31 — train RMSE 226.206562 - train R2 0.847045 — val RMSE 192.038912 - val R2 0.889301\n",
      "Epoch 32 — train RMSE 259.133202 - train R2 0.799276 — val RMSE 182.095567 - val R2 0.900468\n",
      "Epoch 33 — train RMSE 355.964805 - train R2 0.621237 — val RMSE 181.163851 - val R2 0.901483\n",
      "Epoch 34 — train RMSE 265.417813 - train R2 0.789422 — val RMSE 159.715170 - val R2 0.923430\n",
      "Epoch 35 — train RMSE 306.755837 - train R2 0.718720 — val RMSE 196.574565 - val R2 0.884010\n",
      "Epoch 36 — train RMSE 256.137437 - train R2 0.803890 — val RMSE 159.520236 - val R2 0.923617\n",
      "Epoch 37 — train RMSE 235.462933 - train R2 0.834271 — val RMSE 152.478531 - val R2 0.930212\n",
      "Epoch 38 — train RMSE 218.932489 - train R2 0.856724 — val RMSE 136.664372 - val R2 0.943937\n",
      "Epoch 39 — train RMSE 230.989833 - train R2 0.840508 — val RMSE 143.059515 - val R2 0.938567\n",
      "Epoch 40 — train RMSE 260.077186 - train R2 0.797811 — val RMSE 136.348251 - val R2 0.944196\n",
      "Epoch 41 — train RMSE 233.566206 - train R2 0.836930 — val RMSE 133.127245 - val R2 0.946801\n",
      "Epoch 42 — train RMSE 316.320462 - train R2 0.700906 — val RMSE 126.203953 - val R2 0.952191\n",
      "Epoch 43 — train RMSE 229.002659 - train R2 0.843240 — val RMSE 120.698305 - val R2 0.956271\n",
      "Epoch 44 — train RMSE 212.728847 - train R2 0.864729 — val RMSE 129.892204 - val R2 0.949356\n",
      "Epoch 45 — train RMSE 139.738545 - train R2 0.941631 — val RMSE 119.858008 - val R2 0.956878\n",
      "Epoch 46 — train RMSE 212.243352 - train R2 0.865345 — val RMSE 116.648295 - val R2 0.959157\n",
      "Epoch 47 — train RMSE 267.851775 - train R2 0.785542 — val RMSE 117.217233 - val R2 0.958757\n",
      "Epoch 48 — train RMSE 255.336622 - train R2 0.805114 — val RMSE 133.869038 - val R2 0.946207\n",
      "Epoch 49 — train RMSE 241.040287 - train R2 0.826327 — val RMSE 121.765114 - val R2 0.955495\n",
      "Epoch 50 — train RMSE 241.068267 - train R2 0.826287 — val RMSE 124.510898 - val R2 0.953465\n"
     ]
    }
   ],
   "source": [
    "train, validation = model_selection.train_test_split(train, test_size=0.1, random_state=1, stratify= train['Concentration'])\n",
    "\n",
    "odel_red = train_model(configs, train, validation, 50, 8, True, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test RMSE 175.141344 — Test R2 0.915253\n"
     ]
    }
   ],
   "source": [
    "model_red.eval()\n",
    "\n",
    "#Normalize test data\n",
    "test.columns = test.columns.astype(str)\n",
    "scaler = StandardScaler()\n",
    "normalized = scaler.fit_transform(test.iloc[:,1:].T).T\n",
    "X_test_n = pd.DataFrame(normalized, columns = test.iloc[:,1:].columns)\n",
    "\n",
    "stand = StandardScaler()\n",
    "scaled = stand.fit_transform(X_test_n.iloc[:,1:])#Data leackage fix!!!!!\n",
    "X_test_n = pd.DataFrame(scaled, columns = X_test_n.iloc[:,1:].columns)\n",
    "\n",
    "X_test = torch.from_numpy(X_test_n.to_numpy().astype('float32')).unsqueeze(-1) \n",
    "y_test = torch.from_numpy(test.iloc[:,0].to_numpy().astype('float32')).unsqueeze(1).unsqueeze(1)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "X_test = X_test.to(device)\n",
    "y_test = y_test.to(device)\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    y_pred = model_red(X_test, None, None, None)\n",
    "\n",
    "r2_metric = R2Score() \n",
    "criterion = torch.nn.MSELoss()\n",
    "r2_metric.update(y_pred.squeeze(), y_test.squeeze())\n",
    "test_r2 = r2_metric.compute()\n",
    "test_rmse = sqrt(criterion(y_test, y_pred))\n",
    "\n",
    "\n",
    "print(f\"Test RMSE {test_rmse:.6f} — Test R2 {test_r2:.6f}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Oversample minority classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13.0      102\n",
       "130.0     102\n",
       "1300.0    102\n",
       "1.3        30\n",
       "0.0        20\n",
       "Name: Concentration, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['Concentration'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = model_selection.train_test_split(data, test_size=0.1, random_state=1, stratify= data['Concentration'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13.0      92\n",
       "1300.0    92\n",
       "130.0     91\n",
       "1.3       27\n",
       "0.0       18\n",
       "Name: Concentration, dtype: int64"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['Concentration'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "train['Concentration'].replace([0.0, 1.3, 13.0, 130.0, 1300.0],[0, 1, 2, 3, 4], inplace=True)\n",
    "\n",
    "smote = SMOTE(sampling_strategy={0 : 70, 1: 70})\n",
    "X_resampled, y_resampled = smote.fit_resample(train.iloc[:,1:], train['Concentration'])\n",
    "\n",
    "y_resampled = pd.DataFrame(y_resampled, columns=[\"Concentration\"])\n",
    "y_resampled['Concentration'].replace([0, 1, 2, 3, 4], [0.0, 1.3, 13.0, 130.0, 1300.0], inplace=True)\n",
    "train['Concentration'].replace([0, 1, 2, 3, 4], [0.0, 1.3, 13.0, 130.0, 1300.0], inplace=True)\n",
    "\n",
    "oversampled = pd.concat([y_resampled, X_resampled], axis = 1) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13.0      92\n",
       "1300.0    92\n",
       "130.0     91\n",
       "0.0       70\n",
       "1.3       70\n",
       "Name: Concentration, dtype: int64"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oversampled['Concentration'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d28d748b15a4737afd0a3278e1ec7bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  1 — train RMSE 593.641703 - train R2 -0.316990 — val RMSE 702.236399 - val R2 -0.464091\n",
      "Epoch  2 — train RMSE 566.706380 - train R2 -0.200189 — val RMSE 657.431760 - val R2 -0.283225\n",
      "Epoch  3 — train RMSE 539.815623 - train R2 -0.088992 — val RMSE 628.685563 - val R2 -0.173460\n",
      "Epoch  4 — train RMSE 535.783225 - train R2 -0.072783 — val RMSE 616.608485 - val R2 -0.128809\n",
      "Epoch  5 — train RMSE 513.406271 - train R2 0.014956 — val RMSE 590.490068 - val R2 -0.035206\n",
      "Epoch  6 — train RMSE 503.490061 - train R2 0.052639 — val RMSE 574.554885 - val R2 0.019913\n",
      "Epoch  7 — train RMSE 480.828113 - train R2 0.136001 — val RMSE 539.836692 - val R2 0.134781\n",
      "Epoch  8 — train RMSE 493.570057 - train R2 0.089602 — val RMSE 540.229189 - val R2 0.133522\n",
      "Epoch  9 — train RMSE 456.939802 - train R2 0.219718 — val RMSE 504.981248 - val R2 0.242902\n",
      "Epoch 10 — train RMSE 446.412113 - train R2 0.255259 — val RMSE 465.117741 - val R2 0.357716\n",
      "Epoch 11 — train RMSE 411.343704 - train R2 0.367671 — val RMSE 438.441379 - val R2 0.429278\n",
      "Epoch 12 — train RMSE 393.137723 - train R2 0.422406 — val RMSE 423.487946 - val R2 0.467544\n",
      "Epoch 13 — train RMSE 388.763985 - train R2 0.435186 — val RMSE 387.520066 - val R2 0.554149\n",
      "Epoch 14 — train RMSE 346.077614 - train R2 0.552410 — val RMSE 361.350673 - val R2 0.612333\n",
      "Epoch 15 — train RMSE 338.972977 - train R2 0.570598 — val RMSE 323.624011 - val R2 0.689056\n",
      "Epoch 16 — train RMSE 321.510292 - train R2 0.613701 — val RMSE 354.539628 - val R2 0.626809\n",
      "Epoch 17 — train RMSE 333.899411 - train R2 0.583356 — val RMSE 283.803388 - val R2 0.760869\n",
      "Epoch 18 — train RMSE 283.403199 - train R2 0.699847 — val RMSE 243.922088 - val R2 0.823354\n",
      "Epoch 19 — train RMSE 283.390762 - train R2 0.699873 — val RMSE 279.068187 - val R2 0.768782\n",
      "Epoch 20 — train RMSE 277.731360 - train R2 0.711741 — val RMSE 192.222978 - val R2 0.890299\n",
      "Epoch 21 — train RMSE 232.196731 - train R2 0.798514 — val RMSE 198.404042 - val R2 0.883130\n",
      "Epoch 22 — train RMSE 228.515779 - train R2 0.804851 — val RMSE 179.761862 - val R2 0.904061\n",
      "Epoch 23 — train RMSE 294.201972 - train R2 0.676537 — val RMSE 284.147097 - val R2 0.760289\n",
      "Epoch 24 — train RMSE 210.104959 - train R2 0.835030 — val RMSE 277.828818 - val R2 0.770831\n",
      "Epoch 25 — train RMSE 179.393580 - train R2 0.879733 — val RMSE 220.973225 - val R2 0.855029\n",
      "Epoch 26 — train RMSE 219.589988 - train R2 0.819798 — val RMSE 123.470878 - val R2 0.954738\n",
      "Epoch 27 — train RMSE 215.870320 - train R2 0.825852 — val RMSE 115.719347 - val R2 0.960243\n",
      "Epoch 28 — train RMSE 191.497594 - train R2 0.862956 — val RMSE 108.213846 - val R2 0.965233\n",
      "Epoch 29 — train RMSE 165.759416 - train R2 0.897319 — val RMSE 164.559555 - val R2 0.919602\n",
      "Epoch 30 — train RMSE 212.975577 - train R2 0.830491 — val RMSE 86.899313 - val R2 0.977580\n",
      "Epoch 31 — train RMSE 190.583286 - train R2 0.864262 — val RMSE 111.876264 - val R2 0.962840\n",
      "Epoch 32 — train RMSE 251.994356 - train R2 0.762690 — val RMSE 142.454484 - val R2 0.939750\n",
      "Epoch 33 — train RMSE 162.227208 - train R2 0.901648 — val RMSE 113.029860 - val R2 0.962070\n",
      "Epoch 34 — train RMSE 175.930983 - train R2 0.884331 — val RMSE 112.085653 - val R2 0.962701\n",
      "Epoch 35 — train RMSE 224.247863 - train R2 0.812073 — val RMSE 152.803845 - val R2 0.930678\n",
      "Epoch 36 — train RMSE 199.230536 - train R2 0.851664 — val RMSE 111.854593 - val R2 0.962854\n",
      "Epoch 37 — train RMSE 188.247704 - train R2 0.867568 — val RMSE 91.963447 - val R2 0.974891\n",
      "Epoch 38 — train RMSE 200.218573 - train R2 0.850189 — val RMSE 93.357286 - val R2 0.974124\n",
      "Epoch 39 — train RMSE 250.521099 - train R2 0.765457 — val RMSE 238.734755 - val R2 0.830787\n",
      "Epoch 40 — train RMSE 198.935394 - train R2 0.852104 — val RMSE 133.892021 - val R2 0.946776\n",
      "Epoch 41 — train RMSE 175.460999 - train R2 0.884948 — val RMSE 92.011901 - val R2 0.974864\n",
      "Epoch 42 — train RMSE 147.397013 - train R2 0.918808 — val RMSE 84.166806 - val R2 0.978968\n",
      "Epoch 43 — train RMSE 162.345391 - train R2 0.901505 — val RMSE 67.901746 - val R2 0.986311\n",
      "Epoch 44 — train RMSE 201.633304 - train R2 0.848065 — val RMSE 92.611830 - val R2 0.974536\n",
      "Epoch 45 — train RMSE 256.121160 - train R2 0.754854 — val RMSE 117.287211 - val R2 0.959158\n",
      "Epoch 46 — train RMSE 212.779177 - train R2 0.830803 — val RMSE 237.610408 - val R2 0.832378\n",
      "Epoch 47 — train RMSE 222.631054 - train R2 0.814773 — val RMSE 91.507739 - val R2 0.975139\n",
      "Epoch 48 — train RMSE 175.735764 - train R2 0.884587 — val RMSE 126.446354 - val R2 0.952531\n",
      "Epoch 49 — train RMSE 210.936821 - train R2 0.833721 — val RMSE 114.688735 - val R2 0.960948\n",
      "Epoch 50 — train RMSE 199.494970 - train R2 0.851270 — val RMSE 105.259504 - val R2 0.967105\n"
     ]
    }
   ],
   "source": [
    "train, validation = model_selection.train_test_split(oversampled, test_size=0.1, random_state=1, stratify= train['Concentration'])\n",
    "\n",
    "model_overs = train_model(configs, train, validation, 50, 8, True, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test RMSE 237.536181 — Test R2 0.818570\n"
     ]
    }
   ],
   "source": [
    "model_overs.eval()\n",
    "\n",
    "#Normalize test data\n",
    "test.columns = test.columns.astype(str)\n",
    "scaler = StandardScaler()\n",
    "normalized = scaler.fit_transform(test.iloc[:,1:].T).T\n",
    "X_test_n = pd.DataFrame(normalized, columns = test.iloc[:,1:].columns)\n",
    "\n",
    "stand = StandardScaler()\n",
    "scaled = stand.fit_transform(X_test_n.iloc[:,1:])#Data leackage fix!!!!!\n",
    "X_test_n = pd.DataFrame(scaled, columns = X_test_n.iloc[:,1:].columns)\n",
    "\n",
    "X_test = torch.from_numpy(X_test_n.to_numpy().astype('float32')).unsqueeze(-1) \n",
    "y_test = torch.from_numpy(test.iloc[:,0].to_numpy().astype('float32')).unsqueeze(1).unsqueeze(1)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "X_test = X_test.to(device)\n",
    "y_test = y_test.to(device)\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    y_pred = model_overs(X_test, None, None, None)\n",
    "\n",
    "r2_metric = R2Score() \n",
    "criterion = torch.nn.MSELoss()\n",
    "r2_metric.update(y_pred.squeeze(), y_test.squeeze())\n",
    "test_r2 = r2_metric.compute()\n",
    "test_rmse = sqrt(criterion(y_test, y_pred))\n",
    "\n",
    "\n",
    "print(f\"Test RMSE {test_rmse:.6f} — Test R2 {test_r2:.6f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Other LLMs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GPT 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "configs = Details(\n",
    "    pred_len = 1,      \n",
    "    seq_len  = 350,\n",
    "    d_ff = 32,\n",
    "    patch_len= 16, \n",
    "    stride = 8, \n",
    "    llm_layers = 12,\n",
    "    description = \"This dataset contains near-infrared (NIR) spectroscopy measurements of lactate concentration in a \" \\\n",
    "                  \"phosphate-buffered saline (PBS) solution. Each spectrum includes absorbance values across \" \\\n",
    "                  \"350 wavelengths. \" ,\n",
    "    dropout = 0.1,\n",
    "    n_heads = 2,\n",
    "    d_model = 16,\n",
    "    enc_in = 1,\n",
    "    d_llm = 768,\n",
    "    llm_model = 'GPT2',\n",
    "    lr = 0.0001\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  1 — train RMSE 699.866052 - train R2 -0.534255 — val RMSE 689.531963 - val R2 -0.507918\n",
      "Epoch  2 — train RMSE 691.786341 - train R2 -0.499034 — val RMSE 669.607295 - val R2 -0.422031\n",
      "Epoch  3 — train RMSE 673.612560 - train R2 -0.421307 — val RMSE 653.521955 - val R2 -0.354532\n",
      "Epoch  4 — train RMSE 659.323884 - train R2 -0.361649 — val RMSE 637.989659 - val R2 -0.290910\n",
      "Epoch  5 — train RMSE 650.613045 - train R2 -0.325907 — val RMSE 628.292888 - val R2 -0.251968\n",
      "Epoch  6 — train RMSE 640.812781 - train R2 -0.286263 — val RMSE 618.309426 - val R2 -0.212497\n",
      "Epoch  7 — train RMSE 632.917790 - train R2 -0.254764 — val RMSE 610.643871 - val R2 -0.182619\n",
      "Epoch  8 — train RMSE 625.976690 - train R2 -0.227394 — val RMSE 602.727855 - val R2 -0.152156\n",
      "Epoch  9 — train RMSE 616.776536 - train R2 -0.191580 — val RMSE 592.097945 - val R2 -0.111875\n",
      "Epoch 10 — train RMSE 619.583623 - train R2 -0.202451 — val RMSE 583.461104 - val R2 -0.079674\n",
      "Epoch 11 — train RMSE 599.243970 - train R2 -0.124799 — val RMSE 571.837902 - val R2 -0.037086\n",
      "Epoch 12 — train RMSE 591.179729 - train R2 -0.094729 — val RMSE 556.937518 - val R2 0.016257\n",
      "Epoch 13 — train RMSE 571.487609 - train R2 -0.023013 — val RMSE 547.167954 - val R2 0.050467\n",
      "Epoch 14 — train RMSE 569.681647 - train R2 -0.016558 — val RMSE 544.385157 - val R2 0.060101\n",
      "Epoch 15 — train RMSE 545.787258 - train R2 0.066930 — val RMSE 520.657363 - val R2 0.140249\n",
      "Epoch 16 — train RMSE 541.745376 - train R2 0.080698 — val RMSE 513.677097 - val R2 0.163147\n",
      "Epoch 17 — train RMSE 549.558730 - train R2 0.053990 — val RMSE 486.245557 - val R2 0.250140\n",
      "Epoch 18 — train RMSE 514.862466 - train R2 0.169672 — val RMSE 472.395243 - val R2 0.292250\n",
      "Epoch 19 — train RMSE 489.076265 - train R2 0.250760 — val RMSE 450.849909 - val R2 0.355337\n",
      "Epoch 20 — train RMSE 492.092679 - train R2 0.241490 — val RMSE 439.437789 - val R2 0.387560\n",
      "Epoch 21 — train RMSE 501.262799 - train R2 0.212957 — val RMSE 421.716202 - val R2 0.435961\n",
      "Epoch 22 — train RMSE 458.775257 - train R2 0.340724 — val RMSE 405.467559 - val R2 0.478588\n",
      "Epoch 23 — train RMSE 458.856736 - train R2 0.340490 — val RMSE 395.156377 - val R2 0.504770\n",
      "Epoch 24 — train RMSE 435.827869 - train R2 0.405027 — val RMSE 387.991385 - val R2 0.522566\n",
      "Epoch 25 — train RMSE 424.153804 - train R2 0.436473 — val RMSE 361.136879 - val R2 0.586370\n",
      "Epoch 26 — train RMSE 400.015264 - train R2 0.498789 — val RMSE 345.606549 - val R2 0.621180\n",
      "Epoch 27 — train RMSE 396.015047 - train R2 0.508763 — val RMSE 336.040429 - val R2 0.641861\n",
      "Epoch 28 — train RMSE 401.589476 - train R2 0.494836 — val RMSE 319.395563 - val R2 0.676461\n",
      "Epoch 29 — train RMSE 378.013385 - train R2 0.552408 — val RMSE 325.722598 - val R2 0.663516\n",
      "Epoch 30 — train RMSE 379.466913 - train R2 0.548960 — val RMSE 300.468332 - val R2 0.713670\n",
      "Epoch 31 — train RMSE 391.465923 - train R2 0.519984 — val RMSE 283.996479 - val R2 0.744203\n",
      "Epoch 32 — train RMSE 346.204419 - train R2 0.624567 — val RMSE 280.868724 - val R2 0.749807\n",
      "Epoch 33 — train RMSE 335.541564 - train R2 0.647337 — val RMSE 261.689256 - val R2 0.782810\n",
      "Epoch 34 — train RMSE 363.591366 - train R2 0.585910 — val RMSE 252.159257 - val R2 0.798341\n",
      "Epoch 35 — train RMSE 406.029608 - train R2 0.483604 — val RMSE 243.547574 - val R2 0.811879\n",
      "Epoch 36 — train RMSE 339.697922 - train R2 0.638546 — val RMSE 232.671054 - val R2 0.828307\n",
      "Epoch 37 — train RMSE 333.057139 - train R2 0.652540 — val RMSE 239.563943 - val R2 0.817983\n",
      "Epoch 38 — train RMSE 296.318872 - train R2 0.724966 — val RMSE 215.841060 - val R2 0.852247\n",
      "Epoch 39 — train RMSE 288.594578 - train R2 0.739118 — val RMSE 207.778503 - val R2 0.863079\n",
      "Epoch 40 — train RMSE 311.172123 - train R2 0.696702 — val RMSE 198.530743 - val R2 0.874996\n",
      "Epoch 41 — train RMSE 310.678941 - train R2 0.697663 — val RMSE 189.804222 - val R2 0.885744\n",
      "Epoch 42 — train RMSE 255.219488 - train R2 0.795969 — val RMSE 181.345417 - val R2 0.895701\n",
      "Epoch 43 — train RMSE 264.404116 - train R2 0.781020 — val RMSE 172.727819 - val R2 0.905378\n",
      "Epoch 44 — train RMSE 259.384393 - train R2 0.789256 — val RMSE 166.296541 - val R2 0.912293\n",
      "Epoch 45 — train RMSE 274.180006 - train R2 0.764528 — val RMSE 158.217602 - val R2 0.920608\n",
      "Epoch 46 — train RMSE 280.241953 - train R2 0.754001 — val RMSE 160.566893 - val R2 0.918233\n",
      "Epoch 47 — train RMSE 247.404193 - train R2 0.808274 — val RMSE 146.150677 - val R2 0.932256\n",
      "Epoch 48 — train RMSE 277.659408 - train R2 0.758514 — val RMSE 149.562581 - val R2 0.929056\n",
      "Epoch 49 — train RMSE 243.609662 - train R2 0.814110 — val RMSE 147.284980 - val R2 0.931200\n",
      "Epoch 50 — train RMSE 292.039981 - train R2 0.732852 — val RMSE 153.862927 - val R2 0.924918\n",
      "Epoch 51 — train RMSE 246.324529 - train R2 0.809943 — val RMSE 126.549129 - val R2 0.949209\n",
      "Epoch 52 — train RMSE 286.618225 - train R2 0.742679 — val RMSE 120.929908 - val R2 0.953619\n",
      "Epoch 53 — train RMSE 275.569962 - train R2 0.762135 — val RMSE 126.065604 - val R2 0.949596\n",
      "Epoch 54 — train RMSE 239.493136 - train R2 0.820339 — val RMSE 116.174478 - val R2 0.957195\n",
      "Epoch 55 — train RMSE 211.790259 - train R2 0.859499 — val RMSE 111.562113 - val R2 0.960527\n",
      "Epoch 56 — train RMSE 275.905237 - train R2 0.761555 — val RMSE 142.577506 - val R2 0.935528\n",
      "Epoch 57 — train RMSE 200.522786 - train R2 0.874051 — val RMSE 118.890342 - val R2 0.955171\n",
      "Epoch 58 — train RMSE 263.173091 - train R2 0.783055 — val RMSE 125.045419 - val R2 0.950409\n",
      "Epoch 59 — train RMSE 273.693869 - train R2 0.765362 — val RMSE 105.812147 - val R2 0.964491\n",
      "Epoch 60 — train RMSE 264.268224 - train R2 0.781245 — val RMSE 180.759216 - val R2 0.896374\n",
      "Epoch 61 — train RMSE 271.200618 - train R2 0.769618 — val RMSE 279.216861 - val R2 0.752741\n",
      "Epoch 62 — train RMSE 290.185192 - train R2 0.736234 — val RMSE 149.339352 - val R2 0.929268\n",
      "Epoch 63 — train RMSE 150.891664 - train R2 0.928682 — val RMSE 125.183185 - val R2 0.950300\n",
      "Epoch 64 — train RMSE 205.643280 - train R2 0.867536 — val RMSE 95.213235 - val R2 0.971248\n",
      "Epoch 65 — train RMSE 161.314645 - train R2 0.918489 — val RMSE 92.560237 - val R2 0.972828\n",
      "Epoch 66 — train RMSE 203.367843 - train R2 0.870452 — val RMSE 87.740558 - val R2 0.975584\n",
      "Epoch 67 — train RMSE 207.842963 - train R2 0.864687 — val RMSE 85.150097 - val R2 0.977005\n",
      "Epoch 68 — train RMSE 218.479357 - train R2 0.850484 — val RMSE 98.721904 - val R2 0.969090\n",
      "Epoch 69 — train RMSE 201.125191 - train R2 0.873293 — val RMSE 97.783222 - val R2 0.969675\n",
      "Epoch 70 — train RMSE 240.931767 - train R2 0.818174 — val RMSE 76.998583 - val R2 0.981197\n",
      "Epoch 71 — train RMSE 201.112552 - train R2 0.873309 — val RMSE 75.814841 - val R2 0.981770\n",
      "Epoch 72 — train RMSE 240.059557 - train R2 0.819488 — val RMSE 98.983379 - val R2 0.968926\n",
      "Epoch 73 — train RMSE 199.370586 - train R2 0.875494 — val RMSE 69.128811 - val R2 0.984844\n",
      "Epoch 74 — train RMSE 212.180944 - train R2 0.858980 — val RMSE 85.412724 - val R2 0.976863\n",
      "Epoch 75 — train RMSE 238.514571 - train R2 0.821804 — val RMSE 67.733594 - val R2 0.985450\n",
      "Epoch 76 — train RMSE 213.627364 - train R2 0.857051 — val RMSE 66.025043 - val R2 0.986174\n",
      "Epoch 77 — train RMSE 212.840556 - train R2 0.858102 — val RMSE 86.659644 - val R2 0.976182\n",
      "Epoch 78 — train RMSE 212.360823 - train R2 0.858741 — val RMSE 67.597529 - val R2 0.985508\n",
      "Epoch 79 — train RMSE 272.437496 - train R2 0.767512 — val RMSE 79.371399 - val R2 0.980020\n",
      "Epoch 80 — train RMSE 212.182430 - train R2 0.858978 — val RMSE 64.504136 - val R2 0.986804\n",
      "Epoch 81 — train RMSE 197.483765 - train R2 0.877840 — val RMSE 65.124234 - val R2 0.986549\n",
      "Epoch 82 — train RMSE 197.204129 - train R2 0.878185 — val RMSE 64.065109 - val R2 0.986983\n",
      "Epoch 83 — train RMSE 197.782725 - train R2 0.877469 — val RMSE 101.759685 - val R2 0.967159\n",
      "Epoch 84 — train RMSE 260.891819 - train R2 0.786799 — val RMSE 359.436980 - val R2 0.590254\n",
      "Epoch 85 — train RMSE 318.612225 - train R2 0.682025 — val RMSE 229.660433 - val R2 0.832721\n",
      "Epoch 86 — train RMSE 261.159597 - train R2 0.786362 — val RMSE 233.194399 - val R2 0.827533\n",
      "Epoch 87 — train RMSE 197.843222 - train R2 0.877395 — val RMSE 68.070765 - val R2 0.985304\n",
      "Epoch 88 — train RMSE 305.140132 - train R2 0.708347 — val RMSE 68.837365 - val R2 0.984971\n",
      "Epoch 89 — train RMSE 275.740314 - train R2 0.761840 — val RMSE 69.485968 - val R2 0.984687\n",
      "Epoch 90 — train RMSE 250.593854 - train R2 0.803298 — val RMSE 70.382549 - val R2 0.984289\n",
      "Epoch 91 — train RMSE 284.202666 - train R2 0.746998 — val RMSE 65.543279 - val R2 0.986375\n",
      "Epoch 92 — train RMSE 197.664776 - train R2 0.877616 — val RMSE 62.636387 - val R2 0.987557\n",
      "Epoch 93 — train RMSE 283.314803 - train R2 0.748576 — val RMSE 61.368711 - val R2 0.988056\n",
      "Epoch 94 — train RMSE 240.557109 - train R2 0.818739 — val RMSE 65.164123 - val R2 0.986533\n",
      "Epoch 95 — train RMSE 261.384760 - train R2 0.785993 — val RMSE 61.370587 - val R2 0.988055\n",
      "Epoch 96 — train RMSE 210.936391 - train R2 0.860630 — val RMSE 60.877494 - val R2 0.988246\n",
      "Epoch 97 — train RMSE 212.692590 - train R2 0.858299 — val RMSE 60.153687 - val R2 0.988524\n",
      "Epoch 98 — train RMSE 196.289705 - train R2 0.879312 — val RMSE 59.547015 - val R2 0.988754\n",
      "Epoch 99 — train RMSE 210.951386 - train R2 0.860610 — val RMSE 64.000847 - val R2 0.987009\n",
      "Epoch 100 — train RMSE 271.003435 - train R2 0.769953 — val RMSE 61.341054 - val R2 0.988066\n"
     ]
    }
   ],
   "source": [
    "train, test = model_selection.train_test_split(data, test_size=0.1, random_state=1, stratify= data['Concentration'])\n",
    "\n",
    "train, validation = model_selection.train_test_split(train, test_size=0.1, random_state=1, stratify= train['Concentration'])\n",
    "\n",
    "model ,train_rmse, train_r2, val_rmse, val_r2, stand  = train_model(configs, train, validation, 100, 8, True, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Hyperparameter tunning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial  1  config : {'pred_len': 1, 'seq_len': 350, 'd_ff': 32, 'patch_len': 16, 'stride': 8, 'llm_layers': 4, 'content': 'This dataset contains near-infrared (NIR) spectroscopy measurements of lactate concentration in a phosphate-buffered saline (PBS) solution. Each spectrum includes absorbance values across 350 wavelengths. ', 'dropout': 0.1, 'd_model': 16, 'enc_in': 1, 'n_heads': 16, 'd_llm': 768, 'llm_model': 'GPT2', 'lr': 0.001}, 8\n",
      "Trial  1  results — train RMSE 235.557973 - train R2 0.826195 — val RMSE 92.081445 - val R2 0.973109\n",
      "Trial  2  config : {'pred_len': 1, 'seq_len': 350, 'd_ff': 32, 'patch_len': 16, 'stride': 8, 'llm_layers': 8, 'content': 'This dataset contains near-infrared (NIR) spectroscopy measurements of lactate concentration in a phosphate-buffered saline (PBS) solution. Each spectrum includes absorbance values across 350 wavelengths. ', 'dropout': 0.1, 'd_model': 16, 'enc_in': 1, 'n_heads': 8, 'd_llm': 768, 'llm_model': 'GPT2', 'lr': 0.001}, 8\n",
      "Trial  2  results — train RMSE 271.369398 - train R2 0.769331 — val RMSE 141.070603 - val R2 0.936884\n",
      "Trial  3  config : {'pred_len': 1, 'seq_len': 350, 'd_ff': 64, 'patch_len': 16, 'stride': 8, 'llm_layers': 8, 'content': 'This dataset contains near-infrared (NIR) spectroscopy measurements of lactate concentration in a phosphate-buffered saline (PBS) solution. Each spectrum includes absorbance values across 350 wavelengths. ', 'dropout': 0.1, 'd_model': 16, 'enc_in': 1, 'n_heads': 16, 'd_llm': 768, 'llm_model': 'GPT2', 'lr': 0.001}, 16\n",
      "Trial  3  results — train RMSE 186.358855 - train R2 0.891215 — val RMSE 85.275181 - val R2 0.976937\n",
      "Trial  4  config : {'pred_len': 1, 'seq_len': 350, 'd_ff': 64, 'patch_len': 16, 'stride': 8, 'llm_layers': 8, 'content': 'This dataset contains near-infrared (NIR) spectroscopy measurements of lactate concentration in a phosphate-buffered saline (PBS) solution. Each spectrum includes absorbance values across 350 wavelengths. ', 'dropout': 0.1, 'd_model': 16, 'enc_in': 1, 'n_heads': 4, 'd_llm': 768, 'llm_model': 'GPT2', 'lr': 0.001}, 8\n",
      "Trial  4  results — train RMSE 294.435278 - train R2 0.728452 — val RMSE 208.185919 - val R2 0.862542\n",
      "Trial  5  config : {'pred_len': 1, 'seq_len': 350, 'd_ff': 64, 'patch_len': 16, 'stride': 8, 'llm_layers': 12, 'content': 'This dataset contains near-infrared (NIR) spectroscopy measurements of lactate concentration in a phosphate-buffered saline (PBS) solution. Each spectrum includes absorbance values across 350 wavelengths. ', 'dropout': 0.1, 'd_model': 16, 'enc_in': 1, 'n_heads': 16, 'd_llm': 768, 'llm_model': 'GPT2', 'lr': 0.001}, 16\n",
      "Trial  5  results — train RMSE 258.884734 - train R2 0.790067 — val RMSE 138.155660 - val R2 0.939465\n",
      "Trial  6  config : {'pred_len': 1, 'seq_len': 350, 'd_ff': 64, 'patch_len': 16, 'stride': 8, 'llm_layers': 12, 'content': 'This dataset contains near-infrared (NIR) spectroscopy measurements of lactate concentration in a phosphate-buffered saline (PBS) solution. Each spectrum includes absorbance values across 350 wavelengths. ', 'dropout': 0.1, 'd_model': 16, 'enc_in': 1, 'n_heads': 16, 'd_llm': 768, 'llm_model': 'GPT2', 'lr': 0.001}, 8\n",
      "Trial  6  results — train RMSE 263.907514 - train R2 0.781842 — val RMSE 127.780207 - val R2 0.948216\n",
      "Trial  7  config : {'pred_len': 1, 'seq_len': 350, 'd_ff': 64, 'patch_len': 16, 'stride': 8, 'llm_layers': 8, 'content': 'This dataset contains near-infrared (NIR) spectroscopy measurements of lactate concentration in a phosphate-buffered saline (PBS) solution. Each spectrum includes absorbance values across 350 wavelengths. ', 'dropout': 0.1, 'd_model': 16, 'enc_in': 1, 'n_heads': 16, 'd_llm': 768, 'llm_model': 'GPT2', 'lr': 0.001}, 16\n",
      "Trial  7  results — train RMSE 254.736480 - train R2 0.796741 — val RMSE 100.400908 - val R2 0.968030\n",
      "Trial  8  config : {'pred_len': 1, 'seq_len': 350, 'd_ff': 32, 'patch_len': 16, 'stride': 8, 'llm_layers': 4, 'content': 'This dataset contains near-infrared (NIR) spectroscopy measurements of lactate concentration in a phosphate-buffered saline (PBS) solution. Each spectrum includes absorbance values across 350 wavelengths. ', 'dropout': 0.1, 'd_model': 16, 'enc_in': 1, 'n_heads': 16, 'd_llm': 768, 'llm_model': 'GPT2', 'lr': 0.001}, 8\n",
      "Trial  8  results — train RMSE 280.470362 - train R2 0.753600 — val RMSE 96.633466 - val R2 0.970384\n",
      "Trial  9  config : {'pred_len': 1, 'seq_len': 350, 'd_ff': 64, 'patch_len': 16, 'stride': 8, 'llm_layers': 12, 'content': 'This dataset contains near-infrared (NIR) spectroscopy measurements of lactate concentration in a phosphate-buffered saline (PBS) solution. Each spectrum includes absorbance values across 350 wavelengths. ', 'dropout': 0.1, 'd_model': 16, 'enc_in': 1, 'n_heads': 16, 'd_llm': 768, 'llm_model': 'GPT2', 'lr': 0.001}, 16\n",
      "Trial  9  results — train RMSE 180.031820 - train R2 0.898477 — val RMSE 144.308812 - val R2 0.933953\n",
      "Trial 10  config : {'pred_len': 1, 'seq_len': 350, 'd_ff': 32, 'patch_len': 16, 'stride': 8, 'llm_layers': 8, 'content': 'This dataset contains near-infrared (NIR) spectroscopy measurements of lactate concentration in a phosphate-buffered saline (PBS) solution. Each spectrum includes absorbance values across 350 wavelengths. ', 'dropout': 0.1, 'd_model': 16, 'enc_in': 1, 'n_heads': 8, 'd_llm': 768, 'llm_model': 'GPT2', 'lr': 0.001}, 8\n",
      "Trial 10  results — train RMSE 292.910264 - train R2 0.731257 — val RMSE 94.340189 - val R2 0.971773\n",
      "Trial 11  config : {'pred_len': 1, 'seq_len': 350, 'd_ff': 64, 'patch_len': 16, 'stride': 8, 'llm_layers': 4, 'content': 'This dataset contains near-infrared (NIR) spectroscopy measurements of lactate concentration in a phosphate-buffered saline (PBS) solution. Each spectrum includes absorbance values across 350 wavelengths. ', 'dropout': 0.1, 'd_model': 16, 'enc_in': 1, 'n_heads': 8, 'd_llm': 768, 'llm_model': 'GPT2', 'lr': 0.001}, 8\n",
      "Trial 11  results — train RMSE 277.857205 - train R2 0.758170 — val RMSE 180.494690 - val R2 0.896677\n",
      "Trial 12  config : {'pred_len': 1, 'seq_len': 350, 'd_ff': 64, 'patch_len': 16, 'stride': 8, 'llm_layers': 8, 'content': 'This dataset contains near-infrared (NIR) spectroscopy measurements of lactate concentration in a phosphate-buffered saline (PBS) solution. Each spectrum includes absorbance values across 350 wavelengths. ', 'dropout': 0.1, 'd_model': 16, 'enc_in': 1, 'n_heads': 4, 'd_llm': 768, 'llm_model': 'GPT2', 'lr': 0.001}, 8\n",
      "Trial 12  results — train RMSE 351.619162 - train R2 0.612731 — val RMSE 249.043766 - val R2 0.803293\n",
      "Trial 13  config : {'pred_len': 1, 'seq_len': 350, 'd_ff': 64, 'patch_len': 16, 'stride': 8, 'llm_layers': 4, 'content': 'This dataset contains near-infrared (NIR) spectroscopy measurements of lactate concentration in a phosphate-buffered saline (PBS) solution. Each spectrum includes absorbance values across 350 wavelengths. ', 'dropout': 0.1, 'd_model': 16, 'enc_in': 1, 'n_heads': 16, 'd_llm': 768, 'llm_model': 'GPT2', 'lr': 0.001}, 16\n",
      "Trial 13  results — train RMSE 221.665140 - train R2 0.846092 — val RMSE 86.787830 - val R2 0.976112\n",
      "Trial 14  config : {'pred_len': 1, 'seq_len': 350, 'd_ff': 32, 'patch_len': 16, 'stride': 8, 'llm_layers': 8, 'content': 'This dataset contains near-infrared (NIR) spectroscopy measurements of lactate concentration in a phosphate-buffered saline (PBS) solution. Each spectrum includes absorbance values across 350 wavelengths. ', 'dropout': 0.1, 'd_model': 16, 'enc_in': 1, 'n_heads': 16, 'd_llm': 768, 'llm_model': 'GPT2', 'lr': 0.001}, 8\n",
      "Trial 14  results — train RMSE 286.088948 - train R2 0.743629 — val RMSE 192.397508 - val R2 0.882600\n",
      "Trial 15  config : {'pred_len': 1, 'seq_len': 350, 'd_ff': 64, 'patch_len': 16, 'stride': 8, 'llm_layers': 4, 'content': 'This dataset contains near-infrared (NIR) spectroscopy measurements of lactate concentration in a phosphate-buffered saline (PBS) solution. Each spectrum includes absorbance values across 350 wavelengths. ', 'dropout': 0.1, 'd_model': 16, 'enc_in': 1, 'n_heads': 4, 'd_llm': 768, 'llm_model': 'GPT2', 'lr': 0.001}, 16\n",
      "Trial 15  results — train RMSE 272.205987 - train R2 0.767907 — val RMSE 107.035600 - val R2 0.963665\n",
      "Trial 16  config : {'pred_len': 1, 'seq_len': 350, 'd_ff': 64, 'patch_len': 16, 'stride': 8, 'llm_layers': 12, 'content': 'This dataset contains near-infrared (NIR) spectroscopy measurements of lactate concentration in a phosphate-buffered saline (PBS) solution. Each spectrum includes absorbance values across 350 wavelengths. ', 'dropout': 0.1, 'd_model': 16, 'enc_in': 1, 'n_heads': 4, 'd_llm': 768, 'llm_model': 'GPT2', 'lr': 0.001}, 8\n",
      "Trial 16  results — train RMSE 284.214298 - train R2 0.746977 — val RMSE 84.358816 - val R2 0.977430\n",
      "Trial 17  config : {'pred_len': 1, 'seq_len': 350, 'd_ff': 32, 'patch_len': 16, 'stride': 8, 'llm_layers': 8, 'content': 'This dataset contains near-infrared (NIR) spectroscopy measurements of lactate concentration in a phosphate-buffered saline (PBS) solution. Each spectrum includes absorbance values across 350 wavelengths. ', 'dropout': 0.1, 'd_model': 16, 'enc_in': 1, 'n_heads': 4, 'd_llm': 768, 'llm_model': 'GPT2', 'lr': 0.001}, 8\n",
      "Trial 17  results — train RMSE 263.524996 - train R2 0.782474 — val RMSE 178.592769 - val R2 0.898843\n",
      "Trial 18  config : {'pred_len': 1, 'seq_len': 350, 'd_ff': 64, 'patch_len': 16, 'stride': 8, 'llm_layers': 4, 'content': 'This dataset contains near-infrared (NIR) spectroscopy measurements of lactate concentration in a phosphate-buffered saline (PBS) solution. Each spectrum includes absorbance values across 350 wavelengths. ', 'dropout': 0.1, 'd_model': 16, 'enc_in': 1, 'n_heads': 16, 'd_llm': 768, 'llm_model': 'GPT2', 'lr': 0.001}, 16\n",
      "Trial 18  results — train RMSE 291.674312 - train R2 0.733521 — val RMSE 85.243921 - val R2 0.976954\n",
      "Trial 19  config : {'pred_len': 1, 'seq_len': 350, 'd_ff': 32, 'patch_len': 16, 'stride': 8, 'llm_layers': 8, 'content': 'This dataset contains near-infrared (NIR) spectroscopy measurements of lactate concentration in a phosphate-buffered saline (PBS) solution. Each spectrum includes absorbance values across 350 wavelengths. ', 'dropout': 0.1, 'd_model': 16, 'enc_in': 1, 'n_heads': 8, 'd_llm': 768, 'llm_model': 'GPT2', 'lr': 0.001}, 8\n",
      "Trial 19  results — train RMSE 344.545356 - train R2 0.628156 — val RMSE 216.322173 - val R2 0.851587\n",
      "Trial 20  config : {'pred_len': 1, 'seq_len': 350, 'd_ff': 64, 'patch_len': 16, 'stride': 8, 'llm_layers': 4, 'content': 'This dataset contains near-infrared (NIR) spectroscopy measurements of lactate concentration in a phosphate-buffered saline (PBS) solution. Each spectrum includes absorbance values across 350 wavelengths. ', 'dropout': 0.1, 'd_model': 16, 'enc_in': 1, 'n_heads': 16, 'd_llm': 768, 'llm_model': 'GPT2', 'lr': 0.001}, 8\n",
      "Trial 20  results — train RMSE 216.198503 - train R2 0.853589 — val RMSE 78.198472 - val R2 0.980606\n",
      "Trial 21  config : {'pred_len': 1, 'seq_len': 350, 'd_ff': 32, 'patch_len': 16, 'stride': 8, 'llm_layers': 8, 'content': 'This dataset contains near-infrared (NIR) spectroscopy measurements of lactate concentration in a phosphate-buffered saline (PBS) solution. Each spectrum includes absorbance values across 350 wavelengths. ', 'dropout': 0.1, 'd_model': 16, 'enc_in': 1, 'n_heads': 8, 'd_llm': 768, 'llm_model': 'GPT2', 'lr': 0.001}, 16\n",
      "Trial 21  results — train RMSE 253.786615 - train R2 0.798254 — val RMSE 201.439236 - val R2 0.871306\n",
      "Trial 22  config : {'pred_len': 1, 'seq_len': 350, 'd_ff': 32, 'patch_len': 16, 'stride': 8, 'llm_layers': 8, 'content': 'This dataset contains near-infrared (NIR) spectroscopy measurements of lactate concentration in a phosphate-buffered saline (PBS) solution. Each spectrum includes absorbance values across 350 wavelengths. ', 'dropout': 0.1, 'd_model': 16, 'enc_in': 1, 'n_heads': 8, 'd_llm': 768, 'llm_model': 'GPT2', 'lr': 0.001}, 16\n",
      "Trial 22  results — train RMSE 263.889720 - train R2 0.781871 — val RMSE 153.319833 - val R2 0.925447\n",
      "Trial 23  config : {'pred_len': 1, 'seq_len': 350, 'd_ff': 32, 'patch_len': 16, 'stride': 8, 'llm_layers': 12, 'content': 'This dataset contains near-infrared (NIR) spectroscopy measurements of lactate concentration in a phosphate-buffered saline (PBS) solution. Each spectrum includes absorbance values across 350 wavelengths. ', 'dropout': 0.1, 'd_model': 16, 'enc_in': 1, 'n_heads': 8, 'd_llm': 768, 'llm_model': 'GPT2', 'lr': 0.001}, 16\n",
      "Trial 23  results — train RMSE 291.437150 - train R2 0.733954 — val RMSE 169.050664 - val R2 0.909364\n",
      "Trial 24  config : {'pred_len': 1, 'seq_len': 350, 'd_ff': 32, 'patch_len': 16, 'stride': 8, 'llm_layers': 12, 'content': 'This dataset contains near-infrared (NIR) spectroscopy measurements of lactate concentration in a phosphate-buffered saline (PBS) solution. Each spectrum includes absorbance values across 350 wavelengths. ', 'dropout': 0.1, 'd_model': 16, 'enc_in': 1, 'n_heads': 4, 'd_llm': 768, 'llm_model': 'GPT2', 'lr': 0.001}, 16\n",
      "Trial 24  results — train RMSE 302.969533 - train R2 0.712482 — val RMSE 237.098448 - val R2 0.821710\n",
      "Trial 25  config : {'pred_len': 1, 'seq_len': 350, 'd_ff': 32, 'patch_len': 16, 'stride': 8, 'llm_layers': 4, 'content': 'This dataset contains near-infrared (NIR) spectroscopy measurements of lactate concentration in a phosphate-buffered saline (PBS) solution. Each spectrum includes absorbance values across 350 wavelengths. ', 'dropout': 0.1, 'd_model': 16, 'enc_in': 1, 'n_heads': 8, 'd_llm': 768, 'llm_model': 'GPT2', 'lr': 0.001}, 16\n",
      "Trial 25  results — train RMSE 201.260774 - train R2 0.873122 — val RMSE 90.155588 - val R2 0.974222\n",
      "Trial 26  config : {'pred_len': 1, 'seq_len': 350, 'd_ff': 32, 'patch_len': 16, 'stride': 8, 'llm_layers': 8, 'content': 'This dataset contains near-infrared (NIR) spectroscopy measurements of lactate concentration in a phosphate-buffered saline (PBS) solution. Each spectrum includes absorbance values across 350 wavelengths. ', 'dropout': 0.1, 'd_model': 16, 'enc_in': 1, 'n_heads': 16, 'd_llm': 768, 'llm_model': 'GPT2', 'lr': 0.001}, 16\n",
      "Trial 26  results — train RMSE 228.213860 - train R2 0.836863 — val RMSE 72.640707 - val R2 0.983265\n",
      "Trial 27  config : {'pred_len': 1, 'seq_len': 350, 'd_ff': 64, 'patch_len': 16, 'stride': 8, 'llm_layers': 12, 'content': 'This dataset contains near-infrared (NIR) spectroscopy measurements of lactate concentration in a phosphate-buffered saline (PBS) solution. Each spectrum includes absorbance values across 350 wavelengths. ', 'dropout': 0.1, 'd_model': 16, 'enc_in': 1, 'n_heads': 16, 'd_llm': 768, 'llm_model': 'GPT2', 'lr': 0.001}, 16\n",
      "Trial 27  results — train RMSE 298.064503 - train R2 0.721716 — val RMSE 104.609792 - val R2 0.965293\n",
      "Trial 28  config : {'pred_len': 1, 'seq_len': 350, 'd_ff': 64, 'patch_len': 16, 'stride': 8, 'llm_layers': 8, 'content': 'This dataset contains near-infrared (NIR) spectroscopy measurements of lactate concentration in a phosphate-buffered saline (PBS) solution. Each spectrum includes absorbance values across 350 wavelengths. ', 'dropout': 0.1, 'd_model': 16, 'enc_in': 1, 'n_heads': 4, 'd_llm': 768, 'llm_model': 'GPT2', 'lr': 0.001}, 16\n",
      "Trial 28  results — train RMSE 251.094671 - train R2 0.802511 — val RMSE 141.567881 - val R2 0.936438\n",
      "Trial 29  config : {'pred_len': 1, 'seq_len': 350, 'd_ff': 32, 'patch_len': 16, 'stride': 8, 'llm_layers': 4, 'content': 'This dataset contains near-infrared (NIR) spectroscopy measurements of lactate concentration in a phosphate-buffered saline (PBS) solution. Each spectrum includes absorbance values across 350 wavelengths. ', 'dropout': 0.1, 'd_model': 16, 'enc_in': 1, 'n_heads': 4, 'd_llm': 768, 'llm_model': 'GPT2', 'lr': 0.001}, 16\n",
      "Trial 29  results — train RMSE 322.630320 - train R2 0.673955 — val RMSE 198.607395 - val R2 0.874899\n",
      "Trial 30  config : {'pred_len': 1, 'seq_len': 350, 'd_ff': 32, 'patch_len': 16, 'stride': 8, 'llm_layers': 4, 'content': 'This dataset contains near-infrared (NIR) spectroscopy measurements of lactate concentration in a phosphate-buffered saline (PBS) solution. Each spectrum includes absorbance values across 350 wavelengths. ', 'dropout': 0.1, 'd_model': 16, 'enc_in': 1, 'n_heads': 8, 'd_llm': 768, 'llm_model': 'GPT2', 'lr': 0.001}, 8\n",
      "Trial 30  results — train RMSE 251.143048 - train R2 0.802435 — val RMSE 101.589047 - val R2 0.967269\n",
      "Best config : {'pred_len': 1, 'seq_len': 350, 'd_ff': 32, 'patch_len': 16, 'stride': 8, 'llm_layers': 8, 'content': 'This dataset contains near-infrared (NIR) spectroscopy measurements of lactate concentration in a phosphate-buffered saline (PBS) solution. Each spectrum includes absorbance values across 350 wavelengths. ', 'dropout': 0.1, 'd_model': 16, 'enc_in': 1, 'n_heads': 16, 'd_llm': 768, 'llm_model': 'GPT2', 'lr': 0.001}, 16 - best val RMSE : 72.640707\n"
     ]
    }
   ],
   "source": [
    "random_trials = 30\n",
    "\n",
    "best_config = None\n",
    "best_rmse = float('inf')\n",
    "results = {}\n",
    "\n",
    "for r in range(random_trials):\n",
    "    configs = Details(\n",
    "        pred_len = 1,      \n",
    "        seq_len  = 350,\n",
    "        d_ff = random.choice([32, 64]),\n",
    "        patch_len= 16, \n",
    "        stride = 8, \n",
    "        llm_layers = random.choice([4, 8, 12]),\n",
    "        description = \"This dataset contains near-infrared (NIR) spectroscopy measurements of lactate concentration in a \" \\\n",
    "                    \"phosphate-buffered saline (PBS) solution. Each spectrum includes absorbance values across \" \\\n",
    "                    \"350 wavelengths. \" ,\n",
    "        dropout = 0.1,\n",
    "        n_heads = random.choice([4, 8, 16]),\n",
    "        d_model = 16,\n",
    "        enc_in = 1,\n",
    "        d_llm = 768,\n",
    "        llm_model = 'GPT2',\n",
    "        lr = 0.001,\n",
    "    )\n",
    "    \n",
    "    batch_size = random.choice([8, 16])\n",
    "\n",
    "    train, test = model_selection.train_test_split(data, test_size=0.1, random_state=1, stratify= data['Concentration'])\n",
    "\n",
    "    train, validation = model_selection.train_test_split(train, test_size=0.1, random_state=1, stratify= train['Concentration'])\n",
    "\n",
    "    model ,train_rmse, train_r2, val_rmse, val_r2, stand  = train_model(configs, train, validation, 30, batch_size, False, False)\n",
    "\n",
    "    results[r] = {'config': configs, 'train_rmse' : train_rmse, 'train_r2': train_r2, 'val_rmse' : val_rmse, 'val_r2' : val_r2}\n",
    "\n",
    "    if val_rmse < best_rmse:\n",
    "        best_rmse = val_rmse\n",
    "        best_config = configs\n",
    "        best_batch = batch_size\n",
    "\n",
    "    print(f\"Trial {r + 1:2d}  config : {vars(configs)}, {batch_size}\")\n",
    "    print(f\"Trial {r + 1:2d}  results — train RMSE {train_rmse:.6f} - train R2 {train_r2:.6f} — val RMSE {val_rmse:.6f} - val R2 {val_r2:.6f}\")\n",
    "print(f\"Best config : {vars(best_config)}, {best_batch} - best val RMSE : {best_rmse:.6f}\")\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train best model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "configs = Details(\n",
    "    pred_len = 1,      \n",
    "    seq_len  = 350,\n",
    "    d_ff = 32,\n",
    "    patch_len= 16, \n",
    "    stride = 8, \n",
    "    llm_layers = 8,\n",
    "    description = \"This dataset contains near-infrared (NIR) spectroscopy measurements of lactate concentration in a \" \\\n",
    "                  \"phosphate-buffered saline (PBS) solution. Each spectrum includes absorbance values across \" \\\n",
    "                  \"350 wavelengths. \" ,\n",
    "    dropout = 0.1,\n",
    "    n_heads = 16,\n",
    "    d_model = 16,\n",
    "    enc_in = 1,\n",
    "    d_llm = 768,\n",
    "    llm_model = 'GPT2',\n",
    "    lr = 0.0001\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  1 — train RMSE 699.606266 - train R2 -0.533116 — val RMSE 688.550777 - val R2 -0.503629\n",
      "Epoch  2 — train RMSE 695.441935 - train R2 -0.514919 — val RMSE 683.797713 - val R2 -0.482942\n",
      "Epoch  3 — train RMSE 689.477974 - train R2 -0.489047 — val RMSE 674.220986 - val R2 -0.441695\n",
      "Epoch  4 — train RMSE 679.620722 - train R2 -0.446774 — val RMSE 660.675648 - val R2 -0.384349\n",
      "Epoch  5 — train RMSE 669.355795 - train R2 -0.403400 — val RMSE 648.567544 - val R2 -0.334072\n",
      "Epoch  6 — train RMSE 656.391654 - train R2 -0.349564 — val RMSE 637.925373 - val R2 -0.290650\n",
      "Epoch  7 — train RMSE 651.075653 - train R2 -0.327793 — val RMSE 629.596572 - val R2 -0.257169\n",
      "Epoch  8 — train RMSE 638.624485 - train R2 -0.277493 — val RMSE 620.476178 - val R2 -0.221010\n",
      "Epoch  9 — train RMSE 630.253468 - train R2 -0.244223 — val RMSE 613.532268 - val R2 -0.193833\n",
      "Epoch 10 — train RMSE 622.052594 - train R2 -0.212054 — val RMSE 607.603541 - val R2 -0.170872\n",
      "Epoch 11 — train RMSE 618.313217 - train R2 -0.197525 — val RMSE 602.203973 - val R2 -0.150154\n",
      "Epoch 12 — train RMSE 607.495941 - train R2 -0.155991 — val RMSE 595.770273 - val R2 -0.125710\n",
      "Epoch 13 — train RMSE 608.634350 - train R2 -0.160327 — val RMSE 585.499746 - val R2 -0.087232\n",
      "Epoch 14 — train RMSE 599.386715 - train R2 -0.125335 — val RMSE 573.830141 - val R2 -0.044325\n",
      "Epoch 15 — train RMSE 591.730231 - train R2 -0.096769 — val RMSE 565.804378 - val R2 -0.015316\n",
      "Epoch 16 — train RMSE 583.792400 - train R2 -0.067541 — val RMSE 557.138518 - val R2 0.015547\n",
      "Epoch 17 — train RMSE 572.935137 - train R2 -0.028202 — val RMSE 548.689746 - val R2 0.045178\n",
      "Epoch 18 — train RMSE 570.720031 - train R2 -0.020267 — val RMSE 551.270918 - val R2 0.036173\n",
      "Epoch 19 — train RMSE 577.157246 - train R2 -0.043412 — val RMSE 534.450543 - val R2 0.094092\n",
      "Epoch 20 — train RMSE 560.803788 - train R2 0.014879 — val RMSE 529.620250 - val R2 0.110393\n",
      "Epoch 21 — train RMSE 554.957787 - train R2 0.035311 — val RMSE 519.380821 - val R2 0.144459\n",
      "Epoch 22 — train RMSE 547.450238 - train R2 0.061235 — val RMSE 512.549822 - val R2 0.166816\n",
      "Epoch 23 — train RMSE 549.511913 - train R2 0.054151 — val RMSE 504.750635 - val R2 0.191979\n",
      "Epoch 24 — train RMSE 543.883526 - train R2 0.073428 — val RMSE 497.889813 - val R2 0.213796\n",
      "Epoch 25 — train RMSE 538.170636 - train R2 0.092791 — val RMSE 491.651903 - val R2 0.233373\n",
      "Epoch 26 — train RMSE 522.741048 - train R2 0.144065 — val RMSE 489.507413 - val R2 0.240046\n",
      "Epoch 27 — train RMSE 516.829687 - train R2 0.163314 — val RMSE 479.631434 - val R2 0.270401\n",
      "Epoch 28 — train RMSE 517.606648 - train R2 0.160796 — val RMSE 471.542706 - val R2 0.294802\n",
      "Epoch 29 — train RMSE 486.139668 - train R2 0.259731 — val RMSE 461.756954 - val R2 0.323768\n",
      "Epoch 30 — train RMSE 493.658340 - train R2 0.236656 — val RMSE 456.587230 - val R2 0.338825\n",
      "Epoch 31 — train RMSE 506.727403 - train R2 0.195703 — val RMSE 451.656828 - val R2 0.353027\n",
      "Epoch 32 — train RMSE 492.179182 - train R2 0.241223 — val RMSE 446.514619 - val R2 0.367675\n",
      "Epoch 33 — train RMSE 491.527417 - train R2 0.243232 — val RMSE 441.914938 - val R2 0.380636\n",
      "Epoch 34 — train RMSE 477.834575 - train R2 0.284808 — val RMSE 436.363569 - val R2 0.396099\n",
      "Epoch 35 — train RMSE 484.737601 - train R2 0.263995 — val RMSE 429.338172 - val R2 0.415388\n",
      "Epoch 36 — train RMSE 461.549534 - train R2 0.332726 — val RMSE 422.849245 - val R2 0.432926\n",
      "Epoch 37 — train RMSE 447.396710 - train R2 0.373021 — val RMSE 416.321184 - val R2 0.450300\n",
      "Epoch 38 — train RMSE 448.441059 - train R2 0.370090 — val RMSE 411.546056 - val R2 0.462837\n",
      "Epoch 39 — train RMSE 442.905457 - train R2 0.385546 — val RMSE 403.278701 - val R2 0.484202\n",
      "Epoch 40 — train RMSE 431.010407 - train R2 0.418107 — val RMSE 400.451464 - val R2 0.491409\n",
      "Epoch 41 — train RMSE 451.126776 - train R2 0.362523 — val RMSE 396.684846 - val R2 0.500932\n",
      "Epoch 42 — train RMSE 456.162745 - train R2 0.348211 — val RMSE 395.827861 - val R2 0.503086\n",
      "Epoch 43 — train RMSE 488.188857 - train R2 0.253477 — val RMSE 388.638323 - val R2 0.520973\n",
      "Epoch 44 — train RMSE 440.920341 - train R2 0.391041 — val RMSE 380.098892 - val R2 0.541793\n",
      "Epoch 45 — train RMSE 434.680980 - train R2 0.408154 — val RMSE 375.588450 - val R2 0.552603\n",
      "Epoch 46 — train RMSE 400.206327 - train R2 0.498310 — val RMSE 365.895931 - val R2 0.575396\n",
      "Epoch 47 — train RMSE 402.928573 - train R2 0.491462 — val RMSE 362.405526 - val R2 0.583458\n",
      "Epoch 48 — train RMSE 455.403184 - train R2 0.350379 — val RMSE 353.169488 - val R2 0.604419\n",
      "Epoch 49 — train RMSE 427.678213 - train R2 0.427070 — val RMSE 343.003462 - val R2 0.626865\n",
      "Epoch 50 — train RMSE 414.609031 - train R2 0.461551 — val RMSE 342.819428 - val R2 0.627265\n",
      "Epoch 51 — train RMSE 403.390888 - train R2 0.490294 — val RMSE 332.506426 - val R2 0.649354\n",
      "Epoch 52 — train RMSE 389.754806 - train R2 0.524171 — val RMSE 329.799868 - val R2 0.655039\n",
      "Epoch 53 — train RMSE 428.899717 - train R2 0.423792 — val RMSE 331.907531 - val R2 0.650616\n",
      "Epoch 54 — train RMSE 397.661707 - train R2 0.504669 — val RMSE 326.228998 - val R2 0.662469\n",
      "Epoch 55 — train RMSE 376.277958 - train R2 0.556509 — val RMSE 312.372086 - val R2 0.690534\n",
      "Epoch 56 — train RMSE 372.677624 - train R2 0.564955 — val RMSE 311.848414 - val R2 0.691571\n",
      "Epoch 57 — train RMSE 380.450555 - train R2 0.546618 — val RMSE 301.030937 - val R2 0.712597\n",
      "Epoch 58 — train RMSE 377.380975 - train R2 0.553905 — val RMSE 304.141903 - val R2 0.706626\n",
      "Epoch 59 — train RMSE 398.927659 - train R2 0.501511 — val RMSE 294.357258 - val R2 0.725199\n",
      "Epoch 60 — train RMSE 372.670649 - train R2 0.564971 — val RMSE 282.365489 - val R2 0.747133\n",
      "Epoch 61 — train RMSE 363.560784 - train R2 0.585980 — val RMSE 272.995629 - val R2 0.763637\n",
      "Epoch 62 — train RMSE 336.865416 - train R2 0.644548 — val RMSE 272.831371 - val R2 0.763921\n",
      "Epoch 63 — train RMSE 354.528119 - train R2 0.606297 — val RMSE 267.444803 - val R2 0.773151\n",
      "Epoch 64 — train RMSE 324.711445 - train R2 0.669735 — val RMSE 257.814924 - val R2 0.789193\n",
      "Epoch 65 — train RMSE 328.001293 - train R2 0.663009 — val RMSE 259.792037 - val R2 0.785947\n",
      "Epoch 66 — train RMSE 278.250634 - train R2 0.757484 — val RMSE 252.686259 - val R2 0.797497\n",
      "Epoch 67 — train RMSE 316.681601 - train R2 0.685867 — val RMSE 247.722820 - val R2 0.805374\n",
      "Epoch 68 — train RMSE 329.532993 - train R2 0.659854 — val RMSE 245.893166 - val R2 0.808238\n",
      "Epoch 69 — train RMSE 338.412274 - train R2 0.641277 — val RMSE 240.319847 - val R2 0.816833\n",
      "Epoch 70 — train RMSE 290.184120 - train R2 0.736236 — val RMSE 226.181227 - val R2 0.837751\n",
      "Epoch 71 — train RMSE 309.127124 - train R2 0.700676 — val RMSE 219.085207 - val R2 0.847772\n",
      "Epoch 72 — train RMSE 321.511470 - train R2 0.676212 — val RMSE 218.561262 - val R2 0.848499\n",
      "Epoch 73 — train RMSE 292.971891 - train R2 0.731144 — val RMSE 219.377466 - val R2 0.847365\n",
      "Epoch 74 — train RMSE 319.406282 - train R2 0.680438 — val RMSE 208.679797 - val R2 0.861889\n",
      "Epoch 75 — train RMSE 285.929446 - train R2 0.743914 — val RMSE 204.322810 - val R2 0.867596\n",
      "Epoch 76 — train RMSE 255.216890 - train R2 0.795974 — val RMSE 205.225852 - val R2 0.866423\n",
      "Epoch 77 — train RMSE 271.115902 - train R2 0.769762 — val RMSE 190.203848 - val R2 0.885262\n",
      "Epoch 78 — train RMSE 289.144084 - train R2 0.738124 — val RMSE 183.077228 - val R2 0.893699\n",
      "Epoch 79 — train RMSE 229.343042 - train R2 0.835245 — val RMSE 186.997232 - val R2 0.889098\n",
      "Epoch 80 — train RMSE 307.298318 - train R2 0.704207 — val RMSE 180.210511 - val R2 0.897002\n",
      "Epoch 81 — train RMSE 270.050971 - train R2 0.771567 — val RMSE 174.465540 - val R2 0.903464\n",
      "Epoch 82 — train RMSE 277.492981 - train R2 0.758803 — val RMSE 169.594142 - val R2 0.908780\n",
      "Epoch 83 — train RMSE 241.118764 - train R2 0.817892 — val RMSE 172.042600 - val R2 0.906127\n",
      "Epoch 84 — train RMSE 264.438782 - train R2 0.780963 — val RMSE 158.620167 - val R2 0.920203\n",
      "Epoch 85 — train RMSE 319.574359 - train R2 0.680102 — val RMSE 151.689129 - val R2 0.927024\n",
      "Epoch 86 — train RMSE 235.919005 - train R2 0.825661 — val RMSE 159.863272 - val R2 0.918948\n",
      "Epoch 87 — train RMSE 278.473642 - train R2 0.757095 — val RMSE 154.299441 - val R2 0.924491\n",
      "Epoch 88 — train RMSE 240.920268 - train R2 0.818192 — val RMSE 155.041678 - val R2 0.923763\n",
      "Epoch 89 — train RMSE 241.316102 - train R2 0.817594 — val RMSE 139.850328 - val R2 0.937971\n",
      "Epoch 90 — train RMSE 228.029223 - train R2 0.837127 — val RMSE 149.508371 - val R2 0.929108\n",
      "Epoch 91 — train RMSE 238.824335 - train R2 0.821341 — val RMSE 146.354907 - val R2 0.932067\n",
      "Epoch 92 — train RMSE 224.307101 - train R2 0.842401 — val RMSE 127.242845 - val R2 0.948651\n",
      "Epoch 93 — train RMSE 225.008037 - train R2 0.841414 — val RMSE 127.601281 - val R2 0.948361\n",
      "Epoch 94 — train RMSE 279.664071 - train R2 0.755014 — val RMSE 122.645310 - val R2 0.952294\n",
      "Epoch 95 — train RMSE 264.828416 - train R2 0.780317 — val RMSE 124.959089 - val R2 0.950477\n",
      "Epoch 96 — train RMSE 298.267320 - train R2 0.721337 — val RMSE 132.077954 - val R2 0.944674\n",
      "Epoch 97 — train RMSE 242.477186 - train R2 0.815834 — val RMSE 141.528231 - val R2 0.936473\n",
      "Epoch 98 — train RMSE 240.894135 - train R2 0.818231 — val RMSE 127.223054 - val R2 0.948667\n",
      "Epoch 99 — train RMSE 262.953737 - train R2 0.783416 — val RMSE 114.597847 - val R2 0.958349\n",
      "Epoch 100 — train RMSE 238.119009 - train R2 0.822395 — val RMSE 120.295938 - val R2 0.954104\n",
      "Epoch 101 — train RMSE 271.471231 - train R2 0.769158 — val RMSE 111.596297 - val R2 0.960503\n",
      "Epoch 102 — train RMSE 237.595711 - train R2 0.823175 — val RMSE 107.490743 - val R2 0.963355\n",
      "Epoch 103 — train RMSE 247.405970 - train R2 0.808271 — val RMSE 101.004484 - val R2 0.967644\n",
      "Epoch 104 — train RMSE 196.243349 - train R2 0.879369 — val RMSE 106.632865 - val R2 0.963938\n",
      "Epoch 105 — train RMSE 236.417911 - train R2 0.824923 — val RMSE 104.794327 - val R2 0.965171\n",
      "Epoch 106 — train RMSE 176.215874 - train R2 0.902735 — val RMSE 99.389332 - val R2 0.968671\n",
      "Epoch 107 — train RMSE 258.854005 - train R2 0.790117 — val RMSE 90.547451 - val R2 0.973997\n",
      "Epoch 108 — train RMSE 297.546661 - train R2 0.722682 — val RMSE 96.619760 - val R2 0.970393\n",
      "Epoch 109 — train RMSE 189.180798 - train R2 0.887896 — val RMSE 90.593160 - val R2 0.973971\n",
      "Epoch 110 — train RMSE 277.874295 - train R2 0.758140 — val RMSE 97.394336 - val R2 0.969916\n",
      "Epoch 111 — train RMSE 203.755895 - train R2 0.869957 — val RMSE 91.754377 - val R2 0.973299\n",
      "Epoch 112 — train RMSE 243.625436 - train R2 0.814086 — val RMSE 94.456138 - val R2 0.971704\n",
      "Epoch 113 — train RMSE 267.488368 - train R2 0.775882 — val RMSE 94.728480 - val R2 0.971540\n",
      "Epoch 114 — train RMSE 229.693763 - train R2 0.834741 — val RMSE 103.453827 - val R2 0.966056\n",
      "Epoch 115 — train RMSE 275.197430 - train R2 0.762777 — val RMSE 88.462753 - val R2 0.975181\n",
      "Epoch 116 — train RMSE 204.158493 - train R2 0.869442 — val RMSE 82.187454 - val R2 0.978577\n",
      "Epoch 117 — train RMSE 215.915677 - train R2 0.853972 — val RMSE 88.336935 - val R2 0.975251\n",
      "Epoch 118 — train RMSE 188.671399 - train R2 0.888499 — val RMSE 85.334664 - val R2 0.976905\n",
      "Epoch 119 — train RMSE 253.182087 - train R2 0.799214 — val RMSE 79.979964 - val R2 0.979712\n",
      "Epoch 120 — train RMSE 216.376192 - train R2 0.853348 — val RMSE 88.555443 - val R2 0.975129\n",
      "Epoch 121 — train RMSE 186.208693 - train R2 0.891391 — val RMSE 75.520322 - val R2 0.981912\n",
      "Epoch 122 — train RMSE 284.541977 - train R2 0.746394 — val RMSE 75.464827 - val R2 0.981938\n",
      "Epoch 123 — train RMSE 274.366279 - train R2 0.764208 — val RMSE 75.106662 - val R2 0.982109\n",
      "Epoch 124 — train RMSE 263.461458 - train R2 0.782579 — val RMSE 80.092698 - val R2 0.979655\n",
      "Epoch 125 — train RMSE 254.426776 - train R2 0.797235 — val RMSE 94.587552 - val R2 0.971625\n",
      "Epoch 126 — train RMSE 151.738405 - train R2 0.927880 — val RMSE 79.618868 - val R2 0.979895\n",
      "Epoch 127 — train RMSE 227.643765 - train R2 0.837677 — val RMSE 91.262607 - val R2 0.973585\n",
      "Epoch 128 — train RMSE 229.008842 - train R2 0.835725 — val RMSE 84.376411 - val R2 0.977421\n",
      "Epoch 129 — train RMSE 251.248132 - train R2 0.802270 — val RMSE 88.480831 - val R2 0.975171\n",
      "Epoch 130 — train RMSE 214.776206 - train R2 0.855509 — val RMSE 82.345173 - val R2 0.978495\n",
      "Epoch 131 — train RMSE 305.922640 - train R2 0.706849 — val RMSE 73.040618 - val R2 0.983080\n",
      "Epoch 132 — train RMSE 229.540884 - train R2 0.834961 — val RMSE 88.032092 - val R2 0.975422\n",
      "Epoch 133 — train RMSE 252.769459 - train R2 0.799868 — val RMSE 144.345083 - val R2 0.933920\n",
      "Epoch 134 — train RMSE 168.583685 - train R2 0.910978 — val RMSE 76.087280 - val R2 0.981639\n",
      "Epoch 135 — train RMSE 167.184327 - train R2 0.912449 — val RMSE 82.787712 - val R2 0.978263\n",
      "Epoch 136 — train RMSE 228.042409 - train R2 0.837108 — val RMSE 77.164280 - val R2 0.981116\n",
      "Epoch 137 — train RMSE 213.892575 - train R2 0.856696 — val RMSE 77.015598 - val R2 0.981188\n",
      "Epoch 138 — train RMSE 200.102223 - train R2 0.874579 — val RMSE 87.495513 - val R2 0.975720\n",
      "Epoch 139 — train RMSE 212.758231 - train R2 0.858212 — val RMSE 73.822581 - val R2 0.982716\n",
      "Epoch 140 — train RMSE 184.774997 - train R2 0.893057 — val RMSE 80.433216 - val R2 0.979482\n",
      "Epoch 141 — train RMSE 184.333613 - train R2 0.893567 — val RMSE 74.928545 - val R2 0.982194\n",
      "Epoch 142 — train RMSE 197.377115 - train R2 0.877972 — val RMSE 76.497340 - val R2 0.981441\n",
      "Epoch 143 — train RMSE 198.126183 - train R2 0.877044 — val RMSE 68.501328 - val R2 0.985118\n",
      "Epoch 144 — train RMSE 183.316274 - train R2 0.894738 — val RMSE 66.743908 - val R2 0.985872\n",
      "Epoch 145 — train RMSE 238.601283 - train R2 0.821675 — val RMSE 87.615298 - val R2 0.975654\n",
      "Epoch 146 — train RMSE 263.719374 - train R2 0.782153 — val RMSE 85.219022 - val R2 0.976967\n",
      "Epoch 147 — train RMSE 273.368509 - train R2 0.765920 — val RMSE 70.893382 - val R2 0.984060\n",
      "Epoch 148 — train RMSE 250.238105 - train R2 0.803856 — val RMSE 81.545883 - val R2 0.978910\n",
      "Epoch 149 — train RMSE 182.307686 - train R2 0.895894 — val RMSE 68.809087 - val R2 0.984984\n",
      "Epoch 150 — train RMSE 198.206182 - train R2 0.876944 — val RMSE 74.931691 - val R2 0.982193\n",
      "Epoch 151 — train RMSE 197.530542 - train R2 0.877782 — val RMSE 72.893032 - val R2 0.983148\n",
      "Epoch 152 — train RMSE 197.780035 - train R2 0.877473 — val RMSE 68.825968 - val R2 0.984976\n",
      "Epoch 153 — train RMSE 312.320405 - train R2 0.694460 — val RMSE 105.479394 - val R2 0.964714\n",
      "Epoch 154 — train RMSE 250.245809 - train R2 0.803844 — val RMSE 68.584789 - val R2 0.985082\n",
      "Epoch 155 — train RMSE 225.567463 - train R2 0.840625 — val RMSE 87.144411 - val R2 0.975915\n",
      "Epoch 156 — train RMSE 283.819120 - train R2 0.747681 — val RMSE 69.082048 - val R2 0.984864\n",
      "Epoch 157 — train RMSE 183.944257 - train R2 0.894016 — val RMSE 77.715059 - val R2 0.980845\n",
      "Epoch 158 — train RMSE 213.187988 - train R2 0.857638 — val RMSE 70.574527 - val R2 0.984203\n",
      "Epoch 159 — train RMSE 239.100217 - train R2 0.820928 — val RMSE 75.238900 - val R2 0.982046\n",
      "Epoch 160 — train RMSE 197.865982 - train R2 0.877366 — val RMSE 84.631751 - val R2 0.977284\n",
      "Epoch 161 — train RMSE 183.047058 - train R2 0.895047 — val RMSE 81.032833 - val R2 0.979175\n",
      "Epoch 162 — train RMSE 181.714598 - train R2 0.896570 — val RMSE 68.906774 - val R2 0.984941\n",
      "Epoch 163 — train RMSE 165.082043 - train R2 0.914638 — val RMSE 83.212105 - val R2 0.978040\n",
      "Epoch 164 — train RMSE 260.158939 - train R2 0.787996 — val RMSE 59.299168 - val R2 0.988848\n",
      "Epoch 165 — train RMSE 181.456785 - train R2 0.896863 — val RMSE 67.197087 - val R2 0.985679\n",
      "Epoch 166 — train RMSE 209.305880 - train R2 0.862776 — val RMSE 59.452854 - val R2 0.988790\n",
      "Epoch 167 — train RMSE 210.286977 - train R2 0.861486 — val RMSE 68.380593 - val R2 0.985170\n",
      "Epoch 168 — train RMSE 249.798055 - train R2 0.804545 — val RMSE 71.076088 - val R2 0.983978\n",
      "Epoch 169 — train RMSE 223.804408 - train R2 0.843107 — val RMSE 66.924233 - val R2 0.985795\n",
      "Epoch 170 — train RMSE 292.705330 - train R2 0.731633 — val RMSE 69.529429 - val R2 0.984668\n",
      "Epoch 171 — train RMSE 210.367968 - train R2 0.861380 — val RMSE 95.438855 - val R2 0.971112\n",
      "Epoch 172 — train RMSE 146.832414 - train R2 0.932468 — val RMSE 63.007212 - val R2 0.987409\n",
      "Epoch 173 — train RMSE 223.692817 - train R2 0.843263 — val RMSE 67.300546 - val R2 0.985635\n",
      "Epoch 174 — train RMSE 210.238518 - train R2 0.861550 — val RMSE 62.611974 - val R2 0.987567\n",
      "Epoch 175 — train RMSE 224.766544 - train R2 0.841755 — val RMSE 64.946135 - val R2 0.986623\n",
      "Epoch 176 — train RMSE 124.548665 - train R2 0.951410 — val RMSE 60.310679 - val R2 0.988464\n",
      "Epoch 177 — train RMSE 211.484354 - train R2 0.859905 — val RMSE 77.869120 - val R2 0.980769\n",
      "Epoch 178 — train RMSE 273.078219 - train R2 0.766417 — val RMSE 65.407734 - val R2 0.986432\n",
      "Epoch 179 — train RMSE 259.905858 - train R2 0.788408 — val RMSE 65.266893 - val R2 0.986490\n",
      "Epoch 180 — train RMSE 271.131244 - train R2 0.769736 — val RMSE 60.760632 - val R2 0.988291\n",
      "Epoch 181 — train RMSE 180.012252 - train R2 0.898499 — val RMSE 62.213755 - val R2 0.987724\n",
      "Epoch 182 — train RMSE 247.395492 - train R2 0.808287 — val RMSE 62.331140 - val R2 0.987678\n",
      "Epoch 183 — train RMSE 94.393177 - train R2 0.972091 — val RMSE 94.765881 - val R2 0.971518\n",
      "Epoch 184 — train RMSE 224.825815 - train R2 0.841671 — val RMSE 61.828331 - val R2 0.987876\n",
      "Epoch 185 — train RMSE 248.985134 - train R2 0.805816 — val RMSE 62.322022 - val R2 0.987682\n",
      "Epoch 186 — train RMSE 180.765039 - train R2 0.897648 — val RMSE 61.764083 - val R2 0.987901\n",
      "Epoch 187 — train RMSE 259.730543 - train R2 0.788693 — val RMSE 63.450435 - val R2 0.987232\n",
      "Epoch 188 — train RMSE 247.999848 - train R2 0.807349 — val RMSE 62.228722 - val R2 0.987719\n",
      "Epoch 189 — train RMSE 259.505388 - train R2 0.789059 — val RMSE 62.513828 - val R2 0.987606\n",
      "Epoch 190 — train RMSE 223.395854 - train R2 0.843679 — val RMSE 66.166781 - val R2 0.986115\n",
      "Epoch 191 — train RMSE 270.299840 - train R2 0.771146 — val RMSE 61.174646 - val R2 0.988131\n",
      "Epoch 192 — train RMSE 209.316095 - train R2 0.862763 — val RMSE 64.075825 - val R2 0.986979\n",
      "Epoch 193 — train RMSE 248.327713 - train R2 0.806840 — val RMSE 59.086069 - val R2 0.988928\n",
      "Epoch 194 — train RMSE 210.863896 - train R2 0.860725 — val RMSE 61.474882 - val R2 0.988014\n",
      "Epoch 195 — train RMSE 178.101055 - train R2 0.900643 — val RMSE 59.134901 - val R2 0.988909\n",
      "Epoch 196 — train RMSE 269.838978 - train R2 0.771925 — val RMSE 61.508558 - val R2 0.988001\n",
      "Epoch 197 — train RMSE 209.163744 - train R2 0.862962 — val RMSE 61.544966 - val R2 0.987987\n",
      "Epoch 198 — train RMSE 310.369538 - train R2 0.698265 — val RMSE 65.958085 - val R2 0.986202\n",
      "Epoch 199 — train RMSE 260.361682 - train R2 0.787665 — val RMSE 227.967945 - val R2 0.835178\n",
      "Epoch 200 — train RMSE 232.149288 - train R2 0.831188 — val RMSE 184.419383 - val R2 0.892135\n"
     ]
    }
   ],
   "source": [
    "train, test = model_selection.train_test_split(data, test_size=0.1, random_state=1, stratify= data['Concentration'])\n",
    "\n",
    "train, validation = model_selection.train_test_split(train, test_size=0.1, random_state=1, stratify= train['Concentration'])\n",
    "\n",
    "model ,train_rmse, train_r2, val_rmse, val_r2, stand  = train_model(configs, train, validation, 200, 16, True, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grid search with 5-fold cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial  1  config : {'pred_len': 1, 'seq_len': 350, 'd_ff': 32, 'patch_len': 16, 'stride': 8, 'llm_layers': 4, 'content': 'This dataset contains near-infrared (NIR) spectroscopy measurements of lactate concentration in a phosphate-buffered saline (PBS) solution. Each spectrum includes absorbance values across 350 wavelengths. ', 'dropout': 0.1, 'd_model': 16, 'enc_in': 1, 'n_heads': 4, 'd_llm': 768, 'llm_model': 'GPT2', 'lr': 0.001}, 8\n",
      "Trial  1 CV results — train RMSE 356.232323 - train R2 0.592366 — val RMSE 271.255699 - val R2 0.735146\n",
      "Trial  2  config : {'pred_len': 1, 'seq_len': 350, 'd_ff': 32, 'patch_len': 16, 'stride': 8, 'llm_layers': 4, 'content': 'This dataset contains near-infrared (NIR) spectroscopy measurements of lactate concentration in a phosphate-buffered saline (PBS) solution. Each spectrum includes absorbance values across 350 wavelengths. ', 'dropout': 0.1, 'd_model': 16, 'enc_in': 1, 'n_heads': 8, 'd_llm': 768, 'llm_model': 'GPT2', 'lr': 0.001}, 8\n",
      "Trial  2 CV results — train RMSE 348.578071 - train R2 0.609410 — val RMSE 276.064421 - val R2 0.744711\n",
      "Trial  3  config : {'pred_len': 1, 'seq_len': 350, 'd_ff': 32, 'patch_len': 16, 'stride': 8, 'llm_layers': 4, 'content': 'This dataset contains near-infrared (NIR) spectroscopy measurements of lactate concentration in a phosphate-buffered saline (PBS) solution. Each spectrum includes absorbance values across 350 wavelengths. ', 'dropout': 0.1, 'd_model': 16, 'enc_in': 1, 'n_heads': 16, 'd_llm': 768, 'llm_model': 'GPT2', 'lr': 0.001}, 8\n",
      "Trial  3 CV results — train RMSE 269.964883 - train R2 0.769227 — val RMSE 161.130500 - val R2 0.913372\n",
      "Trial  4  config : {'pred_len': 1, 'seq_len': 350, 'd_ff': 32, 'patch_len': 16, 'stride': 8, 'llm_layers': 8, 'content': 'This dataset contains near-infrared (NIR) spectroscopy measurements of lactate concentration in a phosphate-buffered saline (PBS) solution. Each spectrum includes absorbance values across 350 wavelengths. ', 'dropout': 0.1, 'd_model': 16, 'enc_in': 1, 'n_heads': 4, 'd_llm': 768, 'llm_model': 'GPT2', 'lr': 0.001}, 8\n",
      "Trial  4 CV results — train RMSE 312.361616 - train R2 0.679352 — val RMSE 199.323946 - val R2 0.865318\n",
      "Trial  5  config : {'pred_len': 1, 'seq_len': 350, 'd_ff': 32, 'patch_len': 16, 'stride': 8, 'llm_layers': 8, 'content': 'This dataset contains near-infrared (NIR) spectroscopy measurements of lactate concentration in a phosphate-buffered saline (PBS) solution. Each spectrum includes absorbance values across 350 wavelengths. ', 'dropout': 0.1, 'd_model': 16, 'enc_in': 1, 'n_heads': 8, 'd_llm': 768, 'llm_model': 'GPT2', 'lr': 0.001}, 8\n",
      "Trial  5 CV results — train RMSE 316.580501 - train R2 0.678596 — val RMSE 222.092662 - val R2 0.823995\n",
      "Trial  6  config : {'pred_len': 1, 'seq_len': 350, 'd_ff': 32, 'patch_len': 16, 'stride': 8, 'llm_layers': 8, 'content': 'This dataset contains near-infrared (NIR) spectroscopy measurements of lactate concentration in a phosphate-buffered saline (PBS) solution. Each spectrum includes absorbance values across 350 wavelengths. ', 'dropout': 0.1, 'd_model': 16, 'enc_in': 1, 'n_heads': 16, 'd_llm': 768, 'llm_model': 'GPT2', 'lr': 0.001}, 8\n",
      "Trial  6 CV results — train RMSE 335.786906 - train R2 0.614549 — val RMSE 230.843714 - val R2 0.763786\n",
      "Trial  7  config : {'pred_len': 1, 'seq_len': 350, 'd_ff': 32, 'patch_len': 16, 'stride': 8, 'llm_layers': 12, 'content': 'This dataset contains near-infrared (NIR) spectroscopy measurements of lactate concentration in a phosphate-buffered saline (PBS) solution. Each spectrum includes absorbance values across 350 wavelengths. ', 'dropout': 0.1, 'd_model': 16, 'enc_in': 1, 'n_heads': 4, 'd_llm': 768, 'llm_model': 'GPT2', 'lr': 0.001}, 8\n",
      "Trial  7 CV results — train RMSE 349.441115 - train R2 0.613967 — val RMSE 230.079828 - val R2 0.828661\n",
      "Trial  8  config : {'pred_len': 1, 'seq_len': 350, 'd_ff': 32, 'patch_len': 16, 'stride': 8, 'llm_layers': 12, 'content': 'This dataset contains near-infrared (NIR) spectroscopy measurements of lactate concentration in a phosphate-buffered saline (PBS) solution. Each spectrum includes absorbance values across 350 wavelengths. ', 'dropout': 0.1, 'd_model': 16, 'enc_in': 1, 'n_heads': 8, 'd_llm': 768, 'llm_model': 'GPT2', 'lr': 0.001}, 8\n",
      "Trial  8 CV results — train RMSE 331.583991 - train R2 0.650051 — val RMSE 217.290785 - val R2 0.836864\n",
      "Trial  9  config : {'pred_len': 1, 'seq_len': 350, 'd_ff': 32, 'patch_len': 16, 'stride': 8, 'llm_layers': 12, 'content': 'This dataset contains near-infrared (NIR) spectroscopy measurements of lactate concentration in a phosphate-buffered saline (PBS) solution. Each spectrum includes absorbance values across 350 wavelengths. ', 'dropout': 0.1, 'd_model': 16, 'enc_in': 1, 'n_heads': 16, 'd_llm': 768, 'llm_model': 'GPT2', 'lr': 0.001}, 8\n",
      "Trial  9 CV results — train RMSE 269.402121 - train R2 0.770800 — val RMSE 188.911970 - val R2 0.883269\n",
      "Trial 10  config : {'pred_len': 1, 'seq_len': 350, 'd_ff': 32, 'patch_len': 16, 'stride': 8, 'llm_layers': 4, 'content': 'This dataset contains near-infrared (NIR) spectroscopy measurements of lactate concentration in a phosphate-buffered saline (PBS) solution. Each spectrum includes absorbance values across 350 wavelengths. ', 'dropout': 0.1, 'd_model': 16, 'enc_in': 1, 'n_heads': 4, 'd_llm': 768, 'llm_model': 'GPT2', 'lr': 0.001}, 16\n",
      "Trial 10 CV results — train RMSE 325.794791 - train R2 0.655950 — val RMSE 224.897021 - val R2 0.817427\n",
      "Trial 11  config : {'pred_len': 1, 'seq_len': 350, 'd_ff': 32, 'patch_len': 16, 'stride': 8, 'llm_layers': 4, 'content': 'This dataset contains near-infrared (NIR) spectroscopy measurements of lactate concentration in a phosphate-buffered saline (PBS) solution. Each spectrum includes absorbance values across 350 wavelengths. ', 'dropout': 0.1, 'd_model': 16, 'enc_in': 1, 'n_heads': 8, 'd_llm': 768, 'llm_model': 'GPT2', 'lr': 0.001}, 16\n",
      "Trial 11 CV results — train RMSE 318.646279 - train R2 0.675605 — val RMSE 180.248101 - val R2 0.891569\n",
      "Trial 12  config : {'pred_len': 1, 'seq_len': 350, 'd_ff': 32, 'patch_len': 16, 'stride': 8, 'llm_layers': 4, 'content': 'This dataset contains near-infrared (NIR) spectroscopy measurements of lactate concentration in a phosphate-buffered saline (PBS) solution. Each spectrum includes absorbance values across 350 wavelengths. ', 'dropout': 0.1, 'd_model': 16, 'enc_in': 1, 'n_heads': 16, 'd_llm': 768, 'llm_model': 'GPT2', 'lr': 0.001}, 16\n",
      "Trial 12 CV results — train RMSE 317.960935 - train R2 0.675797 — val RMSE 196.635303 - val R2 0.870563\n",
      "Trial 13  config : {'pred_len': 1, 'seq_len': 350, 'd_ff': 32, 'patch_len': 16, 'stride': 8, 'llm_layers': 8, 'content': 'This dataset contains near-infrared (NIR) spectroscopy measurements of lactate concentration in a phosphate-buffered saline (PBS) solution. Each spectrum includes absorbance values across 350 wavelengths. ', 'dropout': 0.1, 'd_model': 16, 'enc_in': 1, 'n_heads': 4, 'd_llm': 768, 'llm_model': 'GPT2', 'lr': 0.001}, 16\n",
      "Trial 13 CV results — train RMSE 331.711919 - train R2 0.635857 — val RMSE 243.923913 - val R2 0.787347\n",
      "Trial 14  config : {'pred_len': 1, 'seq_len': 350, 'd_ff': 32, 'patch_len': 16, 'stride': 8, 'llm_layers': 8, 'content': 'This dataset contains near-infrared (NIR) spectroscopy measurements of lactate concentration in a phosphate-buffered saline (PBS) solution. Each spectrum includes absorbance values across 350 wavelengths. ', 'dropout': 0.1, 'd_model': 16, 'enc_in': 1, 'n_heads': 8, 'd_llm': 768, 'llm_model': 'GPT2', 'lr': 0.001}, 16\n",
      "Trial 14 CV results — train RMSE 303.970915 - train R2 0.707213 — val RMSE 190.917023 - val R2 0.872890\n",
      "Trial 15  config : {'pred_len': 1, 'seq_len': 350, 'd_ff': 32, 'patch_len': 16, 'stride': 8, 'llm_layers': 8, 'content': 'This dataset contains near-infrared (NIR) spectroscopy measurements of lactate concentration in a phosphate-buffered saline (PBS) solution. Each spectrum includes absorbance values across 350 wavelengths. ', 'dropout': 0.1, 'd_model': 16, 'enc_in': 1, 'n_heads': 16, 'd_llm': 768, 'llm_model': 'GPT2', 'lr': 0.001}, 16\n",
      "Trial 15 CV results — train RMSE 297.567017 - train R2 0.710024 — val RMSE 198.117728 - val R2 0.866865\n",
      "Trial 16  config : {'pred_len': 1, 'seq_len': 350, 'd_ff': 32, 'patch_len': 16, 'stride': 8, 'llm_layers': 12, 'content': 'This dataset contains near-infrared (NIR) spectroscopy measurements of lactate concentration in a phosphate-buffered saline (PBS) solution. Each spectrum includes absorbance values across 350 wavelengths. ', 'dropout': 0.1, 'd_model': 16, 'enc_in': 1, 'n_heads': 4, 'd_llm': 768, 'llm_model': 'GPT2', 'lr': 0.001}, 16\n",
      "Trial 16 CV results — train RMSE 382.100049 - train R2 0.540046 — val RMSE 292.751718 - val R2 0.716965\n",
      "Trial 17  config : {'pred_len': 1, 'seq_len': 350, 'd_ff': 32, 'patch_len': 16, 'stride': 8, 'llm_layers': 12, 'content': 'This dataset contains near-infrared (NIR) spectroscopy measurements of lactate concentration in a phosphate-buffered saline (PBS) solution. Each spectrum includes absorbance values across 350 wavelengths. ', 'dropout': 0.1, 'd_model': 16, 'enc_in': 1, 'n_heads': 8, 'd_llm': 768, 'llm_model': 'GPT2', 'lr': 0.001}, 16\n",
      "Trial 17 CV results — train RMSE 328.834604 - train R2 0.648261 — val RMSE 239.227340 - val R2 0.751966\n",
      "Trial 18  config : {'pred_len': 1, 'seq_len': 350, 'd_ff': 32, 'patch_len': 16, 'stride': 8, 'llm_layers': 12, 'content': 'This dataset contains near-infrared (NIR) spectroscopy measurements of lactate concentration in a phosphate-buffered saline (PBS) solution. Each spectrum includes absorbance values across 350 wavelengths. ', 'dropout': 0.1, 'd_model': 16, 'enc_in': 1, 'n_heads': 16, 'd_llm': 768, 'llm_model': 'GPT2', 'lr': 0.001}, 16\n",
      "Trial 18 CV results — train RMSE 301.873289 - train R2 0.710395 — val RMSE 218.161748 - val R2 0.839009\n",
      "Trial 19  config : {'pred_len': 1, 'seq_len': 350, 'd_ff': 64, 'patch_len': 16, 'stride': 8, 'llm_layers': 4, 'content': 'This dataset contains near-infrared (NIR) spectroscopy measurements of lactate concentration in a phosphate-buffered saline (PBS) solution. Each spectrum includes absorbance values across 350 wavelengths. ', 'dropout': 0.1, 'd_model': 16, 'enc_in': 1, 'n_heads': 4, 'd_llm': 768, 'llm_model': 'GPT2', 'lr': 0.001}, 8\n",
      "Trial 19 CV results — train RMSE 296.969793 - train R2 0.703893 — val RMSE 209.361271 - val R2 0.845465\n",
      "Trial 20  config : {'pred_len': 1, 'seq_len': 350, 'd_ff': 64, 'patch_len': 16, 'stride': 8, 'llm_layers': 4, 'content': 'This dataset contains near-infrared (NIR) spectroscopy measurements of lactate concentration in a phosphate-buffered saline (PBS) solution. Each spectrum includes absorbance values across 350 wavelengths. ', 'dropout': 0.1, 'd_model': 16, 'enc_in': 1, 'n_heads': 8, 'd_llm': 768, 'llm_model': 'GPT2', 'lr': 0.001}, 8\n",
      "Trial 20 CV results — train RMSE 294.082785 - train R2 0.723784 — val RMSE 170.020265 - val R2 0.899567\n",
      "Trial 21  config : {'pred_len': 1, 'seq_len': 350, 'd_ff': 64, 'patch_len': 16, 'stride': 8, 'llm_layers': 4, 'content': 'This dataset contains near-infrared (NIR) spectroscopy measurements of lactate concentration in a phosphate-buffered saline (PBS) solution. Each spectrum includes absorbance values across 350 wavelengths. ', 'dropout': 0.1, 'd_model': 16, 'enc_in': 1, 'n_heads': 16, 'd_llm': 768, 'llm_model': 'GPT2', 'lr': 0.001}, 8\n",
      "Trial 21 CV results — train RMSE 270.334160 - train R2 0.768621 — val RMSE 120.297329 - val R2 0.950361\n",
      "Trial 22  config : {'pred_len': 1, 'seq_len': 350, 'd_ff': 64, 'patch_len': 16, 'stride': 8, 'llm_layers': 8, 'content': 'This dataset contains near-infrared (NIR) spectroscopy measurements of lactate concentration in a phosphate-buffered saline (PBS) solution. Each spectrum includes absorbance values across 350 wavelengths. ', 'dropout': 0.1, 'd_model': 16, 'enc_in': 1, 'n_heads': 4, 'd_llm': 768, 'llm_model': 'GPT2', 'lr': 0.001}, 8\n",
      "Trial 22 CV results — train RMSE 307.512161 - train R2 0.684029 — val RMSE 241.700915 - val R2 0.801581\n",
      "Trial 23  config : {'pred_len': 1, 'seq_len': 350, 'd_ff': 64, 'patch_len': 16, 'stride': 8, 'llm_layers': 8, 'content': 'This dataset contains near-infrared (NIR) spectroscopy measurements of lactate concentration in a phosphate-buffered saline (PBS) solution. Each spectrum includes absorbance values across 350 wavelengths. ', 'dropout': 0.1, 'd_model': 16, 'enc_in': 1, 'n_heads': 8, 'd_llm': 768, 'llm_model': 'GPT2', 'lr': 0.001}, 8\n",
      "Trial 23 CV results — train RMSE 252.219737 - train R2 0.798799 — val RMSE 160.998103 - val R2 0.890108\n",
      "Trial 24  config : {'pred_len': 1, 'seq_len': 350, 'd_ff': 64, 'patch_len': 16, 'stride': 8, 'llm_layers': 8, 'content': 'This dataset contains near-infrared (NIR) spectroscopy measurements of lactate concentration in a phosphate-buffered saline (PBS) solution. Each spectrum includes absorbance values across 350 wavelengths. ', 'dropout': 0.1, 'd_model': 16, 'enc_in': 1, 'n_heads': 16, 'd_llm': 768, 'llm_model': 'GPT2', 'lr': 0.001}, 8\n",
      "Trial 24 CV results — train RMSE 238.174724 - train R2 0.817882 — val RMSE 180.248483 - val R2 0.874650\n",
      "Trial 25  config : {'pred_len': 1, 'seq_len': 350, 'd_ff': 64, 'patch_len': 16, 'stride': 8, 'llm_layers': 12, 'content': 'This dataset contains near-infrared (NIR) spectroscopy measurements of lactate concentration in a phosphate-buffered saline (PBS) solution. Each spectrum includes absorbance values across 350 wavelengths. ', 'dropout': 0.1, 'd_model': 16, 'enc_in': 1, 'n_heads': 4, 'd_llm': 768, 'llm_model': 'GPT2', 'lr': 0.001}, 8\n",
      "Trial 25 CV results — train RMSE 336.129234 - train R2 0.627519 — val RMSE 206.333927 - val R2 0.842428\n",
      "Trial 26  config : {'pred_len': 1, 'seq_len': 350, 'd_ff': 64, 'patch_len': 16, 'stride': 8, 'llm_layers': 12, 'content': 'This dataset contains near-infrared (NIR) spectroscopy measurements of lactate concentration in a phosphate-buffered saline (PBS) solution. Each spectrum includes absorbance values across 350 wavelengths. ', 'dropout': 0.1, 'd_model': 16, 'enc_in': 1, 'n_heads': 8, 'd_llm': 768, 'llm_model': 'GPT2', 'lr': 0.001}, 8\n",
      "Trial 26 CV results — train RMSE 304.066927 - train R2 0.704999 — val RMSE 198.583072 - val R2 0.854323\n",
      "Trial 27  config : {'pred_len': 1, 'seq_len': 350, 'd_ff': 64, 'patch_len': 16, 'stride': 8, 'llm_layers': 12, 'content': 'This dataset contains near-infrared (NIR) spectroscopy measurements of lactate concentration in a phosphate-buffered saline (PBS) solution. Each spectrum includes absorbance values across 350 wavelengths. ', 'dropout': 0.1, 'd_model': 16, 'enc_in': 1, 'n_heads': 16, 'd_llm': 768, 'llm_model': 'GPT2', 'lr': 0.001}, 8\n",
      "Trial 27 CV results — train RMSE 263.406493 - train R2 0.779963 — val RMSE 136.801851 - val R2 0.934521\n",
      "Trial 28  config : {'pred_len': 1, 'seq_len': 350, 'd_ff': 64, 'patch_len': 16, 'stride': 8, 'llm_layers': 4, 'content': 'This dataset contains near-infrared (NIR) spectroscopy measurements of lactate concentration in a phosphate-buffered saline (PBS) solution. Each spectrum includes absorbance values across 350 wavelengths. ', 'dropout': 0.1, 'd_model': 16, 'enc_in': 1, 'n_heads': 4, 'd_llm': 768, 'llm_model': 'GPT2', 'lr': 0.001}, 16\n",
      "Trial 28 CV results — train RMSE 320.663994 - train R2 0.662923 — val RMSE 201.092241 - val R2 0.833837\n",
      "Trial 29  config : {'pred_len': 1, 'seq_len': 350, 'd_ff': 64, 'patch_len': 16, 'stride': 8, 'llm_layers': 4, 'content': 'This dataset contains near-infrared (NIR) spectroscopy measurements of lactate concentration in a phosphate-buffered saline (PBS) solution. Each spectrum includes absorbance values across 350 wavelengths. ', 'dropout': 0.1, 'd_model': 16, 'enc_in': 1, 'n_heads': 8, 'd_llm': 768, 'llm_model': 'GPT2', 'lr': 0.001}, 16\n",
      "Trial 29 CV results — train RMSE 271.784531 - train R2 0.764655 — val RMSE 161.537784 - val R2 0.902871\n",
      "Trial 30  config : {'pred_len': 1, 'seq_len': 350, 'd_ff': 64, 'patch_len': 16, 'stride': 8, 'llm_layers': 4, 'content': 'This dataset contains near-infrared (NIR) spectroscopy measurements of lactate concentration in a phosphate-buffered saline (PBS) solution. Each spectrum includes absorbance values across 350 wavelengths. ', 'dropout': 0.1, 'd_model': 16, 'enc_in': 1, 'n_heads': 16, 'd_llm': 768, 'llm_model': 'GPT2', 'lr': 0.001}, 16\n",
      "Trial 30 CV results — train RMSE 275.395169 - train R2 0.756548 — val RMSE 139.752825 - val R2 0.934366\n",
      "Trial 31  config : {'pred_len': 1, 'seq_len': 350, 'd_ff': 64, 'patch_len': 16, 'stride': 8, 'llm_layers': 8, 'content': 'This dataset contains near-infrared (NIR) spectroscopy measurements of lactate concentration in a phosphate-buffered saline (PBS) solution. Each spectrum includes absorbance values across 350 wavelengths. ', 'dropout': 0.1, 'd_model': 16, 'enc_in': 1, 'n_heads': 4, 'd_llm': 768, 'llm_model': 'GPT2', 'lr': 0.001}, 16\n",
      "Trial 31 CV results — train RMSE 258.007586 - train R2 0.784099 — val RMSE 133.560020 - val R2 0.941170\n",
      "Trial 32  config : {'pred_len': 1, 'seq_len': 350, 'd_ff': 64, 'patch_len': 16, 'stride': 8, 'llm_layers': 8, 'content': 'This dataset contains near-infrared (NIR) spectroscopy measurements of lactate concentration in a phosphate-buffered saline (PBS) solution. Each spectrum includes absorbance values across 350 wavelengths. ', 'dropout': 0.1, 'd_model': 16, 'enc_in': 1, 'n_heads': 8, 'd_llm': 768, 'llm_model': 'GPT2', 'lr': 0.001}, 16\n",
      "Trial 32 CV results — train RMSE 245.526998 - train R2 0.803387 — val RMSE 156.204095 - val R2 0.917394\n",
      "Trial 33  config : {'pred_len': 1, 'seq_len': 350, 'd_ff': 64, 'patch_len': 16, 'stride': 8, 'llm_layers': 8, 'content': 'This dataset contains near-infrared (NIR) spectroscopy measurements of lactate concentration in a phosphate-buffered saline (PBS) solution. Each spectrum includes absorbance values across 350 wavelengths. ', 'dropout': 0.1, 'd_model': 16, 'enc_in': 1, 'n_heads': 16, 'd_llm': 768, 'llm_model': 'GPT2', 'lr': 0.001}, 16\n",
      "Trial 33 CV results — train RMSE 277.412059 - train R2 0.755043 — val RMSE 159.728650 - val R2 0.910617\n",
      "Trial 34  config : {'pred_len': 1, 'seq_len': 350, 'd_ff': 64, 'patch_len': 16, 'stride': 8, 'llm_layers': 12, 'content': 'This dataset contains near-infrared (NIR) spectroscopy measurements of lactate concentration in a phosphate-buffered saline (PBS) solution. Each spectrum includes absorbance values across 350 wavelengths. ', 'dropout': 0.1, 'd_model': 16, 'enc_in': 1, 'n_heads': 4, 'd_llm': 768, 'llm_model': 'GPT2', 'lr': 0.001}, 16\n",
      "Trial 34 CV results — train RMSE 309.411874 - train R2 0.694674 — val RMSE 184.490321 - val R2 0.865555\n",
      "Trial 35  config : {'pred_len': 1, 'seq_len': 350, 'd_ff': 64, 'patch_len': 16, 'stride': 8, 'llm_layers': 12, 'content': 'This dataset contains near-infrared (NIR) spectroscopy measurements of lactate concentration in a phosphate-buffered saline (PBS) solution. Each spectrum includes absorbance values across 350 wavelengths. ', 'dropout': 0.1, 'd_model': 16, 'enc_in': 1, 'n_heads': 8, 'd_llm': 768, 'llm_model': 'GPT2', 'lr': 0.001}, 16\n",
      "Trial 35 CV results — train RMSE 316.426777 - train R2 0.679889 — val RMSE 247.430534 - val R2 0.737004\n",
      "Trial 36  config : {'pred_len': 1, 'seq_len': 350, 'd_ff': 64, 'patch_len': 16, 'stride': 8, 'llm_layers': 12, 'content': 'This dataset contains near-infrared (NIR) spectroscopy measurements of lactate concentration in a phosphate-buffered saline (PBS) solution. Each spectrum includes absorbance values across 350 wavelengths. ', 'dropout': 0.1, 'd_model': 16, 'enc_in': 1, 'n_heads': 16, 'd_llm': 768, 'llm_model': 'GPT2', 'lr': 0.001}, 16\n",
      "Trial 36 CV results — train RMSE 273.785343 - train R2 0.758774 — val RMSE 159.587989 - val R2 0.913318\n",
      "Best config : ({'pred_len': 1, 'seq_len': 350, 'd_ff': 64, 'patch_len': 16, 'stride': 8, 'llm_layers': 4, 'content': 'This dataset contains near-infrared (NIR) spectroscopy measurements of lactate concentration in a phosphate-buffered saline (PBS) solution. Each spectrum includes absorbance values across 350 wavelengths. ', 'dropout': 0.1, 'd_model': 16, 'enc_in': 1, 'n_heads': 16, 'd_llm': 768, 'llm_model': 'GPT2', 'lr': 0.001}, {16}) - best cv val RMSE : 120.297329\n"
     ]
    }
   ],
   "source": [
    "param_grid = {'d_ff' : [32, 64], 'batch_size' : [8, 16], 'llm_layers' : [4, 8, 12], 'n_heads' :  [4, 8, 16]}\n",
    "\n",
    "keys = list(param_grid.keys())\n",
    "all_configs = list(itertools.product(*[param_grid[k] for k in keys]))\n",
    "\n",
    "best_config = None\n",
    "best_rmse = float('inf')\n",
    "results = []\n",
    "r = 0\n",
    "\n",
    "train, test = model_selection.train_test_split(data, test_size=0.1, random_state=1, stratify= data['Concentration'])\n",
    "\n",
    "for idx, values in enumerate(all_configs):\n",
    "    config_dict = dict(zip(keys, values))\n",
    "    \n",
    "    configs = Details(\n",
    "        pred_len = 1,      \n",
    "        seq_len  = 350,\n",
    "        d_ff = config_dict['d_ff'],\n",
    "        patch_len= 16, \n",
    "        stride = 8, \n",
    "        llm_layers = config_dict['llm_layers'],\n",
    "        description = \"This dataset contains near-infrared (NIR) spectroscopy measurements of lactate concentration in a \" \\\n",
    "                    \"phosphate-buffered saline (PBS) solution. Each spectrum includes absorbance values across \" \\\n",
    "                    \"350 wavelengths. \" ,\n",
    "        dropout = 0.1,\n",
    "        n_heads = config_dict['n_heads'],\n",
    "        d_model = 16,\n",
    "        enc_in = 1,\n",
    "        d_llm = 768,\n",
    "        llm_model = 'GPT2',\n",
    "        lr = 0.001,\n",
    "    )\n",
    "    \n",
    "    batch_size = config_dict['batch_size']\n",
    "\n",
    "    k = 5\n",
    "    kf = KFold(n_splits=k, shuffle=True, random_state=42)\n",
    "\n",
    "    train_rmse_list = []\n",
    "    train_r2_list = []\n",
    "    val_rmse_list = []\n",
    "    val_r2_list = []\n",
    "\n",
    "    for f, (train_i, val_i) in enumerate(kf.split(train)):\n",
    "        \n",
    "        #Split data \n",
    "        train_ds = train.iloc[train_i,:]\n",
    "        val_ds = train.iloc[val_i,:]\n",
    "\n",
    "        model, train_rmse, train_r2, val_rmse, val_r2, stand = train_model(configs, train_ds, val_ds, 20, batch_size, False, False)\n",
    "\n",
    "        train_rmse_list.append(train_rmse)\n",
    "        train_r2_list.append(train_r2)\n",
    "        val_rmse_list.append(val_rmse)\n",
    "        val_r2_list.append(val_r2)\n",
    "\n",
    "    \n",
    "    cv_train_rmse = sum(train_rmse_list)/k\n",
    "    cv_train_r2 = sum(train_r2_list)/k\n",
    "    cv_val_rmse = sum(val_rmse_list)/k\n",
    "    cv_val_r2 = sum(val_r2_list)/k\n",
    "    results.append({'config': configs, 'train_rmse' : cv_train_rmse, 'train_r2': cv_train_r2, 'val_rmse' : cv_val_rmse, 'val_r2' : cv_val_r2})\n",
    "    \n",
    "    if cv_val_rmse < best_rmse:\n",
    "        best_rmse = cv_val_rmse\n",
    "        best_config = configs\n",
    "    r += 1\n",
    "    print(f\"Trial {r:2d}  config : {vars(configs)}, {batch_size}\")\n",
    "    print(f\"Trial {r:2d} CV results — train RMSE {cv_train_rmse:.6f} - train R2 {cv_train_r2:.6f} — val RMSE {cv_val_rmse:.6f} - val R2 {cv_val_r2:.6f}\")\n",
    "print(f\"Best config : {vars(best_config), {batch_size}} - best cv val RMSE : {best_rmse:.6f}\")\n",
    "\n",
    "#Time 160 min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'pred_len': 1,\n",
       " 'seq_len': 350,\n",
       " 'd_ff': 64,\n",
       " 'patch_len': 16,\n",
       " 'stride': 8,\n",
       " 'llm_layers': 4,\n",
       " 'content': 'This dataset contains near-infrared (NIR) spectroscopy measurements of lactate concentration in a phosphate-buffered saline (PBS) solution. Each spectrum includes absorbance values across 350 wavelengths. ',\n",
       " 'dropout': 0.1,\n",
       " 'd_model': 16,\n",
       " 'enc_in': 1,\n",
       " 'n_heads': 16,\n",
       " 'd_llm': 768,\n",
       " 'llm_model': 'GPT2',\n",
       " 'lr': 0.001}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vars(best_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best model after cross val grid search:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "configs = Details(\n",
    "    pred_len = 1,      \n",
    "    seq_len  = 350,\n",
    "    d_ff = 64,\n",
    "    patch_len= 16, \n",
    "    stride = 8, \n",
    "    llm_layers = 4,\n",
    "    description = \"This dataset contains near-infrared (NIR) spectroscopy measurements of lactate concentration in a \" \\\n",
    "                  \"phosphate-buffered saline (PBS) solution. Each spectrum includes absorbance values across \" \\\n",
    "                  \"350 wavelengths. \" ,\n",
    "    dropout = 0.1,\n",
    "    n_heads = 16,\n",
    "    d_model = 16,\n",
    "    enc_in = 1,\n",
    "    d_llm = 768,\n",
    "    llm_model = 'GPT2',\n",
    "    lr = 0.001\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  1 — train RMSE 679.981435 - train R2 -0.448310 — val RMSE 619.665358 - val R2 -0.217821\n",
      "Epoch  2 — train RMSE 589.206206 - train R2 -0.087432 — val RMSE 520.359303 - val R2 0.141233\n",
      "Epoch  3 — train RMSE 523.847132 - train R2 0.140439 — val RMSE 431.297606 - val R2 0.410039\n",
      "Epoch  4 — train RMSE 435.018560 - train R2 0.407234 — val RMSE 349.578340 - val R2 0.612423\n",
      "Epoch  5 — train RMSE 400.969415 - train R2 0.496395 — val RMSE 249.202172 - val R2 0.803043\n",
      "Epoch  6 — train RMSE 327.380243 - train R2 0.664284 — val RMSE 194.058353 - val R2 0.880564\n",
      "Epoch  7 — train RMSE 308.438738 - train R2 0.702007 — val RMSE 179.989843 - val R2 0.897254\n",
      "Epoch  8 — train RMSE 192.725634 - train R2 0.883655 — val RMSE 144.191635 - val R2 0.934060\n",
      "Epoch  9 — train RMSE 253.295722 - train R2 0.799034 — val RMSE 182.655600 - val R2 0.894188\n",
      "Epoch 10 — train RMSE 287.214615 - train R2 0.741607 — val RMSE 137.982937 - val R2 0.939616\n",
      "Epoch 11 — train RMSE 231.039435 - train R2 0.832799 — val RMSE 118.558345 - val R2 0.955421\n",
      "Epoch 12 — train RMSE 269.841981 - train R2 0.771920 — val RMSE 119.890512 - val R2 0.954413\n",
      "Epoch 13 — train RMSE 231.054340 - train R2 0.832777 — val RMSE 102.417031 - val R2 0.966733\n",
      "Epoch 14 — train RMSE 227.397464 - train R2 0.838028 — val RMSE 124.835947 - val R2 0.950575\n",
      "Epoch 15 — train RMSE 218.341579 - train R2 0.850672 — val RMSE 134.012236 - val R2 0.943042\n",
      "Epoch 16 — train RMSE 222.755975 - train R2 0.844573 — val RMSE 123.041290 - val R2 0.951986\n",
      "Epoch 17 — train RMSE 259.261974 - train R2 0.789455 — val RMSE 167.137066 - val R2 0.911404\n",
      "Epoch 18 — train RMSE 201.392663 - train R2 0.872956 — val RMSE 212.965196 - val R2 0.856158\n",
      "Epoch 19 — train RMSE 228.181587 - train R2 0.836909 — val RMSE 154.238373 - val R2 0.924551\n",
      "Epoch 20 — train RMSE 226.846701 - train R2 0.838812 — val RMSE 195.144343 - val R2 0.879224\n",
      "Epoch 21 — train RMSE 281.018195 - train R2 0.752636 — val RMSE 90.276561 - val R2 0.974153\n",
      "Epoch 22 — train RMSE 235.710118 - train R2 0.825970 — val RMSE 209.290319 - val R2 0.861079\n",
      "Epoch 23 — train RMSE 270.140271 - train R2 0.771416 — val RMSE 97.239965 - val R2 0.970011\n",
      "Epoch 24 — train RMSE 236.819775 - train R2 0.824328 — val RMSE 125.097099 - val R2 0.950368\n",
      "Epoch 25 — train RMSE 219.636896 - train R2 0.848895 — val RMSE 80.930990 - val R2 0.979227\n",
      "Epoch 26 — train RMSE 277.573093 - train R2 0.758664 — val RMSE 91.151228 - val R2 0.973649\n",
      "Epoch 27 — train RMSE 218.148996 - train R2 0.850936 — val RMSE 74.045728 - val R2 0.982611\n",
      "Epoch 28 — train RMSE 229.198517 - train R2 0.835453 — val RMSE 75.309610 - val R2 0.982013\n",
      "Epoch 29 — train RMSE 218.335493 - train R2 0.850681 — val RMSE 91.434640 - val R2 0.973485\n",
      "Epoch 30 — train RMSE 203.697027 - train R2 0.870032 — val RMSE 74.664312 - val R2 0.982319\n",
      "Epoch 31 — train RMSE 230.597959 - train R2 0.833437 — val RMSE 68.637045 - val R2 0.985059\n",
      "Epoch 32 — train RMSE 255.808401 - train R2 0.795027 — val RMSE 87.784176 - val R2 0.975560\n",
      "Epoch 33 — train RMSE 223.501934 - train R2 0.843530 — val RMSE 115.807915 - val R2 0.957465\n",
      "Epoch 34 — train RMSE 197.173507 - train R2 0.878223 — val RMSE 81.880719 - val R2 0.978737\n",
      "Epoch 35 — train RMSE 181.302272 - train R2 0.897039 — val RMSE 113.882103 - val R2 0.958868\n",
      "Epoch 36 — train RMSE 282.783471 - train R2 0.749519 — val RMSE 103.670115 - val R2 0.965914\n",
      "Epoch 37 — train RMSE 287.802909 - train R2 0.740547 — val RMSE 112.361027 - val R2 0.959959\n",
      "Epoch 38 — train RMSE 203.614447 - train R2 0.870137 — val RMSE 112.020127 - val R2 0.960202\n",
      "Epoch 39 — train RMSE 207.537126 - train R2 0.865085 — val RMSE 110.514091 - val R2 0.961265\n",
      "Epoch 40 — train RMSE 220.947681 - train R2 0.847086 — val RMSE 107.059212 - val R2 0.963649\n",
      "Epoch 41 — train RMSE 221.722943 - train R2 0.846011 — val RMSE 137.995747 - val R2 0.939605\n",
      "Early stopping triggered; Best val RMSE : 68.637045\n"
     ]
    }
   ],
   "source": [
    "train, test = model_selection.train_test_split(data, test_size=0.1, random_state=1, stratify= data['Concentration'])\n",
    "\n",
    "train, validation = model_selection.train_test_split(train, test_size=0.1, random_state=1, stratify= train['Concentration'])\n",
    "\n",
    "model ,train_rmse, train_r2, val_rmse, val_r2, stand  = train_model(configs, train, validation, 200, 16, True, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "configs = Details(\n",
    "    pred_len = 1,      \n",
    "    seq_len  = 350,\n",
    "    d_ff = 32,\n",
    "    patch_len= 16, \n",
    "    stride = 8, \n",
    "    llm_layers = 12,\n",
    "    description = \"This dataset contains near-infrared (NIR) spectroscopy measurements of lactate concentration in a \" \\\n",
    "                  \"phosphate-buffered saline (PBS) solution. Each spectrum includes absorbance values across \" \\\n",
    "                  \"350 wavelengths. \" ,\n",
    "    dropout = 0.1,\n",
    "    n_heads = 2,\n",
    "    d_model = 16,\n",
    "    enc_in = 1,\n",
    "    d_llm = 768,\n",
    "    llm_model = 'BERT',\n",
    "    lr = 0.0001\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  1 — train RMSE 701.261647 - train R2 -0.540379 — val RMSE 692.688851 - val R2 -0.521757\n",
      "Epoch  2 — train RMSE 698.651181 - train R2 -0.528932 — val RMSE 683.037111 - val R2 -0.479645\n",
      "Epoch  3 — train RMSE 679.288854 - train R2 -0.445361 — val RMSE 648.757978 - val R2 -0.334855\n",
      "Epoch  4 — train RMSE 648.301735 - train R2 -0.316503 — val RMSE 626.188040 - val R2 -0.243593\n",
      "Epoch  5 — train RMSE 635.494208 - train R2 -0.265000 — val RMSE 615.509602 - val R2 -0.201541\n",
      "Epoch  6 — train RMSE 635.495429 - train R2 -0.265006 — val RMSE 610.346552 - val R2 -0.181468\n",
      "Epoch  7 — train RMSE 623.095301 - train R2 -0.216120 — val RMSE 605.763806 - val R2 -0.163792\n",
      "Epoch  8 — train RMSE 617.661029 - train R2 -0.195000 — val RMSE 589.623490 - val R2 -0.102601\n",
      "Epoch  9 — train RMSE 618.882298 - train R2 -0.199731 — val RMSE 586.974306 - val R2 -0.092715\n",
      "Epoch 10 — train RMSE 593.004224 - train R2 -0.101497 — val RMSE 568.990595 - val R2 -0.026784\n",
      "Epoch 11 — train RMSE 590.598001 - train R2 -0.092576 — val RMSE 560.187720 - val R2 0.004741\n",
      "Epoch 12 — train RMSE 580.781678 - train R2 -0.056558 — val RMSE 550.556906 - val R2 0.038668\n",
      "Epoch 13 — train RMSE 565.159553 - train R2 -0.000483 — val RMSE 544.045925 - val R2 0.061272\n",
      "Epoch 14 — train RMSE 557.894527 - train R2 0.025074 — val RMSE 537.479524 - val R2 0.083795\n",
      "Epoch 15 — train RMSE 550.952689 - train R2 0.049185 — val RMSE 536.494932 - val R2 0.087149\n",
      "Epoch 16 — train RMSE 565.145265 - train R2 -0.000432 — val RMSE 513.860538 - val R2 0.162549\n",
      "Epoch 17 — train RMSE 524.225021 - train R2 0.139199 — val RMSE 506.850543 - val R2 0.185242\n",
      "Epoch 18 — train RMSE 525.541695 - train R2 0.134869 — val RMSE 462.144593 - val R2 0.322632\n",
      "Epoch 19 — train RMSE 501.373683 - train R2 0.212609 — val RMSE 448.294197 - val R2 0.362625\n",
      "Epoch 20 — train RMSE 474.689794 - train R2 0.294191 — val RMSE 433.503546 - val R2 0.403989\n",
      "Epoch 21 — train RMSE 470.977316 - train R2 0.305188 — val RMSE 415.080936 - val R2 0.453570\n",
      "Epoch 22 — train RMSE 462.600718 - train R2 0.329683 — val RMSE 406.276744 - val R2 0.476505\n",
      "Epoch 23 — train RMSE 455.467905 - train R2 0.350195 — val RMSE 405.262560 - val R2 0.479115\n",
      "Epoch 24 — train RMSE 436.488001 - train R2 0.403223 — val RMSE 377.170158 - val R2 0.548827\n",
      "Epoch 25 — train RMSE 422.204571 - train R2 0.441641 — val RMSE 366.949676 - val R2 0.572947\n",
      "Epoch 26 — train RMSE 442.228152 - train R2 0.387424 — val RMSE 364.010423 - val R2 0.579761\n",
      "Epoch 27 — train RMSE 412.205661 - train R2 0.467775 — val RMSE 356.558266 - val R2 0.596791\n",
      "Epoch 28 — train RMSE 393.301868 - train R2 0.515471 — val RMSE 330.511680 - val R2 0.653549\n",
      "Epoch 29 — train RMSE 380.529781 - train R2 0.546429 — val RMSE 323.343323 - val R2 0.668414\n",
      "Epoch 30 — train RMSE 380.357963 - train R2 0.546839 — val RMSE 304.647914 - val R2 0.705649\n",
      "Epoch 31 — train RMSE 330.086333 - train R2 0.658711 — val RMSE 293.617723 - val R2 0.726578\n",
      "Epoch 32 — train RMSE 339.462136 - train R2 0.639047 — val RMSE 280.791338 - val R2 0.749945\n",
      "Epoch 33 — train RMSE 313.702742 - train R2 0.691749 — val RMSE 272.235845 - val R2 0.764950\n",
      "Epoch 34 — train RMSE 329.578829 - train R2 0.659759 — val RMSE 258.729921 - val R2 0.787694\n",
      "Epoch 35 — train RMSE 338.654275 - train R2 0.640763 — val RMSE 249.550803 - val R2 0.802491\n",
      "Epoch 36 — train RMSE 328.177087 - train R2 0.662647 — val RMSE 240.104282 - val R2 0.817161\n",
      "Epoch 37 — train RMSE 336.470887 - train R2 0.645380 — val RMSE 237.790452 - val R2 0.820668\n",
      "Epoch 38 — train RMSE 302.800513 - train R2 0.712802 — val RMSE 226.963225 - val R2 0.836627\n",
      "Epoch 39 — train RMSE 305.626395 - train R2 0.707417 — val RMSE 220.464356 - val R2 0.845849\n",
      "Epoch 40 — train RMSE 306.386254 - train R2 0.705960 — val RMSE 207.511153 - val R2 0.863431\n",
      "Epoch 41 — train RMSE 297.102429 - train R2 0.723510 — val RMSE 196.577218 - val R2 0.877444\n",
      "Epoch 42 — train RMSE 295.907009 - train R2 0.725730 — val RMSE 192.588859 - val R2 0.882366\n",
      "Epoch 43 — train RMSE 285.904416 - train R2 0.743959 — val RMSE 179.728781 - val R2 0.897552\n",
      "Epoch 44 — train RMSE 323.683389 - train R2 0.671823 — val RMSE 173.109792 - val R2 0.904959\n",
      "Epoch 45 — train RMSE 242.221435 - train R2 0.816222 — val RMSE 162.981434 - val R2 0.915755\n",
      "Epoch 46 — train RMSE 249.689812 - train R2 0.804715 — val RMSE 155.033725 - val R2 0.923771\n",
      "Epoch 47 — train RMSE 275.049234 - train R2 0.763033 — val RMSE 149.247082 - val R2 0.929355\n",
      "Epoch 48 — train RMSE 301.408711 - train R2 0.715437 — val RMSE 164.647725 - val R2 0.914023\n",
      "Epoch 49 — train RMSE 256.964561 - train R2 0.793170 — val RMSE 139.928743 - val R2 0.937901\n",
      "Epoch 50 — train RMSE 254.096029 - train R2 0.797762 — val RMSE 134.744126 - val R2 0.942418\n",
      "Epoch 51 — train RMSE 281.338301 - train R2 0.752072 — val RMSE 132.085593 - val R2 0.944668\n",
      "Epoch 52 — train RMSE 260.508505 - train R2 0.787425 — val RMSE 130.142345 - val R2 0.946284\n",
      "Epoch 53 — train RMSE 209.478757 - train R2 0.862549 — val RMSE 116.220232 - val R2 0.957162\n",
      "Epoch 54 — train RMSE 266.035988 - train R2 0.778309 — val RMSE 115.362597 - val R2 0.957792\n",
      "Epoch 55 — train RMSE 267.976327 - train R2 0.775063 — val RMSE 122.162428 - val R2 0.952669\n",
      "Epoch 56 — train RMSE 253.667143 - train R2 0.798444 — val RMSE 107.510505 - val R2 0.963342\n",
      "Epoch 57 — train RMSE 270.934992 - train R2 0.770069 — val RMSE 105.226424 - val R2 0.964883\n",
      "Epoch 58 — train RMSE 236.263329 - train R2 0.825152 — val RMSE 100.403113 - val R2 0.968028\n",
      "Epoch 59 — train RMSE 234.675547 - train R2 0.827494 — val RMSE 96.547555 - val R2 0.970437\n",
      "Epoch 60 — train RMSE 193.407231 - train R2 0.882831 — val RMSE 93.961033 - val R2 0.972000\n",
      "Epoch 61 — train RMSE 235.299462 - train R2 0.826576 — val RMSE 92.111387 - val R2 0.973091\n",
      "Epoch 62 — train RMSE 268.688125 - train R2 0.773867 — val RMSE 122.965522 - val R2 0.952045\n",
      "Epoch 63 — train RMSE 176.395243 - train R2 0.902537 — val RMSE 91.308025 - val R2 0.973558\n",
      "Epoch 64 — train RMSE 266.918765 - train R2 0.776835 — val RMSE 87.897768 - val R2 0.975497\n",
      "Epoch 65 — train RMSE 217.316838 - train R2 0.852071 — val RMSE 85.970422 - val R2 0.976560\n",
      "Epoch 66 — train RMSE 215.162664 - train R2 0.854989 — val RMSE 93.866256 - val R2 0.972056\n",
      "Epoch 67 — train RMSE 254.511378 - train R2 0.797100 — val RMSE 103.337859 - val R2 0.966132\n",
      "Epoch 68 — train RMSE 239.824676 - train R2 0.819841 — val RMSE 82.667784 - val R2 0.978326\n",
      "Epoch 69 — train RMSE 216.129066 - train R2 0.853683 — val RMSE 78.126632 - val R2 0.980642\n",
      "Epoch 70 — train RMSE 231.385778 - train R2 0.832297 — val RMSE 84.066339 - val R2 0.977586\n",
      "Epoch 71 — train RMSE 201.783550 - train R2 0.872462 — val RMSE 76.211025 - val R2 0.981579\n",
      "Epoch 72 — train RMSE 240.504862 - train R2 0.818818 — val RMSE 72.723299 - val R2 0.983227\n",
      "Epoch 73 — train RMSE 199.752786 - train R2 0.875016 — val RMSE 74.132764 - val R2 0.982570\n",
      "Epoch 74 — train RMSE 263.415998 - train R2 0.782654 — val RMSE 70.796524 - val R2 0.984104\n",
      "Epoch 75 — train RMSE 263.001395 - train R2 0.783337 — val RMSE 70.632092 - val R2 0.984178\n",
      "Epoch 76 — train RMSE 273.711145 - train R2 0.765333 — val RMSE 69.707698 - val R2 0.984589\n",
      "Epoch 77 — train RMSE 214.659650 - train R2 0.855666 — val RMSE 70.070504 - val R2 0.984428\n",
      "Epoch 78 — train RMSE 212.762405 - train R2 0.858206 — val RMSE 69.225730 - val R2 0.984801\n",
      "Epoch 79 — train RMSE 212.477912 - train R2 0.858585 — val RMSE 66.121466 - val R2 0.986134\n",
      "Epoch 80 — train RMSE 196.989125 - train R2 0.878451 — val RMSE 67.622384 - val R2 0.985497\n",
      "Epoch 81 — train RMSE 210.523901 - train R2 0.861174 — val RMSE 71.256779 - val R2 0.983896\n",
      "Epoch 82 — train RMSE 226.520379 - train R2 0.839276 — val RMSE 72.197839 - val R2 0.983468\n",
      "Epoch 83 — train RMSE 240.082398 - train R2 0.819454 — val RMSE 68.552294 - val R2 0.985096\n",
      "Epoch 84 — train RMSE 224.304851 - train R2 0.842404 — val RMSE 67.465199 - val R2 0.985565\n",
      "Epoch 85 — train RMSE 300.405589 - train R2 0.717328 — val RMSE 108.163344 - val R2 0.962895\n",
      "Epoch 86 — train RMSE 360.846870 - train R2 0.592138 — val RMSE 160.092775 - val R2 0.918715\n",
      "Epoch 87 — train RMSE 277.384149 - train R2 0.758992 — val RMSE 114.255929 - val R2 0.958598\n",
      "Epoch 88 — train RMSE 240.550892 - train R2 0.818749 — val RMSE 78.901961 - val R2 0.980256\n",
      "Epoch 89 — train RMSE 266.365116 - train R2 0.777760 — val RMSE 79.826322 - val R2 0.979790\n",
      "Epoch 90 — train RMSE 217.256636 - train R2 0.852153 — val RMSE 69.232816 - val R2 0.984798\n",
      "Epoch 91 — train RMSE 293.993222 - train R2 0.729266 — val RMSE 74.518960 - val R2 0.982388\n",
      "Epoch 92 — train RMSE 200.315419 - train R2 0.874311 — val RMSE 104.810390 - val R2 0.965160\n",
      "Epoch 93 — train RMSE 227.488010 - train R2 0.837899 — val RMSE 75.836106 - val R2 0.981760\n",
      "Epoch 94 — train RMSE 198.395944 - train R2 0.876709 — val RMSE 76.374948 - val R2 0.981500\n",
      "Epoch 95 — train RMSE 266.512513 - train R2 0.777514 — val RMSE 83.632565 - val R2 0.977817\n",
      "Epoch 96 — train RMSE 227.816272 - train R2 0.837431 — val RMSE 99.192264 - val R2 0.968795\n",
      "Epoch 97 — train RMSE 254.222487 - train R2 0.797560 — val RMSE 72.316167 - val R2 0.983414\n",
      "Epoch 98 — train RMSE 251.018553 - train R2 0.802631 — val RMSE 71.826733 - val R2 0.983638\n",
      "Epoch 99 — train RMSE 228.926486 - train R2 0.835843 — val RMSE 81.539730 - val R2 0.978913\n",
      "Epoch 100 — train RMSE 254.711536 - train R2 0.796781 — val RMSE 155.285849 - val R2 0.923523\n"
     ]
    }
   ],
   "source": [
    "train, test = model_selection.train_test_split(data, test_size=0.1, random_state=1, stratify= data['Concentration'])\n",
    "\n",
    "train, validation = model_selection.train_test_split(train, test_size=0.1, random_state=1, stratify= train['Concentration'])\n",
    "\n",
    "model ,train_rmse, train_r2, val_rmse, val_r2, stand  = train_model(configs, train, validation, 100, 8, True, False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter tunning"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Grid search over batch size and learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7b0c7c2fb9a48f4b4cb471f24691f08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial  1  lr : 0.0001,  batch size :8\n",
      "Trial  1  results — train RMSE 327.358446 - train R2 0.664328 — val RMSE 256.896361 - val R2 0.790693\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "188c3c19c6e5422f9ef3cecd73d8a4c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial  2  lr : 0.0001,  batch size :16\n",
      "Trial  2  results — train RMSE 436.169210 - train R2 0.404094 — val RMSE 410.000372 - val R2 0.466865\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a41cb4cf34b54b308e0211e596e841d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial  3  lr : 0.0001,  batch size :32\n",
      "Trial  3  results — train RMSE 573.834509 - train R2 -0.031433 — val RMSE 562.577411 - val R2 -0.003768\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e845ede34ef4315ad759ccfef83fcc5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial  4  lr : 0.001,  batch size :8\n",
      "Trial  4  results — train RMSE 228.295154 - train R2 0.836747 — val RMSE 72.072755 - val R2 0.983526\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d79b33d24b94400a9571a628d15175e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial  5  lr : 0.001,  batch size :16\n",
      "Trial  5  results — train RMSE 159.647615 - train R2 0.920165 — val RMSE 91.528695 - val R2 0.973431\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "166cef1e7d904ef894ac247b57aada47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial  6  lr : 0.001,  batch size :32\n",
      "Trial  6  results — train RMSE 237.470926 - train R2 0.823360 — val RMSE 84.568597 - val R2 0.977318\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c0141a52d064f059d41b8d5172d42cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial  7  lr : 0.01,  batch size :8\n",
      "Trial  7  results — train RMSE 299.918828 - train R2 0.718243 — val RMSE 120.420760 - val R2 0.954009\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2523e614f7d49e680391c305cfb2f11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial  8  lr : 0.01,  batch size :16\n",
      "Trial  8  results — train RMSE 495.287300 - train R2 0.231609 — val RMSE 623.551196 - val R2 -0.233142\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02536a38c5654912840ee2c884fcbc89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial  9  lr : 0.01,  batch size :32\n",
      "Trial  9  results — train RMSE 362.301451 - train R2 0.588843 — val RMSE 219.073965 - val R2 0.847787\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09ab69a1383944e8b76b6536f502aca5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 10  lr : 0.1,  batch size :8\n",
      "Trial 10  results — train RMSE 328.919276 - train R2 0.661120 — val RMSE 243.672153 - val R2 0.811687\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "407019b3db944994832d8038957d6948",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 11  lr : 0.1,  batch size :16\n",
      "Trial 11  results — train RMSE 294.321334 - train R2 0.728662 — val RMSE 135.515707 - val R2 0.941756\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2cf955eb2374e90977c83f6ce6be181",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 12  lr : 0.1,  batch size :32\n",
      "Trial 12  results — train RMSE 293.043622 - train R2 0.731012 — val RMSE 133.844148 - val R2 0.943184\n",
      "Best config : {'pred_len': 1, 'seq_len': 350, 'd_ff': 32, 'patch_len': 16, 'stride': 8, 'llm_layers': 4, 'content': 'This dataset contains near-infrared (NIR) spectroscopy measurements of lactate concentration in a phosphate-buffered saline (PBS) solution. Each spectrum includes absorbance values across 350 wavelengths. ', 'dropout': 0.1, 'd_model': 16, 'enc_in': 1, 'n_heads': 2, 'd_llm': 4096, 'llm_model': 'LLAMA', 'lr': 0.1} - best val RMSE : 72.072755\n"
     ]
    }
   ],
   "source": [
    "param_grid = {'lr' : [1e-4, 1e-3, 1e-2, 1e-1], 'batch_size' : [8, 16, 32]}\n",
    "\n",
    "keys = list(param_grid.keys())\n",
    "all_configs = list(itertools.product(*[param_grid[k] for k in keys]))\n",
    "\n",
    "best_config = None\n",
    "best_rmse = float('inf')\n",
    "results = []\n",
    "r = 0\n",
    "for idx, values in enumerate(all_configs):\n",
    "    config_dict = dict(zip(keys, values))\n",
    "    \n",
    "    configs = Details(\n",
    "    pred_len = 1,      \n",
    "    seq_len  = 350,\n",
    "    d_ff = 32,\n",
    "    patch_len= 16, \n",
    "    stride = 8, \n",
    "    llm_layers = 4,\n",
    "    description = \"This dataset contains near-infrared (NIR) spectroscopy measurements of lactate concentration in a \" \\\n",
    "                  \"phosphate-buffered saline (PBS) solution. Each spectrum includes absorbance values across \" \\\n",
    "                  \"350 wavelengths. \" ,\n",
    "    dropout = 0.1,\n",
    "    n_heads = 2,\n",
    "    d_model = 16,\n",
    "    enc_in = 1,\n",
    "    d_llm = 4096,\n",
    "    llm_model = 'LLAMA',\n",
    "    lr = config_dict['lr']\n",
    "    )\n",
    "    \n",
    "    batch_size = config_dict['batch_size']\n",
    "\n",
    "    train, test = model_selection.train_test_split(data, test_size=0.1, random_state=1, stratify= data['Concentration'])\n",
    "\n",
    "    train, validation = model_selection.train_test_split(train, test_size=0.1, random_state=1, stratify= train['Concentration'])\n",
    "\n",
    "    model ,train_rmse, train_r2, val_rmse, val_r2, stand  = train_model(configs, train, validation, 20, batch_size, False, False)\n",
    "\n",
    "    results.append({'config': configs, 'train_rmse' : train_rmse, 'train_r2': train_r2, 'val_rmse' : val_rmse, 'val_r2' : val_r2})\n",
    "    \n",
    "    if val_rmse < best_rmse:\n",
    "        best_rmse = val_rmse\n",
    "        best_config = configs\n",
    "    r += 1\n",
    "    print(f\"Trial {r:2d}  lr : {config_dict['lr']},  batch size :{batch_size}\")\n",
    "    print(f\"Trial {r:2d}  results — train RMSE {train_rmse:.6f} - train R2 {train_r2:.6f} — val RMSE {val_rmse:.6f} - val R2 {val_r2:.6f}\")\n",
    "print(f\"Best config : {vars(best_config)} - best val RMSE : {best_rmse:.6f}\")\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'pred_len': 1,\n",
       " 'seq_len': 350,\n",
       " 'd_ff': 32,\n",
       " 'patch_len': 16,\n",
       " 'stride': 8,\n",
       " 'llm_layers': 4,\n",
       " 'content': 'This dataset contains near-infrared (NIR) spectroscopy measurements of lactate concentration in a phosphate-buffered saline (PBS) solution. Each spectrum includes absorbance values across 350 wavelengths. ',\n",
       " 'dropout': 0.1,\n",
       " 'd_model': 16,\n",
       " 'enc_in': 1,\n",
       " 'n_heads': 2,\n",
       " 'd_llm': 4096,\n",
       " 'llm_model': 'LLAMA',\n",
       " 'lr': 0.001}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vars(best_config)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c282f8786ba4cc9befe19d2b759300e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 36\u001b[0m\n\u001b[1;32m     32\u001b[0m train, test \u001b[39m=\u001b[39m model_selection\u001b[39m.\u001b[39mtrain_test_split(data, test_size\u001b[39m=\u001b[39m\u001b[39m0.1\u001b[39m, random_state\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, stratify\u001b[39m=\u001b[39m data[\u001b[39m'\u001b[39m\u001b[39mConcentration\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m     34\u001b[0m train, validation \u001b[39m=\u001b[39m model_selection\u001b[39m.\u001b[39mtrain_test_split(train, test_size\u001b[39m=\u001b[39m\u001b[39m0.1\u001b[39m, random_state\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, stratify\u001b[39m=\u001b[39m train[\u001b[39m'\u001b[39m\u001b[39mConcentration\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m---> 36\u001b[0m model ,train_rmse, train_r2, val_rmse, val_r2, stand  \u001b[39m=\u001b[39m train_model(configs, train, validation, \u001b[39m20\u001b[39;49m, batch_size, \u001b[39mFalse\u001b[39;49;00m)\n\u001b[1;32m     38\u001b[0m results[r] \u001b[39m=\u001b[39m {\u001b[39m'\u001b[39m\u001b[39mconfig\u001b[39m\u001b[39m'\u001b[39m: configs, \u001b[39m'\u001b[39m\u001b[39mtrain_rmse\u001b[39m\u001b[39m'\u001b[39m : train_rmse, \u001b[39m'\u001b[39m\u001b[39mtrain_r2\u001b[39m\u001b[39m'\u001b[39m: train_r2, \u001b[39m'\u001b[39m\u001b[39mval_rmse\u001b[39m\u001b[39m'\u001b[39m : val_rmse, \u001b[39m'\u001b[39m\u001b[39mval_r2\u001b[39m\u001b[39m'\u001b[39m : val_r2}\n\u001b[1;32m     40\u001b[0m \u001b[39m# Create a trial name (optional but helpful)\u001b[39;00m\n\u001b[1;32m     41\u001b[0m \u001b[39m#trial_name = f\"trial_{r:02d}\"\u001b[39;00m\n\u001b[1;32m     42\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     48\u001b[0m \n\u001b[1;32m     49\u001b[0m \u001b[39m# Log hyperparameters (grouped with final val_rmse)\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[4], line 44\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(configs, train, validation, epochs, batch_size, updates)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[39m#Train model \u001b[39;00m\n\u001b[1;32m     43\u001b[0m device \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mdevice(\u001b[39m\"\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mis_available() \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> 44\u001b[0m model \u001b[39m=\u001b[39m Model(configs)\u001b[39m.\u001b[39;49mto(device)\n\u001b[1;32m     45\u001b[0m \u001b[39m#opt = torch.optim.AdamW(model.parameters(), lr=configs.lr, weight_decay=1e-2)\u001b[39;00m\n\u001b[1;32m     46\u001b[0m opt \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39moptim\u001b[39m.\u001b[39mAdam(\u001b[39mfilter\u001b[39m(\u001b[39mlambda\u001b[39;00m p: p\u001b[39m.\u001b[39mrequires_grad, model\u001b[39m.\u001b[39mparameters()), lr\u001b[39m=\u001b[39mconfigs\u001b[39m.\u001b[39mlr)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1355\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1352\u001b[0m         \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1353\u001b[0m             \u001b[39mraise\u001b[39;00m\n\u001b[0;32m-> 1355\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_apply(convert)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:915\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    913\u001b[0m \u001b[39mif\u001b[39;00m recurse:\n\u001b[1;32m    914\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren():\n\u001b[0;32m--> 915\u001b[0m         module\u001b[39m.\u001b[39;49m_apply(fn)\n\u001b[1;32m    917\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    918\u001b[0m     \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    919\u001b[0m         \u001b[39m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    920\u001b[0m         \u001b[39m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    925\u001b[0m         \u001b[39m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    926\u001b[0m         \u001b[39m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:915\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    913\u001b[0m \u001b[39mif\u001b[39;00m recurse:\n\u001b[1;32m    914\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren():\n\u001b[0;32m--> 915\u001b[0m         module\u001b[39m.\u001b[39;49m_apply(fn)\n\u001b[1;32m    917\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    918\u001b[0m     \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    919\u001b[0m         \u001b[39m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    920\u001b[0m         \u001b[39m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    925\u001b[0m         \u001b[39m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    926\u001b[0m         \u001b[39m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:942\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    938\u001b[0m \u001b[39m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    939\u001b[0m \u001b[39m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    940\u001b[0m \u001b[39m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    941\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m--> 942\u001b[0m     param_applied \u001b[39m=\u001b[39m fn(param)\n\u001b[1;32m    943\u001b[0m p_should_use_set_data \u001b[39m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    945\u001b[0m \u001b[39m# subclasses may have multiple child tensors so we need to use swap_tensors\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1341\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1334\u001b[0m     \u001b[39mif\u001b[39;00m convert_to_format \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m t\u001b[39m.\u001b[39mdim() \u001b[39min\u001b[39;00m (\u001b[39m4\u001b[39m, \u001b[39m5\u001b[39m):\n\u001b[1;32m   1335\u001b[0m         \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39mto(\n\u001b[1;32m   1336\u001b[0m             device,\n\u001b[1;32m   1337\u001b[0m             dtype \u001b[39mif\u001b[39;00m t\u001b[39m.\u001b[39mis_floating_point() \u001b[39mor\u001b[39;00m t\u001b[39m.\u001b[39mis_complex() \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   1338\u001b[0m             non_blocking,\n\u001b[1;32m   1339\u001b[0m             memory_format\u001b[39m=\u001b[39mconvert_to_format,\n\u001b[1;32m   1340\u001b[0m         )\n\u001b[0;32m-> 1341\u001b[0m     \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39;49mto(\n\u001b[1;32m   1342\u001b[0m         device,\n\u001b[1;32m   1343\u001b[0m         dtype \u001b[39mif\u001b[39;49;00m t\u001b[39m.\u001b[39;49mis_floating_point() \u001b[39mor\u001b[39;49;00m t\u001b[39m.\u001b[39;49mis_complex() \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m   1344\u001b[0m         non_blocking,\n\u001b[1;32m   1345\u001b[0m     )\n\u001b[1;32m   1346\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mNotImplementedError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m   1347\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mstr\u001b[39m(e) \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mCannot copy out of meta tensor; no data!\u001b[39m\u001b[39m\"\u001b[39m:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "#timestamp = time.strftime('%Y%m%d-%H%M%S')\n",
    "#writer = SummaryWriter(log_dir=f\"runs/random_search_{timestamp}\")\n",
    "\n",
    "random_trials = 20\n",
    "\n",
    "best_config = None\n",
    "best_rmse = float('inf')\n",
    "results = {}\n",
    "\n",
    "for r in range(random_trials):\n",
    "    configs = Details(\n",
    "        pred_len = 1,      \n",
    "        seq_len  = 350,\n",
    "        d_ff = random.choice([32, 64]),\n",
    "        patch_len= random.choice([8, 16]), \n",
    "        stride = random.choice([4, 8, 16]), \n",
    "        llm_layers = random.choice([4, 8, 16,32]),\n",
    "        description = \"This dataset contains near-infrared (NIR) spectroscopy measurements of lactate concentration in a \" \\\n",
    "                    \"phosphate-buffered saline (PBS) solution. Each spectrum includes absorbance values across \" \\\n",
    "                    \"350 wavelengths. \" ,\n",
    "        dropout = 0.1,\n",
    "        n_heads = random.choice([4, 8]),\n",
    "        d_model = 16,\n",
    "        enc_in = 1,\n",
    "        d_llm = 4096,\n",
    "        llm_model = 'LLAMA',\n",
    "        lr = 0.001\n",
    "    )\n",
    "    \n",
    "    batch_size = random.choice([8, 16, 32])\n",
    "\n",
    "    train, test = model_selection.train_test_split(data, test_size=0.1, random_state=1, stratify= data['Concentration'])\n",
    "\n",
    "    train, validation = model_selection.train_test_split(train, test_size=0.1, random_state=1, stratify= train['Concentration'])\n",
    "\n",
    "    model ,train_rmse, train_r2, val_rmse, val_r2, stand  = train_model(configs, train, validation, 20, batch_size, False, False)\n",
    "\n",
    "    results[r] = {'config': configs, 'train_rmse' : train_rmse, 'train_r2': train_r2, 'val_rmse' : val_rmse, 'val_r2' : val_r2}\n",
    "\n",
    "    # Create a trial name (optional but helpful)\n",
    "    #trial_name = f\"trial_{r:02d}\"\n",
    "\n",
    "    # Log scalar metrics (per trial)\n",
    "    #writer.add_scalar(f\"{trial_name}/RMSE/train\", train_rmse, r)\n",
    "    #writer.add_scalar(f\"{trial_name}/RMSE/val\", val_rmse, r)\n",
    "    #writer.add_scalar(f\"{trial_name}/R2/train\", train_r2, r)\n",
    "    #writer.add_scalar(f\"{trial_name}/R2/val\", val_r2, r)\n",
    "\n",
    "    # Log hyperparameters (grouped with final val_rmse)\n",
    "    '''\n",
    "    hparams = {\n",
    "        'd_ff': configs.d_ff,\n",
    "        'patch_len': configs.patch_len,\n",
    "        'stride': configs.stride,\n",
    "        'llm_layers': configs.llm_layers,\n",
    "        'dropout': configs.dropout,\n",
    "        'n_heads': configs.n_heads,\n",
    "        'lr': configs.lr,\n",
    "        'd_model': configs.d_model,\n",
    "        'batch_size': batch_size\n",
    "    }\n",
    "    metrics = {\n",
    "        'val_rmse': val_rmse,\n",
    "        'val_r2': val_r2\n",
    "    }\n",
    "    writer.add_hparams(hparams, metrics, run_name=trial_name)\n",
    "\n",
    "    '''\n",
    "    if val_rmse < best_rmse:\n",
    "        best_rmse = val_rmse\n",
    "        best_config = configs\n",
    "\n",
    "    print(f\"Trial {r + 1:2d}  config : {vars(configs)}, {batch_size}\")\n",
    "    print(f\"Trial {r + 1:2d}  results — train RMSE {train_rmse:.6f} - train R2 {train_r2:.6f} — val RMSE {val_rmse:.6f} - val R2 {val_r2:.6f}\")\n",
    "print(f\"Best config : {vars(configs)} - best val RMSE : {best_rmse:.6f}\")\n",
    "\n",
    "#writer.close()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    }
   ],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ERROR: Could not find `tensorboard`. Please ensure that your PATH\n",
       "contains an executable `tensorboard` program, or explicitly specify\n",
       "the path to a TensorBoard binary by setting the `TENSORBOARD_BINARY`\n",
       "environment variable."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tensorboard --logdir= runs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Log Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Log concentration\n",
    "data_log = data.copy()\n",
    "data_log['Concentration'] = np.log1p(data_log['Concentration']) #log(cancentration + 1)\n",
    "\n",
    "train, test = model_selection.train_test_split(data_log, test_size=0.1, random_state=1, stratify= data_log['Concentration'])\n",
    "\n",
    "scaler_out = MinMaxScaler()\n",
    "train['Concentration'] = scaler_out.fit_transform(train[['Concentration']])\n",
    "test['Concentration'] = scaler_out.transform(test[['Concentration']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "configs = Details(\n",
    "    pred_len = 1,      \n",
    "    seq_len  = 350,\n",
    "    d_ff = 32,\n",
    "    patch_len= 16, \n",
    "    stride = 8, \n",
    "    llm_layers = 4,\n",
    "    description = \"This dataset contains near-infrared (NIR) spectroscopy measurements of lactate concentration in a \" \\\n",
    "                  \"phosphate-buffered saline (PBS) solution. Each spectrum includes absorbance values across \" \\\n",
    "                  \"350 wavelengths. \" ,\n",
    "    dropout = 0.1,\n",
    "    n_heads = 2,\n",
    "    d_model = 16,\n",
    "    enc_in = 1,\n",
    "    d_llm = 4096,\n",
    "    llm_model = 'LLAMA',\n",
    "    lr = 0.0001\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e19371ee6374fc4acf9350dee1ec96e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  1 — train RMSE 0.499592 - train R2 -1.424678 — val RMSE 0.358797 - val R2 -0.212155\n",
      "Epoch  2 — train RMSE 0.388769 - train R2 -0.468265 — val RMSE 0.284850 - val R2 0.235999\n",
      "Epoch  3 — train RMSE 0.292347 - train R2 0.169729 — val RMSE 0.244936 - val R2 0.435108\n",
      "Epoch  4 — train RMSE 0.325728 - train R2 -0.030697 — val RMSE 0.204326 - val R2 0.606895\n",
      "Epoch  5 — train RMSE 0.259499 - train R2 0.345824 — val RMSE 0.181257 - val R2 0.690648\n",
      "Epoch  6 — train RMSE 0.311179 - train R2 0.059319 — val RMSE 0.224908 - val R2 0.523712\n",
      "Epoch  7 — train RMSE 0.265646 - train R2 0.314466 — val RMSE 0.192117 - val R2 0.652471\n",
      "Epoch  8 — train RMSE 0.243041 - train R2 0.426173 — val RMSE 0.151927 - val R2 0.782663\n",
      "Epoch  9 — train RMSE 0.259983 - train R2 0.343381 — val RMSE 0.176166 - val R2 0.707783\n",
      "Epoch 10 — train RMSE 0.272781 - train R2 0.277148 — val RMSE 0.174799 - val R2 0.712301\n",
      "Epoch 11 — train RMSE 0.275445 - train R2 0.262956 — val RMSE 0.149571 - val R2 0.789352\n",
      "Epoch 12 — train RMSE 0.311352 - train R2 0.058270 — val RMSE 0.208004 - val R2 0.592616\n",
      "Epoch 13 — train RMSE 0.280963 - train R2 0.233131 — val RMSE 0.167839 - val R2 0.734754\n",
      "Epoch 14 — train RMSE 0.281534 - train R2 0.230013 — val RMSE 0.131618 - val R2 0.836885\n",
      "Epoch 15 — train RMSE 0.250097 - train R2 0.392372 — val RMSE 0.140459 - val R2 0.814236\n",
      "Epoch 16 — train RMSE 0.267386 - train R2 0.305456 — val RMSE 0.201611 - val R2 0.617272\n",
      "Epoch 17 — train RMSE 0.289416 - train R2 0.186292 — val RMSE 0.177891 - val R2 0.702034\n",
      "Epoch 18 — train RMSE 0.262534 - train R2 0.330433 — val RMSE 0.160269 - val R2 0.758143\n",
      "Epoch 19 — train RMSE 0.278454 - train R2 0.246769 — val RMSE 0.213585 - val R2 0.570463\n",
      "Epoch 20 — train RMSE 0.265880 - train R2 0.313258 — val RMSE 0.148537 - val R2 0.792254\n",
      "Epoch 21 — train RMSE 0.258434 - train R2 0.351186 — val RMSE 0.133314 - val R2 0.832655\n",
      "Epoch 22 — train RMSE 0.233122 - train R2 0.472053 — val RMSE 0.162469 - val R2 0.751456\n",
      "Epoch 23 — train RMSE 0.270702 - train R2 0.288121 — val RMSE 0.162044 - val R2 0.752754\n",
      "Epoch 24 — train RMSE 0.259391 - train R2 0.346369 — val RMSE 0.114414 - val R2 0.876740\n",
      "Epoch 25 — train RMSE 0.236568 - train R2 0.456331 — val RMSE 0.138533 - val R2 0.819296\n",
      "Epoch 26 — train RMSE 0.233193 - train R2 0.471732 — val RMSE 0.133020 - val R2 0.833393\n",
      "Epoch 27 — train RMSE 0.225092 - train R2 0.507797 — val RMSE 0.155171 - val R2 0.773284\n",
      "Epoch 28 — train RMSE 0.213208 - train R2 0.558400 — val RMSE 0.142705 - val R2 0.808248\n",
      "Epoch 29 — train RMSE 0.254495 - train R2 0.370811 — val RMSE 0.139517 - val R2 0.816720\n",
      "Epoch 30 — train RMSE 0.259056 - train R2 0.348057 — val RMSE 0.144489 - val R2 0.803424\n",
      "Epoch 31 — train RMSE 0.261453 - train R2 0.335936 — val RMSE 0.141767 - val R2 0.810760\n",
      "Epoch 32 — train RMSE 0.273269 - train R2 0.274555 — val RMSE 0.123428 - val R2 0.856555\n",
      "Epoch 33 — train RMSE 0.264840 - train R2 0.318618 — val RMSE 0.128028 - val R2 0.845663\n",
      "Epoch 34 — train RMSE 0.236336 - train R2 0.457399 — val RMSE 0.126777 - val R2 0.848665\n",
      "Epoch 35 — train RMSE 0.241237 - train R2 0.434658 — val RMSE 0.197654 - val R2 0.632148\n",
      "Epoch 36 — train RMSE 0.272794 - train R2 0.277079 — val RMSE 0.126207 - val R2 0.850023\n",
      "Epoch 37 — train RMSE 0.231443 - train R2 0.479632 — val RMSE 0.134698 - val R2 0.829162\n",
      "Epoch 38 — train RMSE 0.221867 - train R2 0.521801 — val RMSE 0.148203 - val R2 0.793188\n",
      "Epoch 39 — train RMSE 0.297025 - train R2 0.142943 — val RMSE 0.114723 - val R2 0.876075\n",
      "Epoch 40 — train RMSE 0.213968 - train R2 0.555244 — val RMSE 0.102850 - val R2 0.900398\n",
      "Epoch 41 — train RMSE 0.234366 - train R2 0.466407 — val RMSE 0.122368 - val R2 0.859007\n",
      "Epoch 42 — train RMSE 0.274755 - train R2 0.266647 — val RMSE 0.142894 - val R2 0.807741\n",
      "Epoch 43 — train RMSE 0.222691 - train R2 0.518243 — val RMSE 0.166182 - val R2 0.739966\n",
      "Epoch 44 — train RMSE 0.226084 - train R2 0.503452 — val RMSE 0.160273 - val R2 0.758131\n",
      "Epoch 45 — train RMSE 0.290688 - train R2 0.179125 — val RMSE 0.139820 - val R2 0.815924\n",
      "Epoch 46 — train RMSE 0.285741 - train R2 0.206827 — val RMSE 0.131843 - val R2 0.836327\n",
      "Epoch 47 — train RMSE 0.277005 - train R2 0.254586 — val RMSE 0.138811 - val R2 0.818570\n",
      "Epoch 48 — train RMSE 0.280706 - train R2 0.234533 — val RMSE 0.189074 - val R2 0.663392\n",
      "Epoch 49 — train RMSE 0.262589 - train R2 0.330156 — val RMSE 0.119196 - val R2 0.866223\n",
      "Epoch 50 — train RMSE 0.303139 - train R2 0.107300 — val RMSE 0.129105 - val R2 0.843056\n"
     ]
    }
   ],
   "source": [
    "train, validation = model_selection.train_test_split(train, test_size=0.1, random_state=1, stratify= train['Concentration'])\n",
    "\n",
    "model_log = train_model(configs, train, validation, 50, 8, True, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Log concentration\n",
    "data_log = data.copy()\n",
    "data_log['Concentration'] = np.log1p(data_log['Concentration']) #log(cancentration + 1)\n",
    "\n",
    "train, test = model_selection.train_test_split(data_log, test_size=0.1, random_state=1, stratify= data_log['Concentration'])\n",
    "\n",
    "scaler_out = StandardScaler()\n",
    "train['Concentration'] = scaler_out.fit_transform(train[['Concentration']])\n",
    "test['Concentration'] = scaler_out.transform(test[['Concentration']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "configs = Details(\n",
    "    pred_len = 1,      \n",
    "    seq_len  = 350,\n",
    "    d_ff = 32,\n",
    "    patch_len= 16, \n",
    "    stride = 8, \n",
    "    llm_layers = 4,\n",
    "    description = \"This dataset contains near-infrared (NIR) spectroscopy measurements of lactate concentration in a \" \\\n",
    "                  \"phosphate-buffered saline (PBS) solution. Each spectrum includes absorbance values across \" \\\n",
    "                  \"350 wavelengths. \" ,\n",
    "    dropout = 0.1,\n",
    "    n_heads = 2,\n",
    "    d_model = 16,\n",
    "    enc_in = 1,\n",
    "    d_llm = 4096,\n",
    "    llm_model = 'LLAMA',\n",
    "    lr = 0.0001\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e213705acee4bc3b7b88621207f1d1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  1 — train RMSE 1.034861 - train R2 -0.074440 — val RMSE 0.981588 - val R2 0.063052\n",
      "Epoch  2 — train RMSE 0.758395 - train R2 0.422955 — val RMSE 0.465333 - val R2 0.789436\n",
      "Epoch  3 — train RMSE 0.542620 - train R2 0.704600 — val RMSE 0.437158 - val R2 0.814163\n",
      "Epoch  4 — train RMSE 0.613829 - train R2 0.621982 — val RMSE 0.487894 - val R2 0.768523\n",
      "Epoch  5 — train RMSE 0.565360 - train R2 0.679322 — val RMSE 0.416975 - val R2 0.830926\n",
      "Epoch  6 — train RMSE 0.534705 - train R2 0.713156 — val RMSE 0.597645 - val R2 0.652669\n",
      "Epoch  7 — train RMSE 0.539374 - train R2 0.708124 — val RMSE 0.372241 - val R2 0.865257\n",
      "Epoch  8 — train RMSE 0.493696 - train R2 0.755467 — val RMSE 0.380283 - val R2 0.859373\n",
      "Epoch  9 — train RMSE 0.465467 - train R2 0.782631 — val RMSE 0.400578 - val R2 0.843962\n",
      "Epoch 10 — train RMSE 0.527872 - train R2 0.720440 — val RMSE 0.405031 - val R2 0.840473\n",
      "Epoch 11 — train RMSE 0.446384 - train R2 0.800090 — val RMSE 0.361640 - val R2 0.872823\n",
      "Epoch 12 — train RMSE 0.533834 - train R2 0.714089 — val RMSE 0.491287 - val R2 0.765292\n",
      "Epoch 13 — train RMSE 0.542458 - train R2 0.704777 — val RMSE 0.392029 - val R2 0.850551\n",
      "Epoch 14 — train RMSE 0.479917 - train R2 0.768926 — val RMSE 0.398343 - val R2 0.845698\n",
      "Epoch 15 — train RMSE 0.458345 - train R2 0.789233 — val RMSE 0.365032 - val R2 0.870426\n",
      "Epoch 16 — train RMSE 0.526381 - train R2 0.722017 — val RMSE 0.347742 - val R2 0.882410\n",
      "Epoch 17 — train RMSE 0.491984 - train R2 0.757160 — val RMSE 0.375577 - val R2 0.862832\n",
      "Epoch 18 — train RMSE 0.429801 - train R2 0.814667 — val RMSE 0.342696 - val R2 0.885798\n",
      "Epoch 19 — train RMSE 0.442914 - train R2 0.803185 — val RMSE 0.345249 - val R2 0.884090\n",
      "Epoch 20 — train RMSE 0.422800 - train R2 0.820656 — val RMSE 0.477883 - val R2 0.777925\n",
      "Epoch 21 — train RMSE 0.451936 - train R2 0.795086 — val RMSE 0.405608 - val R2 0.840019\n",
      "Epoch 22 — train RMSE 0.419936 - train R2 0.823077 — val RMSE 0.366884 - val R2 0.869107\n",
      "Epoch 23 — train RMSE 0.493138 - train R2 0.756019 — val RMSE 0.322068 - val R2 0.899132\n",
      "Epoch 24 — train RMSE 0.434277 - train R2 0.810787 — val RMSE 0.326585 - val R2 0.896283\n",
      "Epoch 25 — train RMSE 0.476984 - train R2 0.771742 — val RMSE 0.431812 - val R2 0.818680\n",
      "Epoch 26 — train RMSE 0.481600 - train R2 0.767303 — val RMSE 0.340806 - val R2 0.887054\n",
      "Epoch 27 — train RMSE 0.426053 - train R2 0.817885 — val RMSE 0.356167 - val R2 0.876643\n",
      "Epoch 28 — train RMSE 0.427494 - train R2 0.816651 — val RMSE 0.346732 - val R2 0.883092\n",
      "Epoch 29 — train RMSE 0.402007 - train R2 0.837862 — val RMSE 0.315063 - val R2 0.903472\n",
      "Epoch 30 — train RMSE 0.430546 - train R2 0.814024 — val RMSE 0.310977 - val R2 0.905960\n",
      "Epoch 31 — train RMSE 0.397925 - train R2 0.841138 — val RMSE 0.293059 - val R2 0.916485\n",
      "Epoch 32 — train RMSE 0.433553 - train R2 0.811417 — val RMSE 0.356212 - val R2 0.876612\n",
      "Epoch 33 — train RMSE 0.473402 - train R2 0.775158 — val RMSE 0.311612 - val R2 0.905575\n",
      "Epoch 34 — train RMSE 0.389757 - train R2 0.847592 — val RMSE 0.288425 - val R2 0.919105\n",
      "Epoch 35 — train RMSE 0.392802 - train R2 0.845202 — val RMSE 0.319690 - val R2 0.900616\n",
      "Epoch 36 — train RMSE 0.440503 - train R2 0.805323 — val RMSE 0.328960 - val R2 0.894769\n",
      "Epoch 37 — train RMSE 0.422828 - train R2 0.820632 — val RMSE 0.337132 - val R2 0.889476\n",
      "Epoch 38 — train RMSE 0.454060 - train R2 0.793155 — val RMSE 0.324458 - val R2 0.897630\n",
      "Epoch 39 — train RMSE 0.437667 - train R2 0.807821 — val RMSE 0.282931 - val R2 0.922157\n",
      "Epoch 40 — train RMSE 0.436358 - train R2 0.808969 — val RMSE 0.350158 - val R2 0.880770\n",
      "Epoch 41 — train RMSE 0.392404 - train R2 0.845516 — val RMSE 0.312600 - val R2 0.904976\n",
      "Epoch 42 — train RMSE 0.427388 - train R2 0.816742 — val RMSE 0.313644 - val R2 0.904340\n",
      "Epoch 43 — train RMSE 0.386126 - train R2 0.850419 — val RMSE 0.290062 - val R2 0.918184\n",
      "Epoch 44 — train RMSE 0.500815 - train R2 0.748364 — val RMSE 0.357415 - val R2 0.875777\n",
      "Epoch 45 — train RMSE 0.387179 - train R2 0.849602 — val RMSE 0.312919 - val R2 0.904782\n",
      "Epoch 46 — train RMSE 0.378447 - train R2 0.856309 — val RMSE 0.298787 - val R2 0.913188\n",
      "Epoch 47 — train RMSE 0.380165 - train R2 0.855002 — val RMSE 0.268999 - val R2 0.929635\n",
      "Epoch 48 — train RMSE 0.383725 - train R2 0.852273 — val RMSE 0.305238 - val R2 0.909399\n",
      "Epoch 49 — train RMSE 0.448798 - train R2 0.797922 — val RMSE 0.304395 - val R2 0.909899\n",
      "Epoch 50 — train RMSE 0.332454 - train R2 0.889113 — val RMSE 0.314081 - val R2 0.904073\n"
     ]
    }
   ],
   "source": [
    "train, validation = model_selection.train_test_split(train, test_size=0.1, random_state=1, stratify= train['Concentration'])\n",
    "\n",
    "model_log = train_model(configs, train, validation, 50, 8, True, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "282ce4b6628a4c7fa7d41aa2a5b4cab0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  1 — train RMSE 1.125693 - train R2 -0.271330 — val RMSE 1.086364 - val R2 -0.147645\n",
      "Epoch  2 — train RMSE 1.009709 - train R2 -0.022847 — val RMSE 0.975761 - val R2 0.074143\n",
      "Epoch  3 — train RMSE 0.734508 - train R2 0.458734 — val RMSE 0.589761 - val R2 0.661772\n",
      "Epoch  4 — train RMSE 0.578438 - train R2 0.664315 — val RMSE 0.500950 - val R2 0.755968\n",
      "Epoch  5 — train RMSE 0.601650 - train R2 0.636833 — val RMSE 0.467079 - val R2 0.787853\n",
      "Epoch  6 — train RMSE 0.536076 - train R2 0.711682 — val RMSE 0.401567 - val R2 0.843191\n",
      "Epoch  7 — train RMSE 0.529969 - train R2 0.718214 — val RMSE 0.397022 - val R2 0.846720\n",
      "Epoch  8 — train RMSE 0.503560 - train R2 0.745598 — val RMSE 0.375701 - val R2 0.862741\n",
      "Epoch  9 — train RMSE 0.490690 - train R2 0.758436 — val RMSE 0.407362 - val R2 0.838632\n",
      "Epoch 10 — train RMSE 0.556919 - train R2 0.688826 — val RMSE 0.463304 - val R2 0.791268\n",
      "Epoch 11 — train RMSE 0.526516 - train R2 0.721874 — val RMSE 0.394272 - val R2 0.848836\n",
      "Epoch 12 — train RMSE 0.476304 - train R2 0.772392 — val RMSE 0.332941 - val R2 0.892207\n",
      "Epoch 13 — train RMSE 0.509176 - train R2 0.739892 — val RMSE 0.341429 - val R2 0.886641\n",
      "Epoch 14 — train RMSE 0.420216 - train R2 0.822841 — val RMSE 0.370818 - val R2 0.866286\n",
      "Epoch 15 — train RMSE 0.440141 - train R2 0.805642 — val RMSE 0.338468 - val R2 0.888598\n",
      "Epoch 16 — train RMSE 0.456427 - train R2 0.790993 — val RMSE 0.308291 - val R2 0.907578\n",
      "Epoch 17 — train RMSE 0.408183 - train R2 0.832841 — val RMSE 0.324225 - val R2 0.897776\n",
      "Epoch 18 — train RMSE 0.513874 - train R2 0.735070 — val RMSE 0.338198 - val R2 0.888776\n",
      "Epoch 19 — train RMSE 0.406920 - train R2 0.833875 — val RMSE 0.290724 - val R2 0.917810\n",
      "Epoch 20 — train RMSE 0.488508 - train R2 0.760579 — val RMSE 0.385037 - val R2 0.855835\n",
      "Epoch 21 — train RMSE 0.459997 - train R2 0.787710 — val RMSE 0.288199 - val R2 0.919231\n",
      "Epoch 22 — train RMSE 0.466638 - train R2 0.781537 — val RMSE 0.296563 - val R2 0.914476\n",
      "Epoch 23 — train RMSE 0.409302 - train R2 0.831924 — val RMSE 0.307956 - val R2 0.907778\n",
      "Epoch 24 — train RMSE 0.494184 - train R2 0.754983 — val RMSE 0.275339 - val R2 0.926279\n",
      "Epoch 25 — train RMSE 0.449955 - train R2 0.796878 — val RMSE 0.282910 - val R2 0.922169\n",
      "Epoch 26 — train RMSE 0.450174 - train R2 0.796680 — val RMSE 0.418956 - val R2 0.829316\n",
      "Epoch 27 — train RMSE 0.404303 - train R2 0.836004 — val RMSE 0.386429 - val R2 0.854790\n",
      "Epoch 28 — train RMSE 0.420205 - train R2 0.822851 — val RMSE 0.336385 - val R2 0.889965\n",
      "Epoch 29 — train RMSE 0.499442 - train R2 0.749741 — val RMSE 0.300280 - val R2 0.912318\n",
      "Epoch 30 — train RMSE 0.468012 - train R2 0.780248 — val RMSE 0.491522 - val R2 0.765067\n",
      "Epoch 31 — train RMSE 0.474662 - train R2 0.773959 — val RMSE 0.386049 - val R2 0.855076\n",
      "Epoch 32 — train RMSE 0.380370 - train R2 0.854845 — val RMSE 0.345239 - val R2 0.884097\n",
      "Epoch 33 — train RMSE 0.417537 - train R2 0.825093 — val RMSE 0.300030 - val R2 0.912464\n",
      "Epoch 34 — train RMSE 0.447052 - train R2 0.799491 — val RMSE 0.330964 - val R2 0.893483\n",
      "Early stopping triggered; Best val RMSE : 0.275339\n"
     ]
    }
   ],
   "source": [
    "train, validation = model_selection.train_test_split(train, test_size=0.1, random_state=1, stratify= train['Concentration'])\n",
    "\n",
    "model_log = train_model(configs, train, validation, 100, 8, True, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([134.84718917])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.expm1(0.275339*scaler_out.scale_ + scaler_out.mean_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try predict wavelength"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1014.08</th>\n",
       "      <th>1015.85</th>\n",
       "      <th>1017.62</th>\n",
       "      <th>1019.40</th>\n",
       "      <th>1021.19</th>\n",
       "      <th>1022.98</th>\n",
       "      <th>1024.77</th>\n",
       "      <th>1026.58</th>\n",
       "      <th>1028.39</th>\n",
       "      <th>1030.21</th>\n",
       "      <th>...</th>\n",
       "      <th>2481.26</th>\n",
       "      <th>2491.87</th>\n",
       "      <th>2502.56</th>\n",
       "      <th>2513.35</th>\n",
       "      <th>2524.23</th>\n",
       "      <th>2535.20</th>\n",
       "      <th>2546.27</th>\n",
       "      <th>2557.44</th>\n",
       "      <th>2568.71</th>\n",
       "      <th>2580.07</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.005231</td>\n",
       "      <td>0.038095</td>\n",
       "      <td>0.082302</td>\n",
       "      <td>0.082237</td>\n",
       "      <td>0.076576</td>\n",
       "      <td>0.106940</td>\n",
       "      <td>0.095478</td>\n",
       "      <td>0.089268</td>\n",
       "      <td>0.086707</td>\n",
       "      <td>0.053949</td>\n",
       "      <td>...</td>\n",
       "      <td>0.026194</td>\n",
       "      <td>0.021901</td>\n",
       "      <td>0.019588</td>\n",
       "      <td>0.029467</td>\n",
       "      <td>0.030502</td>\n",
       "      <td>0.012769</td>\n",
       "      <td>0.000612</td>\n",
       "      <td>-0.015370</td>\n",
       "      <td>-0.047555</td>\n",
       "      <td>-0.068557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.081353</td>\n",
       "      <td>-0.131399</td>\n",
       "      <td>-0.160719</td>\n",
       "      <td>-0.190526</td>\n",
       "      <td>-0.224999</td>\n",
       "      <td>-0.239949</td>\n",
       "      <td>-0.231437</td>\n",
       "      <td>-0.181605</td>\n",
       "      <td>-0.190708</td>\n",
       "      <td>-0.171021</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012446</td>\n",
       "      <td>0.048297</td>\n",
       "      <td>0.064985</td>\n",
       "      <td>0.043647</td>\n",
       "      <td>0.008629</td>\n",
       "      <td>-0.035054</td>\n",
       "      <td>-0.086570</td>\n",
       "      <td>-0.152690</td>\n",
       "      <td>-0.234232</td>\n",
       "      <td>-0.326223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.143674</td>\n",
       "      <td>-0.116576</td>\n",
       "      <td>-0.123858</td>\n",
       "      <td>-0.097288</td>\n",
       "      <td>-0.035242</td>\n",
       "      <td>0.023129</td>\n",
       "      <td>0.056915</td>\n",
       "      <td>0.098068</td>\n",
       "      <td>0.075592</td>\n",
       "      <td>0.067727</td>\n",
       "      <td>...</td>\n",
       "      <td>0.011739</td>\n",
       "      <td>-0.003521</td>\n",
       "      <td>-0.014063</td>\n",
       "      <td>-0.008997</td>\n",
       "      <td>0.002582</td>\n",
       "      <td>0.017829</td>\n",
       "      <td>0.048742</td>\n",
       "      <td>0.084157</td>\n",
       "      <td>0.115354</td>\n",
       "      <td>0.151298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.096424</td>\n",
       "      <td>0.010348</td>\n",
       "      <td>0.037800</td>\n",
       "      <td>0.007231</td>\n",
       "      <td>-0.021712</td>\n",
       "      <td>-0.037742</td>\n",
       "      <td>-0.082596</td>\n",
       "      <td>-0.089286</td>\n",
       "      <td>-0.070054</td>\n",
       "      <td>-0.067586</td>\n",
       "      <td>...</td>\n",
       "      <td>0.091157</td>\n",
       "      <td>0.068864</td>\n",
       "      <td>0.056702</td>\n",
       "      <td>0.034800</td>\n",
       "      <td>0.020370</td>\n",
       "      <td>0.016746</td>\n",
       "      <td>0.001978</td>\n",
       "      <td>0.007347</td>\n",
       "      <td>0.025432</td>\n",
       "      <td>0.049032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.013245</td>\n",
       "      <td>0.088878</td>\n",
       "      <td>0.055112</td>\n",
       "      <td>0.057282</td>\n",
       "      <td>0.044607</td>\n",
       "      <td>0.047680</td>\n",
       "      <td>0.033936</td>\n",
       "      <td>0.050192</td>\n",
       "      <td>0.005173</td>\n",
       "      <td>-0.038979</td>\n",
       "      <td>...</td>\n",
       "      <td>0.074989</td>\n",
       "      <td>0.022504</td>\n",
       "      <td>-0.014653</td>\n",
       "      <td>-0.054191</td>\n",
       "      <td>-0.089806</td>\n",
       "      <td>-0.118453</td>\n",
       "      <td>-0.134508</td>\n",
       "      <td>-0.148832</td>\n",
       "      <td>-0.151126</td>\n",
       "      <td>-0.144606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>351</th>\n",
       "      <td>0.013892</td>\n",
       "      <td>-0.023256</td>\n",
       "      <td>-0.031826</td>\n",
       "      <td>-0.053169</td>\n",
       "      <td>-0.084421</td>\n",
       "      <td>-0.100099</td>\n",
       "      <td>-0.078321</td>\n",
       "      <td>-0.081930</td>\n",
       "      <td>-0.054463</td>\n",
       "      <td>-0.031788</td>\n",
       "      <td>...</td>\n",
       "      <td>0.149447</td>\n",
       "      <td>0.165580</td>\n",
       "      <td>0.169533</td>\n",
       "      <td>0.151979</td>\n",
       "      <td>0.127341</td>\n",
       "      <td>0.088787</td>\n",
       "      <td>0.039029</td>\n",
       "      <td>-0.012803</td>\n",
       "      <td>-0.064349</td>\n",
       "      <td>-0.110492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>352</th>\n",
       "      <td>-0.300830</td>\n",
       "      <td>-0.334294</td>\n",
       "      <td>-0.300479</td>\n",
       "      <td>-0.285832</td>\n",
       "      <td>-0.234678</td>\n",
       "      <td>-0.182459</td>\n",
       "      <td>-0.151797</td>\n",
       "      <td>-0.119183</td>\n",
       "      <td>-0.052906</td>\n",
       "      <td>-0.067668</td>\n",
       "      <td>...</td>\n",
       "      <td>0.063646</td>\n",
       "      <td>0.027490</td>\n",
       "      <td>0.003725</td>\n",
       "      <td>-0.002946</td>\n",
       "      <td>0.010705</td>\n",
       "      <td>0.029308</td>\n",
       "      <td>0.049670</td>\n",
       "      <td>0.079636</td>\n",
       "      <td>0.110156</td>\n",
       "      <td>0.135482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>353</th>\n",
       "      <td>-0.199545</td>\n",
       "      <td>-0.220742</td>\n",
       "      <td>-0.222265</td>\n",
       "      <td>-0.198332</td>\n",
       "      <td>-0.153330</td>\n",
       "      <td>-0.132572</td>\n",
       "      <td>-0.199724</td>\n",
       "      <td>-0.168876</td>\n",
       "      <td>-0.109947</td>\n",
       "      <td>-0.049659</td>\n",
       "      <td>...</td>\n",
       "      <td>0.096368</td>\n",
       "      <td>0.073663</td>\n",
       "      <td>0.063638</td>\n",
       "      <td>0.061432</td>\n",
       "      <td>0.066310</td>\n",
       "      <td>0.063311</td>\n",
       "      <td>0.060929</td>\n",
       "      <td>0.077316</td>\n",
       "      <td>0.088265</td>\n",
       "      <td>0.111251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>354</th>\n",
       "      <td>-0.107494</td>\n",
       "      <td>-0.126795</td>\n",
       "      <td>-0.182586</td>\n",
       "      <td>-0.215886</td>\n",
       "      <td>-0.246338</td>\n",
       "      <td>-0.249917</td>\n",
       "      <td>-0.254625</td>\n",
       "      <td>-0.288691</td>\n",
       "      <td>-0.253613</td>\n",
       "      <td>-0.208179</td>\n",
       "      <td>...</td>\n",
       "      <td>0.123685</td>\n",
       "      <td>0.132682</td>\n",
       "      <td>0.137421</td>\n",
       "      <td>0.141735</td>\n",
       "      <td>0.143292</td>\n",
       "      <td>0.140572</td>\n",
       "      <td>0.137399</td>\n",
       "      <td>0.136859</td>\n",
       "      <td>0.126693</td>\n",
       "      <td>0.115020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>355</th>\n",
       "      <td>0.027243</td>\n",
       "      <td>0.027325</td>\n",
       "      <td>0.045799</td>\n",
       "      <td>-0.001712</td>\n",
       "      <td>0.018226</td>\n",
       "      <td>-0.049751</td>\n",
       "      <td>-0.056777</td>\n",
       "      <td>-0.107068</td>\n",
       "      <td>-0.135225</td>\n",
       "      <td>-0.143717</td>\n",
       "      <td>...</td>\n",
       "      <td>0.084787</td>\n",
       "      <td>0.070592</td>\n",
       "      <td>0.061971</td>\n",
       "      <td>0.066389</td>\n",
       "      <td>0.073253</td>\n",
       "      <td>0.083245</td>\n",
       "      <td>0.103166</td>\n",
       "      <td>0.116690</td>\n",
       "      <td>0.136836</td>\n",
       "      <td>0.146329</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>356 rows × 350 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      1014.08   1015.85   1017.62   1019.40   1021.19   1022.98   1024.77  \\\n",
       "0   -0.005231  0.038095  0.082302  0.082237  0.076576  0.106940  0.095478   \n",
       "1   -0.081353 -0.131399 -0.160719 -0.190526 -0.224999 -0.239949 -0.231437   \n",
       "2   -0.143674 -0.116576 -0.123858 -0.097288 -0.035242  0.023129  0.056915   \n",
       "3    0.096424  0.010348  0.037800  0.007231 -0.021712 -0.037742 -0.082596   \n",
       "4    0.013245  0.088878  0.055112  0.057282  0.044607  0.047680  0.033936   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "351  0.013892 -0.023256 -0.031826 -0.053169 -0.084421 -0.100099 -0.078321   \n",
       "352 -0.300830 -0.334294 -0.300479 -0.285832 -0.234678 -0.182459 -0.151797   \n",
       "353 -0.199545 -0.220742 -0.222265 -0.198332 -0.153330 -0.132572 -0.199724   \n",
       "354 -0.107494 -0.126795 -0.182586 -0.215886 -0.246338 -0.249917 -0.254625   \n",
       "355  0.027243  0.027325  0.045799 -0.001712  0.018226 -0.049751 -0.056777   \n",
       "\n",
       "      1026.58   1028.39   1030.21  ...   2481.26   2491.87   2502.56  \\\n",
       "0    0.089268  0.086707  0.053949  ...  0.026194  0.021901  0.019588   \n",
       "1   -0.181605 -0.190708 -0.171021  ...  0.012446  0.048297  0.064985   \n",
       "2    0.098068  0.075592  0.067727  ...  0.011739 -0.003521 -0.014063   \n",
       "3   -0.089286 -0.070054 -0.067586  ...  0.091157  0.068864  0.056702   \n",
       "4    0.050192  0.005173 -0.038979  ...  0.074989  0.022504 -0.014653   \n",
       "..        ...       ...       ...  ...       ...       ...       ...   \n",
       "351 -0.081930 -0.054463 -0.031788  ...  0.149447  0.165580  0.169533   \n",
       "352 -0.119183 -0.052906 -0.067668  ...  0.063646  0.027490  0.003725   \n",
       "353 -0.168876 -0.109947 -0.049659  ...  0.096368  0.073663  0.063638   \n",
       "354 -0.288691 -0.253613 -0.208179  ...  0.123685  0.132682  0.137421   \n",
       "355 -0.107068 -0.135225 -0.143717  ...  0.084787  0.070592  0.061971   \n",
       "\n",
       "      2513.35   2524.23   2535.20   2546.27   2557.44   2568.71   2580.07  \n",
       "0    0.029467  0.030502  0.012769  0.000612 -0.015370 -0.047555 -0.068557  \n",
       "1    0.043647  0.008629 -0.035054 -0.086570 -0.152690 -0.234232 -0.326223  \n",
       "2   -0.008997  0.002582  0.017829  0.048742  0.084157  0.115354  0.151298  \n",
       "3    0.034800  0.020370  0.016746  0.001978  0.007347  0.025432  0.049032  \n",
       "4   -0.054191 -0.089806 -0.118453 -0.134508 -0.148832 -0.151126 -0.144606  \n",
       "..        ...       ...       ...       ...       ...       ...       ...  \n",
       "351  0.151979  0.127341  0.088787  0.039029 -0.012803 -0.064349 -0.110492  \n",
       "352 -0.002946  0.010705  0.029308  0.049670  0.079636  0.110156  0.135482  \n",
       "353  0.061432  0.066310  0.063311  0.060929  0.077316  0.088265  0.111251  \n",
       "354  0.141735  0.143292  0.140572  0.137399  0.136859  0.126693  0.115020  \n",
       "355  0.066389  0.073253  0.083245  0.103166  0.116690  0.136836  0.146329  \n",
       "\n",
       "[356 rows x 350 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data2 = data.iloc[:,1:]\n",
    "data2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ae0708abff2439c89e3c4ad3c2ee95d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  1 — train RMSE 0.177001 - train R2 -2.246421 — val RMSE 0.114138 - val R2 -0.188732\n",
      "Epoch  2 — train RMSE 0.120968 - train R2 -0.516326 — val RMSE 0.156551 - val R2 -1.236340\n",
      "Epoch  3 — train RMSE 0.138621 - train R2 -0.991188 — val RMSE 0.142573 - val R2 -0.854802\n",
      "Epoch  4 — train RMSE 0.116000 - train R2 -0.394332 — val RMSE 0.098859 - val R2 0.108220\n",
      "Epoch  5 — train RMSE 0.108171 - train R2 -0.212480 — val RMSE 0.101436 - val R2 0.061116\n",
      "Epoch  6 — train RMSE 0.121804 - train R2 -0.537370 — val RMSE 0.083533 - val R2 0.363298\n",
      "Epoch  7 — train RMSE 0.098091 - train R2 0.002958 — val RMSE 0.100113 - val R2 0.085443\n",
      "Epoch  8 — train RMSE 0.095377 - train R2 0.057373 — val RMSE 0.085272 - val R2 0.336505\n",
      "Epoch  9 — train RMSE 0.087803 - train R2 0.201135 — val RMSE 0.060405 - val R2 0.667052\n",
      "Epoch 10 — train RMSE 0.093472 - train R2 0.094655 — val RMSE 0.114163 - val R2 -0.189260\n",
      "Epoch 11 — train RMSE 0.095565 - train R2 0.053645 — val RMSE 0.062111 - val R2 0.647984\n",
      "Epoch 12 — train RMSE 0.095884 - train R2 0.047317 — val RMSE 0.149332 - val R2 -1.034837\n",
      "Epoch 13 — train RMSE 0.091633 - train R2 0.129935 — val RMSE 0.061824 - val R2 0.651225\n",
      "Epoch 14 — train RMSE 0.092815 - train R2 0.107338 — val RMSE 0.065809 - val R2 0.604823\n",
      "Epoch 15 — train RMSE 0.087774 - train R2 0.201658 — val RMSE 0.054367 - val R2 0.730287\n",
      "Epoch 16 — train RMSE 0.088515 - train R2 0.188126 — val RMSE 0.082202 - val R2 0.383426\n",
      "Epoch 17 — train RMSE 0.108234 - train R2 -0.213900 — val RMSE 0.082812 - val R2 0.374229\n",
      "Epoch 18 — train RMSE 0.082086 - train R2 0.301775 — val RMSE 0.064991 - val R2 0.614582\n",
      "Epoch 19 — train RMSE 0.089656 - train R2 0.167074 — val RMSE 0.075996 - val R2 0.473008\n",
      "Epoch 20 — train RMSE 0.089415 - train R2 0.171534 — val RMSE 0.066367 - val R2 0.598090\n",
      "Epoch 21 — train RMSE 0.081927 - train R2 0.304485 — val RMSE 0.070337 - val R2 0.548573\n",
      "Epoch 22 — train RMSE 0.099924 - train R2 -0.034650 — val RMSE 0.071742 - val R2 0.530354\n",
      "Epoch 23 — train RMSE 0.090094 - train R2 0.158899 — val RMSE 0.060887 - val R2 0.661718\n",
      "Epoch 24 — train RMSE 0.087504 - train R2 0.206568 — val RMSE 0.064014 - val R2 0.626080\n",
      "Epoch 25 — train RMSE 0.082132 - train R2 0.301006 — val RMSE 0.060140 - val R2 0.669972\n",
      "Epoch 26 — train RMSE 0.088020 - train R2 0.197178 — val RMSE 0.084504 - val R2 0.348404\n",
      "Epoch 27 — train RMSE 0.085431 - train R2 0.243721 — val RMSE 0.058446 - val R2 0.688302\n",
      "Epoch 28 — train RMSE 0.095106 - train R2 0.062715 — val RMSE 0.064914 - val R2 0.615492\n",
      "Epoch 29 — train RMSE 0.088151 - train R2 0.194802 — val RMSE 0.083188 - val R2 0.368533\n",
      "Epoch 30 — train RMSE 0.098787 - train R2 -0.011230 — val RMSE 0.089864 - val R2 0.263112\n",
      "Epoch 31 — train RMSE 0.085078 - train R2 0.249959 — val RMSE 0.053437 - val R2 0.739442\n",
      "Epoch 32 — train RMSE 0.084477 - train R2 0.260517 — val RMSE 0.053656 - val R2 0.737297\n",
      "Epoch 33 — train RMSE 0.077454 - train R2 0.378355 — val RMSE 0.051656 - val R2 0.756520\n",
      "Epoch 34 — train RMSE 0.079193 - train R2 0.350125 — val RMSE 0.049448 - val R2 0.776884\n",
      "Epoch 35 — train RMSE 0.094486 - train R2 0.074894 — val RMSE 0.062619 - val R2 0.642202\n",
      "Epoch 36 — train RMSE 0.099029 - train R2 -0.016197 — val RMSE 0.099959 - val R2 0.088262\n",
      "Epoch 37 — train RMSE 0.094547 - train R2 0.073713 — val RMSE 0.088274 - val R2 0.288963\n",
      "Epoch 38 — train RMSE 0.097550 - train R2 0.013927 — val RMSE 0.057853 - val R2 0.694595\n",
      "Epoch 39 — train RMSE 0.084315 - train R2 0.263352 — val RMSE 0.070504 - val R2 0.546427\n",
      "Epoch 40 — train RMSE 0.085521 - train R2 0.242131 — val RMSE 0.066992 - val R2 0.590480\n",
      "Epoch 41 — train RMSE 0.093489 - train R2 0.094320 — val RMSE 0.083521 - val R2 0.363480\n",
      "Epoch 42 — train RMSE 0.091976 - train R2 0.123408 — val RMSE 0.061331 - val R2 0.656773\n",
      "Epoch 43 — train RMSE 0.081712 - train R2 0.308136 — val RMSE 0.054683 - val R2 0.727143\n",
      "Epoch 44 — train RMSE 0.085661 - train R2 0.239645 — val RMSE 0.066373 - val R2 0.598017\n",
      "Epoch 45 — train RMSE 0.084429 - train R2 0.261358 — val RMSE 0.055158 - val R2 0.722382\n",
      "Epoch 46 — train RMSE 0.105729 - train R2 -0.158360 — val RMSE 0.052472 - val R2 0.748768\n",
      "Epoch 47 — train RMSE 0.082801 - train R2 0.289563 — val RMSE 0.068331 - val R2 0.573949\n",
      "Epoch 48 — train RMSE 0.079776 - train R2 0.340521 — val RMSE 0.058587 - val R2 0.686794\n",
      "Epoch 49 — train RMSE 0.079630 - train R2 0.342938 — val RMSE 0.052462 - val R2 0.748859\n",
      "Epoch 50 — train RMSE 0.076318 - train R2 0.396453 — val RMSE 0.051832 - val R2 0.754859\n",
      "Epoch 51 — train RMSE 0.083017 - train R2 0.285851 — val RMSE 0.057619 - val R2 0.697059\n",
      "Epoch 52 — train RMSE 0.084802 - train R2 0.254807 — val RMSE 0.056314 - val R2 0.710624\n",
      "Epoch 53 — train RMSE 0.078731 - train R2 0.357686 — val RMSE 0.053144 - val R2 0.742290\n",
      "Epoch 54 — train RMSE 0.078998 - train R2 0.353325 — val RMSE 0.068260 - val R2 0.574840\n",
      "Epoch 55 — train RMSE 0.083065 - train R2 0.285023 — val RMSE 0.066759 - val R2 0.593325\n",
      "Epoch 56 — train RMSE 0.091795 - train R2 0.126854 — val RMSE 0.069655 - val R2 0.557275\n",
      "Epoch 57 — train RMSE 0.080006 - train R2 0.336716 — val RMSE 0.050732 - val R2 0.765147\n",
      "Epoch 58 — train RMSE 0.083267 - train R2 0.281547 — val RMSE 0.060357 - val R2 0.667587\n",
      "Epoch 59 — train RMSE 0.080314 - train R2 0.331599 — val RMSE 0.048952 - val R2 0.781342\n",
      "Epoch 60 — train RMSE 0.076662 - train R2 0.391009 — val RMSE 0.053370 - val R2 0.740089\n",
      "Epoch 61 — train RMSE 0.079906 - train R2 0.338373 — val RMSE 0.067995 - val R2 0.578132\n",
      "Epoch 62 — train RMSE 0.079769 - train R2 0.340634 — val RMSE 0.049270 - val R2 0.778489\n",
      "Epoch 63 — train RMSE 0.076911 - train R2 0.387050 — val RMSE 0.068175 - val R2 0.575894\n",
      "Epoch 64 — train RMSE 0.090144 - train R2 0.157970 — val RMSE 0.064023 - val R2 0.625982\n",
      "Epoch 65 — train RMSE 0.081410 - train R2 0.313228 — val RMSE 0.046246 - val R2 0.804851\n",
      "Epoch 66 — train RMSE 0.078942 - train R2 0.354249 — val RMSE 0.060840 - val R2 0.662243\n",
      "Epoch 67 — train RMSE 0.073313 - train R2 0.443057 — val RMSE 0.042512 - val R2 0.835086\n",
      "Epoch 68 — train RMSE 0.080921 - train R2 0.321455 — val RMSE 0.054122 - val R2 0.732712\n",
      "Epoch 69 — train RMSE 0.084874 - train R2 0.253547 — val RMSE 0.070670 - val R2 0.544277\n",
      "Epoch 70 — train RMSE 0.082302 - train R2 0.298096 — val RMSE 0.052853 - val R2 0.745103\n",
      "Epoch 71 — train RMSE 0.083176 - train R2 0.283119 — val RMSE 0.053573 - val R2 0.738111\n",
      "Epoch 72 — train RMSE 0.076213 - train R2 0.398121 — val RMSE 0.057464 - val R2 0.698691\n",
      "Epoch 73 — train RMSE 0.071487 - train R2 0.470456 — val RMSE 0.041964 - val R2 0.839311\n",
      "Epoch 74 — train RMSE 0.084468 - train R2 0.260677 — val RMSE 0.054813 - val R2 0.725847\n",
      "Epoch 75 — train RMSE 0.084807 - train R2 0.254733 — val RMSE 0.058538 - val R2 0.687322\n",
      "Epoch 76 — train RMSE 0.081135 - train R2 0.317874 — val RMSE 0.063074 - val R2 0.636986\n",
      "Epoch 77 — train RMSE 0.080003 - train R2 0.336776 — val RMSE 0.054272 - val R2 0.731233\n",
      "Epoch 78 — train RMSE 0.074482 - train R2 0.425157 — val RMSE 0.054800 - val R2 0.725978\n",
      "Epoch 79 — train RMSE 0.077347 - train R2 0.380068 — val RMSE 0.045485 - val R2 0.811215\n",
      "Epoch 80 — train RMSE 0.073900 - train R2 0.434103 — val RMSE 0.049350 - val R2 0.777775\n",
      "Epoch 81 — train RMSE 0.075311 - train R2 0.412288 — val RMSE 0.077988 - val R2 0.445020\n",
      "Epoch 82 — train RMSE 0.074428 - train R2 0.425988 — val RMSE 0.064578 - val R2 0.619460\n",
      "Epoch 83 — train RMSE 0.077327 - train R2 0.380400 — val RMSE 0.047831 - val R2 0.791243\n",
      "Epoch 84 — train RMSE 0.072569 - train R2 0.454295 — val RMSE 0.046473 - val R2 0.802927\n",
      "Epoch 85 — train RMSE 0.078184 - train R2 0.366582 — val RMSE 0.055581 - val R2 0.718107\n",
      "Epoch 86 — train RMSE 0.077741 - train R2 0.373741 — val RMSE 0.048529 - val R2 0.785108\n",
      "Epoch 87 — train RMSE 0.079748 - train R2 0.340985 — val RMSE 0.051918 - val R2 0.754038\n",
      "Epoch 88 — train RMSE 0.071146 - train R2 0.475484 — val RMSE 0.050835 - val R2 0.764200\n",
      "Epoch 89 — train RMSE 0.073818 - train R2 0.435359 — val RMSE 0.047766 - val R2 0.791805\n",
      "Epoch 90 — train RMSE 0.078307 - train R2 0.364593 — val RMSE 0.057664 - val R2 0.696590\n",
      "Epoch 91 — train RMSE 0.072866 - train R2 0.449824 — val RMSE 0.047095 - val R2 0.797615\n",
      "Epoch 92 — train RMSE 0.077411 - train R2 0.379052 — val RMSE 0.045526 - val R2 0.810879\n",
      "Epoch 93 — train RMSE 0.077967 - train R2 0.370093 — val RMSE 0.046959 - val R2 0.798782\n",
      "Epoch 94 — train RMSE 0.076573 - train R2 0.392415 — val RMSE 0.051319 - val R2 0.759682\n",
      "Epoch 95 — train RMSE 0.077853 - train R2 0.371935 — val RMSE 0.043516 - val R2 0.827205\n",
      "Epoch 96 — train RMSE 0.078940 - train R2 0.354271 — val RMSE 0.050590 - val R2 0.766462\n",
      "Epoch 97 — train RMSE 0.078261 - train R2 0.365340 — val RMSE 0.049598 - val R2 0.775535\n",
      "Epoch 98 — train RMSE 0.078369 - train R2 0.363588 — val RMSE 0.063569 - val R2 0.631268\n",
      "Epoch 99 — train RMSE 0.073160 - train R2 0.445368 — val RMSE 0.066473 - val R2 0.596806\n",
      "Epoch 100 — train RMSE 0.077664 - train R2 0.374986 — val RMSE 0.056310 - val R2 0.710671\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'stand' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 24\u001b[0m\n\u001b[1;32m     20\u001b[0m train, test \u001b[38;5;241m=\u001b[39m model_selection\u001b[38;5;241m.\u001b[39mtrain_test_split(data2, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     22\u001b[0m train, validation \u001b[38;5;241m=\u001b[39m model_selection\u001b[38;5;241m.\u001b[39mtrain_test_split(train, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 24\u001b[0m model ,train_rmse, train_r2, val_rmse, val_r2, stand  \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfigs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[14], line 113\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(configs, train, validation, epochs, batch_size, updates, earlystop, patience)\u001b[0m\n\u001b[1;32m    111\u001b[0m                 \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEarly stopping triggered; Best val RMSE : \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbest_val_rmse\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.6f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    112\u001b[0m                 \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m--> 113\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model, train_rmse, train_r2, val_rmse, val_r2, \u001b[43mstand\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'stand' is not defined"
     ]
    }
   ],
   "source": [
    "configs = Details(\n",
    "    pred_len = 1,      \n",
    "    seq_len  = 349,\n",
    "    d_ff = 32,\n",
    "    patch_len= 16, \n",
    "    stride = 8, \n",
    "    llm_layers = 4,\n",
    "    description = \"This dataset contains near-infrared (NIR) spectroscopy measurements of lactate concentration in a \" \\\n",
    "                  \"phosphate-buffered saline (PBS) solution. Each spectrum includes absorbance values across \" \\\n",
    "                  \"349 wavelengths. Try predict the next wavelength \" ,\n",
    "    dropout = 0.1,\n",
    "    n_heads = 2,\n",
    "    d_model = 16,\n",
    "    enc_in = 1,\n",
    "    d_llm = 4096,\n",
    "    llm_model = 'LLAMA',\n",
    "    lr = 0.0001\n",
    ")\n",
    "\n",
    "train, test = model_selection.train_test_split(data2, test_size=0.1, random_state=1)\n",
    "\n",
    "train, validation = model_selection.train_test_split(train, test_size=0.1, random_state=1)\n",
    "\n",
    "model ,train_rmse, train_r2, val_rmse, val_r2, stand  = train_model(configs, train, validation, 100, 8, True, False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interference Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Lactate</th>\n",
       "      <th>Urea</th>\n",
       "      <th>Glucose</th>\n",
       "      <th>1350.0</th>\n",
       "      <th>1351.0</th>\n",
       "      <th>1352.0</th>\n",
       "      <th>1353.0</th>\n",
       "      <th>1354.0</th>\n",
       "      <th>1355.0</th>\n",
       "      <th>1356.0</th>\n",
       "      <th>...</th>\n",
       "      <th>2491.0</th>\n",
       "      <th>2492.0</th>\n",
       "      <th>2493.0</th>\n",
       "      <th>2494.0</th>\n",
       "      <th>2495.0</th>\n",
       "      <th>2496.0</th>\n",
       "      <th>2497.0</th>\n",
       "      <th>2498.0</th>\n",
       "      <th>2499.0</th>\n",
       "      <th>2500.0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.325606</td>\n",
       "      <td>0.326118</td>\n",
       "      <td>0.326777</td>\n",
       "      <td>0.327594</td>\n",
       "      <td>0.328574</td>\n",
       "      <td>0.329736</td>\n",
       "      <td>0.331045</td>\n",
       "      <td>...</td>\n",
       "      <td>3.884846</td>\n",
       "      <td>3.891455</td>\n",
       "      <td>3.897297</td>\n",
       "      <td>3.902262</td>\n",
       "      <td>3.906273</td>\n",
       "      <td>3.909308</td>\n",
       "      <td>3.911418</td>\n",
       "      <td>3.912773</td>\n",
       "      <td>3.913718</td>\n",
       "      <td>3.914531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.325919</td>\n",
       "      <td>0.326437</td>\n",
       "      <td>0.327103</td>\n",
       "      <td>0.327927</td>\n",
       "      <td>0.328911</td>\n",
       "      <td>0.330068</td>\n",
       "      <td>0.331363</td>\n",
       "      <td>...</td>\n",
       "      <td>3.902861</td>\n",
       "      <td>3.909777</td>\n",
       "      <td>3.914507</td>\n",
       "      <td>3.917156</td>\n",
       "      <td>3.917897</td>\n",
       "      <td>3.916987</td>\n",
       "      <td>3.914795</td>\n",
       "      <td>3.911844</td>\n",
       "      <td>3.908879</td>\n",
       "      <td>3.906618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.327546</td>\n",
       "      <td>0.328058</td>\n",
       "      <td>0.328717</td>\n",
       "      <td>0.329531</td>\n",
       "      <td>0.330507</td>\n",
       "      <td>0.331664</td>\n",
       "      <td>0.332967</td>\n",
       "      <td>...</td>\n",
       "      <td>3.890589</td>\n",
       "      <td>3.900686</td>\n",
       "      <td>3.909336</td>\n",
       "      <td>3.916392</td>\n",
       "      <td>3.921788</td>\n",
       "      <td>3.925556</td>\n",
       "      <td>3.927858</td>\n",
       "      <td>3.929030</td>\n",
       "      <td>3.929648</td>\n",
       "      <td>3.930290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.326307</td>\n",
       "      <td>0.326820</td>\n",
       "      <td>0.327485</td>\n",
       "      <td>0.328309</td>\n",
       "      <td>0.329295</td>\n",
       "      <td>0.330458</td>\n",
       "      <td>0.331761</td>\n",
       "      <td>...</td>\n",
       "      <td>3.869466</td>\n",
       "      <td>3.876229</td>\n",
       "      <td>3.882600</td>\n",
       "      <td>3.888400</td>\n",
       "      <td>3.893481</td>\n",
       "      <td>3.897737</td>\n",
       "      <td>3.901139</td>\n",
       "      <td>3.903763</td>\n",
       "      <td>3.905857</td>\n",
       "      <td>3.907591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.325683</td>\n",
       "      <td>0.326201</td>\n",
       "      <td>0.326867</td>\n",
       "      <td>0.327691</td>\n",
       "      <td>0.328678</td>\n",
       "      <td>0.329846</td>\n",
       "      <td>0.331160</td>\n",
       "      <td>...</td>\n",
       "      <td>3.865839</td>\n",
       "      <td>3.870089</td>\n",
       "      <td>3.873236</td>\n",
       "      <td>3.875318</td>\n",
       "      <td>3.876395</td>\n",
       "      <td>3.876562</td>\n",
       "      <td>3.875976</td>\n",
       "      <td>3.874894</td>\n",
       "      <td>3.873732</td>\n",
       "      <td>3.872815</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1154 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Lactate  Urea  Glucose    1350.0    1351.0    1352.0    1353.0    1354.0  \\\n",
       "0      9.0   0.0      0.0  0.325606  0.326118  0.326777  0.327594  0.328574   \n",
       "1      9.0   0.0      0.0  0.325919  0.326437  0.327103  0.327927  0.328911   \n",
       "2      9.0   0.0      0.0  0.327546  0.328058  0.328717  0.329531  0.330507   \n",
       "3      9.0   0.0      0.0  0.326307  0.326820  0.327485  0.328309  0.329295   \n",
       "4      9.0   0.0      0.0  0.325683  0.326201  0.326867  0.327691  0.328678   \n",
       "\n",
       "     1355.0    1356.0  ...    2491.0    2492.0    2493.0    2494.0    2495.0  \\\n",
       "0  0.329736  0.331045  ...  3.884846  3.891455  3.897297  3.902262  3.906273   \n",
       "1  0.330068  0.331363  ...  3.902861  3.909777  3.914507  3.917156  3.917897   \n",
       "2  0.331664  0.332967  ...  3.890589  3.900686  3.909336  3.916392  3.921788   \n",
       "3  0.330458  0.331761  ...  3.869466  3.876229  3.882600  3.888400  3.893481   \n",
       "4  0.329846  0.331160  ...  3.865839  3.870089  3.873236  3.875318  3.876395   \n",
       "\n",
       "     2496.0    2497.0    2498.0    2499.0    2500.0  \n",
       "0  3.909308  3.911418  3.912773  3.913718  3.914531  \n",
       "1  3.916987  3.914795  3.911844  3.908879  3.906618  \n",
       "2  3.925556  3.927858  3.929030  3.929648  3.930290  \n",
       "3  3.897737  3.901139  3.903763  3.905857  3.907591  \n",
       "4  3.876562  3.875976  3.874894  3.873732  3.872815  \n",
       "\n",
       "[5 rows x 1154 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"Interference.csv\")\n",
    "del data['Albumin']\n",
    "del data['Flag']\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = data['Lactate']\n",
    "u = data['Urea']\n",
    "g = data['Glucose']\n",
    "X = data.iloc[:,3:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "lactate = pd.concat([l, X], axis=1)\n",
    "\n",
    "urea = pd.concat([u, X], axis=1)\n",
    "\n",
    "glucose = pd.concat([g, X], axis=1)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lactate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    9100.000000\n",
       "mean      101.197802\n",
       "std        93.504179\n",
       "min         9.000000\n",
       "25%        25.000000\n",
       "50%        72.000000\n",
       "75%       147.000000\n",
       "max       300.000000\n",
       "Name: Lactate, dtype: float64"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lactate['Lactate'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "72.0     1000\n",
       "103.0    1000\n",
       "210.0    1000\n",
       "300.0    1000\n",
       "147.0     900\n",
       "9.0       700\n",
       "12.0      700\n",
       "17.0      700\n",
       "25.0      700\n",
       "35.0      700\n",
       "50.0      700\n",
       "Name: Lactate, dtype: int64"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lactate['Lactate'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.hist(lactate['Lactate'])\n",
    "plt.title(f'Distribution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "plt.figure()\n",
    "sns.boxplot(data=lactate, x='Lactate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1352.0</th>\n",
       "      <th>1353.0</th>\n",
       "      <th>1354.0</th>\n",
       "      <th>1355.0</th>\n",
       "      <th>1356.0</th>\n",
       "      <th>1357.0</th>\n",
       "      <th>1358.0</th>\n",
       "      <th>1359.0</th>\n",
       "      <th>1360.0</th>\n",
       "      <th>1361.0</th>\n",
       "      <th>...</th>\n",
       "      <th>2491.0</th>\n",
       "      <th>2492.0</th>\n",
       "      <th>2493.0</th>\n",
       "      <th>2494.0</th>\n",
       "      <th>2495.0</th>\n",
       "      <th>2496.0</th>\n",
       "      <th>2497.0</th>\n",
       "      <th>2498.0</th>\n",
       "      <th>2499.0</th>\n",
       "      <th>2500.0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.326777</td>\n",
       "      <td>0.327594</td>\n",
       "      <td>0.328574</td>\n",
       "      <td>0.329736</td>\n",
       "      <td>0.331045</td>\n",
       "      <td>0.332443</td>\n",
       "      <td>0.333808</td>\n",
       "      <td>0.335060</td>\n",
       "      <td>0.336272</td>\n",
       "      <td>0.337447</td>\n",
       "      <td>...</td>\n",
       "      <td>3.884846</td>\n",
       "      <td>3.891455</td>\n",
       "      <td>3.897297</td>\n",
       "      <td>3.902262</td>\n",
       "      <td>3.906273</td>\n",
       "      <td>3.909308</td>\n",
       "      <td>3.911418</td>\n",
       "      <td>3.912773</td>\n",
       "      <td>3.913718</td>\n",
       "      <td>3.914531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.327103</td>\n",
       "      <td>0.327927</td>\n",
       "      <td>0.328911</td>\n",
       "      <td>0.330068</td>\n",
       "      <td>0.331363</td>\n",
       "      <td>0.332741</td>\n",
       "      <td>0.334091</td>\n",
       "      <td>0.335334</td>\n",
       "      <td>0.336545</td>\n",
       "      <td>0.337726</td>\n",
       "      <td>...</td>\n",
       "      <td>3.902861</td>\n",
       "      <td>3.909777</td>\n",
       "      <td>3.914507</td>\n",
       "      <td>3.917156</td>\n",
       "      <td>3.917897</td>\n",
       "      <td>3.916987</td>\n",
       "      <td>3.914795</td>\n",
       "      <td>3.911844</td>\n",
       "      <td>3.908879</td>\n",
       "      <td>3.906618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.328717</td>\n",
       "      <td>0.329531</td>\n",
       "      <td>0.330507</td>\n",
       "      <td>0.331664</td>\n",
       "      <td>0.332967</td>\n",
       "      <td>0.334358</td>\n",
       "      <td>0.335716</td>\n",
       "      <td>0.336960</td>\n",
       "      <td>0.338168</td>\n",
       "      <td>0.339345</td>\n",
       "      <td>...</td>\n",
       "      <td>3.890589</td>\n",
       "      <td>3.900686</td>\n",
       "      <td>3.909336</td>\n",
       "      <td>3.916392</td>\n",
       "      <td>3.921788</td>\n",
       "      <td>3.925556</td>\n",
       "      <td>3.927858</td>\n",
       "      <td>3.929030</td>\n",
       "      <td>3.929648</td>\n",
       "      <td>3.930290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.327485</td>\n",
       "      <td>0.328309</td>\n",
       "      <td>0.329295</td>\n",
       "      <td>0.330458</td>\n",
       "      <td>0.331761</td>\n",
       "      <td>0.333149</td>\n",
       "      <td>0.334505</td>\n",
       "      <td>0.335749</td>\n",
       "      <td>0.336956</td>\n",
       "      <td>0.338131</td>\n",
       "      <td>...</td>\n",
       "      <td>3.869466</td>\n",
       "      <td>3.876229</td>\n",
       "      <td>3.882600</td>\n",
       "      <td>3.888400</td>\n",
       "      <td>3.893481</td>\n",
       "      <td>3.897737</td>\n",
       "      <td>3.901139</td>\n",
       "      <td>3.903763</td>\n",
       "      <td>3.905857</td>\n",
       "      <td>3.907591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.326867</td>\n",
       "      <td>0.327691</td>\n",
       "      <td>0.328678</td>\n",
       "      <td>0.329846</td>\n",
       "      <td>0.331160</td>\n",
       "      <td>0.332560</td>\n",
       "      <td>0.333924</td>\n",
       "      <td>0.335169</td>\n",
       "      <td>0.336374</td>\n",
       "      <td>0.337544</td>\n",
       "      <td>...</td>\n",
       "      <td>3.865839</td>\n",
       "      <td>3.870089</td>\n",
       "      <td>3.873236</td>\n",
       "      <td>3.875318</td>\n",
       "      <td>3.876395</td>\n",
       "      <td>3.876562</td>\n",
       "      <td>3.875976</td>\n",
       "      <td>3.874894</td>\n",
       "      <td>3.873732</td>\n",
       "      <td>3.872815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9095</th>\n",
       "      <td>0.325582</td>\n",
       "      <td>0.326674</td>\n",
       "      <td>0.327948</td>\n",
       "      <td>0.329424</td>\n",
       "      <td>0.331054</td>\n",
       "      <td>0.332754</td>\n",
       "      <td>0.334386</td>\n",
       "      <td>0.335833</td>\n",
       "      <td>0.337177</td>\n",
       "      <td>0.338424</td>\n",
       "      <td>...</td>\n",
       "      <td>4.500144</td>\n",
       "      <td>4.512827</td>\n",
       "      <td>4.526333</td>\n",
       "      <td>4.540226</td>\n",
       "      <td>4.554142</td>\n",
       "      <td>4.567758</td>\n",
       "      <td>4.580768</td>\n",
       "      <td>4.592866</td>\n",
       "      <td>4.603732</td>\n",
       "      <td>4.612983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9096</th>\n",
       "      <td>0.325347</td>\n",
       "      <td>0.326446</td>\n",
       "      <td>0.327726</td>\n",
       "      <td>0.329207</td>\n",
       "      <td>0.330841</td>\n",
       "      <td>0.332543</td>\n",
       "      <td>0.334175</td>\n",
       "      <td>0.335623</td>\n",
       "      <td>0.336967</td>\n",
       "      <td>0.338212</td>\n",
       "      <td>...</td>\n",
       "      <td>4.548688</td>\n",
       "      <td>4.554267</td>\n",
       "      <td>4.555407</td>\n",
       "      <td>4.552824</td>\n",
       "      <td>4.547324</td>\n",
       "      <td>4.539770</td>\n",
       "      <td>4.531062</td>\n",
       "      <td>4.522119</td>\n",
       "      <td>4.513868</td>\n",
       "      <td>4.507194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9097</th>\n",
       "      <td>0.325563</td>\n",
       "      <td>0.326648</td>\n",
       "      <td>0.327911</td>\n",
       "      <td>0.329370</td>\n",
       "      <td>0.330981</td>\n",
       "      <td>0.332661</td>\n",
       "      <td>0.334278</td>\n",
       "      <td>0.335718</td>\n",
       "      <td>0.337061</td>\n",
       "      <td>0.338311</td>\n",
       "      <td>...</td>\n",
       "      <td>4.504238</td>\n",
       "      <td>4.512583</td>\n",
       "      <td>4.517647</td>\n",
       "      <td>4.519844</td>\n",
       "      <td>4.519682</td>\n",
       "      <td>4.517728</td>\n",
       "      <td>4.514587</td>\n",
       "      <td>4.510887</td>\n",
       "      <td>4.507268</td>\n",
       "      <td>4.504331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9098</th>\n",
       "      <td>0.325854</td>\n",
       "      <td>0.326950</td>\n",
       "      <td>0.328228</td>\n",
       "      <td>0.329704</td>\n",
       "      <td>0.331331</td>\n",
       "      <td>0.333025</td>\n",
       "      <td>0.334650</td>\n",
       "      <td>0.336089</td>\n",
       "      <td>0.337429</td>\n",
       "      <td>0.338673</td>\n",
       "      <td>...</td>\n",
       "      <td>4.569032</td>\n",
       "      <td>4.576238</td>\n",
       "      <td>4.579017</td>\n",
       "      <td>4.578017</td>\n",
       "      <td>4.573987</td>\n",
       "      <td>4.567748</td>\n",
       "      <td>4.560176</td>\n",
       "      <td>4.552182</td>\n",
       "      <td>4.544703</td>\n",
       "      <td>4.538660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9099</th>\n",
       "      <td>0.325434</td>\n",
       "      <td>0.326538</td>\n",
       "      <td>0.327827</td>\n",
       "      <td>0.329312</td>\n",
       "      <td>0.330945</td>\n",
       "      <td>0.332644</td>\n",
       "      <td>0.334274</td>\n",
       "      <td>0.335721</td>\n",
       "      <td>0.337068</td>\n",
       "      <td>0.338319</td>\n",
       "      <td>...</td>\n",
       "      <td>4.615527</td>\n",
       "      <td>4.624899</td>\n",
       "      <td>4.627401</td>\n",
       "      <td>4.623963</td>\n",
       "      <td>4.615687</td>\n",
       "      <td>4.603824</td>\n",
       "      <td>4.589769</td>\n",
       "      <td>4.575056</td>\n",
       "      <td>4.561365</td>\n",
       "      <td>4.550496</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9100 rows × 1149 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        1352.0    1353.0    1354.0    1355.0    1356.0    1357.0    1358.0  \\\n",
       "0     0.326777  0.327594  0.328574  0.329736  0.331045  0.332443  0.333808   \n",
       "1     0.327103  0.327927  0.328911  0.330068  0.331363  0.332741  0.334091   \n",
       "2     0.328717  0.329531  0.330507  0.331664  0.332967  0.334358  0.335716   \n",
       "3     0.327485  0.328309  0.329295  0.330458  0.331761  0.333149  0.334505   \n",
       "4     0.326867  0.327691  0.328678  0.329846  0.331160  0.332560  0.333924   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "9095  0.325582  0.326674  0.327948  0.329424  0.331054  0.332754  0.334386   \n",
       "9096  0.325347  0.326446  0.327726  0.329207  0.330841  0.332543  0.334175   \n",
       "9097  0.325563  0.326648  0.327911  0.329370  0.330981  0.332661  0.334278   \n",
       "9098  0.325854  0.326950  0.328228  0.329704  0.331331  0.333025  0.334650   \n",
       "9099  0.325434  0.326538  0.327827  0.329312  0.330945  0.332644  0.334274   \n",
       "\n",
       "        1359.0    1360.0    1361.0  ...    2491.0    2492.0    2493.0  \\\n",
       "0     0.335060  0.336272  0.337447  ...  3.884846  3.891455  3.897297   \n",
       "1     0.335334  0.336545  0.337726  ...  3.902861  3.909777  3.914507   \n",
       "2     0.336960  0.338168  0.339345  ...  3.890589  3.900686  3.909336   \n",
       "3     0.335749  0.336956  0.338131  ...  3.869466  3.876229  3.882600   \n",
       "4     0.335169  0.336374  0.337544  ...  3.865839  3.870089  3.873236   \n",
       "...        ...       ...       ...  ...       ...       ...       ...   \n",
       "9095  0.335833  0.337177  0.338424  ...  4.500144  4.512827  4.526333   \n",
       "9096  0.335623  0.336967  0.338212  ...  4.548688  4.554267  4.555407   \n",
       "9097  0.335718  0.337061  0.338311  ...  4.504238  4.512583  4.517647   \n",
       "9098  0.336089  0.337429  0.338673  ...  4.569032  4.576238  4.579017   \n",
       "9099  0.335721  0.337068  0.338319  ...  4.615527  4.624899  4.627401   \n",
       "\n",
       "        2494.0    2495.0    2496.0    2497.0    2498.0    2499.0    2500.0  \n",
       "0     3.902262  3.906273  3.909308  3.911418  3.912773  3.913718  3.914531  \n",
       "1     3.917156  3.917897  3.916987  3.914795  3.911844  3.908879  3.906618  \n",
       "2     3.916392  3.921788  3.925556  3.927858  3.929030  3.929648  3.930290  \n",
       "3     3.888400  3.893481  3.897737  3.901139  3.903763  3.905857  3.907591  \n",
       "4     3.875318  3.876395  3.876562  3.875976  3.874894  3.873732  3.872815  \n",
       "...        ...       ...       ...       ...       ...       ...       ...  \n",
       "9095  4.540226  4.554142  4.567758  4.580768  4.592866  4.603732  4.612983  \n",
       "9096  4.552824  4.547324  4.539770  4.531062  4.522119  4.513868  4.507194  \n",
       "9097  4.519844  4.519682  4.517728  4.514587  4.510887  4.507268  4.504331  \n",
       "9098  4.578017  4.573987  4.567748  4.560176  4.552182  4.544703  4.538660  \n",
       "9099  4.623963  4.615687  4.603824  4.589769  4.575056  4.561365  4.550496  \n",
       "\n",
       "[9100 rows x 1149 columns]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spectra = urea.iloc[:,3:]\n",
    "spectra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAiUAAAGdCAYAAADNHANuAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy88F64QAAAACXBIWXMAAA9hAAAPYQGoP6dpAABVzUlEQVR4nO3deXiU5dU/8O9smayTkH0nCUsICZusAQEFBcEFlVqtVq1WW6xLNeVHi/b1bfXtG9+KFa0KUkFKcW2DOyCoEFAWCYSdhCUhCdnXmawzmZnn98csJJBAJpmZ55nJ93Ndc9UZnsmcUH1ycu5zn1smCIIAIiIiIpHJxQ6AiIiICGBSQkRERBLBpISIiIgkgUkJERERSQKTEiIiIpIEJiVEREQkCUxKiIiISBKYlBAREZEkKMUOoC/MZjMqKioQFBQEmUwmdjhERETUB4IgoLm5GbGxsZDLr14H8YikpKKiAgkJCWKHQURERP1QVlaG+Pj4q17nEUlJUFAQAMs3pdFoRI6GiIiI+kKn0yEhIcH+c/xqPCIpsS3ZaDQaJiVEREQepq+tF2x0JSIiIklgUkJERESSwKSEiIiIJIFJCREREUkCkxIiIiKSBCYlREREJAlMSoiIiEgSmJQQERGRJDApISIiIklgUkJERESSwKSEiIiIJIFJCREREUkCkxIiEk21rgNv7TwLbVun2KEQkQQwKSEi0by0pQB/3VqIX/0rT+xQiEgCmJQQkWg+yS8HAOwvboAgCCJHQ0RiG1BSkp2dDZlMhqeffvqK1+Xm5mLixInw9fVFSkoKVq9ePZCPJSIv0GYwdnve0GoQKRIikop+JyUHDhzAmjVrMHbs2CteV1xcjIULF2LmzJnIz8/Hs88+i6eeego5OTn9/Wgi8gJV2o5uz8/Xt4oUCRFJRb+SkpaWFtx33334xz/+gSFDhlzx2tWrVyMxMRErV65EWloaHnnkETz88MNYsWJFvwImIu9QpeuelFTr9CJFQkRS0a+k5PHHH8fNN9+MG2644arX7t27F/Pmzev22vz585GXl4fOTnbcEw1WNZckIXUtTEqIBjulo2/48MMPcejQIRw4cKBP11dVVSEqKqrba1FRUTAajairq0NMTMxl79Hr9dDrL96gdDqdo2ESkcRd2kNS18KeEqLBzqFKSVlZGX77299i48aN8PX17fP7ZDJZt+e2LvtLX7fJzs5GcHCw/ZGQkOBImETkAZrau1dK61kpIRr0HEpKDh48iJqaGkycOBFKpRJKpRK5ubl4/fXXoVQqYTKZLntPdHQ0qqqqur1WU1MDpVKJsLCwHj9n+fLl0Gq19kdZWZkjYRKRB9C2WSojGl9LwbaelRKiQc+h5Zu5c+fi2LFj3V576KGHMGrUKPz+97+HQqG47D2ZmZn44osvur22bds2TJo0CSqVqsfPUavVUKvVjoRGRB7GVikZGhaAY+VaNOvZY0Y02DmUlAQFBSEjI6PbawEBAQgLC7O/vnz5cpSXl2PDhg0AgCVLluCNN95AVlYWHn30Uezduxdr167FBx984KRvgYg8UaN1tHxCqB+OlWuhazde5R1E5O2cPtG1srISpaWl9ufJycnYvHkzdu7cifHjx+PFF1/E66+/jsWLFzv7o4nIg2itlZL4If4AAF0HKyVEg53Du28utXPnzm7P169ff9k1s2fPxqFDhwb6UUTkRVr1lspIbLClaV7XzqSEyF06TWZ88GMpSurbMGdUJKYPC+t184k7DTgpISLqD1tSEhPiBwBo7jBCEARJ3BiJvF3Wx0fwxZEKAMDa74sxc0Q4/njzaKRGB4kaFw/kIyJRtNgrJZakxGgW0N55+Q4+InKub05W44sjFVDKZbh1XCx8FHLsPlOH/cX1YofGSgkRuZ8gCPZKSUSQGgq5DCazAF27Ef4+vC0RudIbO84CAH45MxnLF6ShtL4N7+4pxr1TEkWOjJUSIhJBR6cZZssMRQSoFfZZJWx2JXKtszXNOFzWBIVchl9emwwASAzzx3/fmg6lQvyUQPwIiGjQsS3dAECAjxIaP8vMomYmJUQu9e+8CwCA61MjEBnU98ns7sKkhIjczrZ04++jgFwug8bXkpRwVgmR6xhNZmzKLwcA3DVJmse3MCkhIrezVUoC1JZlG40fl2+IXC33dC1qm/UIC/DBnFGRYofTIyYlROR2tkpJkDUpCVLbKiVMSohc5YMfLYNNb58QB5UE+kd6Is2oiMirtRp6q5Rw+YbIFYrrWvFtQQ0A4N6p4u+y6Q2TEiJyuxa9ZR5JgNpyiGeg2tboyqSEyBXe/aEYggDMGRWJYRGBYofTKyYlROR2tuWbQNvyjXVLcAtPCiZyOm1bp33XjW0bsFQxKSEit2u9pNHVlpSwUkLkfB8cKEV7pwmjooMwfViY2OFcEZMSInK7S3ff2CslTEqI+kTb3olKbftVr+s0mfHPPecBAA/PSJb82VJMSojI7S5dvmFPCVHfVWrbccPfcjHjpe/woXVHTW+2HK9CpbYD4YE+uG18rJsi7D8mJUTkdvZGV5/ulRLOKSG6urW7i1HbrIdZALK3FEBv7P0gy3XfFwMA7ps6FL4qhbtC7DcmJUTkdhd7Siw3yYuNrqyUEF3NN6eq7f+sbe/Et6dqerxuf1E9Dpc1wUchx8+nDXVXeAPCpISI3K633TdcviG6stpmPc7Xt0EmA34+zTJv5BPr6PhL/f07y2nAiyfGIyJI7bYYB4JJCRG53eWNrir764IgiBYXkdQdLGkEAIyMDMK9UyzVj12na9FmMF5yXQO+P1sHpVyG31w3zO1x9heTEiJyO9tE14uNrpb/NZkFtHf2vj5ONNjll1mSkmuGDkFaTBASQv2gN5qRW1hrv0YQBPx1ayEAYPE18UgI9Rcl1v5gUkJEbtdqn+hqSUb8fRSQW3cqcgmHqHdnq1sAAKNjNZDJZJg/OhoA8PWJKvs1Hx4ow/7iBviq5HhiznBR4uwvJiVE5HYtlzS6ymQye7WESQlR787WWpKS4dZR8QvGxAAANh+vQrWuA98VVOP5z44DAJ6+YaRHVUkAQCl2AEQ0+Fza6ApY+kp0HUY0c1swUY86Ok0oa2gDAAyLDAAAXJMYgklDhyCvpBE/+8c+lNa3wWgWcPPYGPxqZoqY4fYLKyVE5FZms4A2g2X5pmtSEuJvaXZtbDOIEheR1JXUt8EsWHarRQRadtPIZDIsX5gGlUKGotpWGM0C7pgQh5V3j4dcLu3prT1hpYSI3Kq1yy6BQN+Lt6AojS9OVOhQo9OLERaR5J2tsS7dRAZ2Gxc/cegQfPzrTGw5XoVJQ4fgxtFRkh8n3xsmJUTkVrZ+EpVCBrXy4oTJSOschWomJUQ9OmftJxlm7SfpakLiEExIHOLukJyOyzdE5Fa2Q/e6Lt0AQKTGFwBQ3dzh9piIPMGVkhJvwaSEiNzq0sFpNrZKCZdviHrWdfnGWzEpISK3aulh5w1g6SkBgBpWSoguYzYLKKptBQAMiwgQORrXYVJCRG7V03ZggJUSoiup1HWgvdMElULmcbNHHMGkhIjcyjYcrevOG+BipaS2RQ+TmeffEHV1zrp0MzQsACqF9/7o9t7vjIgkqbeekoggNVQKGUxmAVU6LuEQdXWxydV7l24AB5OSVatWYezYsdBoNNBoNMjMzMSWLVt6vX7nzp2QyWSXPQoKCgYcOBF5JtvyTdAlSYlCLkPCEEtZuqSu1e1xEUnZYNh5Azg4pyQ+Ph4vvfQShg+3HPDzz3/+E4sWLUJ+fj7S09N7fV9hYSE0Go39eURERD/DJSJPYjIL+H3OUchlwEt3joVcLkNzLz0lADA0zB9Fda04X9+G6Z51jhiRS9maXFOYlFx06623dnv+l7/8BatWrcK+ffuumJRERkYiJCSkXwESkef65lQ1/nPwAgBg5ogI3Dou1l4puXT5BrCslwO1KGlgpYSoKy7fXIXJZMKHH36I1tZWZGZmXvHaCRMmICYmBnPnzsWOHTuu+rX1ej10Ol23BxF5nkOljfZ//rc1ObENTwvy7blSAgAldW2X/dnq3HO49e/fI/d0rStCJZKs5o5O+6Rjb6+UOJyUHDt2DIGBgVCr1ViyZAk++eQTjB49usdrY2JisGbNGuTk5GDTpk1ITU3F3LlzsWvXrit+RnZ2NoKDg+2PhIQER8MkIgnomlwcKG6A0WTutdEVAJLCLL8F2n4rtNlXVI+XthTgWLkWz3x0GHqjyYVRE0mLbekmIkiNYD+VyNG4lsNn36SmpuLw4cNoampCTk4OHnzwQeTm5vaYmKSmpiI1NdX+PDMzE2VlZVixYgVmzZrV62csX74cWVlZ9uc6nY6JCZEHKmm4mJS0d5pQUNUMXXvvlZLRsZbes3O1LWgzGOHvY7nm/f2l9msaWg34/kwd5qZFuTJ0IskoqrMk6Snh3r10A/SjUuLj44Phw4dj0qRJyM7Oxrhx4/Daa6/1+f3Tpk3DmTNnrniNWq227/CxPYjI89S1WErO4YE+AID8sqYur6kvuz5K44sojRpmAThRYVm21bZ34usTVQCAjDjLvWBfUb3LYyeSinM11kmuXjxe3mbAc0oEQYBe3/cJjPn5+YiJiRnoxxKRxAmCgMZWAwBgzqhIAEB+aeNlicqlxsSFAACOXtACAD47XA690YzUqCA8mJkEADhezj4zGjwGy3ZgwMHlm2effRYLFixAQkICmpub8eGHH2Lnzp3YunUrAMuyS3l5OTZs2AAAWLlyJZKSkpCeng6DwYCNGzciJycHOTk5zv9OiEhSmvVGGK2TWWeNjMDHeRdwuKwJOmuja0+VEgAYnxCMb05VY19RPR6ekWRfuvnZlAT7QWS2cjbRYDAYzryxcSgpqa6uxv3334/KykoEBwdj7Nix2Lp1K2688UYAQGVlJUpLL679GgwGLF26FOXl5fDz80N6ejq++uorLFy40LnfBRFJjq1K4u+jwMShQwBcvLmqFLJeG/bmjIrCim2nset0Lb4/W4eCqmaolXLccU28/ZpqnR7NHZ0I8vXupj8ik1lAcZ0tKWGlpJu1a9de8c/Xr1/f7fmyZcuwbNkyh4MiIs/XYE1Khvj7IFrji9AAH/trwyODIJPJenxfWkwQEkP9UdrQhvvX/ggAuH18nD2JiQhSo7ZZj6LaVoxLCHH9N0IkoguNbTCYzFAr5YgN8RM7HJfj2TdE5BK2ZRqNnwoymQzpsRcb1scnBPf6PplMhsVdqiI+CjmenHtxvKttBwKXcGgwsPWTJIcHQCHvOZH3JkxKiMgl7EPSrPNIbuiyhffG0VfezvvIzGTMHhmBILUS/3NHBuKHXDyqPdmalJTUXz5gjcjb2PtJBsHOG6Afc0qIiPqiRd8JAAi0ziO5a1I8Cqp00PiqMHtk5BXfG6BW4p8PT4EgCJct80QH+wIAqnmSMA0C9p03g2BGCcCkhIhcpLmj+8F7/j5KZN851qGv0VPfSYw1KanUMikh7zeYZpQAXL4hIhexjZMP7GFy60BEaSxJSRWTEhoEzg6iGSUAkxIicpFLe0qcJSbYsgOhiss35OXqWvRoaDVAJmNSQkQ0IFc6eG8goq2Vkqa2TnR08mA+8l6nq5oBAEND/eHnoxA5GvdgUkJELtFmsCQMzk5KNH5K+Kosty42u5I3K6y2JCUjo4JEjsR9mJQQkUu0W6sYfirn/oYnk8ns1RL2lZA3O11t6SdhUkJENEDt1kqJn4/zbzO2c3PqrRNiibzRaVulJJpJCRHRgFyslDh/8kCY9YTh+pa+n1BO5EkEQbD3lIyMGhxNrgCTEiJykYuVEuc36IWxUkJerkrXgWa9EUq5DCnhTEqIiAakw0U9JQAQHmCrlDApIe9UaK2SJIcHwEc5eH5UD57vlIjcylWNrgAQaktKWrl8Q96poGrw7bwBmJQQkYvYkxIXNLral29YKSEvdfRCEwBgTHzvJ2p7IyYlROQStp4SXxdUSuyNruwpIS91pEwLABjLpISIaGBMZgF6oxmA5SA+ZwsLsFVKuHxD3qemuQPlTe2QyYAxcUxKiIgGpOv4d1f0lNgqJU3tnTCazE7/+kRiOmqtkgyPCESQr0rkaNyLSQkROV17l6RE7YKdA0P8fSCTAYIANLZ1Ov3rE4npiLWfZFxCiKhxiIFJCRE53cV+EjnkcpnTv75CLkOoP3fgkHc6cL4BADCeSQkR0cC5ckaJTShnlZAX6ug04VBJEwBg+rAwcYMRAZMSInI6V84oseEOHPJGB0saYTCZEa3xRXJ4gNjhuB2TEiJyOvvyjQtGzNtcnFXC5RvyHnvO1QGwVElkMucvfUodkxIicro2a6XE35VJiXX5poGVEvIi35+tBwBMHx4uciTiYFJCRE7XYXDD8o11Vkkde0rIS9Q0d9gnuc4cwaSEiMgpbD0lrpjmahMaaKuUcPmGvMOOghoIAjAuPhhRGl+xwxEFkxIicjp3JCU8KZi8zfaTNQCAuWlRIkciHiYlROR0+k7LlFWXVkrYU0JepFVvxPdnawEAc9MiRY5GPExKiMjpbOfeuGKaq41t900dd9+QF9h+shodnWYkhwdgdIxG7HBEw6SEiJzOYE1KfFyZlFgrJboOo/3ziDzVZ4fLAQC3jYsdlFuBbRy6Y6xatQpjx46FRqOBRqNBZmYmtmzZcsX35ObmYuLEifD19UVKSgpWr149oICJSPr0RktPiSsrJcF+KiisI+wb27iEQ56rvkWPXWcs80luGx8rcjTicuiOER8fj5deegl5eXnIy8vDnDlzsGjRIpw4caLH64uLi7Fw4ULMnDkT+fn5ePbZZ/HUU08hJyfHKcETkTS5o1Iil8swxHr+DZdwyJNtPlYJk1lARpwGwyICxQ5HVEpHLr711lu7Pf/LX/6CVatWYd++fUhPT7/s+tWrVyMxMRErV64EAKSlpSEvLw8rVqzA4sWL+x81EUnaxZ4S1zW6AkB4oA/qWvRsdiWPJQgCPvixDABw+/g4kaMRX79/jTGZTPjwww/R2tqKzMzMHq/Zu3cv5s2b1+21+fPnIy8vD52dvR83rtfrodPpuj2IyHMY3NDoCvBQPvJ8h0qbcLJSB7VSjp9MjBc7HNE5fMc4duwYAgMDoVarsWTJEnzyyScYPXp0j9dWVVUhKqr7fuuoqCgYjUbU1dX1+hnZ2dkIDg62PxISEhwNk4hE5I6eEqDL+TeslJCH+tfe8wAsDa4h1uXIwczhO0ZqaioOHz6Mffv24bHHHsODDz6IkydP9nr9pV3EgiD0+HpXy5cvh1artT/KysocDZOIRGQwub6nBLi4A4eH8pEnqtZ14MujlQCABzKTxA1GIhzqKQEAHx8fDB8+HAAwadIkHDhwAK+99hrefvvty66Njo5GVVVVt9dqamqgVCoRFhbW62eo1Wqo1WpHQyMiibANT3N5pYQD1MiDbdh7HkazgClJoRgTHyx2OJIw4DuGIAjQ63v+LSUzMxPbt2/v9tq2bdswadIkqFSqgX40EUmUuyoltvNveCgfeZp2gwnv7y8FADx8bZK4wUiIQ3eMZ599Frt378b58+dx7NgxPPfcc9i5cyfuu+8+AJZllwceeMB+/ZIlS1BSUoKsrCycOnUK69atw9q1a7F06VLnfhdEJCkXKyWu3X1jOymYh/KRp/kkvxyNbZ1ICPXDjaOjxQ5HMhxavqmursb999+PyspKBAcHY+zYsdi6dStuvPFGAEBlZSVKS0vt1ycnJ2Pz5s145pln8OabbyI2Nhavv/46twMTeTm9rVKicHWjq7WnhMs35GHe218CAHgwM8k+BJAcTErWrl17xT9fv379Za/Nnj0bhw4dcigoIvJseuspwWqVm3pKuHxDHqSgSocTFTqoFDIsvobbgLvi2TdE5HQGt1VKLMs3zXojOqyJEJHU5Ry8AACYMyoSQwK4DbgrJiVE5HT2nhKVa3tKNL5KezNtbTP7Skj6jCYzPsmvAABWSXrApISInM5dlRKZTIbIIEu1pIZJCXmAH87Vo65Fj9AAH1yXGil2OJLDpISInM5dPSUAEGFNSlgpIU+w5ZhlWNqCjGiXb5n3RPwbISKnc1elBAAirH0ltZzqShJnNJmx7WQ1AGBBRozI0UgTkxIicipBEC6eEuyGSkmkxpqU6Dpc/llEA/Hj+QY0tBowxF+FqSmhYocjSUxKiMipjGYB1iOuoFa4ttEVACICfQGwUkLSt+WY5diVG0dHQeWGKqIn4t8KETmVrUoCuLlSwp4SkjBBEPDtKcvSzU0ZnODaGyYlRORUhi5JiTt7Srj7hqSstKENFdoOqBQyZKaEix2OZDEpISKn0hstO29UChnkbhifzd035An2nqsHAIxPCIGfj+uXNT0VkxIicipbpcQdVRLg4vJNXYseZrPgls8kctS+IktSMi0lTORIpI1JCRE51cWdN+75bdB2UnCnSUBTe6dbPpPIEYIgYK81KclkUnJFTEqIyKncXSnxUcoRaj0/pJrbgkmCiutaUa3Tw0chxzVDh4gdjqQxKSEip7L1lLhj541NtMayLbhKy6SEpMdWJZmQGAJfN1UQPRWTEiJyKr2bKyUAEBviBwAob2p322cS9dW+ogYA7CfpCyYlRORU7pzmahMbYqmUVGqZlJC0CIJg33mTOYxJydUwKSEip3J3TwlwsVJS0cTlG5KWc7UtqGvRQ62UY0JiiNjhSB6TEiJyKnulROm+tXMu35BU7bUu3UwcOsSt/014KiYlRORU9kqJG49ljw3m8g1J075znE/iCCYlRORU9t037kxKrJWSKm0HB6iRZAiCYB+axn6SvmFSQkROJUalJDJIDbnMMkCtjqcFk0Scrm5BfasBfioFxsWHiB2OR2BSQkROJUZPiVIht88qYV8JSYWtSjIpaYhbk3RPxr8lInIqMSolABDDHTgkMXvZT+IwJiVE5FRi9JQAQGKoPwCgpKHVrZ9L1BOzWcC+YiYljmJSQkROZbAv37j39jI0zJqU1LW59XOJelJY3Yymtk74+ygwNj5Y7HA8BpMSInIqvUhJSVJYAADgfD0rJSQ+29LN5KRQqNw4SNDT8W+KiJxKrJ4Se6WknpUSEt9ebgXuFyYlRORUYuy+AS5WSqp0HWg3mNz62URdmcwC9hexn6Q/mJQQkVOJVSkZEuCDYD8VAKC0gdUSEs+pSh10HUYEqpXIiNWIHY5HYVJCRE4l1u4bAEiyLuGwr4TEZJtPMiU5FEr2kzjEob+t7OxsTJ48GUFBQYiMjMTtt9+OwsLCK75n586dkMlklz0KCgoGFDgRSZNepEoJAAy1NbvWMSkh8diaXDO5dOMwh+4aubm5ePzxx7Fv3z5s374dRqMR8+bNQ2vr1W8AhYWFqKystD9GjBjR76CJSLrE6ikBgORwS1JyrrbF7Z9NBFj6SX4stpwMzH4SxykduXjr1q3dnr/77ruIjIzEwYMHMWvWrCu+NzIyEiEhIQ4HSESeRawtwQAwMioIgOXMESIxHC/XollvRJCvEqPZT+KwAd01tFotACA0NPSq106YMAExMTGYO3cuduzYccVr9Xo9dDpdtwcReQaxGl0BIDU6EABwprqZpwWTKPZ0GS2vkMtEjsbz9PuuIQgCsrKycO211yIjI6PX62JiYrBmzRrk5ORg06ZNSE1Nxdy5c7Fr165e35OdnY3g4GD7IyEhob9hEpGbidnoOjQsAD4KOVoNJh7MR6LYc64OADCD80n6xaHlm66eeOIJHD16FN9///0Vr0tNTUVqaqr9eWZmJsrKyrBixYpel3yWL1+OrKws+3OdTsfEhMhDiFkpUSnkSIkIQEFVM87UNCPBeh4OkTsYjGYcOG/pJ5k+PFzkaDxTv+4aTz75JD7//HPs2LED8fHxDr9/2rRpOHPmTK9/rlarodFouj2IyDOI2egKXOwrKaxiXwm5V35pIzo6zQgP9MGIyECxw/FIDlVKBEHAk08+iU8++QQ7d+5EcnJyvz40Pz8fMTEx/XovEUmbmJUSABgZZflhcLq6WZTPp8HL1k+SOSwcMhn7SfrDoaTk8ccfx/vvv4/PPvsMQUFBqKqqAgAEBwfDz88PgGXppby8HBs2bAAArFy5EklJSUhPT4fBYMDGjRuRk5ODnJwcJ38rRCQFYvaUAMCoaEtl9WQFG+TJvWzzSaazn6TfHEpKVq1aBQC47rrrur3+7rvv4he/+AUAoLKyEqWlpfY/MxgMWLp0KcrLy+Hn54f09HR89dVXWLhw4cAiJyLJEQTBXikRKykZYz0m/kxNM9oMRvj79Lt1jqjP2gxG5Jc1AgBmDGM/SX85vHxzNevXr+/2fNmyZVi2bJlDQRGRZzKaBdh24orVUxKl8UVEkBq1zXqcqtRh4tCrjywgGqgfztaj0yQgIdQPCaF+YofjsTiUn4icxlYlAcTrKQGAMXGWasmxC1rRYqDB5buCGgDAnNRI9pMMAJMSInIavUSSkgxbUlLOvhJyPUEQsLPQkpRcNypS5Gg8G5MSInIaW6VEKZeJOs3SVik5Xs5KCbleQVUzKrUd8FXJeQjfADEpISKnEXvnjY0tKbE1uxK5km3pZvqwcPiqxOml8hZMSojIacSeUWITpVEjPFANswCcquQSDrnW9pPVAIDruXQzYExKiMhpxJ7maiOTyTDOujX4SBmXcMh1ypvacbisCTIZMH90lNjheDwmJUTkNHqJVEoAYFxCCADgyIUmUeMg77b1uGWI6OShoYjU+IocjecT/85BRF5DKj0lQJekpKxJ1DjIu205VgkAWDAmWuRIvIP4dw4i8hpS6SkBYF++OV/fhqY2g8jRkDeq0nYgr8QyxfWmDCYlziD+nYOIvIZe5BHzXYX4+yApzB8AcIRD1MgFvj5hWbq5JjEEMcGc4uoM4t85iMhrSKlSAnAJh1zrk/xyAMDCMTz13lmkcecgIq9wsdFVGrMaxsWHAGBSQs53uroZh8uaoJTLsGh8nNjheA0mJUTkNLZKia/UKiUXmvp0oChRX318oAwAMGdUJCKC1CJH4z2kcecgIq9g330jkamW6bEaKOUy1LUYUKHtEDsc8hIGo9m+dPPTSQkiR+NdmJQQkdNIqdEVAHxVCoyKCQLAJRxynu8KalDfakBEkBrXpUaIHY5Xkcadg4i8gr5TWo2uAPtKyPk+zrMs3Sy+Jh5KhXT+XfcG/NskIqeR0vA0G1tfyWEmJeQE1boO7Cy0HMB316R4kaPxPtK5cxCRx5PK2TddjbcmJcfKtTCZ2exKA5Nz6ALMAjBp6BAMiwgUOxyvw6SEiJxGipWSYRGBCPBRoM1gwtmaFrHDIQ8mCAL+nXcBABtcXUU6dw4i8ni2nhK1Sjq3FoVchjH2E4ObxA2GPFpeSSOK61rh76PAzWM5MM0VpHPnICKPJ8XlG6BLXwlPDKYB+Mg6m+SWsTEIUCtFjsY7MSkhIqeR4vINwB04NHAteiO+Omo5EZhLN64jrTsHEXk0g8TmlNiMibMs35yubrYnTkSO+OpoBdo7TUgJD8DEoUPEDsdrSevOQUQezb58I5GJrjbxQ/wQ4q9Cp0lAYVWz2OGQB7I1uN41KQEymUzkaLwXkxIichqpTXS1kclkyIi1VEuOl+tEjoY8TUl9K/JKGiGXAXdew8P3XEladw4i8mhS7SkBgAzrEs6xcq3IkZCnsZ1zM2N4OKI0viJH492kd+cgIo8lxTHzNra+kuNMSsgBgiBg0yFLUsIqietJ785BRB5LqluCASAjTgMAKKxqtjfkEl3NwZJGlDa0wd9Hgfnp0WKH4/WYlBCR00h5+SYx1B8aXyUMJjNOV7PZlfpmk3Xp5qaMaPj7cDaJq0nvzkFEHstWKfGV0ERXG5lMZu8r4RIO9UVHpwlfHqkAYDkRmFzPoTtHdnY2Jk+ejKCgIERGRuL2229HYWHhVd+Xm5uLiRMnwtfXFykpKVi9enW/AyYi6bKPmZfg8g3Qpa+kgkkJXd2OghroOoyICfbFtJQwscMZFBxKSnJzc/H4449j37592L59O4xGI+bNm4fW1tZe31NcXIyFCxdi5syZyM/Px7PPPounnnoKOTk5Aw6eiKRDEARJL98AQLp9Bw63BdPV5VgbXBeNj4NCztkk7uDQAtnWrVu7PX/33XcRGRmJgwcPYtasWT2+Z/Xq1UhMTMTKlSsBAGlpacjLy8OKFSuwePHi/kVNRJJjNAswC5Z/lnql5FSlDp0mM1QKaSZPJL76Fj12FtYA4K4bdxrQf5FaraUEGhoa2us1e/fuxbx587q9Nn/+fOTl5aGzs7PH9+j1euh0um4PIpK2rjtapHRKcFdDQ/0RpFbCYDTjbE2L2OGQhH15tBJGs4CMOA1GRgWJHc6g0e87hyAIyMrKwrXXXouMjIxer6uqqkJUVFS316KiomA0GlFXV9fje7KzsxEcHGx/JCTw8CMiqdN3SUp8JFqBkMtlGB1r2RrMIWp0JZsOWcbK3zmBDa7u1O87xxNPPIGjR4/igw8+uOq1l54TIAhCj6/bLF++HFqt1v4oKyvrb5hE5Ca2fhIfhRxyCa+/c4gaXc3ZmhYcuaCFQi7DbeNjxQ5nUOnXpusnn3wSn3/+OXbt2oX4+CtnkdHR0aiqqur2Wk1NDZRKJcLCeu5mVqvVUKvV/QmNiERyceeNNKskNmPimZTQlX2Sb6mSzB4ZgfBA/ixyJ4fuHoIg4IknnsCmTZvw3XffITk5+arvyczMxPbt27u9tm3bNkyaNAkqlcqxaIlIsmzLN1IcMd9VuvVgvpOVOhhNnOxK3ZnNAj7Nt8wmYYOr+zl093j88cexceNGvP/++wgKCkJVVRWqqqrQ3t5uv2b58uV44IEH7M+XLFmCkpISZGVl4dSpU1i3bh3Wrl2LpUuXOu+7ICLRSX07sE1KeAACfBTo6DTjXG3v4wxocNpf3IDypnYEqZW4IS3q6m8gp3Lo7rFq1SpotVpcd911iImJsT8++ugj+zWVlZUoLS21P09OTsbmzZuxc+dOjB8/Hi+++CJef/11bgcm8jL2c29U0twObCOXy+zVEi7h0KVsDa43j42Br8T/XfZGDvWU2BpUr2T9+vWXvTZ79mwcOnTIkY8iIg/jKT0lAJAep8GP5xtwrFyLxRO5u4Is2g0mbDlu6YG8YwKXbsQg/bsHEXkET1m+AbgDh3q27WQVWvRGxA/xw+Sk3udvketI/+5BRB7Bvnwj0WmuXdmSkpOVOpjMV68A0+CwyTpW/s4JcZLe1u7NmJQQkVPYKyUSnebaVUpEIPxUCrQZTCiu42RXAmqaO7D7TC0A4A6eCCwa6d89iMgjeFJPiYKTXekSnx+ugFkAJiSGIDk8QOxwBi3p3z2IyCMYTJ6zfAN07Svh2VrUfemGxMOkhIicwpMqJQCQYU1KWCmhkxU6nKzUQaWQ4ZaxHCsvJs+4exCR5HlSTwkAZMRZlm9OVuhgZrProJZjnU1yQ1oUhgT4iBzN4OYZdw8ikjxP2n0DAMMjAuGrkqNFb8T5ek52Haw6TWZ8mm9ZuvkJZ9aIjkkJETmFp5x9Y6NUyJEWw2bXwS63sBb1rQaEB/pg1sgIscMZ9Dzj7kFEktdusCzfeNJo7gyOmx/0/nPQsnRz+/g4qBT8kSg2/j9ARE7R3mlJSvw8KCkZw2bXQa2x1YBvC6oBgMcNSASTEiJyClulxN/Hc5IS2w6cE+Vsdh2Mcg5dQKdJQHqsxr6UR+JiUkJETuGJlZIRUYFQK+Vo1htRVMdm18HEbBawcV8JAOC+qUNFjoZsmJQQkVPYe0o8qFKiUsgxITEEAPBjcYO4wZBbfX+2Dufr2xCkVmLReM4mkQomJUTkFG3WSom/B1VKAGBKchgA4MfiepEjIXfasNdSJVk8MR4BaqXI0ZANkxIicooOa6XEz4MqJQAwNdlyRP3+4gYIAvtKBoMLjW34ztrg+vNpXLqREiYlROQUtp4ST9oSDADXJA6BUi5DpbYDFxrbxQ6H3OC9/aUwC8CM4WEYHhkodjjUBZMSInKKNoPnNboClsrO2HjLLpz97Cvxes0dnfYG1/unJYkbDF2GSQkROUVHp+dtCbax9ZXsK2Jfibd7b38pmjuMGBYRgHmjo8QOhy7BpISIBkwQhItbgj0wKZk+zJKU7D5Ty74SL9bRacLa74sBAEtmD4NcLhM5IroUkxIiGjCDyQyTdfiYp/WUAMCU5FD4quSo1ulxurpF7HDIRf5z8AJqm/WIDfbFovFxYodDPWBSQkQD1mEw2//ZE5dvfFUKTEuxVEtyT9eIHA25QrvBhL9/dwYA8OisFI85OHKw4f8rRDRgtqUbpVzmsYeazRphOSE293StyJGQK6z7oRjVOj3iQvxw79REscOhXnjm3YOIJMUTR8xfanaqJSk5UNyINoNR5GjImRpbDVi98xwAYOn8kVArPfffU2/HpISIBsz2Q9wTm1xtUsIDEBfiB4PJzF04Xua1b8+gWW9EWowGi8axl0TKmJQQ0YB1ePDOGxuZTGavluws5BKOtzhRocWGvecBAM8tTOOOG4ljUkJEA9ZubXT15OUbAJiTGgkA2H6ymluDvYDZLOC/Pj0OswDcPCYG144IFzskugomJUQ0YN6wfAMA144Ih7+PApXaDhy9oBU7HBqg/xy8gEOlTfD3UeCPt6SJHQ71AZMSIhowb2h0BSxbg6+zLuF8faJK5GhoIJraDHhpawEA4OkbRiAm2E/kiKgvmJQQ0YB1eElSAgDz06MBMCnxdH/9uhANrQaMjArEQzOSxQ6H+ohJCRENmO0wPl8PX74BgOtHRUKlkOFcbSvO1nC6qyc6UtaED34sBQC8sCjDY2fnDEYO/z+1a9cu3HrrrYiNjYVMJsOnn356xet37twJmUx22aOgoKC/MRORxNiWb/y9oFKi8VUhc5ilIZLVEs9jMgv446fHIQjAHRPi7JN6yTM4nJS0trZi3LhxeOONNxx6X2FhISorK+2PESNGOPrRRCRRHQbP3xLc1fx0y+mx25iUeJz3fyzFsXItgtRKLF84SuxwyEFKR9+wYMECLFiwwOEPioyMREhIiMPvIyLpsy3feENPCQDcODoKf/z0OI5c0OJCYxvih/iLHRL1QV2LHi9bm1t/N28kIoN8RY6IHOW2hbYJEyYgJiYGc+fOxY4dO654rV6vh06n6/YgIulq94LhaV1FBvliSlIoAOCro5UiR0N99dKWAug6jBgdo8HPpw0VOxzqB5cnJTExMVizZg1ycnKwadMmpKamYu7cudi1a1ev78nOzkZwcLD9kZCQ4OowiWgAvGVLcFe3josFAHzJpMQjHDjfgP8cvAAAePH2DCjZ3OqRHF6+cVRqaipSU1PtzzMzM1FWVoYVK1Zg1qxZPb5n+fLlyMrKsj/X6XRMTIgkrN3LekoAYEFGNP778xM4Vq7F+bpWJIUHiB0S9aLTZMYfPzkOALh7UgImDh0ickTUX6KkktOmTcOZM2d6/XO1Wg2NRtPtQUTS5Y2VkrBANaYPs+zc+OoYqyVS9o/dRSisbsYQfxV+v4DNrZ5MlKQkPz8fMTExYnw0EblAmxdWSgDglrGW+9QXRypEjoR6U1Lfite+sfyS+8ebRyM0wEfkiGggHF6+aWlpwdmzZ+3Pi4uLcfjwYYSGhiIxMRHLly9HeXk5NmzYAABYuXIlkpKSkJ6eDoPBgI0bNyInJwc5OTnO+y6ISFS2s28C1C5fEXar+enReO6T4yioasbZmmYMjwwSOyTqQhAsM0n0RjOmDwvDndfEiR0SDZDDd5C8vDxcf/319ue23o8HH3wQ69evR2VlJUpLS+1/bjAYsHTpUpSXl8PPzw/p6en46quvsHDhQieET0RS0Kq3VEoCfLwrKQnx98HMEeHYUViLL45U4pkbmZRIyedHKrD7TB18lHL85Y4xkMlkYodEA+TwHeS666674pHe69ev7/Z82bJlWLZsmcOBEZHnaNXbKiXetXwDWHbh7CisxZdHK/D0DSP4g08imtoMePHLkwCAJ68fjmQ2InsF7pkiogGz9ZR4W6UEsAxS81HKca62FQVVzWKHQ1b/t7UAdS0GDI8MxK9nDxM7HHISJiVENCCCIKDV2lPi74WVkiBfFa4bGQEA+PIoG16l4MD5BnzwYxkA4H/vGAMfJX+UeQv+P0lEA9LeaYJtRdcbKyUAcIt1kNoXRyqvuHxNrmcwmvHspmMAgHsmJ2BKcqjIEZEzMSkhogGxNbnKZN41p6SrG9Ii4e+jQGlDGw6VNoodzqC2Ztc5nKlpQXigD/7AmSReh0kJEQ2IbTuwv0oBudw7m0D9fZS4KSMaALDpULnI0QxexXWteP07y0iK/7plNEL8OZPE2zApIaIBsVVK/L1sRsml7pwQD8ByFo7eaBI5msHHZBaw9N9HYDCaMXNEOG6zLqmRd2FSQkQDYmtyDfTypCRzWBiiNGpo2zuxo6BW7HAGnXd2F+FgSSMC1Uq8tHgst2Z7KSYlRDQgthkl/l42Yv5SCrkMt4+3TAzddOiCyNEMLqcqdXhl+2kAwPO3jEZciJ/IEZGrMCkhogHx5hkll7rDOsZ8R2ENGlsNIkczOGjbO/HYxoMwGM2YOyoSd02KFzskciEmJUQ0IPZKiRfOKLnUqGgNRsdo0GkS8CVPDnY5s1nA7z4+gvP1bYgL8cOKu8Zx2cbLMSkhogG5OGLe+yslAOyHvn3CJRyXEgQBz39+HN+cqoaPUo5VP78GQ3gCsNdjUkJEA9JqX77x/koJANw2PhZyGXCotAlnazh23hUEQcCfPj+BjftKIZMBL/9kLMbGh4gdFrkBkxIiGhD7nJJB0FMCAJFBvpgzKgoA8P7+MpGj8T6CIOCFL0/in3tLIJMBf108FousDcbk/ZiUENGA2OaUeOMJwb25b2oiACDn0AV0dHJmibMIgoC/fHUK7/5wHgDw0p1jcNekBHGDIrdiUkJEAzLYekoAYNbICMSF+EHb3onNbHh1CkEQ8NLWArzzfTEAy0F7d09OFDkqcjcmJUQ0IINpS7CNQi7DPZMtv8G/v79U5Gg8nyAIWLGtEG/nFgEAXrw9A/dOZUIyGDEpIaIBaTUMjuFpl/rp5AQo5DLklTSisIoNrwOxOrcIb+44BwD4823puH/aUJEjIrEwKSGiAWmz95QMnkoJAERpfHFjmqXhdf2e8+IG48E+zivD/20tAAA8tzAND05PEjcgEhWTEiIaEFulZLAlJQDw0IwkAJax8/UtenGD8UD5pY147pNjAIDHrhuGR2eliBwRiY1JCRENiL3RdZAt3wDAlORQjI0Pht5oxsZ97C1xREOrAb957xA6TQJuSo/GsvmpYodEEsCkhIgGxDY8bbDMKelKJpPhkZmW3+7/te88twc74PnPjqNS24GU8AC8fBdP/SULJiVENCDNHZ0AgCDfwZeUAMDCjGjEhfihrsWAT/PLxQ7HI2w5Vokvj1ZCIZfhtXsmIMhXJXZIJBFMSoio3wxGMzo6zQAAjd/g/MGiVMjxC2tz5ppdRTCZBXEDkrjmjk48//kJAMBjs4dhTHywyBGRlDApIaJ+s1VJACBwEDa62vxsaiJC/FUoqmvFl0crxA5H0t7ccQ61zXokhwfgybnDxQ6HJIZJCRH1m67D0uQapFZCIR+8PQGBaiUetfaWvPbtGVZLelFS34p11omtf7w5DWrl4GuOpitjUkJE/TbY+0m6eiBzKIL9VCiqZbWkN3/56hQMJjNmjgjHnFGRYodDEsSkhIj6TddurZSwURFBvio8cm0yAOB1Vksu88PZOmw7WQ2FXIbnbxnN3TbUIyYlRNRvtkqJxo+VEgB4cEYSNL5KnGO1pBujyYwXvjgJALh/2lCMiAoSOSKSKiYlRNRvOvvyDSslAKDxVdnnlqz85gw6TWaRI5KGDXtLUFjdjBB/FZ6+YYTY4ZCEMSkhon5rtja6athTYvfwtckIC/BBcV0r/nPwgtjhiK62WY9Xt58GACybPwoh/j4iR0RS5nBSsmvXLtx6662IjY2FTCbDp59+etX35ObmYuLEifD19UVKSgpWr17dn1iJSGJ07ayUXCpQrcTj11u2uq785vSgn/L60pYCNOuNGBMXjLsnJ4gdDkmcw0lJa2srxo0bhzfeeKNP1xcXF2PhwoWYOXMm8vPz8eyzz+Kpp55CTk6Ow8ESkbTYtgSzp6S7+6YlIi7ED9U6PTbsPS92OKLZUVCDnEOWatELi9IH9bZx6huH7yQLFizAggUL+nz96tWrkZiYiJUrVwIA0tLSkJeXhxUrVmDx4sWOfrzXqm/R41i5Fg2tBqgUciSG+mNUTBD38ZOksaekZ2qlAr+9YQSW/eco3tp5DvdMSYRmkP0dNbQa8P/+cxQA8PCMZExIHCJyROQJXP7rzd69ezFv3rxur82fPx9r165FZ2cnVKrL/0PV6/XQ6y8eA67T6Vwdpmh2na7FmzvOYn9xw2V/FqhWYn56NH41KwWp0exWJ+m52FMyuH7g9sWdE+KwZlcRzta04J1dRciaN3hOwTWazPjth/moa9FjRGQglt00eL53GhiXN7pWVVUhKiqq22tRUVEwGo2oq6vr8T3Z2dkIDg62PxISvG8dsqPThN99fAQPrPvRnpAMiwjAzBHhmJIUitAAH7Tojcg5dAE3vbYL/7v5FPTGwb02TdJzsaeEyzeXUirkWDpvJADgne+LUdusv8o7vIPZLOC5T45j95k6+KkUeO2eCfBVseJLfeOWO8mlQ3IEQejxdZvly5cjKyvL/lyn03lVYtLUZsAj/8xDXkkj5DLggcwk/GpWCmJD/OzXmM0CDpU24p3dxdh6ogprdhWhsKoZb98/kf+Bk2TYKyWD9DC+q5mfHo1x8cE4ckGLN3ecxZ9uSxc7JJfq6DTh2U+OYdOhcshlwMp7xmN0rEbssMiDuLxSEh0djaqqqm6v1dTUQKlUIiwsrMf3qNVqaDSabg9v0aI34udr9yOvpBEaXyXef3Qa/nRbereEBADkchkmJYVi9f0T8fb9E+GnUiD3dC2e/vCwPakjEpuOY+avSCaTYdlNowAAG/eV4GxNi8gRuc6Fxjb89O299oTk1bvHY356tNhhkYdxeVKSmZmJ7du3d3tt27ZtmDRpUo/9JN7MYDTjsY0Hcbxch7AAH3y8JBPTUnpOzLqanx6N9Q9Nhkohw9YTVVhrPdCKSGzsKbm6GcPDcUNaJIxmAS98edIrf6n4/kwdbv379zh6QYsh/iqsf2gKFo2PEzss8kAOJyUtLS04fPgwDh8+DMCy5ffw4cMoLS0FYFl6eeCBB+zXL1myBCUlJcjKysKpU6ewbt06rF27FkuXLnXOd+BBsrecsq+zrvvFZIyK7nsFaGpKGJ6/ZTQA4K9fF6KkvtVVYRL1iSAIF8fMs1JyRX+8eTR8FHLsOl2Lb0/ViB2O0wiCgLdzz+GBdfvR2NaJMXHB+OLJazFrZITYoZGHcjgpycvLw4QJEzBhwgQAQFZWFiZMmIDnn38eAFBZWWlPUAAgOTkZmzdvxs6dOzF+/Hi8+OKLeP311wfdduCtxyvx7g/nAQCv/2wCxiWEOPw1fj5tKGYMD4PBaMbzn51wboBEDmo1mGA7c449JVeWFB6Ah62H9b341UmvGKgmCAJe2lqA7C0FMAvAXRPj8e8lmYgf4i92aOTBZIIH1BJ1Oh2Cg4Oh1Wo9sr+ktlmPua/shK7DiF/PSsHyhWn9/lpFtS2Yv3IXOk0C3ntkKmYMD3dipER9V97UjhkvfQcfhRyF/3MTT329iha9EXNW7ERNsx6PXTcMv7f2mniqf+wqwl82nwIA/Ncto/HwjCT+O0CXcfTnN8++cYMXvjwJXYcRGXEaLJ0/sP36KRGBuG/qUADAy18XeuX6NHmGxlYDACDEX8UfRn0QqFbixdszAABv557DodJGkSPqvz1n65C9xZKQPLcwDb+8Npn/DpBTMClxsZ2FNfjiSAXkMuClO8dCpRj4X/lvrh8GP5UCh8ua8I0XrU+TZ9FaZ5SE+HPppq/mp0fjjglxMAvAbz/Mtyd2nqRVb8T/+89RmAXgJxPj8cjMZLFDIi/CpMSFTGYB/2stbz40IxkZccFO+bqRQb54aEYSAOCVbYUwm1ktIfdrbLNVSnjqqyP+dGs6EkL9UNbQjqc+zIfRZBY7JIe8uv00ypvaERfihxcWpbNCQk7FpMSFNh26gNPVLQj2U+GpOSOc+rV/PWsYgtRKFFQ145tT1U792kR90dRmrZSwydUhwf4qrLl/EvxUCuw+U4ff/fsITB7yi8Xp6mas+8EykuB/bs+Avw93XZFzMSlxkY5OE17dfhoA8JvrhiHYySXuYH8V7s+09Ja8ueMse0vI7ZqslZIhrJQ4LC1Gg9fuGQ+lXIbPDlfgqQ/z0W6Q/o6cV7efhlkA5o2OwvWjIsUOh7wQkxIXeX9/KSq0HYgJ9sWD05Nc8hkPX5sMX5UcRy5o8cPZepd8BlFv7JUS9pT0y7z0aLxx7wQo5TJ8dbQSP1m9R9ITX4+Xa7HleBVkMgy4YZ+oN0xKXMBgNOMfu4sAAE/MGe6ys2rCA9W4Z3IiAEu1hMidGu1JCSsl/XVTRgz+9cupCA3wwYkKHW5auQt//uIEyhraxA7tMq9sKwQALBoXi5FRPLWcXINJiQt8ergcldoORAap8ZOJ8S79rF/NSoFSLsPeonocLPHcLYbkeS4u37BSMhCZw8Lw5ZPX2kfRv/vDecx6eQceXn8AW45VwmAUvxH2YEkDdhTWQiGX4ekbRoodDnkxJiVOZjYLWJ17DgDwy2uToVa69kTf2BA/3HmN5YyJt1gtITdq4pZgp4kN8cM7D07GhoenYOaIcAgC8F1BDR577xCm/u83+PMXJ3ChUbzqyYqvLf1xd02MR1J4gGhxkPdjUuJkP5yrQ1FtK4LUStw7NdEtn/nYdcMhlwHfFtTgZIXOLZ9JxC3BzjdrZAT+9cup+O53s7Fk9jBEBqnR2NaJd384jzkrcpG95ZTbR9T/cLYOe4vq4aOQ48m5zt1FSHQpJiVO9t4+y7k/d14ThyA3nZyaHB6AhWNiAACrrFUaIlfTstHVZVIiAvGHBaOw5w9z8O4vJiMzJQwGkxlv5xZh4Wu73TYNVhAE/PVrSy/JvVMTERfi55bPpcGLSYkTVes6sN06M+Re6yh4d3nsumEAgK+OVuB8HU8QJtcSBMG+fMMtwa6jVMhx/ahIvP/oVLzzwCREadQoqmvFT1btcUvVZNvJahwpa4K/jwKPXz/cpZ9FBDApcap/55XBZBYwaegQpEa7tzs9PTYY16VGwCwAb+9itYRcq1lvtA/8CubwNJeTyWS4YXQUtj09G3dax9S/nVuEW/7+PQ6XNbnkMw1GM162VkkenpGMiCC1Sz6HqCsmJU4iCAI+yS8HANwzxT29JJey/SaTc7Ac1boOUWKgwaGp1VIl8VMpXLblnS4X7K/C3+4ej388MAkRQWqcrWnBnW/9gP/bWuD0qsna74txtqYFYQE+eHRWilO/NlFvmJQ4yclKHc7VtkKtlGN+epQoMUxOCsXkpCEwmMx4xzonhcgVGrkdWFQ3jo7C9mdm4fbxsTALwKqd5zDv1V3YUeCcAzovNLbh9W/PAACWL0xjNYzchkmJk3x+pAIAMGdUpNsaXHvym+ss1ZL39pfa50gQOVsDd96ILsTfByvvmYC375+IaI0vShva8ND6A/jVhrwBbR82msx45qPDaO80YUpyKBZbRw4QuQOTEicwmwV8eaQSAHDbuFhRY7kuNQJpMRq0GUz4554SUWMh71XXrAcA9hlIwPz0aHzzu9n2QYrbTlbjhr/l4s0dZ/s1eO3/thbgwPlGBKqV+OvisTwFmNyKSYkTHCptRHlTOwLVStEPqZLJZPadOOv3FKPNYBQ1HvJOtS1MSqQkUK3EswvTsPm3MzElORQdnZYm1Zte24WdhTV9OrBTEAT8/dsz+MduyynA2XeO4aA0cjsmJU5gW7qZlx4liaa/hRnRGBrmj8a2TnzwY5nY4ZAXqmWlRJJGRgXho19Nw6t3j0N4oBpFta34xbsHsOC13diw9zzKm9p7fF+1rgNPfJCPV6wnmy9fMAq3ilz1pcFJKXYAns5oMmPzMWks3dgoFXIsmT0Myzcdwz92FeHn0xJdPu6eBhd7UhLIpERqZDIZ7pgQjzmjovDaN2fwwY+lKKhqxvOfncDzn51AYqg/0mKCEK3xBQAU1bViX1E9Ok0CFHIZnr9ltMtONie6GiYlA/T92TrUtRgQGuCDGcPDxQ7H7s5r4rDym9Oo0nXg0/xy3D1ZnG3K5J1YKZG+YD8Vnr91NH47dwT+fbAMX5+owsGSRpQ2tKG0h1OIJw0dgv+6ZTTGJYS4P1giKyYlA/TZYcvSzS1jY6BSSGc1TK1U4JFrU/CXzaewOrcIP5mYAIWcDWvkHOwp8RzB/io8MjMFj8xMgbatEycqtCioakZjmwEms4CYED9kpoRieKR7Bz4S9YRJyQC0GYz4+kQVAGDReOltm/vZ1ES8seMsiutasfV4FW4eGyN2SOQlWCnxTMH+KkwfHo7pEqrqEnUlnV/tPdD2k9VoM5iQGOqPaxJDxA7nMoFqpX1t+K2dZ/vUgU90NR2dJjR3WHZ1MSkhImdiUjIAH1p3ttw+Playe/kfmp4Efx8FTlTokHu6VuxwyAvYqiRqpRxBahZbich5mJT00+nqZuwtqodcJt5ZN30xJMAHP7PG9+o3Z1gtoQHr2k8i1WSciDwTk5J+eveH8wCAeaOjERviJ24wV7Fk9jD4+yhwpKwJX5+oFjsc8nA1OvaTEJFrMCnph9L6Nvw7z7J089CMJHGD6YOIIDUenpEMAFixrdB+5DxRf9grJZxRQkROxqSkH17ZXgijWcCskRGYmhImdjh98qvZKQjxV+FsTQs2Hbogdjjkwaq0lqmgUdbhW0REzsKkxEEnKrT22STL5qeKHE3faXxVeGy25Uycld+cgd5oEjki8lQVTR0AgLgh0l62JCLP06+k5K233kJycjJ8fX0xceJE7N69u9drd+7cCZlMdtmjoKCg30GL6W/bLGdD3DYuFhlxwSJH45gHpychSqNGeVM73ttXKnY45KFs56dIvZeKiDyPw0nJRx99hKeffhrPPfcc8vPzMXPmTCxYsAClpVf+IVdYWIjKykr7Y8SIEf0OWizHy7X4tqAGchnwzI0jxQ7HYb4qBX471xL3mzvOokXPE4TJcRXWpCQuhMs3RORcDiclf/vb3/DLX/4SjzzyCNLS0rBy5UokJCRg1apVV3xfZGQkoqOj7Q+FwvMOiHtr51kAwC1jY5HsoUd63zUpHklh/qhvNWBN7jmxwyEPYzILqNJalm9YKSEiZ3MoKTEYDDh48CDmzZvX7fV58+Zhz549V3zvhAkTEBMTg7lz52LHjh1XvFav10On03V7iK1G12HfTvub64eJHE3/qRRyLLtpFADg7V1FuNB4+cFcRL2pbdbDaLacJhsZxEoJETmXQ0lJXV0dTCYToqKiur0eFRWFqqqqHt8TExODNWvWICcnB5s2bUJqairmzp2LXbt29fo52dnZCA4Otj8SEhIcCdMlcg6Vw2QWcE1iCEZFa8QOZ0AWZERjanIo9EYz/nfzKbHDIQ9i6yeJ1vjygEcicrp+zYi+dIqjIAi9TnZMTU1FaurFXSqZmZkoKyvDihUrMGvWrB7fs3z5cmRlZdmf63Q6URMTQRDsc0nunix+gjRQMpkMf7otHTe/vhubj1Vhz7k6TB/GA7ro6i72k3Dphoicz6FKSXh4OBQKxWVVkZqamsuqJ1cybdo0nDlzptc/V6vV0Gg03R5iOnJBi6K6VvipFLh5bKyosThLWowG9061jJ9/4YuTMJrMIkdEnqDCvvOGSzdE5HwOJSU+Pj6YOHEitm/f3u317du3Y/r06X3+Ovn5+YiJiXHko0X1uXUuyQ2joxDoRQeQ/e7GVAT7qVBQ1Wwfm090JRXcDkxELuTwT9isrCzcf//9mDRpEjIzM7FmzRqUlpZiyZIlACxLL+Xl5diwYQMAYOXKlUhKSkJ6ejoMBgM2btyInJwc5OTkOPc7cRGTWcCXRy1JyW3jvKNKYjMkwAfLF4zCHzYdwyvbCzEvPQpDwzxzVxG5R0mDpTE6foi/yJEQkTdyOCm5++67UV9fjxdeeAGVlZXIyMjA5s2bMXToUABAZWVlt5klBoMBS5cuRXl5Ofz8/JCeno6vvvoKCxcudN534UL7i+tR06xHsJ8Ks0dGiB2O0909OQGfHa7A3qJ6/CHnGN5/dCpPfqVeFde1AgBSIpi8EpHzyQQPOMtep9MhODgYWq3W7f0lyzcdxQc/luGeyQl4afFYt362u5TUt2L+yl3o6DTjpTvH4J4piWKHRBJkMJox6r+2wCwAPz47F5E8+4aIrsLRn988++YKOjpN2HzM0tTrbUs3XQ0NC8DSeZYdUn/56hRnl1CPShvaYBaAAB8FIoJ4QjAROR+Tkiv44kgFtO2diAvx85jTgPvroRnJuCYxBM16I3738RGYzJIvoJGb2ZZukiMCuMRHRC7BpKQXJrOAtd8XAwB+Pm2o1w+KUshlePXu8fD3UWB/cQP+sbtI7JBIYorrWgAAyeGBIkdCRN6KSUkv/p1XhoKqZmh8lfjZFM8fmNYXQ8MC8Kdb0wEAr2wrxPFyrcgRkZTYKyUeeu4TEUkfk5JLCIKAb09V409fnAAAPDFnOEL8fUSOyn3umhSP+elR6DQJePqjw+joNIkdEknEuVrrzhsmJUTkIt4zCWwADEYz9hfXY9uJamw/WY0qneUU1NkjI/DLa1NEjs69ZDIZsu8ci0Olu3C2pgV/+vyE1+46or4TBAGFVc0AgOGRXL4hItcY1EnJO7uLsO1kNY5eaEJH58Ux6/4+Ctw1MR7LF6Z5fS9JT0IDfPDKXePw4Ls/4sMDZRiXEIKfcZvwoFap7YC2vRMKuYxJCRG5zKBOSo6Va/FjcQMAIDxQjRtHR+LG0VGYPiwcviqFyNGJa9bICCydl4qXvy7Ef392AmkxGoxPCBE7LBLJqUodAGBYRMCg/2+DiFxnUCcld09KwIxh4bhmaAhSwgMhH4RVkSt5bPYwHClrwraT1fj1v/LwyW9m8MyTQcqWlKTFiHs4JhF5t0Hd6Dp9eDh+OjkBwyODmJD0QC6X4ZWfjsPIqEBU6/R46N0D0LZ3ih0WieCUtZ+ESQkRudKgTkro6oJ8VXj3oSmIDFKjsLoZj208CIPRfPU3kldhpYSI3IFJCV1VXIgf1v1iMgJ8FNhzrh5/yDkKDzgyiZxE19Fpn1EymkkJEbkQkxLqk4y4YLx53zVQyGXYlF+O7C0FTEwGiUMljRAEYGiYP8+8ISKXYlJCfXZdaiSy7xgDAFizqwh//+6syBGRO+SdbwQATBoaKnIkROTtmJSQQ346OQH/dctoAMDftp+2nw9E3iuvxLJtflLSEJEjISJvx6SEHPbLa5PxzA0jAQAvfnkSG/aeFzcgchmD0YzDZU0AgMlMSojIxZiUUL88NXc4fjXLMoL/+c9O4O/fnmGPiRc6XGaZdjzEX4UUng5MRC7GpIT6RSaTYfmCUXhqznAAwCvbT+PPX5yEyczExJt8V1ADwDLhl7N8iMjVmJRQv8lkMmTNS7X3mKzfcx4Prz8AXQcHrHmLHdakZM6oSJEjIaLBgEkJDdgvr03GG/dOgK9KjtzTtbjjzR9w3jrXgjzXmepmFFY3QymXYdaICLHDIaJBgEkJOcUtY2Px719PR7TGF+dqW7HozR+Qe7pW7LBoAD49XA4AmD0yAkMCfESOhogGAyYl5DRj4oPx+RMzMC4hBNr2Tvzi3R/xyrZC9pl4IIPRjI/zLgAAbp8QJ3I0RDRYMCkhp4rU+OKjX03Dz6clQhCAv393Fve9sw9V2g6xQyMHfHm0ArXNekQGqTE/PVrscIhokGBSQk7nq1Lgf24fg9fuGY8AHwX2FTXgxr/l4r39JTB7aNXEbBagbetEWUMbmtoMV9z+LAgCTlXq8M895/HKtkJs3FeCsoY2N0Y7MAajGSu/OQMAeHB6EnyUvE0QkXsoxQ6AvNei8XHIiAtG1sdHcKSsCc99chyf5Vfgz4vSJX/a7MGSBnx2uAJHL2hxvr4V2vZOdM1DIoLUmJYShpnDwzEtJQxDAlQoqW/DN6eq8dXRSpypaen29WQyYOGYGPzx5jTEBPu5+btxzNu551Da0IbwQDUempEkdjhENIjIBA+YeKXT6RAcHAytVguNRto/zOhyJrOADXvP4+WvC9FmMEEuA342JRFZN45EWKC0DnhrbDXg2U+OYcvxqh7/3Ecph8FovurX8VHKMX1YGGJD/HC2pgU/FltGtfv7KPD0DSPw0IxkqBTSq0AcL9fijrd+QKdJwKt3j8MdE+LFDomIPJijP7+ZlJDbXGhsQ/bmAnx1rBIAEOSrxNM3jMQDmUMl8QO6StuB+9fux5maFijkMiwaH4s5oyIxPDIQYQFqBPup4KOUo6PThMNlTdhztg67z9bheLkWnSYBgWolpiaHYsGYGMxLj4LGV2X/2icrdHj+s+PIK7EcbjciMhCPzkrBDWlRCJXIzpYLjW1YvGoPqnV6zBsdhbfvnwiZjAPTiKj/mJSQ5O0rqsefvziJU5U6AEBKRACenDMct4yNFS05Ka1vw31r96GsoR3RGl+88+AkZMQF9+m9giCgvdMEP5Xiij/EzWYBOYcuIHtLARpaDfbX40L8EBbog2A/FXxVCvipFPBVyS3/66OAxleFlPAADI8MxNCwAJf0eOwrqsdTH+SjplmP1KggfLwkE8F+qqu/kYjoCpiUkEcwmQV8nFeGFV8Xot76Azom2Bf3TU3E7RPiED/E322xHC/X4hfvHkBdix5Dw/yx8ZdTkRDqus/XtnXivR9L8Gl+OU5Xt1z9DV2oFDIMjwxCWkwQRsdoMDpGg7QYTb/miFRpO7CjsAZfHKnAnnP1AICRUYH458NTJN/3QkSegUkJeRRdRyf+tbcE7/5wHnUtevvr01JCceeEeCwYE40gX+f/xm42CyhrbMO/8y5gze4iGIxmjIoOwoaHpyBS4+v0z+tNfYseJQ1tqG8xoEXfiXaDGe2dJnRYH+0GExpaDThX24Jzta1o0Rt7/Doxwb5Ii9FYk5VgRAf7wk+lgEIuQ5vBiHaDCc16Iy40tuNcbQv2FdWjqPbi1F2FXIafTkrAczenIVDN/ncicg63JCVvvfUWXn75ZVRWViI9PR0rV67EzJkze70+NzcXWVlZOHHiBGJjY7Fs2TIsWbKkz5/HpMT7dXSa8OXRSuQcvIB9xfX2nS4+SjluSIvEovFxuC41AmqlwuGvLQgC9pyrx/aT1Sisaka1rgPlTe3Qd2lYnTMqEq/dM94lCZCzCIKA8qZ2nKzQ4VRlM05V6nCyUofSfm43lsuAMfEhmDc6CreNi3VpdYiIBieXJyUfffQR7r//frz11luYMWMG3n77bbzzzjs4efIkEhMTL7u+uLgYGRkZePTRR/HrX/8aP/zwA37zm9/ggw8+wOLFi13yTZFnK29qx6f55dh06ALOdfltPthPhYVjonH7+DhMTgrt06m1+4rqseLrQnuDaVcKuQxTkkJxf+ZQLMiI9timzuaOThRUWZIUS6LSjMZWA9o7TTCZBfipFAhQK+Dno0SMxhdJ4QGYkBiCaSlh7BshIpdyeVIydepUXHPNNVi1apX9tbS0NNx+++3Izs6+7Prf//73+Pzzz3Hq1Cn7a0uWLMGRI0ewd+/ePn0mk5LBSRAEnKjQ4bPD5fj8SAWqdReXd2KDfXHr+FgszIhBeqwGyksaZA+WNGDlN2ew+0wdAECtlOPOayzJTGyIH2KD/RAT4iuJXT9ERN7K0Z/fDi0eGwwGHDx4EH/4wx+6vT5v3jzs2bOnx/fs3bsX8+bN6/ba/PnzsXbtWnR2dkKluvw3Nb1eD73+4g8gnU7nSJjkJWQyGTLigpERF4w/LEjD/qJ6fHq4HFuOVaFC24G3c4vwdm4RAtVKTEgMQWywH0yCgKMXmuwNpEq5DD+bkogn5gxHlBt7RYiIyHEOJSV1dXUwmUyIiorq9npUVBSqqnoeNlVVVdXj9UajEXV1dYiJibnsPdnZ2fjzn//sSGjk5RRyGaYPD8f04eF4YVEGdhTU4PMjFfjhbB10HUZ7RcRGpZDhzgnxePz64UgMY68EEZEn6Feb/aVr74IgXHE9vqfre3rdZvny5cjKyrI/1+l0SEhI6E+o5IV8VQosGBODBWNiYDJbzpk5WaFDta4DCoUMSWEBmD4sDCH+0hhKRkREfeNQUhIeHg6FQnFZVaSmpuayaohNdHR0j9crlUqEhYX1+B61Wg21Wlrjx0maFPKLSzxEROTZHOry8/HxwcSJE7F9+/Zur2/fvh3Tp0/v8T2ZmZmXXb9t2zZMmjSpx34SIiIiGpwc3nqQlZWFd955B+vWrcOpU6fwzDPPoLS01D53ZPny5XjggQfs1y9ZsgQlJSXIysrCqVOnsG7dOqxduxZLly513ndBREREHs/hnpK7774b9fX1eOGFF1BZWYmMjAxs3rwZQ4cOBQBUVlaitLTUfn1ycjI2b96MZ555Bm+++SZiY2Px+uuv93lGCREREQ0OHDNPRERELuHoz29OjiIiIiJJYFJCREREksCkhIiIiCSBSQkRERFJApMSIiIikgQmJURERCQJTEqIiIhIEpiUEBERkSQwKSEiIiJJcHjMvBhsQ2d1Op3IkRAREVFf2X5u93V4vEckJc3NzQCAhIQEkSMhIiIiRzU3NyM4OPiq13nE2TdmsxkVFRUICgqCTCZz2tfV6XRMdIiIiAD8+OOPSE1NderXFAQBzc3NiI2NhVx+9Y4Rj6iUyOVyxMfHix0GERGR1woMDHTJobd9qZDYsNGViIiIJIFJCREREUmCRyzfuIparcbvf/977Ny5ExcuXEB0dDQqKysBADExMaiqqkJ0dHSv/8trea1UrvWUOHktr+W/49K8tqamBuHh4RCbRzS6EhERkffj8g0RERFJApMSIiIikgQmJURERCQJTEqIiIhIEjxm981vf/tbvPnmmzCZTGKHQkRERFfh7++P999/H4sWLerzezymUtLQ0AB/f3+oVCqxQyEiIqKraGtrw5133omKioo+v8cjtwQ78/wbIiIicp17770X7733Xp+u9ZhKCREREXkWhUKBffv29fl6JiVERETkEoIgQKvV9vl6JiVEREQkCUxKiIiIyCVkMhmCg4P7fD2TEiIiInIJk8mEadOm9fl6j5lTcu7cOaxbtw5Hjx4VOxQiIiLqA7lcjpdffrnP13tMUvLqq6/izTffFDsMIiIi6gN/f39s3LgRsbGxfX6PR84pISIiIu/DnhIiIiKSBCYlREREJAlMSoiIiEgSmJQQERGRJDApISIiIklgUkJERESSwKSEiIiIJIFJCREREUkCkxIiIiKSBCYlREREJAlMSoiIiEgSmJQQERGRJPx/3DebKWRri4AAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i in range(2):\n",
    "    plt.figure()\n",
    "    plt.plot(spectra.iloc[i,:])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Lactate</th>\n",
       "      <th>1350.0</th>\n",
       "      <th>1351.0</th>\n",
       "      <th>1352.0</th>\n",
       "      <th>1353.0</th>\n",
       "      <th>1354.0</th>\n",
       "      <th>1355.0</th>\n",
       "      <th>1356.0</th>\n",
       "      <th>1357.0</th>\n",
       "      <th>1358.0</th>\n",
       "      <th>...</th>\n",
       "      <th>2491.0</th>\n",
       "      <th>2492.0</th>\n",
       "      <th>2493.0</th>\n",
       "      <th>2494.0</th>\n",
       "      <th>2495.0</th>\n",
       "      <th>2496.0</th>\n",
       "      <th>2497.0</th>\n",
       "      <th>2498.0</th>\n",
       "      <th>2499.0</th>\n",
       "      <th>2500.0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9.0</td>\n",
       "      <td>0.325606</td>\n",
       "      <td>0.326118</td>\n",
       "      <td>0.326777</td>\n",
       "      <td>0.327594</td>\n",
       "      <td>0.328574</td>\n",
       "      <td>0.329736</td>\n",
       "      <td>0.331045</td>\n",
       "      <td>0.332443</td>\n",
       "      <td>0.333808</td>\n",
       "      <td>...</td>\n",
       "      <td>3.884846</td>\n",
       "      <td>3.891455</td>\n",
       "      <td>3.897297</td>\n",
       "      <td>3.902262</td>\n",
       "      <td>3.906273</td>\n",
       "      <td>3.909308</td>\n",
       "      <td>3.911418</td>\n",
       "      <td>3.912773</td>\n",
       "      <td>3.913718</td>\n",
       "      <td>3.914531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9.0</td>\n",
       "      <td>0.325919</td>\n",
       "      <td>0.326437</td>\n",
       "      <td>0.327103</td>\n",
       "      <td>0.327927</td>\n",
       "      <td>0.328911</td>\n",
       "      <td>0.330068</td>\n",
       "      <td>0.331363</td>\n",
       "      <td>0.332741</td>\n",
       "      <td>0.334091</td>\n",
       "      <td>...</td>\n",
       "      <td>3.902861</td>\n",
       "      <td>3.909777</td>\n",
       "      <td>3.914507</td>\n",
       "      <td>3.917156</td>\n",
       "      <td>3.917897</td>\n",
       "      <td>3.916987</td>\n",
       "      <td>3.914795</td>\n",
       "      <td>3.911844</td>\n",
       "      <td>3.908879</td>\n",
       "      <td>3.906618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9.0</td>\n",
       "      <td>0.327546</td>\n",
       "      <td>0.328058</td>\n",
       "      <td>0.328717</td>\n",
       "      <td>0.329531</td>\n",
       "      <td>0.330507</td>\n",
       "      <td>0.331664</td>\n",
       "      <td>0.332967</td>\n",
       "      <td>0.334358</td>\n",
       "      <td>0.335716</td>\n",
       "      <td>...</td>\n",
       "      <td>3.890589</td>\n",
       "      <td>3.900686</td>\n",
       "      <td>3.909336</td>\n",
       "      <td>3.916392</td>\n",
       "      <td>3.921788</td>\n",
       "      <td>3.925556</td>\n",
       "      <td>3.927858</td>\n",
       "      <td>3.929030</td>\n",
       "      <td>3.929648</td>\n",
       "      <td>3.930290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9.0</td>\n",
       "      <td>0.326307</td>\n",
       "      <td>0.326820</td>\n",
       "      <td>0.327485</td>\n",
       "      <td>0.328309</td>\n",
       "      <td>0.329295</td>\n",
       "      <td>0.330458</td>\n",
       "      <td>0.331761</td>\n",
       "      <td>0.333149</td>\n",
       "      <td>0.334505</td>\n",
       "      <td>...</td>\n",
       "      <td>3.869466</td>\n",
       "      <td>3.876229</td>\n",
       "      <td>3.882600</td>\n",
       "      <td>3.888400</td>\n",
       "      <td>3.893481</td>\n",
       "      <td>3.897737</td>\n",
       "      <td>3.901139</td>\n",
       "      <td>3.903763</td>\n",
       "      <td>3.905857</td>\n",
       "      <td>3.907591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9.0</td>\n",
       "      <td>0.325683</td>\n",
       "      <td>0.326201</td>\n",
       "      <td>0.326867</td>\n",
       "      <td>0.327691</td>\n",
       "      <td>0.328678</td>\n",
       "      <td>0.329846</td>\n",
       "      <td>0.331160</td>\n",
       "      <td>0.332560</td>\n",
       "      <td>0.333924</td>\n",
       "      <td>...</td>\n",
       "      <td>3.865839</td>\n",
       "      <td>3.870089</td>\n",
       "      <td>3.873236</td>\n",
       "      <td>3.875318</td>\n",
       "      <td>3.876395</td>\n",
       "      <td>3.876562</td>\n",
       "      <td>3.875976</td>\n",
       "      <td>3.874894</td>\n",
       "      <td>3.873732</td>\n",
       "      <td>3.872815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9095</th>\n",
       "      <td>300.0</td>\n",
       "      <td>0.323957</td>\n",
       "      <td>0.324675</td>\n",
       "      <td>0.325582</td>\n",
       "      <td>0.326674</td>\n",
       "      <td>0.327948</td>\n",
       "      <td>0.329424</td>\n",
       "      <td>0.331054</td>\n",
       "      <td>0.332754</td>\n",
       "      <td>0.334386</td>\n",
       "      <td>...</td>\n",
       "      <td>4.500144</td>\n",
       "      <td>4.512827</td>\n",
       "      <td>4.526333</td>\n",
       "      <td>4.540226</td>\n",
       "      <td>4.554142</td>\n",
       "      <td>4.567758</td>\n",
       "      <td>4.580768</td>\n",
       "      <td>4.592866</td>\n",
       "      <td>4.603732</td>\n",
       "      <td>4.612983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9096</th>\n",
       "      <td>300.0</td>\n",
       "      <td>0.323710</td>\n",
       "      <td>0.324435</td>\n",
       "      <td>0.325347</td>\n",
       "      <td>0.326446</td>\n",
       "      <td>0.327726</td>\n",
       "      <td>0.329207</td>\n",
       "      <td>0.330841</td>\n",
       "      <td>0.332543</td>\n",
       "      <td>0.334175</td>\n",
       "      <td>...</td>\n",
       "      <td>4.548688</td>\n",
       "      <td>4.554267</td>\n",
       "      <td>4.555407</td>\n",
       "      <td>4.552824</td>\n",
       "      <td>4.547324</td>\n",
       "      <td>4.539770</td>\n",
       "      <td>4.531062</td>\n",
       "      <td>4.522119</td>\n",
       "      <td>4.513868</td>\n",
       "      <td>4.507194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9097</th>\n",
       "      <td>300.0</td>\n",
       "      <td>0.323934</td>\n",
       "      <td>0.324658</td>\n",
       "      <td>0.325563</td>\n",
       "      <td>0.326648</td>\n",
       "      <td>0.327911</td>\n",
       "      <td>0.329370</td>\n",
       "      <td>0.330981</td>\n",
       "      <td>0.332661</td>\n",
       "      <td>0.334278</td>\n",
       "      <td>...</td>\n",
       "      <td>4.504238</td>\n",
       "      <td>4.512583</td>\n",
       "      <td>4.517647</td>\n",
       "      <td>4.519844</td>\n",
       "      <td>4.519682</td>\n",
       "      <td>4.517728</td>\n",
       "      <td>4.514587</td>\n",
       "      <td>4.510887</td>\n",
       "      <td>4.507268</td>\n",
       "      <td>4.504331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9098</th>\n",
       "      <td>300.0</td>\n",
       "      <td>0.324222</td>\n",
       "      <td>0.324944</td>\n",
       "      <td>0.325854</td>\n",
       "      <td>0.326950</td>\n",
       "      <td>0.328228</td>\n",
       "      <td>0.329704</td>\n",
       "      <td>0.331331</td>\n",
       "      <td>0.333025</td>\n",
       "      <td>0.334650</td>\n",
       "      <td>...</td>\n",
       "      <td>4.569032</td>\n",
       "      <td>4.576238</td>\n",
       "      <td>4.579017</td>\n",
       "      <td>4.578017</td>\n",
       "      <td>4.573987</td>\n",
       "      <td>4.567748</td>\n",
       "      <td>4.560176</td>\n",
       "      <td>4.552182</td>\n",
       "      <td>4.544703</td>\n",
       "      <td>4.538660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9099</th>\n",
       "      <td>300.0</td>\n",
       "      <td>0.323798</td>\n",
       "      <td>0.324520</td>\n",
       "      <td>0.325434</td>\n",
       "      <td>0.326538</td>\n",
       "      <td>0.327827</td>\n",
       "      <td>0.329312</td>\n",
       "      <td>0.330945</td>\n",
       "      <td>0.332644</td>\n",
       "      <td>0.334274</td>\n",
       "      <td>...</td>\n",
       "      <td>4.615527</td>\n",
       "      <td>4.624899</td>\n",
       "      <td>4.627401</td>\n",
       "      <td>4.623963</td>\n",
       "      <td>4.615687</td>\n",
       "      <td>4.603824</td>\n",
       "      <td>4.589769</td>\n",
       "      <td>4.575056</td>\n",
       "      <td>4.561365</td>\n",
       "      <td>4.550496</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9100 rows × 1152 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Lactate    1350.0    1351.0    1352.0    1353.0    1354.0    1355.0  \\\n",
       "0         9.0  0.325606  0.326118  0.326777  0.327594  0.328574  0.329736   \n",
       "1         9.0  0.325919  0.326437  0.327103  0.327927  0.328911  0.330068   \n",
       "2         9.0  0.327546  0.328058  0.328717  0.329531  0.330507  0.331664   \n",
       "3         9.0  0.326307  0.326820  0.327485  0.328309  0.329295  0.330458   \n",
       "4         9.0  0.325683  0.326201  0.326867  0.327691  0.328678  0.329846   \n",
       "...       ...       ...       ...       ...       ...       ...       ...   \n",
       "9095    300.0  0.323957  0.324675  0.325582  0.326674  0.327948  0.329424   \n",
       "9096    300.0  0.323710  0.324435  0.325347  0.326446  0.327726  0.329207   \n",
       "9097    300.0  0.323934  0.324658  0.325563  0.326648  0.327911  0.329370   \n",
       "9098    300.0  0.324222  0.324944  0.325854  0.326950  0.328228  0.329704   \n",
       "9099    300.0  0.323798  0.324520  0.325434  0.326538  0.327827  0.329312   \n",
       "\n",
       "        1356.0    1357.0    1358.0  ...    2491.0    2492.0    2493.0  \\\n",
       "0     0.331045  0.332443  0.333808  ...  3.884846  3.891455  3.897297   \n",
       "1     0.331363  0.332741  0.334091  ...  3.902861  3.909777  3.914507   \n",
       "2     0.332967  0.334358  0.335716  ...  3.890589  3.900686  3.909336   \n",
       "3     0.331761  0.333149  0.334505  ...  3.869466  3.876229  3.882600   \n",
       "4     0.331160  0.332560  0.333924  ...  3.865839  3.870089  3.873236   \n",
       "...        ...       ...       ...  ...       ...       ...       ...   \n",
       "9095  0.331054  0.332754  0.334386  ...  4.500144  4.512827  4.526333   \n",
       "9096  0.330841  0.332543  0.334175  ...  4.548688  4.554267  4.555407   \n",
       "9097  0.330981  0.332661  0.334278  ...  4.504238  4.512583  4.517647   \n",
       "9098  0.331331  0.333025  0.334650  ...  4.569032  4.576238  4.579017   \n",
       "9099  0.330945  0.332644  0.334274  ...  4.615527  4.624899  4.627401   \n",
       "\n",
       "        2494.0    2495.0    2496.0    2497.0    2498.0    2499.0    2500.0  \n",
       "0     3.902262  3.906273  3.909308  3.911418  3.912773  3.913718  3.914531  \n",
       "1     3.917156  3.917897  3.916987  3.914795  3.911844  3.908879  3.906618  \n",
       "2     3.916392  3.921788  3.925556  3.927858  3.929030  3.929648  3.930290  \n",
       "3     3.888400  3.893481  3.897737  3.901139  3.903763  3.905857  3.907591  \n",
       "4     3.875318  3.876395  3.876562  3.875976  3.874894  3.873732  3.872815  \n",
       "...        ...       ...       ...       ...       ...       ...       ...  \n",
       "9095  4.540226  4.554142  4.567758  4.580768  4.592866  4.603732  4.612983  \n",
       "9096  4.552824  4.547324  4.539770  4.531062  4.522119  4.513868  4.507194  \n",
       "9097  4.519844  4.519682  4.517728  4.514587  4.510887  4.507268  4.504331  \n",
       "9098  4.578017  4.573987  4.567748  4.560176  4.552182  4.544703  4.538660  \n",
       "9099  4.623963  4.615687  4.603824  4.589769  4.575056  4.561365  4.550496  \n",
       "\n",
       "[9100 rows x 1152 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lactate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "configs = Details(\n",
    "    pred_len = 1,      \n",
    "    seq_len  = 1151,\n",
    "    d_ff = 32,\n",
    "    patch_len= 16, \n",
    "    stride = 8, \n",
    "    llm_layers = 4,\n",
    "    description = \"This dataset contains near-infrared (NIR) spectroscopy measurements of lactate concentration in a \" \\\n",
    "                  \"phosphate-buffered saline (PBS) solution. Each spectrum includes absorbance values across \" \\\n",
    "                  \"different wavelengths. The goal is to predict the concentration given the different wavelengths \" ,\n",
    "    dropout = 0.1,\n",
    "    n_heads = 2,\n",
    "    d_model = 16,\n",
    "    enc_in = 1,\n",
    "    d_llm = 4096,\n",
    "    llm_model = 'LLAMA',\n",
    "    lr = 0.0001\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bca8803058f444f2b9702dc2c52a92ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  1 — train RMSE 85.300746 - train R2 0.167676 — val RMSE 64.917971 - val R2 0.517924\n",
      "Epoch  2 — train RMSE 63.712840 - train R2 0.535657 — val RMSE 40.864750 - val R2 0.808978\n",
      "Epoch  3 — train RMSE 53.220217 - train R2 0.676004 — val RMSE 25.867819 - val R2 0.923457\n",
      "Epoch  4 — train RMSE 49.406297 - train R2 0.720778 — val RMSE 27.312296 - val R2 0.914670\n",
      "Epoch  5 — train RMSE 49.270164 - train R2 0.722314 — val RMSE 24.470781 - val R2 0.931501\n",
      "Epoch  6 — train RMSE 47.344012 - train R2 0.743602 — val RMSE 20.901695 - val R2 0.950025\n",
      "Epoch  7 — train RMSE 48.642689 - train R2 0.729342 — val RMSE 19.733115 - val R2 0.955457\n",
      "Epoch  8 — train RMSE 47.616392 - train R2 0.740642 — val RMSE 17.387376 - val R2 0.965418\n",
      "Epoch  9 — train RMSE 48.983973 - train R2 0.725531 — val RMSE 19.662605 - val R2 0.955775\n",
      "Epoch 10 — train RMSE 48.265114 - train R2 0.733528 — val RMSE 20.209031 - val R2 0.953283\n",
      "Epoch 11 — train RMSE 46.243009 - train R2 0.755388 — val RMSE 17.210161 - val R2 0.966119\n",
      "Epoch 12 — train RMSE 46.287541 - train R2 0.754917 — val RMSE 14.077420 - val R2 0.977331\n",
      "Epoch 13 — train RMSE 45.863401 - train R2 0.759387 — val RMSE 18.057173 - val R2 0.962702\n",
      "Epoch 14 — train RMSE 46.018020 - train R2 0.757762 — val RMSE 18.607138 - val R2 0.960395\n",
      "Epoch 15 — train RMSE 43.743306 - train R2 0.781119 — val RMSE 17.669187 - val R2 0.964288\n",
      "Epoch 16 — train RMSE 47.666319 - train R2 0.740098 — val RMSE 15.262138 - val R2 0.973355\n",
      "Epoch 17 — train RMSE 42.142292 - train R2 0.796848 — val RMSE 17.817686 - val R2 0.963685\n",
      "Epoch 18 — train RMSE 46.099156 - train R2 0.756908 — val RMSE 17.294117 - val R2 0.965788\n",
      "Epoch 19 — train RMSE 44.279819 - train R2 0.775717 — val RMSE 15.440732 - val R2 0.972728\n",
      "Epoch 20 — train RMSE 43.917784 - train R2 0.779369 — val RMSE 21.109214 - val R2 0.949028\n",
      "Epoch 21 — train RMSE 45.249114 - train R2 0.765790 — val RMSE 16.550547 - val R2 0.968666\n",
      "Epoch 22 — train RMSE 43.664089 - train R2 0.781911 — val RMSE 19.463328 - val R2 0.956667\n",
      "Epoch 23 — train RMSE 44.866673 - train R2 0.769732 — val RMSE 19.731637 - val R2 0.955464\n",
      "Epoch 24 — train RMSE 44.757363 - train R2 0.770853 — val RMSE 17.475382 - val R2 0.965067\n",
      "Epoch 25 — train RMSE 44.650561 - train R2 0.771945 — val RMSE 14.733488 - val R2 0.975169\n",
      "Epoch 26 — train RMSE 43.541665 - train R2 0.783132 — val RMSE 19.533525 - val R2 0.956354\n",
      "Epoch 27 — train RMSE 43.079830 - train R2 0.787708 — val RMSE 19.065679 - val R2 0.958419\n",
      "Epoch 28 — train RMSE 44.073158 - train R2 0.777805 — val RMSE 14.839706 - val R2 0.974810\n",
      "Epoch 29 — train RMSE 46.208228 - train R2 0.755756 — val RMSE 14.916435 - val R2 0.974548\n",
      "Epoch 30 — train RMSE 42.423238 - train R2 0.794130 — val RMSE 19.825537 - val R2 0.955039\n",
      "Epoch 31 — train RMSE 44.616086 - train R2 0.772297 — val RMSE 14.156397 - val R2 0.977076\n",
      "Epoch 32 — train RMSE 41.814982 - train R2 0.799991 — val RMSE 15.999200 - val R2 0.970719\n",
      "Epoch 33 — train RMSE 44.641279 - train R2 0.772040 — val RMSE 16.051419 - val R2 0.970528\n",
      "Epoch 34 — train RMSE 43.037807 - train R2 0.788122 — val RMSE 16.215741 - val R2 0.969921\n",
      "Epoch 35 — train RMSE 42.111841 - train R2 0.797141 — val RMSE 16.609139 - val R2 0.968444\n",
      "Epoch 36 — train RMSE 44.600597 - train R2 0.772456 — val RMSE 14.111695 - val R2 0.977220\n",
      "Epoch 37 — train RMSE 43.419983 - train R2 0.784342 — val RMSE 16.453687 - val R2 0.969032\n",
      "Epoch 38 — train RMSE 44.132099 - train R2 0.777211 — val RMSE 11.733519 - val R2 0.984251\n",
      "Epoch 39 — train RMSE 41.738985 - train R2 0.800717 — val RMSE 16.280884 - val R2 0.969679\n",
      "Epoch 40 — train RMSE 43.891024 - train R2 0.779638 — val RMSE 15.725622 - val R2 0.971712\n",
      "Epoch 41 — train RMSE 43.396889 - train R2 0.784572 — val RMSE 14.897573 - val R2 0.974613\n",
      "Epoch 42 — train RMSE 42.485028 - train R2 0.793530 — val RMSE 14.185980 - val R2 0.976980\n",
      "Epoch 43 — train RMSE 42.577276 - train R2 0.792633 — val RMSE 16.589466 - val R2 0.968519\n",
      "Epoch 44 — train RMSE 44.272827 - train R2 0.775788 — val RMSE 17.272642 - val R2 0.965873\n",
      "Epoch 45 — train RMSE 41.434473 - train R2 0.803615 — val RMSE 14.115024 - val R2 0.977210\n",
      "Epoch 46 — train RMSE 42.929718 - train R2 0.789185 — val RMSE 15.385851 - val R2 0.972921\n",
      "Epoch 47 — train RMSE 44.940509 - train R2 0.768974 — val RMSE 16.023582 - val R2 0.970630\n",
      "Epoch 48 — train RMSE 42.564796 - train R2 0.792754 — val RMSE 18.045843 - val R2 0.962749\n",
      "Epoch 49 — train RMSE 43.696413 - train R2 0.781587 — val RMSE 14.348136 - val R2 0.976451\n",
      "Epoch 50 — train RMSE 43.684092 - train R2 0.781711 — val RMSE 13.099423 - val R2 0.980371\n",
      "Epoch 51 — train RMSE 43.666089 - train R2 0.781891 — val RMSE 13.646583 - val R2 0.978697\n",
      "Epoch 52 — train RMSE 44.869996 - train R2 0.769698 — val RMSE 12.737046 - val R2 0.981442\n",
      "Epoch 53 — train RMSE 44.277737 - train R2 0.775737 — val RMSE 14.072463 - val R2 0.977347\n",
      "Epoch 54 — train RMSE 42.574473 - train R2 0.792660 — val RMSE 15.170023 - val R2 0.973676\n",
      "Epoch 55 — train RMSE 42.636230 - train R2 0.792057 — val RMSE 15.255350 - val R2 0.973379\n",
      "Epoch 56 — train RMSE 42.529972 - train R2 0.793092 — val RMSE 13.597537 - val R2 0.978850\n",
      "Epoch 57 — train RMSE 43.682092 - train R2 0.781731 — val RMSE 15.528087 - val R2 0.972418\n",
      "Epoch 58 — train RMSE 41.585950 - train R2 0.802176 — val RMSE 15.156683 - val R2 0.973722\n",
      "Epoch 59 — train RMSE 45.245216 - train R2 0.765830 — val RMSE 16.810500 - val R2 0.967674\n",
      "Epoch 60 — train RMSE 45.656599 - train R2 0.761552 — val RMSE 15.400573 - val R2 0.972869\n",
      "Epoch 61 — train RMSE 44.829594 - train R2 0.770113 — val RMSE 14.160165 - val R2 0.977064\n",
      "Epoch 62 — train RMSE 44.845706 - train R2 0.769947 — val RMSE 13.978701 - val R2 0.977648\n",
      "Epoch 63 — train RMSE 44.523196 - train R2 0.773245 — val RMSE 12.942946 - val R2 0.980838\n",
      "Epoch 64 — train RMSE 44.235363 - train R2 0.776167 — val RMSE 14.455256 - val R2 0.976098\n",
      "Epoch 65 — train RMSE 42.843364 - train R2 0.790032 — val RMSE 16.139180 - val R2 0.970205\n",
      "Epoch 66 — train RMSE 45.199511 - train R2 0.766303 — val RMSE 15.364480 - val R2 0.972996\n",
      "Epoch 67 — train RMSE 44.382808 - train R2 0.774672 — val RMSE 14.586752 - val R2 0.975661\n",
      "Epoch 68 — train RMSE 40.897057 - train R2 0.808676 — val RMSE 17.254650 - val R2 0.965944\n",
      "Epoch 69 — train RMSE 44.768410 - train R2 0.770739 — val RMSE 14.641939 - val R2 0.975477\n",
      "Epoch 70 — train RMSE 43.516011 - train R2 0.783387 — val RMSE 14.325972 - val R2 0.976523\n",
      "Epoch 71 — train RMSE 44.795604 - train R2 0.770461 — val RMSE 14.718643 - val R2 0.975219\n",
      "Epoch 72 — train RMSE 43.233781 - train R2 0.786188 — val RMSE 13.623585 - val R2 0.978769\n",
      "Epoch 73 — train RMSE 44.332961 - train R2 0.775178 — val RMSE 12.881882 - val R2 0.981018\n",
      "Epoch 74 — train RMSE 44.373115 - train R2 0.774771 — val RMSE 14.886205 - val R2 0.974651\n",
      "Epoch 75 — train RMSE 45.426602 - train R2 0.763949 — val RMSE 13.889185 - val R2 0.977933\n",
      "Epoch 76 — train RMSE 42.801132 - train R2 0.790446 — val RMSE 15.779437 - val R2 0.971518\n",
      "Epoch 77 — train RMSE 44.851146 - train R2 0.769892 — val RMSE 11.641928 - val R2 0.984496\n",
      "Epoch 78 — train RMSE 42.794208 - train R2 0.790513 — val RMSE 15.328597 - val R2 0.973122\n",
      "Epoch 79 — train RMSE 40.045109 - train R2 0.816564 — val RMSE 15.390508 - val R2 0.972905\n",
      "Epoch 80 — train RMSE 44.750144 - train R2 0.770927 — val RMSE 13.639710 - val R2 0.978719\n",
      "Epoch 81 — train RMSE 42.370261 - train R2 0.794644 — val RMSE 15.341063 - val R2 0.973079\n",
      "Epoch 82 — train RMSE 41.679032 - train R2 0.801289 — val RMSE 13.588332 - val R2 0.978879\n",
      "Epoch 83 — train RMSE 42.369694 - train R2 0.794649 — val RMSE 12.418804 - val R2 0.982358\n",
      "Epoch 84 — train RMSE 43.704590 - train R2 0.781506 — val RMSE 13.404531 - val R2 0.979446\n",
      "Epoch 85 — train RMSE 44.406177 - train R2 0.774435 — val RMSE 14.459622 - val R2 0.976083\n",
      "Epoch 86 — train RMSE 42.097058 - train R2 0.797283 — val RMSE 14.496312 - val R2 0.975962\n",
      "Epoch 87 — train RMSE 44.331207 - train R2 0.775196 — val RMSE 12.831681 - val R2 0.981166\n",
      "Epoch 88 — train RMSE 39.602940 - train R2 0.820593 — val RMSE 14.351965 - val R2 0.976438\n",
      "Epoch 89 — train RMSE 45.238055 - train R2 0.765904 — val RMSE 12.166626 - val R2 0.983067\n",
      "Epoch 90 — train RMSE 43.863916 - train R2 0.779910 — val RMSE 15.074769 - val R2 0.974005\n",
      "Epoch 91 — train RMSE 42.137008 - train R2 0.796898 — val RMSE 14.988099 - val R2 0.974303\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m train, test \u001b[38;5;241m=\u001b[39m model_selection\u001b[38;5;241m.\u001b[39mtrain_test_split(lactate, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, stratify\u001b[38;5;241m=\u001b[39m lactate[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLactate\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m      3\u001b[0m train, validation \u001b[38;5;241m=\u001b[39m model_selection\u001b[38;5;241m.\u001b[39mtrain_test_split(train, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, stratify\u001b[38;5;241m=\u001b[39m train[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLactate\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m----> 5\u001b[0m model ,train_rmse, train_r2, val_rmse, val_r2, stand  \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfigs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[9], line 61\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(configs, train, validation, epochs, batch_size, updates)\u001b[0m\n\u001b[1;32m     59\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     60\u001b[0m opt\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m---> 61\u001b[0m sum_mse \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m*\u001b[39mx_enc\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     62\u001b[0m \u001b[38;5;66;03m#r2_metric.update(y_pred.squeeze(), y_true.squeeze())\u001b[39;00m\n\u001b[1;32m     63\u001b[0m r2_metric\u001b[38;5;241m.\u001b[39mupdate(y_pred\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m), y_true\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train, test = model_selection.train_test_split(lactate, test_size=0.1, random_state=1, stratify= lactate['Lactate'])\n",
    "\n",
    "train, validation = model_selection.train_test_split(train, test_size=0.1, random_state=1, stratify= train['Lactate'])\n",
    "\n",
    "model ,train_rmse, train_r2, val_rmse, val_r2, stand  = train_model(configs, train, validation, 100, 8, True, False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Urea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAggAAAGwCAYAAADMjZ3mAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy88F64QAAAACXBIWXMAAA9hAAAPYQGoP6dpAAASlElEQVR4nO3df2zUd/3A8dcxyrWyFjc3aCvI0GUukYUFRIXoXFCZVXA/otnmPywmLtOxhGxZotEETEy2mDn/2YyJmU6N+aJRIIuCBMIPWcgSRECcRpENQaEhTkcLo+VH398/dCfda2xIgM8VHo+kyfU+n7u+j1c/3DOfu+ZqpZQSAACnGFX1AgCA5iMQAIBEIAAAiUAAABKBAAAkAgEASAQCAJCMPtsbDg0Nxf79+6O9vT1qtdq5XBMAcJ6UUqK/vz+6u7tj1KjTnyc460DYv39/TJo06WxvDgBUaN++fTFx4sTTbj/rQGhvb2/8gI6OjrO9GwDgAurr64tJkyY1nsdP56wD4dWXFTo6OgQCAIwwb/b2AG9SBAASgQAAJAIBAEgEAgCQCAQAIBEIAEAiEACARCAAAIlAAAASgQAAJAIBAEgEAgCQCAQAIBEIAEAiEACARCAAAIlAAAASgQAAJAIBAEgEAgCQCAQAIBEIAEAiEACARCAAAIlAAAASgQAAJAIBAEgEAgCQCAQAIBEIAEAiEACARCAAAMnoqhcAVSmlxMDAQNXLqFQpJQYHByMiol6vR61Wq3hFwKlaW1srOy4FApesgYGB6OnpqXoZAKe1atWqaGtrq+Rne4kBAEicQYCIOHzj3VFGXYKHw8nj0b5jaURE9E+7K+KylooXBNSGTsTl2/+v6mUIBIiIf8fBpf7keFmLfwNoAqXqBfyHlxgAgEQgAACJQAAAEoEAACQCAQBIBAIAkAgEACARCABAIhAAgEQgAACJQAAAEoEAACQCAQBIBAIAkAgEACARCABAIhAAgEQgAACJQAAAEoEAACQCAQBIBAIAkAgEACARCABAIhAAgEQgAACJQAAAEoEAACQCAQBIBAIAkAgEACARCABAIhAAgEQgAACJQAAAEoEAACQCAQBIBAIAkAgEACARCABAIhAAgEQgAACJQAAAEoEAACQCAQBIBAIAkAgEACARCABAIhAAgEQgAACJQAAAEoEAACQCAQBIBAIAkAgEACARCABAIhAAgEQgAACJQAAAEoEAACQCAQBIBAIAkAgEACARCABAIhAAgEQgAACJQAAAEoEAACQCAQBIRle9gFOVUmJgYCAiIlpbW6NWq1W8IgC4wEo55WJ5gx3Pr6Y6gzAwMBA9PT3R09PTCAUAuKQMnWhcHBwcrGwZTRUIAEBzEAgAQCIQAIBEIAAAiUAAABKBAAAkAgEASAQCAJAIBAAgEQgAQCIQAIBEIAAAiUAAABKBAAAkAgEASAQCAJAIBAAgEQgAQCIQAIBEIAAAiUAAABKBAAAkAgEASAQCAJAIBAAgEQgAQCIQAIBEIAAAiUAAABKBAAAkAgEASAQCAJAIBAAgEQgAQCIQAIBEIAAAiUAAABKBAAAkAgEASAQCAJAIBAAgEQgAQCIQAIBEIAAAiUAAABKBAAAkAgEASAQCAJAIBAAgEQgAQCIQAIBEIAAAiUAAABKBAAAkAgEASAQCAJAIBAAgEQgAQCIQAIBEIAAAiUAAABKBAAAkAgEASAQCAJAIBAAgEQgAQCIQAIBEIAAAiUAAABKBAAAkAgEASAQCAJCMrnoBpyqlNC4PDAxUuBIuBcN+x0753QPgfwiEwcHBGBwcbHzf19d3zhdz6v3ffvvt5/z+4bSGTkTEmKpXAdA0zvglhkceeSTGjRvX+Jo0adL5XBcAUKEzPoPw5S9/OR588MHG9319fec8Eur1euPy8uXLo7W19ZzeP5xqYGDgv2eqRjXVq20AlTvj/xXr9fqwJ/DzoVarNS63trZGW1vbef150HDK7x4A/ooBAHgdAgEASAQCAJAIBAAgEQgAQCIQAIBEIAAAiUAAABKBAAAkAgEASAQCAJAIBAAgEQgAQCIQAIBEIAAAiUAAABKBAAAkAgEASAQCAJAIBAAgEQgAQCIQAIBEIAAAiUAAABKBAAAkAgEASAQCAJAIBAAgEQgAQCIQAIBEIAAAiUAAABKBAAAkAgEASAQCAJAIBAAgEQgAQCIQAIBEIAAAiUAAABKBAAAkAgEASAQCAJAIBAAgEQgAQCIQAIBEIAAAiUAAABKBAAAkAgEASAQCAJAIBAAgEQgAQCIQAIBEIAAAiUAAABKBAAAkAgEASAQCAJAIBAAgEQgAQCIQAIBEIAAAiUAAABKBAAAkAgEASAQCAJAIBAAgEQgAQCIQAIBEIAAAiUAAAJLRVS/gVK2trbFq1arGZQC45Iz671NzvV6vbBlNFQi1Wi3a2tqqXgYAVKdWO+Vi7Q12PL+8xAAAJAIBAEgEAgCQCAQAIBEIAEAiEACARCAAAIlAAAASgQAAJAIBAEgEAgCQCAQAIBEIAEAiEACARCAAAIlAAAASgQAAJAIBAEgEAgCQCAQAIBEIAEAiEACARCAAAIlAAAASgQAAJAIBAEgEAgCQCAQAIBEIAEAiEACARCAAAIlAAAASgQAAJAIBAEgEAgCQCAQAIBEIAEAiEACARCAAAIlAAAASgQAAJAIBAEgEAgCQCAQAIBEIAEAiEACARCAAAIlAAAASgQAAJAIBAEgEAgCQCAQAIBEIAEAiEACARCAAAIlAAAASgQAAJAIBAEgEAgCQCAQAIBEIAEAiEACARCAAAIlAAAASgQAAJAIBAEgEAgCQCAQAIBEIAEAyuuoFQDOoDZ2IUvUiqnDy+OtfBipTGzpR9RIiQiBARERcvv3/ql5C5dp3LK16CUAT8RIDAJA4g8Alq7W1NVatWlX1MipVSonBwcGIiKjX61Gr1SpeEXCq1tbWyn62QOCSVavVoq2treplVO4tb3lL1UsAmpCXGACARCAAAIlAAAASgQAAJAIBAEgEAgCQCAQAIBEIAEAiEACARCAAAIlAAAASgQAAJAIBAEgEAgCQCAQAIBEIAEAiEACARCAAAIlAAAASgQAAJAIBAEgEAgCQCAQAIBEIAEAiEACARCAAAIlAAAASgQAAJAIBAEgEAgCQCAQAIBEIAEAiEACAZPTZ3rCUEhERfX1952wxAMD59erz9qvP46dz1oHQ398fERGTJk0627sAACrS398f48aNO+32WnmzhDiNoaGh2L9/f7S3t0etVjvrBb5WX19fTJo0Kfbt2xcdHR3n7H45v8xt5DGzkcncRp5mm1kpJfr7+6O7uztGjTr9Ow3O+gzCqFGjYuLEiWd78zfV0dHRFP+Q/G/MbeQxs5HJ3EaeZprZG505eJU3KQIAiUAAAJKmC4R6vR6LFy+Oer1e9VL4H5jbyGNmI5O5jTwjdWZn/SZFAODi1XRnEACA6gkEACARCABAIhAAgKTpAuHb3/52TJkyJVpbW2PGjBmxadOmqpfEfyxZsiRqtdqwr87Ozsb2UkosWbIkuru7o62tLW6++eZ4/vnnK1zxpenXv/51zJ8/P7q7u6NWq8WKFSuGbT+TOQ0ODsYDDzwQV111VYwdOzY+9alPxd/+9rcL+CguLW82s3vuuScdex/4wAeG7WNmF9YjjzwSM2fOjPb29hg/fnzcdttt8ac//WnYPiP9WGuqQPjJT34SixYtiq985Suxbdu2+NCHPhQ9PT2xd+/eqpfGf7znPe+JAwcONL527tzZ2PaNb3wjHn/88XjiiSdiy5Yt0dnZGR/72Mcan9vBhXHkyJGYNm1aPPHEE6+7/UzmtGjRoli+fHksXbo0nn322Th8+HDMmzcvTp48eaEexiXlzWYWEfHxj3982LG3cuXKYdvN7MLauHFj3H///fHcc8/FmjVr4sSJEzF37tw4cuRIY58Rf6yVJvK+972v3HfffcOuu/7668uXvvSlilbEqRYvXlymTZv2utuGhoZKZ2dnefTRRxvXDQwMlHHjxpXvfOc7F2iFvFZElOXLlze+P5M5vfzyy6WlpaUsXbq0sc/f//73MmrUqPKrX/3qgq39UvXamZVSyoIFC8qtt9562tuYWfUOHjxYIqJs3LixlHJxHGtNcwbh2LFjsXXr1pg7d+6w6+fOnRubN2+uaFW81q5du6K7uzumTJkSd911V7zwwgsREfHiiy9Gb2/vsPnV6/X48Ic/bH5N5EzmtHXr1jh+/Piwfbq7u2Pq1KlmWaENGzbE+PHj47rrrovPf/7zcfDgwcY2M6veoUOHIiLiyiuvjIiL41hrmkD4xz/+ESdPnowJEyYMu37ChAnR29tb0ao41fvf//744Q9/GKtXr47vfve70dvbG7Nnz46XXnqpMSPza25nMqfe3t4YM2ZMXHHFFafdhwurp6cnfvzjH8e6devim9/8ZmzZsiXmzJkTg4ODEWFmVSulxIMPPhgf/OAHY+rUqRFxcRxrZ/1pjufLaz86upRyTj9OmrPX09PTuHzDDTfErFmz4l3velf84Ac/aLxhyvxGhrOZk1lW584772xcnjp1arz3ve+NyZMnxy9/+cu44447Tns7M7swFi5cGL/73e/i2WefTdtG8rHWNGcQrrrqqrjssstSNR08eDAVGM1h7NixccMNN8SuXbsaf81gfs3tTObU2dkZx44di3/961+n3YdqdXV1xeTJk2PXrl0RYWZVeuCBB+KZZ56J9evXx8SJExvXXwzHWtMEwpgxY2LGjBmxZs2aYdevWbMmZs+eXdGqeCODg4Pxxz/+Mbq6umLKlCnR2dk5bH7Hjh2LjRs3ml8TOZM5zZgxI1paWobtc+DAgfj9739vlk3ipZdein379kVXV1dEmFkVSimxcOHCWLZsWaxbty6mTJkybPtFcaxV9vbI17F06dLS0tJSnnrqqfKHP/yhLFq0qIwdO7bs2bOn6qVRSnnooYfKhg0bygsvvFCee+65Mm/evNLe3t6Yz6OPPlrGjRtXli1bVnbu3Fnuvvvu0tXVVfr6+ipe+aWlv7+/bNu2rWzbtq1ERHn88cfLtm3byl//+tdSypnN6b777isTJ04sa9euLb/97W/LnDlzyrRp08qJEyeqelgXtTeaWX9/f3nooYfK5s2by4svvljWr19fZs2aVd7+9rebWYW+8IUvlHHjxpUNGzaUAwcONL5eeeWVxj4j/VhrqkAopZQnn3yyTJ48uYwZM6ZMnz698ScjVO/OO+8sXV1dpaWlpXR3d5c77rijPP/8843tQ0NDZfHixaWzs7PU6/Vy0003lZ07d1a44kvT+vXrS0SkrwULFpRSzmxOR48eLQsXLixXXnllaWtrK/PmzSt79+6t4NFcGt5oZq+88kqZO3duufrqq0tLS0t5xzveURYsWJDmYWYX1uvNKyLK97///cY+I/1Y83HPAEDSNO9BAACah0AAABKBAAAkAgEASAQCAJAIBAAgEQgAQCIQAIBEIAAAiUCAi9jNN98cixYtStevWLGiKT5OFmheAgEY5vjx41UvAWgCAgEucUuWLIkbb7wxvve978U73/nOqNfrUUqJQ4cOxb333hvjx4+Pjo6OmDNnTuzYsaNxu927d8ett94aEyZMiMsvvzxmzpwZa9eurfCRAOeSQADiL3/5S/z0pz+Nn//857F9+/aIiPjkJz8Zvb29sXLlyti6dWtMnz49PvKRj8Q///nPiIg4fPhwfOITn4i1a9fGtm3b4pZbbon58+fH3r17K3wkwLkyuuoFANU7duxY/OhHP4qrr746IiLWrVsXO3fujIMHD0a9Xo+IiMceeyxWrFgRP/vZz+Lee++NadOmxbRp0xr38fWvfz2WL18ezzzzTCxcuLCSxwGcOwIBiMmTJzfiICJi69atcfjw4Xjb2942bL+jR4/G7t27IyLiyJEj8bWvfS1+8YtfxP79++PEiRNx9OhRZxDgIiEQ4CLW0dERhw4dSte//PLL0dHR0fh+7Nixw7YPDQ1FV1dXbNiwId32rW99a0REPPzww7F69ep47LHH4tprr422trb49Kc/HceOHTunjwGohkCAi9j1118fq1atStdv2bIl3v3ud5/2dtOnT4/e3t4YPXp0XHPNNa+7z6ZNm+Kee+6J22+/PSL+/Z6EPXv2nItlA03AmxThIvbFL34xdu/eHffff3/s2LEj/vznP8eTTz4ZTz31VDz88MOnvd1HP/rRmDVrVtx2222xevXq2LNnT2zevDm++tWvxm9+85uIiLj22mtj2bJlsX379tixY0d89rOfjaGhoQv10IDzTCDAReyaa66JTZs2xe7du2Pu3Lkxc+bMePrpp+Ppp5+Oz3zmM6e9Xa1Wi5UrV8ZNN90Un/vc5+K6666Lu+66K/bs2RMTJkyIiIhvfetbccUVV8Ts2bNj/vz5ccstt8T06dMv1EMDzrNaKaVUvQgAoLk4gwAAJAIBAEgEAgCQCAQAIBEIAEAiEACARCAAAIlAAAASgQAAJAIBAEgEAgCQ/D++g+mAUqYfLAAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.hist(urea['Urea'])\n",
    "plt.title(f'Distribution')\n",
    "plt.show()\n",
    "\n",
    "import seaborn as sns\n",
    "plt.figure()\n",
    "sns.boxplot(data=urea, x='Urea')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "101.0    2760\n",
       "206.0    2760\n",
       "71.0     2660\n",
       "0.0       920\n",
       "Name: Urea, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "urea['Urea'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "configs = Details(\n",
    "    pred_len = 1,      \n",
    "    seq_len  = 1151,\n",
    "    d_ff = 32,\n",
    "    patch_len= 16, \n",
    "    stride = 8, \n",
    "    llm_layers = 4,\n",
    "    description = \"This dataset contains near-infrared (NIR) spectroscopy measurements of urea concentration in a \" \\\n",
    "                  \"phosphate-buffered saline (PBS) solution. Each spectrum includes absorbance values across \" \\\n",
    "                  \"different wavelengths. The goal is to predict the concentration given the different wavelengths \" ,\n",
    "    dropout = 0.1,\n",
    "    n_heads = 2,\n",
    "    d_model = 16,\n",
    "    enc_in = 1,\n",
    "    d_llm = 4096,\n",
    "    llm_model = 'LLAMA',\n",
    "    lr = 0.001\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dfc00fadd3e24f97b066db9091a5c4f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  1 — train RMSE 72.941657 - train R2 -0.189498 — val RMSE 53.437408 - val R2 0.361307\n",
      "Epoch  2 — train RMSE 65.123902 - train R2 0.051817 — val RMSE 56.262258 - val R2 0.291996\n",
      "Epoch  3 — train RMSE 64.347899 - train R2 0.074277 — val RMSE 49.787524 - val R2 0.445576\n",
      "Epoch  4 — train RMSE 61.960860 - train R2 0.141685 — val RMSE 48.773918 - val R2 0.467921\n",
      "Epoch  5 — train RMSE 62.039235 - train R2 0.139512 — val RMSE 52.156091 - val R2 0.391569\n",
      "Epoch  6 — train RMSE 63.312308 - train R2 0.103837 — val RMSE 46.564806 - val R2 0.515028\n",
      "Epoch  7 — train RMSE 60.310069 - train R2 0.186813 — val RMSE 51.834915 - val R2 0.399040\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m train, test \u001b[38;5;241m=\u001b[39m model_selection\u001b[38;5;241m.\u001b[39mtrain_test_split(urea, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, stratify\u001b[38;5;241m=\u001b[39m urea[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUrea\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m      3\u001b[0m train, validation \u001b[38;5;241m=\u001b[39m model_selection\u001b[38;5;241m.\u001b[39mtrain_test_split(train, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, stratify\u001b[38;5;241m=\u001b[39m train[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUrea\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m----> 5\u001b[0m model ,train_rmse, train_r2, val_rmse, val_r2, stand  \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfigs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[9], line 61\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(configs, train, validation, epochs, batch_size, updates)\u001b[0m\n\u001b[1;32m     59\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     60\u001b[0m opt\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m---> 61\u001b[0m sum_mse \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m*\u001b[39mx_enc\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     62\u001b[0m \u001b[38;5;66;03m#r2_metric.update(y_pred.squeeze(), y_true.squeeze())\u001b[39;00m\n\u001b[1;32m     63\u001b[0m r2_metric\u001b[38;5;241m.\u001b[39mupdate(y_pred\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m), y_true\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train, test = model_selection.train_test_split(urea, test_size=0.1, random_state=1, stratify= urea['Urea'])\n",
    "\n",
    "train, validation = model_selection.train_test_split(train, test_size=0.1, random_state=1, stratify= train['Urea'])\n",
    "\n",
    "model ,train_rmse, train_r2, val_rmse, val_r2, stand  = train_model(configs, train, validation, 20, 8, True, False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Glucose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.hist(glucose['Glucose'])\n",
    "plt.title(f'Distribution')\n",
    "plt.show()\n",
    "\n",
    "import seaborn as sns\n",
    "plt.figure()\n",
    "sns.boxplot(data=glucose, x='Glucose')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "147.0    2760\n",
       "72.0     2660\n",
       "300.0    2660\n",
       "0.0       920\n",
       "71.0      100\n",
       "Name: Glucose, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glucose['Glucose'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "configs = Details(\n",
    "    pred_len = 1,      \n",
    "    seq_len  = 1151,\n",
    "    d_ff = 32,\n",
    "    patch_len= 16, \n",
    "    stride = 8, \n",
    "    llm_layers = 4,\n",
    "    description = \"This dataset contains near-infrared (NIR) spectroscopy measurements of glucose concentration in a \" \\\n",
    "                  \"phosphate-buffered saline (PBS) solution. Each spectrum includes absorbance values across \" \\\n",
    "                  \"different wavelengths. The goal is to predict the concentration given the different wavelengths \" ,\n",
    "    dropout = 0.1,\n",
    "    n_heads = 2,\n",
    "    d_model = 16,\n",
    "    enc_in = 1,\n",
    "    d_llm = 4096,\n",
    "    llm_model = 'LLAMA',\n",
    "    lr = 0.001\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = model_selection.train_test_split(glucose, test_size=0.1, random_state=1, stratify= glucose['Glucose'])\n",
    "\n",
    "train, validation = model_selection.train_test_split(train, test_size=0.1, random_state=1, stratify= train['Glucose'])\n",
    "\n",
    "model ,train_rmse, train_r2, val_rmse, val_r2, stand  = train_model(configs, train, validation, 50, 8, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "WindowLength = 41\n",
    "X_deriv = savgol_filter(X, window_length = WindowLength, polyorder = 3, deriv = 2, mode='nearest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(X_deriv[0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second derivative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1350.0</th>\n",
       "      <th>1351.0</th>\n",
       "      <th>1352.0</th>\n",
       "      <th>1353.0</th>\n",
       "      <th>1354.0</th>\n",
       "      <th>1355.0</th>\n",
       "      <th>1356.0</th>\n",
       "      <th>1357.0</th>\n",
       "      <th>1358.0</th>\n",
       "      <th>1359.0</th>\n",
       "      <th>...</th>\n",
       "      <th>2491.0</th>\n",
       "      <th>2492.0</th>\n",
       "      <th>2493.0</th>\n",
       "      <th>2494.0</th>\n",
       "      <th>2495.0</th>\n",
       "      <th>2496.0</th>\n",
       "      <th>2497.0</th>\n",
       "      <th>2498.0</th>\n",
       "      <th>2499.0</th>\n",
       "      <th>2500.0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000084</td>\n",
       "      <td>0.000099</td>\n",
       "      <td>0.000117</td>\n",
       "      <td>0.000139</td>\n",
       "      <td>0.000165</td>\n",
       "      <td>0.000194</td>\n",
       "      <td>0.000227</td>\n",
       "      <td>0.000261</td>\n",
       "      <td>0.000297</td>\n",
       "      <td>0.000333</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000494</td>\n",
       "      <td>-0.000493</td>\n",
       "      <td>-0.000485</td>\n",
       "      <td>-0.000472</td>\n",
       "      <td>-0.000455</td>\n",
       "      <td>-0.000434</td>\n",
       "      <td>-0.000412</td>\n",
       "      <td>-0.000388</td>\n",
       "      <td>-0.000364</td>\n",
       "      <td>-0.000339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000085</td>\n",
       "      <td>0.000099</td>\n",
       "      <td>0.000118</td>\n",
       "      <td>0.000140</td>\n",
       "      <td>0.000165</td>\n",
       "      <td>0.000195</td>\n",
       "      <td>0.000227</td>\n",
       "      <td>0.000261</td>\n",
       "      <td>0.000297</td>\n",
       "      <td>0.000333</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000519</td>\n",
       "      <td>-0.000527</td>\n",
       "      <td>-0.000528</td>\n",
       "      <td>-0.000523</td>\n",
       "      <td>-0.000512</td>\n",
       "      <td>-0.000495</td>\n",
       "      <td>-0.000474</td>\n",
       "      <td>-0.000447</td>\n",
       "      <td>-0.000416</td>\n",
       "      <td>-0.000381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000084</td>\n",
       "      <td>0.000099</td>\n",
       "      <td>0.000117</td>\n",
       "      <td>0.000139</td>\n",
       "      <td>0.000165</td>\n",
       "      <td>0.000194</td>\n",
       "      <td>0.000227</td>\n",
       "      <td>0.000261</td>\n",
       "      <td>0.000297</td>\n",
       "      <td>0.000333</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000582</td>\n",
       "      <td>-0.000603</td>\n",
       "      <td>-0.000613</td>\n",
       "      <td>-0.000614</td>\n",
       "      <td>-0.000608</td>\n",
       "      <td>-0.000596</td>\n",
       "      <td>-0.000578</td>\n",
       "      <td>-0.000556</td>\n",
       "      <td>-0.000531</td>\n",
       "      <td>-0.000502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000084</td>\n",
       "      <td>0.000099</td>\n",
       "      <td>0.000118</td>\n",
       "      <td>0.000140</td>\n",
       "      <td>0.000165</td>\n",
       "      <td>0.000195</td>\n",
       "      <td>0.000227</td>\n",
       "      <td>0.000262</td>\n",
       "      <td>0.000297</td>\n",
       "      <td>0.000333</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000498</td>\n",
       "      <td>-0.000498</td>\n",
       "      <td>-0.000490</td>\n",
       "      <td>-0.000475</td>\n",
       "      <td>-0.000455</td>\n",
       "      <td>-0.000433</td>\n",
       "      <td>-0.000409</td>\n",
       "      <td>-0.000385</td>\n",
       "      <td>-0.000362</td>\n",
       "      <td>-0.000340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000084</td>\n",
       "      <td>0.000099</td>\n",
       "      <td>0.000118</td>\n",
       "      <td>0.000140</td>\n",
       "      <td>0.000165</td>\n",
       "      <td>0.000195</td>\n",
       "      <td>0.000227</td>\n",
       "      <td>0.000262</td>\n",
       "      <td>0.000297</td>\n",
       "      <td>0.000333</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000529</td>\n",
       "      <td>-0.000513</td>\n",
       "      <td>-0.000489</td>\n",
       "      <td>-0.000459</td>\n",
       "      <td>-0.000424</td>\n",
       "      <td>-0.000388</td>\n",
       "      <td>-0.000351</td>\n",
       "      <td>-0.000315</td>\n",
       "      <td>-0.000281</td>\n",
       "      <td>-0.000249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9095</th>\n",
       "      <td>0.000080</td>\n",
       "      <td>0.000091</td>\n",
       "      <td>0.000104</td>\n",
       "      <td>0.000121</td>\n",
       "      <td>0.000141</td>\n",
       "      <td>0.000164</td>\n",
       "      <td>0.000191</td>\n",
       "      <td>0.000220</td>\n",
       "      <td>0.000250</td>\n",
       "      <td>0.000281</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000308</td>\n",
       "      <td>-0.000381</td>\n",
       "      <td>-0.000447</td>\n",
       "      <td>-0.000506</td>\n",
       "      <td>-0.000556</td>\n",
       "      <td>-0.000598</td>\n",
       "      <td>-0.000630</td>\n",
       "      <td>-0.000652</td>\n",
       "      <td>-0.000664</td>\n",
       "      <td>-0.000665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9096</th>\n",
       "      <td>0.000080</td>\n",
       "      <td>0.000091</td>\n",
       "      <td>0.000104</td>\n",
       "      <td>0.000121</td>\n",
       "      <td>0.000141</td>\n",
       "      <td>0.000164</td>\n",
       "      <td>0.000191</td>\n",
       "      <td>0.000220</td>\n",
       "      <td>0.000250</td>\n",
       "      <td>0.000281</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000765</td>\n",
       "      <td>-0.000775</td>\n",
       "      <td>-0.000771</td>\n",
       "      <td>-0.000754</td>\n",
       "      <td>-0.000726</td>\n",
       "      <td>-0.000687</td>\n",
       "      <td>-0.000639</td>\n",
       "      <td>-0.000583</td>\n",
       "      <td>-0.000520</td>\n",
       "      <td>-0.000451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9097</th>\n",
       "      <td>0.000080</td>\n",
       "      <td>0.000091</td>\n",
       "      <td>0.000104</td>\n",
       "      <td>0.000121</td>\n",
       "      <td>0.000141</td>\n",
       "      <td>0.000164</td>\n",
       "      <td>0.000191</td>\n",
       "      <td>0.000220</td>\n",
       "      <td>0.000250</td>\n",
       "      <td>0.000282</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000534</td>\n",
       "      <td>-0.000577</td>\n",
       "      <td>-0.000608</td>\n",
       "      <td>-0.000629</td>\n",
       "      <td>-0.000640</td>\n",
       "      <td>-0.000639</td>\n",
       "      <td>-0.000626</td>\n",
       "      <td>-0.000603</td>\n",
       "      <td>-0.000569</td>\n",
       "      <td>-0.000525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9098</th>\n",
       "      <td>0.000080</td>\n",
       "      <td>0.000091</td>\n",
       "      <td>0.000104</td>\n",
       "      <td>0.000121</td>\n",
       "      <td>0.000141</td>\n",
       "      <td>0.000164</td>\n",
       "      <td>0.000191</td>\n",
       "      <td>0.000220</td>\n",
       "      <td>0.000250</td>\n",
       "      <td>0.000281</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000779</td>\n",
       "      <td>-0.000798</td>\n",
       "      <td>-0.000803</td>\n",
       "      <td>-0.000794</td>\n",
       "      <td>-0.000772</td>\n",
       "      <td>-0.000740</td>\n",
       "      <td>-0.000697</td>\n",
       "      <td>-0.000645</td>\n",
       "      <td>-0.000584</td>\n",
       "      <td>-0.000517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9099</th>\n",
       "      <td>0.000080</td>\n",
       "      <td>0.000091</td>\n",
       "      <td>0.000104</td>\n",
       "      <td>0.000121</td>\n",
       "      <td>0.000141</td>\n",
       "      <td>0.000164</td>\n",
       "      <td>0.000191</td>\n",
       "      <td>0.000220</td>\n",
       "      <td>0.000250</td>\n",
       "      <td>0.000282</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000896</td>\n",
       "      <td>-0.000934</td>\n",
       "      <td>-0.000954</td>\n",
       "      <td>-0.000957</td>\n",
       "      <td>-0.000944</td>\n",
       "      <td>-0.000915</td>\n",
       "      <td>-0.000871</td>\n",
       "      <td>-0.000811</td>\n",
       "      <td>-0.000738</td>\n",
       "      <td>-0.000652</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9100 rows × 1151 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        1350.0    1351.0    1352.0    1353.0    1354.0    1355.0    1356.0  \\\n",
       "0     0.000084  0.000099  0.000117  0.000139  0.000165  0.000194  0.000227   \n",
       "1     0.000085  0.000099  0.000118  0.000140  0.000165  0.000195  0.000227   \n",
       "2     0.000084  0.000099  0.000117  0.000139  0.000165  0.000194  0.000227   \n",
       "3     0.000084  0.000099  0.000118  0.000140  0.000165  0.000195  0.000227   \n",
       "4     0.000084  0.000099  0.000118  0.000140  0.000165  0.000195  0.000227   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "9095  0.000080  0.000091  0.000104  0.000121  0.000141  0.000164  0.000191   \n",
       "9096  0.000080  0.000091  0.000104  0.000121  0.000141  0.000164  0.000191   \n",
       "9097  0.000080  0.000091  0.000104  0.000121  0.000141  0.000164  0.000191   \n",
       "9098  0.000080  0.000091  0.000104  0.000121  0.000141  0.000164  0.000191   \n",
       "9099  0.000080  0.000091  0.000104  0.000121  0.000141  0.000164  0.000191   \n",
       "\n",
       "        1357.0    1358.0    1359.0  ...    2491.0    2492.0    2493.0  \\\n",
       "0     0.000261  0.000297  0.000333  ... -0.000494 -0.000493 -0.000485   \n",
       "1     0.000261  0.000297  0.000333  ... -0.000519 -0.000527 -0.000528   \n",
       "2     0.000261  0.000297  0.000333  ... -0.000582 -0.000603 -0.000613   \n",
       "3     0.000262  0.000297  0.000333  ... -0.000498 -0.000498 -0.000490   \n",
       "4     0.000262  0.000297  0.000333  ... -0.000529 -0.000513 -0.000489   \n",
       "...        ...       ...       ...  ...       ...       ...       ...   \n",
       "9095  0.000220  0.000250  0.000281  ... -0.000308 -0.000381 -0.000447   \n",
       "9096  0.000220  0.000250  0.000281  ... -0.000765 -0.000775 -0.000771   \n",
       "9097  0.000220  0.000250  0.000282  ... -0.000534 -0.000577 -0.000608   \n",
       "9098  0.000220  0.000250  0.000281  ... -0.000779 -0.000798 -0.000803   \n",
       "9099  0.000220  0.000250  0.000282  ... -0.000896 -0.000934 -0.000954   \n",
       "\n",
       "        2494.0    2495.0    2496.0    2497.0    2498.0    2499.0    2500.0  \n",
       "0    -0.000472 -0.000455 -0.000434 -0.000412 -0.000388 -0.000364 -0.000339  \n",
       "1    -0.000523 -0.000512 -0.000495 -0.000474 -0.000447 -0.000416 -0.000381  \n",
       "2    -0.000614 -0.000608 -0.000596 -0.000578 -0.000556 -0.000531 -0.000502  \n",
       "3    -0.000475 -0.000455 -0.000433 -0.000409 -0.000385 -0.000362 -0.000340  \n",
       "4    -0.000459 -0.000424 -0.000388 -0.000351 -0.000315 -0.000281 -0.000249  \n",
       "...        ...       ...       ...       ...       ...       ...       ...  \n",
       "9095 -0.000506 -0.000556 -0.000598 -0.000630 -0.000652 -0.000664 -0.000665  \n",
       "9096 -0.000754 -0.000726 -0.000687 -0.000639 -0.000583 -0.000520 -0.000451  \n",
       "9097 -0.000629 -0.000640 -0.000639 -0.000626 -0.000603 -0.000569 -0.000525  \n",
       "9098 -0.000794 -0.000772 -0.000740 -0.000697 -0.000645 -0.000584 -0.000517  \n",
       "9099 -0.000957 -0.000944 -0.000915 -0.000871 -0.000811 -0.000738 -0.000652  \n",
       "\n",
       "[9100 rows x 1151 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_preprocessed1 = pd.DataFrame(data=X_deriv, index=X.index, columns=X.columns)\n",
    "X_preprocessed1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "lactate_p = pd.concat([l, X_preprocessed1], axis=1)\n",
    "\n",
    "urea_p = pd.concat([u, X_preprocessed1], axis=1)\n",
    "\n",
    "glucose_p = pd.concat([g, X_preprocessed1], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lactate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9100, 1152)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lactate_p.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa3ea603879846699de5b48b74ca3000",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  1 — train RMSE 92.403943 - train R2 0.023287 — val RMSE 43.817308 - val R2 0.780377\n",
      "Epoch  2 — train RMSE 51.736340 - train R2 0.693820 — val RMSE 29.563951 - val R2 0.900021\n",
      "Epoch  3 — train RMSE 46.735283 - train R2 0.750152 — val RMSE 22.119994 - val R2 0.944030\n",
      "Epoch  4 — train RMSE 46.900197 - train R2 0.748386 — val RMSE 17.000335 - val R2 0.966940\n",
      "Epoch  5 — train RMSE 44.260419 - train R2 0.775913 — val RMSE 19.698291 - val R2 0.955614\n",
      "Epoch  6 — train RMSE 41.902873 - train R2 0.799150 — val RMSE 15.636673 - val R2 0.972031\n",
      "Epoch  7 — train RMSE 44.237851 - train R2 0.776142 — val RMSE 17.106323 - val R2 0.966527\n",
      "Epoch  8 — train RMSE 44.301986 - train R2 0.775492 — val RMSE 18.770082 - val R2 0.959699\n",
      "Epoch  9 — train RMSE 46.260110 - train R2 0.755207 — val RMSE 14.423835 - val R2 0.976202\n",
      "Epoch 10 — train RMSE 42.110844 - train R2 0.797151 — val RMSE 15.755967 - val R2 0.971603\n",
      "Epoch 11 — train RMSE 45.453845 - train R2 0.763666 — val RMSE 14.065297 - val R2 0.977370\n",
      "Epoch 12 — train RMSE 46.592485 - train R2 0.751677 — val RMSE 14.035938 - val R2 0.977464\n",
      "Epoch 13 — train RMSE 46.332519 - train R2 0.754440 — val RMSE 17.487636 - val R2 0.965018\n",
      "Epoch 14 — train RMSE 44.531286 - train R2 0.773162 — val RMSE 15.574513 - val R2 0.972253\n",
      "Epoch 15 — train RMSE 43.226334 - train R2 0.786261 — val RMSE 16.828787 - val R2 0.967604\n",
      "Epoch 16 — train RMSE 43.110332 - train R2 0.787407 — val RMSE 15.877959 - val R2 0.971161\n",
      "Epoch 17 — train RMSE 39.726982 - train R2 0.819467 — val RMSE 16.147520 - val R2 0.970174\n",
      "Epoch 18 — train RMSE 46.374022 - train R2 0.754000 — val RMSE 14.218948 - val R2 0.976873\n",
      "Epoch 19 — train RMSE 44.159840 - train R2 0.776930 — val RMSE 13.416386 - val R2 0.979410\n",
      "Epoch 20 — train RMSE 42.570524 - train R2 0.792698 — val RMSE 13.021954 - val R2 0.980603\n",
      "Epoch 21 — train RMSE 43.793326 - train R2 0.780618 — val RMSE 16.915975 - val R2 0.967267\n",
      "Epoch 22 — train RMSE 44.687034 - train R2 0.771572 — val RMSE 15.572773 - val R2 0.972259\n",
      "Epoch 23 — train RMSE 44.440831 - train R2 0.774082 — val RMSE 13.742608 - val R2 0.978396\n",
      "Epoch 24 — train RMSE 43.798439 - train R2 0.780566 — val RMSE 14.135966 - val R2 0.977142\n",
      "Epoch 25 — train RMSE 43.545581 - train R2 0.783093 — val RMSE 12.687657 - val R2 0.981586\n",
      "Epoch 26 — train RMSE 44.809549 - train R2 0.770318 — val RMSE 14.599308 - val R2 0.975619\n",
      "Epoch 27 — train RMSE 42.534252 - train R2 0.793051 — val RMSE 13.926231 - val R2 0.977815\n",
      "Epoch 28 — train RMSE 41.039876 - train R2 0.807337 — val RMSE 15.436919 - val R2 0.972741\n",
      "Epoch 29 — train RMSE 43.268347 - train R2 0.785846 — val RMSE 14.080885 - val R2 0.977320\n",
      "Epoch 30 — train RMSE 43.609962 - train R2 0.782451 — val RMSE 14.220535 - val R2 0.976868\n",
      "Epoch 31 — train RMSE 44.554413 - train R2 0.772926 — val RMSE 11.836265 - val R2 0.983974\n",
      "Epoch 32 — train RMSE 45.889605 - train R2 0.759112 — val RMSE 13.330024 - val R2 0.979674\n",
      "Epoch 33 — train RMSE 44.584211 - train R2 0.772622 — val RMSE 10.173813 - val R2 0.988160\n",
      "Epoch 34 — train RMSE 44.319045 - train R2 0.775319 — val RMSE 13.540541 - val R2 0.979027\n",
      "Epoch 35 — train RMSE 45.237636 - train R2 0.765909 — val RMSE 13.921312 - val R2 0.977831\n",
      "Epoch 36 — train RMSE 42.326645 - train R2 0.795066 — val RMSE 14.815540 - val R2 0.974891\n",
      "Epoch 37 — train RMSE 43.602498 - train R2 0.782526 — val RMSE 12.491648 - val R2 0.982151\n",
      "Epoch 38 — train RMSE 44.726915 - train R2 0.771164 — val RMSE 13.552354 - val R2 0.978991\n",
      "Epoch 39 — train RMSE 42.781419 - train R2 0.790639 — val RMSE 15.301695 - val R2 0.973217\n",
      "Epoch 40 — train RMSE 44.177025 - train R2 0.776757 — val RMSE 14.396092 - val R2 0.976293\n",
      "Epoch 41 — train RMSE 43.009101 - train R2 0.788404 — val RMSE 14.924794 - val R2 0.974520\n",
      "Epoch 42 — train RMSE 45.511582 - train R2 0.763065 — val RMSE 13.828250 - val R2 0.978126\n",
      "Epoch 43 — train RMSE 47.462815 - train R2 0.742313 — val RMSE 14.121568 - val R2 0.977189\n",
      "Early stopping triggered; Best val RMSE : 10.173813\n"
     ]
    }
   ],
   "source": [
    "configs = Details(\n",
    "    pred_len = 1,      \n",
    "    seq_len  = 1151,\n",
    "    d_ff = 32,\n",
    "    patch_len= 16, \n",
    "    stride = 8, \n",
    "    llm_layers = 4,\n",
    "    description = \"This dataset contains near-infrared (NIR) spectroscopy measurements of lactate concentration in a \" \\\n",
    "                  \"phosphate-buffered saline (PBS) solution. Each spectrum includes absorbance values across \" \\\n",
    "                  \"different wavelengths. The goal is to predict the concentration given the different wavelengths \" ,\n",
    "    dropout = 0.1,\n",
    "    n_heads = 2,\n",
    "    d_model = 16,\n",
    "    enc_in = 1,\n",
    "    d_llm = 4096,\n",
    "    llm_model = 'LLAMA',\n",
    "    lr = 0.0001\n",
    ")\n",
    "\n",
    "train, test = model_selection.train_test_split(lactate_p, test_size=0.60, random_state=1, stratify= lactate_p['Lactate'])\n",
    "\n",
    "train, validation = model_selection.train_test_split(train, test_size=0.1, random_state=1, stratify= train['Lactate'])\n",
    "\n",
    "model ,train_rmse, train_r2, val_rmse, val_r2, stand  = train_model(configs, train, validation, 50, 8, True, True)\n",
    "\n",
    "#180 min"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second derivative, cropped and standarized data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "U_filter = np.ones(np.shape(X_deriv))\n",
    "\n",
    "''' Applying the filter to the two wavelength regions\n",
    "# Wavelength 1350 - 2499 nm - indices 0 - 1150\n",
    "# Regions of wavelength being zeroed. \n",
    "## R1 - 1380-1480 nm - 30:130\n",
    "## R2 - 1850 - 2050 nm - 500:700\n",
    "'''\n",
    "U_filter[:,30:150] = 0\n",
    "U_filter[:,500:700] = 0\n",
    "\n",
    "#Apply filter\n",
    "X_filtered = X_deriv*U_filter\n",
    "\n",
    "#print(X_test*U_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(threshold=np.inf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(X_filtered[0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1350.0</th>\n",
       "      <th>1351.0</th>\n",
       "      <th>1352.0</th>\n",
       "      <th>1353.0</th>\n",
       "      <th>1354.0</th>\n",
       "      <th>1355.0</th>\n",
       "      <th>1356.0</th>\n",
       "      <th>1357.0</th>\n",
       "      <th>1358.0</th>\n",
       "      <th>1359.0</th>\n",
       "      <th>1360.0</th>\n",
       "      <th>1361.0</th>\n",
       "      <th>1362.0</th>\n",
       "      <th>1363.0</th>\n",
       "      <th>1364.0</th>\n",
       "      <th>1365.0</th>\n",
       "      <th>1366.0</th>\n",
       "      <th>1367.0</th>\n",
       "      <th>1368.0</th>\n",
       "      <th>1369.0</th>\n",
       "      <th>1370.0</th>\n",
       "      <th>1371.0</th>\n",
       "      <th>1372.0</th>\n",
       "      <th>1373.0</th>\n",
       "      <th>1374.0</th>\n",
       "      <th>1375.0</th>\n",
       "      <th>1376.0</th>\n",
       "      <th>1377.0</th>\n",
       "      <th>1378.0</th>\n",
       "      <th>1379.0</th>\n",
       "      <th>1380.0</th>\n",
       "      <th>1381.0</th>\n",
       "      <th>1382.0</th>\n",
       "      <th>1383.0</th>\n",
       "      <th>1384.0</th>\n",
       "      <th>1385.0</th>\n",
       "      <th>1386.0</th>\n",
       "      <th>1387.0</th>\n",
       "      <th>1388.0</th>\n",
       "      <th>1389.0</th>\n",
       "      <th>1390.0</th>\n",
       "      <th>1391.0</th>\n",
       "      <th>1392.0</th>\n",
       "      <th>1393.0</th>\n",
       "      <th>1394.0</th>\n",
       "      <th>1395.0</th>\n",
       "      <th>1396.0</th>\n",
       "      <th>1397.0</th>\n",
       "      <th>1398.0</th>\n",
       "      <th>1399.0</th>\n",
       "      <th>1400.0</th>\n",
       "      <th>1401.0</th>\n",
       "      <th>1402.0</th>\n",
       "      <th>1403.0</th>\n",
       "      <th>1404.0</th>\n",
       "      <th>1405.0</th>\n",
       "      <th>1406.0</th>\n",
       "      <th>1407.0</th>\n",
       "      <th>1408.0</th>\n",
       "      <th>1409.0</th>\n",
       "      <th>1410.0</th>\n",
       "      <th>1411.0</th>\n",
       "      <th>1412.0</th>\n",
       "      <th>1413.0</th>\n",
       "      <th>1414.0</th>\n",
       "      <th>1415.0</th>\n",
       "      <th>1416.0</th>\n",
       "      <th>1417.0</th>\n",
       "      <th>1418.0</th>\n",
       "      <th>1419.0</th>\n",
       "      <th>1420.0</th>\n",
       "      <th>1421.0</th>\n",
       "      <th>1422.0</th>\n",
       "      <th>1423.0</th>\n",
       "      <th>1424.0</th>\n",
       "      <th>1425.0</th>\n",
       "      <th>1426.0</th>\n",
       "      <th>1427.0</th>\n",
       "      <th>1428.0</th>\n",
       "      <th>1429.0</th>\n",
       "      <th>1430.0</th>\n",
       "      <th>1431.0</th>\n",
       "      <th>1432.0</th>\n",
       "      <th>1433.0</th>\n",
       "      <th>1434.0</th>\n",
       "      <th>1435.0</th>\n",
       "      <th>1436.0</th>\n",
       "      <th>1437.0</th>\n",
       "      <th>1438.0</th>\n",
       "      <th>1439.0</th>\n",
       "      <th>1440.0</th>\n",
       "      <th>1441.0</th>\n",
       "      <th>1442.0</th>\n",
       "      <th>1443.0</th>\n",
       "      <th>1444.0</th>\n",
       "      <th>1445.0</th>\n",
       "      <th>1446.0</th>\n",
       "      <th>1447.0</th>\n",
       "      <th>1448.0</th>\n",
       "      <th>1449.0</th>\n",
       "      <th>1450.0</th>\n",
       "      <th>1451.0</th>\n",
       "      <th>1452.0</th>\n",
       "      <th>1453.0</th>\n",
       "      <th>1454.0</th>\n",
       "      <th>1455.0</th>\n",
       "      <th>1456.0</th>\n",
       "      <th>1457.0</th>\n",
       "      <th>1458.0</th>\n",
       "      <th>1459.0</th>\n",
       "      <th>1460.0</th>\n",
       "      <th>1461.0</th>\n",
       "      <th>1462.0</th>\n",
       "      <th>1463.0</th>\n",
       "      <th>1464.0</th>\n",
       "      <th>1465.0</th>\n",
       "      <th>1466.0</th>\n",
       "      <th>1467.0</th>\n",
       "      <th>1468.0</th>\n",
       "      <th>1469.0</th>\n",
       "      <th>1470.0</th>\n",
       "      <th>1471.0</th>\n",
       "      <th>1472.0</th>\n",
       "      <th>1473.0</th>\n",
       "      <th>1474.0</th>\n",
       "      <th>1475.0</th>\n",
       "      <th>1476.0</th>\n",
       "      <th>1477.0</th>\n",
       "      <th>1478.0</th>\n",
       "      <th>1479.0</th>\n",
       "      <th>1480.0</th>\n",
       "      <th>1481.0</th>\n",
       "      <th>1482.0</th>\n",
       "      <th>1483.0</th>\n",
       "      <th>1484.0</th>\n",
       "      <th>1485.0</th>\n",
       "      <th>1486.0</th>\n",
       "      <th>1487.0</th>\n",
       "      <th>1488.0</th>\n",
       "      <th>1489.0</th>\n",
       "      <th>1490.0</th>\n",
       "      <th>1491.0</th>\n",
       "      <th>1492.0</th>\n",
       "      <th>1493.0</th>\n",
       "      <th>1494.0</th>\n",
       "      <th>1495.0</th>\n",
       "      <th>1496.0</th>\n",
       "      <th>1497.0</th>\n",
       "      <th>1498.0</th>\n",
       "      <th>1499.0</th>\n",
       "      <th>1500.0</th>\n",
       "      <th>1501.0</th>\n",
       "      <th>1502.0</th>\n",
       "      <th>1503.0</th>\n",
       "      <th>1504.0</th>\n",
       "      <th>1505.0</th>\n",
       "      <th>1506.0</th>\n",
       "      <th>1507.0</th>\n",
       "      <th>1508.0</th>\n",
       "      <th>1509.0</th>\n",
       "      <th>1510.0</th>\n",
       "      <th>1511.0</th>\n",
       "      <th>1512.0</th>\n",
       "      <th>1513.0</th>\n",
       "      <th>1514.0</th>\n",
       "      <th>1515.0</th>\n",
       "      <th>1516.0</th>\n",
       "      <th>1517.0</th>\n",
       "      <th>1518.0</th>\n",
       "      <th>1519.0</th>\n",
       "      <th>1520.0</th>\n",
       "      <th>1521.0</th>\n",
       "      <th>1522.0</th>\n",
       "      <th>1523.0</th>\n",
       "      <th>1524.0</th>\n",
       "      <th>1525.0</th>\n",
       "      <th>1526.0</th>\n",
       "      <th>1527.0</th>\n",
       "      <th>1528.0</th>\n",
       "      <th>1529.0</th>\n",
       "      <th>1530.0</th>\n",
       "      <th>1531.0</th>\n",
       "      <th>1532.0</th>\n",
       "      <th>1533.0</th>\n",
       "      <th>1534.0</th>\n",
       "      <th>1535.0</th>\n",
       "      <th>1536.0</th>\n",
       "      <th>1537.0</th>\n",
       "      <th>1538.0</th>\n",
       "      <th>1539.0</th>\n",
       "      <th>1540.0</th>\n",
       "      <th>1541.0</th>\n",
       "      <th>1542.0</th>\n",
       "      <th>1543.0</th>\n",
       "      <th>1544.0</th>\n",
       "      <th>1545.0</th>\n",
       "      <th>1546.0</th>\n",
       "      <th>1547.0</th>\n",
       "      <th>1548.0</th>\n",
       "      <th>1549.0</th>\n",
       "      <th>1550.0</th>\n",
       "      <th>1551.0</th>\n",
       "      <th>1552.0</th>\n",
       "      <th>1553.0</th>\n",
       "      <th>1554.0</th>\n",
       "      <th>1555.0</th>\n",
       "      <th>1556.0</th>\n",
       "      <th>1557.0</th>\n",
       "      <th>1558.0</th>\n",
       "      <th>1559.0</th>\n",
       "      <th>1560.0</th>\n",
       "      <th>1561.0</th>\n",
       "      <th>1562.0</th>\n",
       "      <th>1563.0</th>\n",
       "      <th>1564.0</th>\n",
       "      <th>1565.0</th>\n",
       "      <th>1566.0</th>\n",
       "      <th>1567.0</th>\n",
       "      <th>1568.0</th>\n",
       "      <th>1569.0</th>\n",
       "      <th>1570.0</th>\n",
       "      <th>1571.0</th>\n",
       "      <th>1572.0</th>\n",
       "      <th>1573.0</th>\n",
       "      <th>1574.0</th>\n",
       "      <th>1575.0</th>\n",
       "      <th>1576.0</th>\n",
       "      <th>1577.0</th>\n",
       "      <th>1578.0</th>\n",
       "      <th>1579.0</th>\n",
       "      <th>1580.0</th>\n",
       "      <th>1581.0</th>\n",
       "      <th>1582.0</th>\n",
       "      <th>1583.0</th>\n",
       "      <th>1584.0</th>\n",
       "      <th>1585.0</th>\n",
       "      <th>1586.0</th>\n",
       "      <th>1587.0</th>\n",
       "      <th>1588.0</th>\n",
       "      <th>1589.0</th>\n",
       "      <th>1590.0</th>\n",
       "      <th>1591.0</th>\n",
       "      <th>1592.0</th>\n",
       "      <th>1593.0</th>\n",
       "      <th>1594.0</th>\n",
       "      <th>1595.0</th>\n",
       "      <th>1596.0</th>\n",
       "      <th>1597.0</th>\n",
       "      <th>1598.0</th>\n",
       "      <th>1599.0</th>\n",
       "      <th>1600.0</th>\n",
       "      <th>1601.0</th>\n",
       "      <th>1602.0</th>\n",
       "      <th>1603.0</th>\n",
       "      <th>1604.0</th>\n",
       "      <th>1605.0</th>\n",
       "      <th>1606.0</th>\n",
       "      <th>1607.0</th>\n",
       "      <th>1608.0</th>\n",
       "      <th>1609.0</th>\n",
       "      <th>1610.0</th>\n",
       "      <th>1611.0</th>\n",
       "      <th>1612.0</th>\n",
       "      <th>1613.0</th>\n",
       "      <th>1614.0</th>\n",
       "      <th>1615.0</th>\n",
       "      <th>1616.0</th>\n",
       "      <th>1617.0</th>\n",
       "      <th>1618.0</th>\n",
       "      <th>1619.0</th>\n",
       "      <th>1620.0</th>\n",
       "      <th>1621.0</th>\n",
       "      <th>1622.0</th>\n",
       "      <th>1623.0</th>\n",
       "      <th>1624.0</th>\n",
       "      <th>1625.0</th>\n",
       "      <th>1626.0</th>\n",
       "      <th>1627.0</th>\n",
       "      <th>1628.0</th>\n",
       "      <th>1629.0</th>\n",
       "      <th>1630.0</th>\n",
       "      <th>1631.0</th>\n",
       "      <th>1632.0</th>\n",
       "      <th>1633.0</th>\n",
       "      <th>1634.0</th>\n",
       "      <th>1635.0</th>\n",
       "      <th>1636.0</th>\n",
       "      <th>1637.0</th>\n",
       "      <th>1638.0</th>\n",
       "      <th>1639.0</th>\n",
       "      <th>1640.0</th>\n",
       "      <th>1641.0</th>\n",
       "      <th>1642.0</th>\n",
       "      <th>1643.0</th>\n",
       "      <th>1644.0</th>\n",
       "      <th>1645.0</th>\n",
       "      <th>1646.0</th>\n",
       "      <th>1647.0</th>\n",
       "      <th>1648.0</th>\n",
       "      <th>1649.0</th>\n",
       "      <th>1650.0</th>\n",
       "      <th>1651.0</th>\n",
       "      <th>1652.0</th>\n",
       "      <th>1653.0</th>\n",
       "      <th>1654.0</th>\n",
       "      <th>1655.0</th>\n",
       "      <th>1656.0</th>\n",
       "      <th>1657.0</th>\n",
       "      <th>1658.0</th>\n",
       "      <th>1659.0</th>\n",
       "      <th>1660.0</th>\n",
       "      <th>1661.0</th>\n",
       "      <th>1662.0</th>\n",
       "      <th>1663.0</th>\n",
       "      <th>1664.0</th>\n",
       "      <th>1665.0</th>\n",
       "      <th>1666.0</th>\n",
       "      <th>1667.0</th>\n",
       "      <th>1668.0</th>\n",
       "      <th>1669.0</th>\n",
       "      <th>1670.0</th>\n",
       "      <th>1671.0</th>\n",
       "      <th>1672.0</th>\n",
       "      <th>1673.0</th>\n",
       "      <th>1674.0</th>\n",
       "      <th>1675.0</th>\n",
       "      <th>1676.0</th>\n",
       "      <th>1677.0</th>\n",
       "      <th>1678.0</th>\n",
       "      <th>1679.0</th>\n",
       "      <th>1680.0</th>\n",
       "      <th>1681.0</th>\n",
       "      <th>1682.0</th>\n",
       "      <th>1683.0</th>\n",
       "      <th>1684.0</th>\n",
       "      <th>1685.0</th>\n",
       "      <th>1686.0</th>\n",
       "      <th>1687.0</th>\n",
       "      <th>1688.0</th>\n",
       "      <th>1689.0</th>\n",
       "      <th>1690.0</th>\n",
       "      <th>1691.0</th>\n",
       "      <th>1692.0</th>\n",
       "      <th>1693.0</th>\n",
       "      <th>1694.0</th>\n",
       "      <th>1695.0</th>\n",
       "      <th>1696.0</th>\n",
       "      <th>1697.0</th>\n",
       "      <th>1698.0</th>\n",
       "      <th>1699.0</th>\n",
       "      <th>1700.0</th>\n",
       "      <th>1701.0</th>\n",
       "      <th>1702.0</th>\n",
       "      <th>1703.0</th>\n",
       "      <th>1704.0</th>\n",
       "      <th>1705.0</th>\n",
       "      <th>1706.0</th>\n",
       "      <th>1707.0</th>\n",
       "      <th>1708.0</th>\n",
       "      <th>1709.0</th>\n",
       "      <th>1710.0</th>\n",
       "      <th>1711.0</th>\n",
       "      <th>1712.0</th>\n",
       "      <th>1713.0</th>\n",
       "      <th>1714.0</th>\n",
       "      <th>1715.0</th>\n",
       "      <th>1716.0</th>\n",
       "      <th>1717.0</th>\n",
       "      <th>1718.0</th>\n",
       "      <th>1719.0</th>\n",
       "      <th>1720.0</th>\n",
       "      <th>1721.0</th>\n",
       "      <th>1722.0</th>\n",
       "      <th>1723.0</th>\n",
       "      <th>1724.0</th>\n",
       "      <th>1725.0</th>\n",
       "      <th>1726.0</th>\n",
       "      <th>1727.0</th>\n",
       "      <th>1728.0</th>\n",
       "      <th>1729.0</th>\n",
       "      <th>1730.0</th>\n",
       "      <th>1731.0</th>\n",
       "      <th>1732.0</th>\n",
       "      <th>1733.0</th>\n",
       "      <th>1734.0</th>\n",
       "      <th>1735.0</th>\n",
       "      <th>1736.0</th>\n",
       "      <th>1737.0</th>\n",
       "      <th>1738.0</th>\n",
       "      <th>1739.0</th>\n",
       "      <th>1740.0</th>\n",
       "      <th>1741.0</th>\n",
       "      <th>1742.0</th>\n",
       "      <th>1743.0</th>\n",
       "      <th>1744.0</th>\n",
       "      <th>1745.0</th>\n",
       "      <th>1746.0</th>\n",
       "      <th>1747.0</th>\n",
       "      <th>1748.0</th>\n",
       "      <th>1749.0</th>\n",
       "      <th>1750.0</th>\n",
       "      <th>1751.0</th>\n",
       "      <th>1752.0</th>\n",
       "      <th>1753.0</th>\n",
       "      <th>1754.0</th>\n",
       "      <th>1755.0</th>\n",
       "      <th>1756.0</th>\n",
       "      <th>1757.0</th>\n",
       "      <th>1758.0</th>\n",
       "      <th>1759.0</th>\n",
       "      <th>1760.0</th>\n",
       "      <th>1761.0</th>\n",
       "      <th>1762.0</th>\n",
       "      <th>1763.0</th>\n",
       "      <th>1764.0</th>\n",
       "      <th>1765.0</th>\n",
       "      <th>1766.0</th>\n",
       "      <th>1767.0</th>\n",
       "      <th>1768.0</th>\n",
       "      <th>1769.0</th>\n",
       "      <th>1770.0</th>\n",
       "      <th>1771.0</th>\n",
       "      <th>1772.0</th>\n",
       "      <th>1773.0</th>\n",
       "      <th>1774.0</th>\n",
       "      <th>1775.0</th>\n",
       "      <th>1776.0</th>\n",
       "      <th>1777.0</th>\n",
       "      <th>1778.0</th>\n",
       "      <th>1779.0</th>\n",
       "      <th>1780.0</th>\n",
       "      <th>1781.0</th>\n",
       "      <th>1782.0</th>\n",
       "      <th>1783.0</th>\n",
       "      <th>1784.0</th>\n",
       "      <th>1785.0</th>\n",
       "      <th>1786.0</th>\n",
       "      <th>1787.0</th>\n",
       "      <th>1788.0</th>\n",
       "      <th>1789.0</th>\n",
       "      <th>1790.0</th>\n",
       "      <th>1791.0</th>\n",
       "      <th>1792.0</th>\n",
       "      <th>1793.0</th>\n",
       "      <th>1794.0</th>\n",
       "      <th>1795.0</th>\n",
       "      <th>1796.0</th>\n",
       "      <th>1797.0</th>\n",
       "      <th>1798.0</th>\n",
       "      <th>1799.0</th>\n",
       "      <th>1800.0</th>\n",
       "      <th>1801.0</th>\n",
       "      <th>1802.0</th>\n",
       "      <th>1803.0</th>\n",
       "      <th>1804.0</th>\n",
       "      <th>1805.0</th>\n",
       "      <th>1806.0</th>\n",
       "      <th>1807.0</th>\n",
       "      <th>1808.0</th>\n",
       "      <th>1809.0</th>\n",
       "      <th>1810.0</th>\n",
       "      <th>1811.0</th>\n",
       "      <th>1812.0</th>\n",
       "      <th>1813.0</th>\n",
       "      <th>1814.0</th>\n",
       "      <th>1815.0</th>\n",
       "      <th>1816.0</th>\n",
       "      <th>1817.0</th>\n",
       "      <th>1818.0</th>\n",
       "      <th>1819.0</th>\n",
       "      <th>1820.0</th>\n",
       "      <th>1821.0</th>\n",
       "      <th>1822.0</th>\n",
       "      <th>1823.0</th>\n",
       "      <th>1824.0</th>\n",
       "      <th>1825.0</th>\n",
       "      <th>1826.0</th>\n",
       "      <th>1827.0</th>\n",
       "      <th>1828.0</th>\n",
       "      <th>1829.0</th>\n",
       "      <th>1830.0</th>\n",
       "      <th>1831.0</th>\n",
       "      <th>1832.0</th>\n",
       "      <th>1833.0</th>\n",
       "      <th>1834.0</th>\n",
       "      <th>1835.0</th>\n",
       "      <th>1836.0</th>\n",
       "      <th>1837.0</th>\n",
       "      <th>1838.0</th>\n",
       "      <th>1839.0</th>\n",
       "      <th>1840.0</th>\n",
       "      <th>1841.0</th>\n",
       "      <th>1842.0</th>\n",
       "      <th>1843.0</th>\n",
       "      <th>1844.0</th>\n",
       "      <th>1845.0</th>\n",
       "      <th>1846.0</th>\n",
       "      <th>1847.0</th>\n",
       "      <th>1848.0</th>\n",
       "      <th>1849.0</th>\n",
       "      <th>1850.0</th>\n",
       "      <th>1851.0</th>\n",
       "      <th>1852.0</th>\n",
       "      <th>1853.0</th>\n",
       "      <th>1854.0</th>\n",
       "      <th>1855.0</th>\n",
       "      <th>1856.0</th>\n",
       "      <th>1857.0</th>\n",
       "      <th>1858.0</th>\n",
       "      <th>1859.0</th>\n",
       "      <th>1860.0</th>\n",
       "      <th>1861.0</th>\n",
       "      <th>1862.0</th>\n",
       "      <th>1863.0</th>\n",
       "      <th>1864.0</th>\n",
       "      <th>1865.0</th>\n",
       "      <th>1866.0</th>\n",
       "      <th>1867.0</th>\n",
       "      <th>1868.0</th>\n",
       "      <th>1869.0</th>\n",
       "      <th>1870.0</th>\n",
       "      <th>1871.0</th>\n",
       "      <th>1872.0</th>\n",
       "      <th>1873.0</th>\n",
       "      <th>1874.0</th>\n",
       "      <th>1875.0</th>\n",
       "      <th>1876.0</th>\n",
       "      <th>1877.0</th>\n",
       "      <th>1878.0</th>\n",
       "      <th>1879.0</th>\n",
       "      <th>1880.0</th>\n",
       "      <th>1881.0</th>\n",
       "      <th>1882.0</th>\n",
       "      <th>1883.0</th>\n",
       "      <th>1884.0</th>\n",
       "      <th>1885.0</th>\n",
       "      <th>1886.0</th>\n",
       "      <th>1887.0</th>\n",
       "      <th>1888.0</th>\n",
       "      <th>1889.0</th>\n",
       "      <th>1890.0</th>\n",
       "      <th>1891.0</th>\n",
       "      <th>1892.0</th>\n",
       "      <th>1893.0</th>\n",
       "      <th>1894.0</th>\n",
       "      <th>1895.0</th>\n",
       "      <th>1896.0</th>\n",
       "      <th>1897.0</th>\n",
       "      <th>1898.0</th>\n",
       "      <th>1899.0</th>\n",
       "      <th>1900.0</th>\n",
       "      <th>1901.0</th>\n",
       "      <th>1902.0</th>\n",
       "      <th>1903.0</th>\n",
       "      <th>1904.0</th>\n",
       "      <th>1905.0</th>\n",
       "      <th>1906.0</th>\n",
       "      <th>1907.0</th>\n",
       "      <th>1908.0</th>\n",
       "      <th>1909.0</th>\n",
       "      <th>1910.0</th>\n",
       "      <th>1911.0</th>\n",
       "      <th>1912.0</th>\n",
       "      <th>1913.0</th>\n",
       "      <th>1914.0</th>\n",
       "      <th>1915.0</th>\n",
       "      <th>1916.0</th>\n",
       "      <th>1917.0</th>\n",
       "      <th>1918.0</th>\n",
       "      <th>1919.0</th>\n",
       "      <th>1920.0</th>\n",
       "      <th>1921.0</th>\n",
       "      <th>1922.0</th>\n",
       "      <th>1923.0</th>\n",
       "      <th>1924.0</th>\n",
       "      <th>1925.0</th>\n",
       "      <th>1926.0</th>\n",
       "      <th>1927.0</th>\n",
       "      <th>1928.0</th>\n",
       "      <th>1929.0</th>\n",
       "      <th>1930.0</th>\n",
       "      <th>1931.0</th>\n",
       "      <th>1932.0</th>\n",
       "      <th>1933.0</th>\n",
       "      <th>1934.0</th>\n",
       "      <th>1935.0</th>\n",
       "      <th>1936.0</th>\n",
       "      <th>1937.0</th>\n",
       "      <th>1938.0</th>\n",
       "      <th>1939.0</th>\n",
       "      <th>1940.0</th>\n",
       "      <th>1941.0</th>\n",
       "      <th>1942.0</th>\n",
       "      <th>1943.0</th>\n",
       "      <th>1944.0</th>\n",
       "      <th>1945.0</th>\n",
       "      <th>1946.0</th>\n",
       "      <th>1947.0</th>\n",
       "      <th>1948.0</th>\n",
       "      <th>1949.0</th>\n",
       "      <th>1950.0</th>\n",
       "      <th>1951.0</th>\n",
       "      <th>1952.0</th>\n",
       "      <th>1953.0</th>\n",
       "      <th>1954.0</th>\n",
       "      <th>1955.0</th>\n",
       "      <th>1956.0</th>\n",
       "      <th>1957.0</th>\n",
       "      <th>1958.0</th>\n",
       "      <th>1959.0</th>\n",
       "      <th>1960.0</th>\n",
       "      <th>1961.0</th>\n",
       "      <th>1962.0</th>\n",
       "      <th>1963.0</th>\n",
       "      <th>1964.0</th>\n",
       "      <th>1965.0</th>\n",
       "      <th>1966.0</th>\n",
       "      <th>1967.0</th>\n",
       "      <th>1968.0</th>\n",
       "      <th>1969.0</th>\n",
       "      <th>1970.0</th>\n",
       "      <th>1971.0</th>\n",
       "      <th>1972.0</th>\n",
       "      <th>1973.0</th>\n",
       "      <th>1974.0</th>\n",
       "      <th>1975.0</th>\n",
       "      <th>1976.0</th>\n",
       "      <th>1977.0</th>\n",
       "      <th>1978.0</th>\n",
       "      <th>1979.0</th>\n",
       "      <th>1980.0</th>\n",
       "      <th>1981.0</th>\n",
       "      <th>1982.0</th>\n",
       "      <th>1983.0</th>\n",
       "      <th>1984.0</th>\n",
       "      <th>1985.0</th>\n",
       "      <th>1986.0</th>\n",
       "      <th>1987.0</th>\n",
       "      <th>1988.0</th>\n",
       "      <th>1989.0</th>\n",
       "      <th>1990.0</th>\n",
       "      <th>1991.0</th>\n",
       "      <th>1992.0</th>\n",
       "      <th>1993.0</th>\n",
       "      <th>1994.0</th>\n",
       "      <th>1995.0</th>\n",
       "      <th>1996.0</th>\n",
       "      <th>1997.0</th>\n",
       "      <th>1998.0</th>\n",
       "      <th>1999.0</th>\n",
       "      <th>2000.0</th>\n",
       "      <th>2001.0</th>\n",
       "      <th>2002.0</th>\n",
       "      <th>2003.0</th>\n",
       "      <th>2004.0</th>\n",
       "      <th>2005.0</th>\n",
       "      <th>2006.0</th>\n",
       "      <th>2007.0</th>\n",
       "      <th>2008.0</th>\n",
       "      <th>2009.0</th>\n",
       "      <th>2010.0</th>\n",
       "      <th>2011.0</th>\n",
       "      <th>2012.0</th>\n",
       "      <th>2013.0</th>\n",
       "      <th>2014.0</th>\n",
       "      <th>2015.0</th>\n",
       "      <th>2016.0</th>\n",
       "      <th>2017.0</th>\n",
       "      <th>2018.0</th>\n",
       "      <th>2019.0</th>\n",
       "      <th>2020.0</th>\n",
       "      <th>2021.0</th>\n",
       "      <th>2022.0</th>\n",
       "      <th>2023.0</th>\n",
       "      <th>2024.0</th>\n",
       "      <th>2025.0</th>\n",
       "      <th>2026.0</th>\n",
       "      <th>2027.0</th>\n",
       "      <th>2028.0</th>\n",
       "      <th>2029.0</th>\n",
       "      <th>2030.0</th>\n",
       "      <th>2031.0</th>\n",
       "      <th>2032.0</th>\n",
       "      <th>2033.0</th>\n",
       "      <th>2034.0</th>\n",
       "      <th>2035.0</th>\n",
       "      <th>2036.0</th>\n",
       "      <th>2037.0</th>\n",
       "      <th>2038.0</th>\n",
       "      <th>2039.0</th>\n",
       "      <th>2040.0</th>\n",
       "      <th>2041.0</th>\n",
       "      <th>2042.0</th>\n",
       "      <th>2043.0</th>\n",
       "      <th>2044.0</th>\n",
       "      <th>2045.0</th>\n",
       "      <th>2046.0</th>\n",
       "      <th>2047.0</th>\n",
       "      <th>2048.0</th>\n",
       "      <th>2049.0</th>\n",
       "      <th>2050.0</th>\n",
       "      <th>2051.0</th>\n",
       "      <th>2052.0</th>\n",
       "      <th>2053.0</th>\n",
       "      <th>2054.0</th>\n",
       "      <th>2055.0</th>\n",
       "      <th>2056.0</th>\n",
       "      <th>2057.0</th>\n",
       "      <th>2058.0</th>\n",
       "      <th>2059.0</th>\n",
       "      <th>2060.0</th>\n",
       "      <th>2061.0</th>\n",
       "      <th>2062.0</th>\n",
       "      <th>2063.0</th>\n",
       "      <th>2064.0</th>\n",
       "      <th>2065.0</th>\n",
       "      <th>2066.0</th>\n",
       "      <th>2067.0</th>\n",
       "      <th>2068.0</th>\n",
       "      <th>2069.0</th>\n",
       "      <th>2070.0</th>\n",
       "      <th>2071.0</th>\n",
       "      <th>2072.0</th>\n",
       "      <th>2073.0</th>\n",
       "      <th>2074.0</th>\n",
       "      <th>2075.0</th>\n",
       "      <th>2076.0</th>\n",
       "      <th>2077.0</th>\n",
       "      <th>2078.0</th>\n",
       "      <th>2079.0</th>\n",
       "      <th>2080.0</th>\n",
       "      <th>2081.0</th>\n",
       "      <th>2082.0</th>\n",
       "      <th>2083.0</th>\n",
       "      <th>2084.0</th>\n",
       "      <th>2085.0</th>\n",
       "      <th>2086.0</th>\n",
       "      <th>2087.0</th>\n",
       "      <th>2088.0</th>\n",
       "      <th>2089.0</th>\n",
       "      <th>2090.0</th>\n",
       "      <th>2091.0</th>\n",
       "      <th>2092.0</th>\n",
       "      <th>2093.0</th>\n",
       "      <th>2094.0</th>\n",
       "      <th>2095.0</th>\n",
       "      <th>2096.0</th>\n",
       "      <th>2097.0</th>\n",
       "      <th>2098.0</th>\n",
       "      <th>2099.0</th>\n",
       "      <th>2100.0</th>\n",
       "      <th>2101.0</th>\n",
       "      <th>2102.0</th>\n",
       "      <th>2103.0</th>\n",
       "      <th>2104.0</th>\n",
       "      <th>2105.0</th>\n",
       "      <th>2106.0</th>\n",
       "      <th>2107.0</th>\n",
       "      <th>2108.0</th>\n",
       "      <th>2109.0</th>\n",
       "      <th>2110.0</th>\n",
       "      <th>2111.0</th>\n",
       "      <th>2112.0</th>\n",
       "      <th>2113.0</th>\n",
       "      <th>2114.0</th>\n",
       "      <th>2115.0</th>\n",
       "      <th>2116.0</th>\n",
       "      <th>2117.0</th>\n",
       "      <th>2118.0</th>\n",
       "      <th>2119.0</th>\n",
       "      <th>2120.0</th>\n",
       "      <th>2121.0</th>\n",
       "      <th>2122.0</th>\n",
       "      <th>2123.0</th>\n",
       "      <th>2124.0</th>\n",
       "      <th>2125.0</th>\n",
       "      <th>2126.0</th>\n",
       "      <th>2127.0</th>\n",
       "      <th>2128.0</th>\n",
       "      <th>2129.0</th>\n",
       "      <th>2130.0</th>\n",
       "      <th>2131.0</th>\n",
       "      <th>2132.0</th>\n",
       "      <th>2133.0</th>\n",
       "      <th>2134.0</th>\n",
       "      <th>2135.0</th>\n",
       "      <th>2136.0</th>\n",
       "      <th>2137.0</th>\n",
       "      <th>2138.0</th>\n",
       "      <th>2139.0</th>\n",
       "      <th>2140.0</th>\n",
       "      <th>2141.0</th>\n",
       "      <th>2142.0</th>\n",
       "      <th>2143.0</th>\n",
       "      <th>2144.0</th>\n",
       "      <th>2145.0</th>\n",
       "      <th>2146.0</th>\n",
       "      <th>2147.0</th>\n",
       "      <th>2148.0</th>\n",
       "      <th>2149.0</th>\n",
       "      <th>2150.0</th>\n",
       "      <th>2151.0</th>\n",
       "      <th>2152.0</th>\n",
       "      <th>2153.0</th>\n",
       "      <th>2154.0</th>\n",
       "      <th>2155.0</th>\n",
       "      <th>2156.0</th>\n",
       "      <th>2157.0</th>\n",
       "      <th>2158.0</th>\n",
       "      <th>2159.0</th>\n",
       "      <th>2160.0</th>\n",
       "      <th>2161.0</th>\n",
       "      <th>2162.0</th>\n",
       "      <th>2163.0</th>\n",
       "      <th>2164.0</th>\n",
       "      <th>2165.0</th>\n",
       "      <th>2166.0</th>\n",
       "      <th>2167.0</th>\n",
       "      <th>2168.0</th>\n",
       "      <th>2169.0</th>\n",
       "      <th>2170.0</th>\n",
       "      <th>2171.0</th>\n",
       "      <th>2172.0</th>\n",
       "      <th>2173.0</th>\n",
       "      <th>2174.0</th>\n",
       "      <th>2175.0</th>\n",
       "      <th>2176.0</th>\n",
       "      <th>2177.0</th>\n",
       "      <th>2178.0</th>\n",
       "      <th>2179.0</th>\n",
       "      <th>2180.0</th>\n",
       "      <th>2181.0</th>\n",
       "      <th>2182.0</th>\n",
       "      <th>2183.0</th>\n",
       "      <th>2184.0</th>\n",
       "      <th>2185.0</th>\n",
       "      <th>2186.0</th>\n",
       "      <th>2187.0</th>\n",
       "      <th>2188.0</th>\n",
       "      <th>2189.0</th>\n",
       "      <th>2190.0</th>\n",
       "      <th>2191.0</th>\n",
       "      <th>2192.0</th>\n",
       "      <th>2193.0</th>\n",
       "      <th>2194.0</th>\n",
       "      <th>2195.0</th>\n",
       "      <th>2196.0</th>\n",
       "      <th>2197.0</th>\n",
       "      <th>2198.0</th>\n",
       "      <th>2199.0</th>\n",
       "      <th>2200.0</th>\n",
       "      <th>2201.0</th>\n",
       "      <th>2202.0</th>\n",
       "      <th>2203.0</th>\n",
       "      <th>2204.0</th>\n",
       "      <th>2205.0</th>\n",
       "      <th>2206.0</th>\n",
       "      <th>2207.0</th>\n",
       "      <th>2208.0</th>\n",
       "      <th>2209.0</th>\n",
       "      <th>2210.0</th>\n",
       "      <th>2211.0</th>\n",
       "      <th>2212.0</th>\n",
       "      <th>2213.0</th>\n",
       "      <th>2214.0</th>\n",
       "      <th>2215.0</th>\n",
       "      <th>2216.0</th>\n",
       "      <th>2217.0</th>\n",
       "      <th>2218.0</th>\n",
       "      <th>2219.0</th>\n",
       "      <th>2220.0</th>\n",
       "      <th>2221.0</th>\n",
       "      <th>2222.0</th>\n",
       "      <th>2223.0</th>\n",
       "      <th>2224.0</th>\n",
       "      <th>2225.0</th>\n",
       "      <th>2226.0</th>\n",
       "      <th>2227.0</th>\n",
       "      <th>2228.0</th>\n",
       "      <th>2229.0</th>\n",
       "      <th>2230.0</th>\n",
       "      <th>2231.0</th>\n",
       "      <th>2232.0</th>\n",
       "      <th>2233.0</th>\n",
       "      <th>2234.0</th>\n",
       "      <th>2235.0</th>\n",
       "      <th>2236.0</th>\n",
       "      <th>2237.0</th>\n",
       "      <th>2238.0</th>\n",
       "      <th>2239.0</th>\n",
       "      <th>2240.0</th>\n",
       "      <th>2241.0</th>\n",
       "      <th>2242.0</th>\n",
       "      <th>2243.0</th>\n",
       "      <th>2244.0</th>\n",
       "      <th>2245.0</th>\n",
       "      <th>2246.0</th>\n",
       "      <th>2247.0</th>\n",
       "      <th>2248.0</th>\n",
       "      <th>2249.0</th>\n",
       "      <th>2250.0</th>\n",
       "      <th>2251.0</th>\n",
       "      <th>2252.0</th>\n",
       "      <th>2253.0</th>\n",
       "      <th>2254.0</th>\n",
       "      <th>2255.0</th>\n",
       "      <th>2256.0</th>\n",
       "      <th>2257.0</th>\n",
       "      <th>2258.0</th>\n",
       "      <th>2259.0</th>\n",
       "      <th>2260.0</th>\n",
       "      <th>2261.0</th>\n",
       "      <th>2262.0</th>\n",
       "      <th>2263.0</th>\n",
       "      <th>2264.0</th>\n",
       "      <th>2265.0</th>\n",
       "      <th>2266.0</th>\n",
       "      <th>2267.0</th>\n",
       "      <th>2268.0</th>\n",
       "      <th>2269.0</th>\n",
       "      <th>2270.0</th>\n",
       "      <th>2271.0</th>\n",
       "      <th>2272.0</th>\n",
       "      <th>2273.0</th>\n",
       "      <th>2274.0</th>\n",
       "      <th>2275.0</th>\n",
       "      <th>2276.0</th>\n",
       "      <th>2277.0</th>\n",
       "      <th>2278.0</th>\n",
       "      <th>2279.0</th>\n",
       "      <th>2280.0</th>\n",
       "      <th>2281.0</th>\n",
       "      <th>2282.0</th>\n",
       "      <th>2283.0</th>\n",
       "      <th>2284.0</th>\n",
       "      <th>2285.0</th>\n",
       "      <th>2286.0</th>\n",
       "      <th>2287.0</th>\n",
       "      <th>2288.0</th>\n",
       "      <th>2289.0</th>\n",
       "      <th>2290.0</th>\n",
       "      <th>2291.0</th>\n",
       "      <th>2292.0</th>\n",
       "      <th>2293.0</th>\n",
       "      <th>2294.0</th>\n",
       "      <th>2295.0</th>\n",
       "      <th>2296.0</th>\n",
       "      <th>2297.0</th>\n",
       "      <th>2298.0</th>\n",
       "      <th>2299.0</th>\n",
       "      <th>2300.0</th>\n",
       "      <th>2301.0</th>\n",
       "      <th>2302.0</th>\n",
       "      <th>2303.0</th>\n",
       "      <th>2304.0</th>\n",
       "      <th>2305.0</th>\n",
       "      <th>2306.0</th>\n",
       "      <th>2307.0</th>\n",
       "      <th>2308.0</th>\n",
       "      <th>2309.0</th>\n",
       "      <th>2310.0</th>\n",
       "      <th>2311.0</th>\n",
       "      <th>2312.0</th>\n",
       "      <th>2313.0</th>\n",
       "      <th>2314.0</th>\n",
       "      <th>2315.0</th>\n",
       "      <th>2316.0</th>\n",
       "      <th>2317.0</th>\n",
       "      <th>2318.0</th>\n",
       "      <th>2319.0</th>\n",
       "      <th>2320.0</th>\n",
       "      <th>2321.0</th>\n",
       "      <th>2322.0</th>\n",
       "      <th>2323.0</th>\n",
       "      <th>2324.0</th>\n",
       "      <th>2325.0</th>\n",
       "      <th>2326.0</th>\n",
       "      <th>2327.0</th>\n",
       "      <th>2328.0</th>\n",
       "      <th>2329.0</th>\n",
       "      <th>2330.0</th>\n",
       "      <th>2331.0</th>\n",
       "      <th>2332.0</th>\n",
       "      <th>2333.0</th>\n",
       "      <th>2334.0</th>\n",
       "      <th>2335.0</th>\n",
       "      <th>2336.0</th>\n",
       "      <th>2337.0</th>\n",
       "      <th>2338.0</th>\n",
       "      <th>2339.0</th>\n",
       "      <th>2340.0</th>\n",
       "      <th>2341.0</th>\n",
       "      <th>2342.0</th>\n",
       "      <th>2343.0</th>\n",
       "      <th>2344.0</th>\n",
       "      <th>2345.0</th>\n",
       "      <th>2346.0</th>\n",
       "      <th>2347.0</th>\n",
       "      <th>2348.0</th>\n",
       "      <th>2349.0</th>\n",
       "      <th>2350.0</th>\n",
       "      <th>2351.0</th>\n",
       "      <th>2352.0</th>\n",
       "      <th>2353.0</th>\n",
       "      <th>2354.0</th>\n",
       "      <th>2355.0</th>\n",
       "      <th>2356.0</th>\n",
       "      <th>2357.0</th>\n",
       "      <th>2358.0</th>\n",
       "      <th>2359.0</th>\n",
       "      <th>2360.0</th>\n",
       "      <th>2361.0</th>\n",
       "      <th>2362.0</th>\n",
       "      <th>2363.0</th>\n",
       "      <th>2364.0</th>\n",
       "      <th>2365.0</th>\n",
       "      <th>2366.0</th>\n",
       "      <th>2367.0</th>\n",
       "      <th>2368.0</th>\n",
       "      <th>2369.0</th>\n",
       "      <th>2370.0</th>\n",
       "      <th>2371.0</th>\n",
       "      <th>2372.0</th>\n",
       "      <th>2373.0</th>\n",
       "      <th>2374.0</th>\n",
       "      <th>2375.0</th>\n",
       "      <th>2376.0</th>\n",
       "      <th>2377.0</th>\n",
       "      <th>2378.0</th>\n",
       "      <th>2379.0</th>\n",
       "      <th>2380.0</th>\n",
       "      <th>2381.0</th>\n",
       "      <th>2382.0</th>\n",
       "      <th>2383.0</th>\n",
       "      <th>2384.0</th>\n",
       "      <th>2385.0</th>\n",
       "      <th>2386.0</th>\n",
       "      <th>2387.0</th>\n",
       "      <th>2388.0</th>\n",
       "      <th>2389.0</th>\n",
       "      <th>2390.0</th>\n",
       "      <th>2391.0</th>\n",
       "      <th>2392.0</th>\n",
       "      <th>2393.0</th>\n",
       "      <th>2394.0</th>\n",
       "      <th>2395.0</th>\n",
       "      <th>2396.0</th>\n",
       "      <th>2397.0</th>\n",
       "      <th>2398.0</th>\n",
       "      <th>2399.0</th>\n",
       "      <th>2400.0</th>\n",
       "      <th>2401.0</th>\n",
       "      <th>2402.0</th>\n",
       "      <th>2403.0</th>\n",
       "      <th>2404.0</th>\n",
       "      <th>2405.0</th>\n",
       "      <th>2406.0</th>\n",
       "      <th>2407.0</th>\n",
       "      <th>2408.0</th>\n",
       "      <th>2409.0</th>\n",
       "      <th>2410.0</th>\n",
       "      <th>2411.0</th>\n",
       "      <th>2412.0</th>\n",
       "      <th>2413.0</th>\n",
       "      <th>2414.0</th>\n",
       "      <th>2415.0</th>\n",
       "      <th>2416.0</th>\n",
       "      <th>2417.0</th>\n",
       "      <th>2418.0</th>\n",
       "      <th>2419.0</th>\n",
       "      <th>2420.0</th>\n",
       "      <th>2421.0</th>\n",
       "      <th>2422.0</th>\n",
       "      <th>2423.0</th>\n",
       "      <th>2424.0</th>\n",
       "      <th>2425.0</th>\n",
       "      <th>2426.0</th>\n",
       "      <th>2427.0</th>\n",
       "      <th>2428.0</th>\n",
       "      <th>2429.0</th>\n",
       "      <th>2430.0</th>\n",
       "      <th>2431.0</th>\n",
       "      <th>2432.0</th>\n",
       "      <th>2433.0</th>\n",
       "      <th>2434.0</th>\n",
       "      <th>2435.0</th>\n",
       "      <th>2436.0</th>\n",
       "      <th>2437.0</th>\n",
       "      <th>2438.0</th>\n",
       "      <th>2439.0</th>\n",
       "      <th>2440.0</th>\n",
       "      <th>2441.0</th>\n",
       "      <th>2442.0</th>\n",
       "      <th>2443.0</th>\n",
       "      <th>2444.0</th>\n",
       "      <th>2445.0</th>\n",
       "      <th>2446.0</th>\n",
       "      <th>2447.0</th>\n",
       "      <th>2448.0</th>\n",
       "      <th>2449.0</th>\n",
       "      <th>2450.0</th>\n",
       "      <th>2451.0</th>\n",
       "      <th>2452.0</th>\n",
       "      <th>2453.0</th>\n",
       "      <th>2454.0</th>\n",
       "      <th>2455.0</th>\n",
       "      <th>2456.0</th>\n",
       "      <th>2457.0</th>\n",
       "      <th>2458.0</th>\n",
       "      <th>2459.0</th>\n",
       "      <th>2460.0</th>\n",
       "      <th>2461.0</th>\n",
       "      <th>2462.0</th>\n",
       "      <th>2463.0</th>\n",
       "      <th>2464.0</th>\n",
       "      <th>2465.0</th>\n",
       "      <th>2466.0</th>\n",
       "      <th>2467.0</th>\n",
       "      <th>2468.0</th>\n",
       "      <th>2469.0</th>\n",
       "      <th>2470.0</th>\n",
       "      <th>2471.0</th>\n",
       "      <th>2472.0</th>\n",
       "      <th>2473.0</th>\n",
       "      <th>2474.0</th>\n",
       "      <th>2475.0</th>\n",
       "      <th>2476.0</th>\n",
       "      <th>2477.0</th>\n",
       "      <th>2478.0</th>\n",
       "      <th>2479.0</th>\n",
       "      <th>2480.0</th>\n",
       "      <th>2481.0</th>\n",
       "      <th>2482.0</th>\n",
       "      <th>2483.0</th>\n",
       "      <th>2484.0</th>\n",
       "      <th>2485.0</th>\n",
       "      <th>2486.0</th>\n",
       "      <th>2487.0</th>\n",
       "      <th>2488.0</th>\n",
       "      <th>2489.0</th>\n",
       "      <th>2490.0</th>\n",
       "      <th>2491.0</th>\n",
       "      <th>2492.0</th>\n",
       "      <th>2493.0</th>\n",
       "      <th>2494.0</th>\n",
       "      <th>2495.0</th>\n",
       "      <th>2496.0</th>\n",
       "      <th>2497.0</th>\n",
       "      <th>2498.0</th>\n",
       "      <th>2499.0</th>\n",
       "      <th>2500.0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000084</td>\n",
       "      <td>0.000099</td>\n",
       "      <td>0.000117</td>\n",
       "      <td>0.000139</td>\n",
       "      <td>0.000165</td>\n",
       "      <td>0.000194</td>\n",
       "      <td>0.000227</td>\n",
       "      <td>0.000261</td>\n",
       "      <td>0.000297</td>\n",
       "      <td>0.000333</td>\n",
       "      <td>0.000367</td>\n",
       "      <td>0.000399</td>\n",
       "      <td>0.000427</td>\n",
       "      <td>0.000450</td>\n",
       "      <td>0.000468</td>\n",
       "      <td>0.000479</td>\n",
       "      <td>0.000484</td>\n",
       "      <td>0.000483</td>\n",
       "      <td>0.000477</td>\n",
       "      <td>0.000465</td>\n",
       "      <td>0.000449</td>\n",
       "      <td>0.000430</td>\n",
       "      <td>0.000409</td>\n",
       "      <td>0.000386</td>\n",
       "      <td>0.000361</td>\n",
       "      <td>0.000336</td>\n",
       "      <td>0.000311</td>\n",
       "      <td>0.000285</td>\n",
       "      <td>0.000259</td>\n",
       "      <td>0.000233</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000080</td>\n",
       "      <td>0.000084</td>\n",
       "      <td>0.000087</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.000091</td>\n",
       "      <td>0.000093</td>\n",
       "      <td>0.000094</td>\n",
       "      <td>0.000095</td>\n",
       "      <td>0.000096</td>\n",
       "      <td>0.000096</td>\n",
       "      <td>0.000096</td>\n",
       "      <td>0.000096</td>\n",
       "      <td>0.000096</td>\n",
       "      <td>0.000096</td>\n",
       "      <td>0.000096</td>\n",
       "      <td>0.000096</td>\n",
       "      <td>0.000096</td>\n",
       "      <td>0.000096</td>\n",
       "      <td>0.000096</td>\n",
       "      <td>0.000096</td>\n",
       "      <td>0.000096</td>\n",
       "      <td>0.000096</td>\n",
       "      <td>0.000095</td>\n",
       "      <td>0.000095</td>\n",
       "      <td>0.000094</td>\n",
       "      <td>0.000093</td>\n",
       "      <td>0.000092</td>\n",
       "      <td>0.000091</td>\n",
       "      <td>0.000090</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.000087</td>\n",
       "      <td>0.000086</td>\n",
       "      <td>0.000084</td>\n",
       "      <td>0.000083</td>\n",
       "      <td>0.000082</td>\n",
       "      <td>0.000080</td>\n",
       "      <td>0.000079</td>\n",
       "      <td>0.000078</td>\n",
       "      <td>0.000077</td>\n",
       "      <td>0.000076</td>\n",
       "      <td>0.000075</td>\n",
       "      <td>0.000075</td>\n",
       "      <td>0.000074</td>\n",
       "      <td>0.000073</td>\n",
       "      <td>0.000073</td>\n",
       "      <td>0.000072</td>\n",
       "      <td>0.000071</td>\n",
       "      <td>0.000071</td>\n",
       "      <td>0.000070</td>\n",
       "      <td>0.000069</td>\n",
       "      <td>0.000069</td>\n",
       "      <td>0.000068</td>\n",
       "      <td>0.000067</td>\n",
       "      <td>0.000067</td>\n",
       "      <td>0.000066</td>\n",
       "      <td>0.000066</td>\n",
       "      <td>0.000065</td>\n",
       "      <td>0.000065</td>\n",
       "      <td>0.000064</td>\n",
       "      <td>0.000064</td>\n",
       "      <td>0.000064</td>\n",
       "      <td>0.000063</td>\n",
       "      <td>0.000063</td>\n",
       "      <td>0.000062</td>\n",
       "      <td>0.000062</td>\n",
       "      <td>0.000061</td>\n",
       "      <td>0.000060</td>\n",
       "      <td>0.000060</td>\n",
       "      <td>0.000059</td>\n",
       "      <td>0.000058</td>\n",
       "      <td>0.000057</td>\n",
       "      <td>0.000056</td>\n",
       "      <td>0.000055</td>\n",
       "      <td>0.000054</td>\n",
       "      <td>0.000054</td>\n",
       "      <td>0.000053</td>\n",
       "      <td>0.000052</td>\n",
       "      <td>0.000051</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>0.000049</td>\n",
       "      <td>0.000049</td>\n",
       "      <td>0.000048</td>\n",
       "      <td>0.000048</td>\n",
       "      <td>0.000048</td>\n",
       "      <td>0.000047</td>\n",
       "      <td>0.000047</td>\n",
       "      <td>0.000047</td>\n",
       "      <td>0.000047</td>\n",
       "      <td>0.000048</td>\n",
       "      <td>0.000048</td>\n",
       "      <td>0.000048</td>\n",
       "      <td>0.000048</td>\n",
       "      <td>0.000049</td>\n",
       "      <td>0.000049</td>\n",
       "      <td>0.000049</td>\n",
       "      <td>0.000049</td>\n",
       "      <td>0.000049</td>\n",
       "      <td>0.000049</td>\n",
       "      <td>0.000049</td>\n",
       "      <td>0.000049</td>\n",
       "      <td>0.000048</td>\n",
       "      <td>0.000047</td>\n",
       "      <td>0.000046</td>\n",
       "      <td>0.000045</td>\n",
       "      <td>0.000043</td>\n",
       "      <td>0.000041</td>\n",
       "      <td>0.000038</td>\n",
       "      <td>0.000036</td>\n",
       "      <td>0.000033</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>0.000028</td>\n",
       "      <td>0.000025</td>\n",
       "      <td>0.000022</td>\n",
       "      <td>0.00002</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>0.000021</td>\n",
       "      <td>0.000025</td>\n",
       "      <td>0.000029</td>\n",
       "      <td>0.000034</td>\n",
       "      <td>0.000040</td>\n",
       "      <td>0.000046</td>\n",
       "      <td>0.000053</td>\n",
       "      <td>0.000061</td>\n",
       "      <td>0.000069</td>\n",
       "      <td>0.000078</td>\n",
       "      <td>0.000088</td>\n",
       "      <td>0.000098</td>\n",
       "      <td>0.000108</td>\n",
       "      <td>0.000118</td>\n",
       "      <td>0.000127</td>\n",
       "      <td>0.000136</td>\n",
       "      <td>0.000144</td>\n",
       "      <td>0.000151</td>\n",
       "      <td>0.000157</td>\n",
       "      <td>0.000161</td>\n",
       "      <td>0.000162</td>\n",
       "      <td>0.000162</td>\n",
       "      <td>0.000160</td>\n",
       "      <td>0.000155</td>\n",
       "      <td>0.000148</td>\n",
       "      <td>0.000139</td>\n",
       "      <td>0.000128</td>\n",
       "      <td>0.000116</td>\n",
       "      <td>0.000102</td>\n",
       "      <td>0.000087</td>\n",
       "      <td>0.000071</td>\n",
       "      <td>0.000056</td>\n",
       "      <td>0.000040</td>\n",
       "      <td>0.000026</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>-0.000007</td>\n",
       "      <td>-0.000014</td>\n",
       "      <td>-0.000017</td>\n",
       "      <td>-0.000018</td>\n",
       "      <td>-0.000015</td>\n",
       "      <td>-0.000009</td>\n",
       "      <td>-9.116918e-07</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.000022</td>\n",
       "      <td>0.000036</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>0.000064</td>\n",
       "      <td>0.000077</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.000099</td>\n",
       "      <td>0.000106</td>\n",
       "      <td>0.000110</td>\n",
       "      <td>0.000111</td>\n",
       "      <td>0.000109</td>\n",
       "      <td>0.000103</td>\n",
       "      <td>0.000094</td>\n",
       "      <td>0.000082</td>\n",
       "      <td>0.000068</td>\n",
       "      <td>0.000051</td>\n",
       "      <td>0.000033</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>-0.000008</td>\n",
       "      <td>-0.000029</td>\n",
       "      <td>-0.000050</td>\n",
       "      <td>-0.000071</td>\n",
       "      <td>-0.000092</td>\n",
       "      <td>-0.000111</td>\n",
       "      <td>-0.000129</td>\n",
       "      <td>-0.000145</td>\n",
       "      <td>-0.000159</td>\n",
       "      <td>-0.000170</td>\n",
       "      <td>-0.000180</td>\n",
       "      <td>-0.000186</td>\n",
       "      <td>-0.000190</td>\n",
       "      <td>-0.000191</td>\n",
       "      <td>-0.000189</td>\n",
       "      <td>-0.000184</td>\n",
       "      <td>-0.000177</td>\n",
       "      <td>-0.000167</td>\n",
       "      <td>-0.000155</td>\n",
       "      <td>-0.000141</td>\n",
       "      <td>-0.000126</td>\n",
       "      <td>-0.000109</td>\n",
       "      <td>-0.000092</td>\n",
       "      <td>-0.000074</td>\n",
       "      <td>-0.000057</td>\n",
       "      <td>-0.000039</td>\n",
       "      <td>-0.000022</td>\n",
       "      <td>-0.000006</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.000024</td>\n",
       "      <td>0.000038</td>\n",
       "      <td>0.000049</td>\n",
       "      <td>0.000059</td>\n",
       "      <td>0.000067</td>\n",
       "      <td>0.000072</td>\n",
       "      <td>0.000076</td>\n",
       "      <td>0.000076</td>\n",
       "      <td>0.000073</td>\n",
       "      <td>0.000068</td>\n",
       "      <td>0.000059</td>\n",
       "      <td>0.000048</td>\n",
       "      <td>0.000034</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>-6.216049e-07</td>\n",
       "      <td>-0.000020</td>\n",
       "      <td>-0.000041</td>\n",
       "      <td>-0.000062</td>\n",
       "      <td>-0.000082</td>\n",
       "      <td>-0.000102</td>\n",
       "      <td>-0.000119</td>\n",
       "      <td>-0.000134</td>\n",
       "      <td>-0.000146</td>\n",
       "      <td>-0.000155</td>\n",
       "      <td>-0.000160</td>\n",
       "      <td>-0.000162</td>\n",
       "      <td>-0.000160</td>\n",
       "      <td>-0.000154</td>\n",
       "      <td>-0.000145</td>\n",
       "      <td>-0.000132</td>\n",
       "      <td>-0.000117</td>\n",
       "      <td>-0.000100</td>\n",
       "      <td>-0.000080</td>\n",
       "      <td>-0.000060</td>\n",
       "      <td>-0.000038</td>\n",
       "      <td>-0.000017</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000024</td>\n",
       "      <td>0.000042</td>\n",
       "      <td>0.000059</td>\n",
       "      <td>0.000074</td>\n",
       "      <td>0.000086</td>\n",
       "      <td>0.000096</td>\n",
       "      <td>0.000102</td>\n",
       "      <td>0.000106</td>\n",
       "      <td>0.000108</td>\n",
       "      <td>0.000106</td>\n",
       "      <td>0.000103</td>\n",
       "      <td>0.000097</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.000080</td>\n",
       "      <td>0.000070</td>\n",
       "      <td>0.000060</td>\n",
       "      <td>0.000048</td>\n",
       "      <td>0.000037</td>\n",
       "      <td>0.000026</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>-0.000002</td>\n",
       "      <td>-0.000010</td>\n",
       "      <td>-0.000017</td>\n",
       "      <td>-0.000023</td>\n",
       "      <td>-0.000027</td>\n",
       "      <td>-0.000031</td>\n",
       "      <td>-0.000034</td>\n",
       "      <td>-0.000036</td>\n",
       "      <td>-0.000037</td>\n",
       "      <td>-0.000038</td>\n",
       "      <td>-0.000038</td>\n",
       "      <td>-0.000037</td>\n",
       "      <td>-0.000035</td>\n",
       "      <td>-0.000034</td>\n",
       "      <td>-0.000031</td>\n",
       "      <td>-0.000029</td>\n",
       "      <td>-0.000026</td>\n",
       "      <td>-0.000023</td>\n",
       "      <td>-0.000019</td>\n",
       "      <td>-0.000016</td>\n",
       "      <td>-0.000012</td>\n",
       "      <td>-0.000008</td>\n",
       "      <td>-4.116338e-06</td>\n",
       "      <td>-1.047570e-07</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>0.000020</td>\n",
       "      <td>0.000024</td>\n",
       "      <td>0.000028</td>\n",
       "      <td>0.000032</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>0.000038</td>\n",
       "      <td>0.000041</td>\n",
       "      <td>0.000043</td>\n",
       "      <td>0.000046</td>\n",
       "      <td>0.000048</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>0.000051</td>\n",
       "      <td>0.000053</td>\n",
       "      <td>0.000055</td>\n",
       "      <td>0.000057</td>\n",
       "      <td>0.000059</td>\n",
       "      <td>0.000060</td>\n",
       "      <td>0.000062</td>\n",
       "      <td>0.000063</td>\n",
       "      <td>0.000065</td>\n",
       "      <td>0.000066</td>\n",
       "      <td>0.000066</td>\n",
       "      <td>0.000067</td>\n",
       "      <td>0.000068</td>\n",
       "      <td>0.000069</td>\n",
       "      <td>0.000070</td>\n",
       "      <td>0.000072</td>\n",
       "      <td>0.000075</td>\n",
       "      <td>0.000079</td>\n",
       "      <td>0.000084</td>\n",
       "      <td>0.000090</td>\n",
       "      <td>0.000098</td>\n",
       "      <td>0.000107</td>\n",
       "      <td>0.000118</td>\n",
       "      <td>0.000130</td>\n",
       "      <td>0.000144</td>\n",
       "      <td>0.000160</td>\n",
       "      <td>0.000177</td>\n",
       "      <td>0.000196</td>\n",
       "      <td>0.000217</td>\n",
       "      <td>0.000241</td>\n",
       "      <td>0.000267</td>\n",
       "      <td>0.000295</td>\n",
       "      <td>0.000327</td>\n",
       "      <td>0.000361</td>\n",
       "      <td>0.000399</td>\n",
       "      <td>0.000441</td>\n",
       "      <td>0.000485</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000157</td>\n",
       "      <td>0.000152</td>\n",
       "      <td>0.000148</td>\n",
       "      <td>0.000142</td>\n",
       "      <td>0.000137</td>\n",
       "      <td>0.000132</td>\n",
       "      <td>0.000126</td>\n",
       "      <td>0.000121</td>\n",
       "      <td>0.000116</td>\n",
       "      <td>0.000112</td>\n",
       "      <td>0.000108</td>\n",
       "      <td>0.000106</td>\n",
       "      <td>0.000104</td>\n",
       "      <td>0.000104</td>\n",
       "      <td>0.000105</td>\n",
       "      <td>0.000107</td>\n",
       "      <td>0.000110</td>\n",
       "      <td>0.000114</td>\n",
       "      <td>0.000120</td>\n",
       "      <td>0.000127</td>\n",
       "      <td>0.000134</td>\n",
       "      <td>0.000143</td>\n",
       "      <td>0.000151</td>\n",
       "      <td>0.000161</td>\n",
       "      <td>0.000170</td>\n",
       "      <td>0.000180</td>\n",
       "      <td>0.000189</td>\n",
       "      <td>0.000198</td>\n",
       "      <td>0.000207</td>\n",
       "      <td>0.000214</td>\n",
       "      <td>0.000222</td>\n",
       "      <td>0.000228</td>\n",
       "      <td>0.000233</td>\n",
       "      <td>0.000236</td>\n",
       "      <td>0.000239</td>\n",
       "      <td>0.000240</td>\n",
       "      <td>0.000240</td>\n",
       "      <td>0.000238</td>\n",
       "      <td>0.000235</td>\n",
       "      <td>0.000230</td>\n",
       "      <td>0.000223</td>\n",
       "      <td>0.000215</td>\n",
       "      <td>0.000206</td>\n",
       "      <td>0.000195</td>\n",
       "      <td>0.000183</td>\n",
       "      <td>0.000169</td>\n",
       "      <td>0.000154</td>\n",
       "      <td>0.000139</td>\n",
       "      <td>0.000123</td>\n",
       "      <td>0.000107</td>\n",
       "      <td>0.000090</td>\n",
       "      <td>0.000073</td>\n",
       "      <td>0.000057</td>\n",
       "      <td>0.000041</td>\n",
       "      <td>0.000026</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>-8.493508e-07</td>\n",
       "      <td>-0.000013</td>\n",
       "      <td>-0.000023</td>\n",
       "      <td>-0.000033</td>\n",
       "      <td>-0.000040</td>\n",
       "      <td>-0.000047</td>\n",
       "      <td>-0.000051</td>\n",
       "      <td>-0.000054</td>\n",
       "      <td>-0.000056</td>\n",
       "      <td>-0.000056</td>\n",
       "      <td>-0.000055</td>\n",
       "      <td>-0.000053</td>\n",
       "      <td>-0.000050</td>\n",
       "      <td>-0.000046</td>\n",
       "      <td>-0.000042</td>\n",
       "      <td>-0.000037</td>\n",
       "      <td>-0.000031</td>\n",
       "      <td>-0.000026</td>\n",
       "      <td>-0.000020</td>\n",
       "      <td>-0.000014</td>\n",
       "      <td>-0.000009</td>\n",
       "      <td>-0.000003</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>0.000019</td>\n",
       "      <td>0.000023</td>\n",
       "      <td>0.000027</td>\n",
       "      <td>0.000031</td>\n",
       "      <td>0.000034</td>\n",
       "      <td>0.000038</td>\n",
       "      <td>0.000043</td>\n",
       "      <td>0.000047</td>\n",
       "      <td>0.000051</td>\n",
       "      <td>0.000056</td>\n",
       "      <td>0.000061</td>\n",
       "      <td>0.000065</td>\n",
       "      <td>0.000070</td>\n",
       "      <td>0.000075</td>\n",
       "      <td>0.000081</td>\n",
       "      <td>0.000086</td>\n",
       "      <td>0.000092</td>\n",
       "      <td>0.000097</td>\n",
       "      <td>0.000103</td>\n",
       "      <td>0.000109</td>\n",
       "      <td>0.000115</td>\n",
       "      <td>0.000121</td>\n",
       "      <td>0.000128</td>\n",
       "      <td>0.000134</td>\n",
       "      <td>0.000141</td>\n",
       "      <td>0.000149</td>\n",
       "      <td>0.000157</td>\n",
       "      <td>0.000165</td>\n",
       "      <td>0.000174</td>\n",
       "      <td>0.000183</td>\n",
       "      <td>0.000193</td>\n",
       "      <td>0.000204</td>\n",
       "      <td>0.000215</td>\n",
       "      <td>0.000226</td>\n",
       "      <td>0.000238</td>\n",
       "      <td>0.000250</td>\n",
       "      <td>0.000262</td>\n",
       "      <td>0.000275</td>\n",
       "      <td>0.000287</td>\n",
       "      <td>0.000299</td>\n",
       "      <td>0.000310</td>\n",
       "      <td>0.000320</td>\n",
       "      <td>0.000329</td>\n",
       "      <td>0.000337</td>\n",
       "      <td>0.000343</td>\n",
       "      <td>0.000348</td>\n",
       "      <td>0.000350</td>\n",
       "      <td>0.000350</td>\n",
       "      <td>0.000346</td>\n",
       "      <td>0.000340</td>\n",
       "      <td>0.000331</td>\n",
       "      <td>0.000319</td>\n",
       "      <td>0.000304</td>\n",
       "      <td>0.000286</td>\n",
       "      <td>0.000265</td>\n",
       "      <td>0.000241</td>\n",
       "      <td>0.000214</td>\n",
       "      <td>0.000185</td>\n",
       "      <td>0.000155</td>\n",
       "      <td>0.000123</td>\n",
       "      <td>0.000090</td>\n",
       "      <td>0.000056</td>\n",
       "      <td>0.000022</td>\n",
       "      <td>-0.000012</td>\n",
       "      <td>-0.000045</td>\n",
       "      <td>-0.000076</td>\n",
       "      <td>-0.000106</td>\n",
       "      <td>-0.000133</td>\n",
       "      <td>-0.000157</td>\n",
       "      <td>-0.000178</td>\n",
       "      <td>-0.000195</td>\n",
       "      <td>-0.000208</td>\n",
       "      <td>-0.000218</td>\n",
       "      <td>-0.000223</td>\n",
       "      <td>-0.000224</td>\n",
       "      <td>-0.000220</td>\n",
       "      <td>-0.000212</td>\n",
       "      <td>-0.000201</td>\n",
       "      <td>-0.000185</td>\n",
       "      <td>-0.000166</td>\n",
       "      <td>-0.000144</td>\n",
       "      <td>-0.000119</td>\n",
       "      <td>-0.000092</td>\n",
       "      <td>-0.000064</td>\n",
       "      <td>-0.000036</td>\n",
       "      <td>-6.592421e-06</td>\n",
       "      <td>0.000022</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>0.000076</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000122</td>\n",
       "      <td>0.000141</td>\n",
       "      <td>0.000156</td>\n",
       "      <td>0.000168</td>\n",
       "      <td>0.000177</td>\n",
       "      <td>0.000182</td>\n",
       "      <td>0.000184</td>\n",
       "      <td>0.000181</td>\n",
       "      <td>0.000176</td>\n",
       "      <td>0.000166</td>\n",
       "      <td>0.000154</td>\n",
       "      <td>0.000139</td>\n",
       "      <td>0.000122</td>\n",
       "      <td>0.000103</td>\n",
       "      <td>0.000082</td>\n",
       "      <td>0.000060</td>\n",
       "      <td>0.000037</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>-0.000009</td>\n",
       "      <td>-0.000032</td>\n",
       "      <td>-0.000053</td>\n",
       "      <td>-0.000073</td>\n",
       "      <td>-0.000091</td>\n",
       "      <td>-0.000107</td>\n",
       "      <td>-0.000120</td>\n",
       "      <td>-0.000131</td>\n",
       "      <td>-0.000140</td>\n",
       "      <td>-0.000146</td>\n",
       "      <td>-0.000149</td>\n",
       "      <td>-0.000149</td>\n",
       "      <td>-0.000146</td>\n",
       "      <td>-0.000141</td>\n",
       "      <td>-0.000133</td>\n",
       "      <td>-0.000123</td>\n",
       "      <td>-0.000111</td>\n",
       "      <td>-0.000098</td>\n",
       "      <td>-0.000083</td>\n",
       "      <td>-0.000067</td>\n",
       "      <td>-0.000050</td>\n",
       "      <td>-0.000033</td>\n",
       "      <td>-1.568699e-05</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>0.000034</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>0.000064</td>\n",
       "      <td>0.000076</td>\n",
       "      <td>0.000087</td>\n",
       "      <td>0.000097</td>\n",
       "      <td>0.000105</td>\n",
       "      <td>0.000112</td>\n",
       "      <td>0.000116</td>\n",
       "      <td>0.000119</td>\n",
       "      <td>0.000121</td>\n",
       "      <td>0.000121</td>\n",
       "      <td>0.000120</td>\n",
       "      <td>0.000117</td>\n",
       "      <td>0.000114</td>\n",
       "      <td>0.000110</td>\n",
       "      <td>0.000105</td>\n",
       "      <td>0.000099</td>\n",
       "      <td>0.000092</td>\n",
       "      <td>0.000085</td>\n",
       "      <td>0.000078</td>\n",
       "      <td>0.000071</td>\n",
       "      <td>0.000063</td>\n",
       "      <td>0.000056</td>\n",
       "      <td>0.000049</td>\n",
       "      <td>0.000042</td>\n",
       "      <td>0.000036</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>0.000025</td>\n",
       "      <td>0.000021</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>0.000019</td>\n",
       "      <td>0.000023</td>\n",
       "      <td>0.000028</td>\n",
       "      <td>0.000033</td>\n",
       "      <td>0.000040</td>\n",
       "      <td>0.000047</td>\n",
       "      <td>0.000055</td>\n",
       "      <td>0.000064</td>\n",
       "      <td>0.000073</td>\n",
       "      <td>0.000082</td>\n",
       "      <td>0.000092</td>\n",
       "      <td>0.000103</td>\n",
       "      <td>0.000113</td>\n",
       "      <td>0.000123</td>\n",
       "      <td>0.000134</td>\n",
       "      <td>0.000144</td>\n",
       "      <td>0.000155</td>\n",
       "      <td>0.000164</td>\n",
       "      <td>0.000173</td>\n",
       "      <td>0.000181</td>\n",
       "      <td>0.000188</td>\n",
       "      <td>0.000194</td>\n",
       "      <td>0.000198</td>\n",
       "      <td>0.000201</td>\n",
       "      <td>0.000203</td>\n",
       "      <td>0.000203</td>\n",
       "      <td>0.000201</td>\n",
       "      <td>0.000198</td>\n",
       "      <td>0.000194</td>\n",
       "      <td>0.000188</td>\n",
       "      <td>0.000181</td>\n",
       "      <td>0.000173</td>\n",
       "      <td>0.000163</td>\n",
       "      <td>0.000152</td>\n",
       "      <td>0.000140</td>\n",
       "      <td>0.000128</td>\n",
       "      <td>0.000115</td>\n",
       "      <td>0.000101</td>\n",
       "      <td>0.000086</td>\n",
       "      <td>0.000072</td>\n",
       "      <td>0.000057</td>\n",
       "      <td>0.000043</td>\n",
       "      <td>0.000029</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>-0.000011</td>\n",
       "      <td>-0.000022</td>\n",
       "      <td>-0.000032</td>\n",
       "      <td>-0.000040</td>\n",
       "      <td>-0.000045</td>\n",
       "      <td>-0.000049</td>\n",
       "      <td>-0.000051</td>\n",
       "      <td>-0.000050</td>\n",
       "      <td>-0.000046</td>\n",
       "      <td>-0.000041</td>\n",
       "      <td>-0.000034</td>\n",
       "      <td>-0.000024</td>\n",
       "      <td>-0.000012</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>0.000034</td>\n",
       "      <td>0.000053</td>\n",
       "      <td>0.000074</td>\n",
       "      <td>0.000097</td>\n",
       "      <td>0.000120</td>\n",
       "      <td>0.000144</td>\n",
       "      <td>0.000168</td>\n",
       "      <td>0.000191</td>\n",
       "      <td>0.000213</td>\n",
       "      <td>0.000234</td>\n",
       "      <td>0.000252</td>\n",
       "      <td>0.000268</td>\n",
       "      <td>0.000282</td>\n",
       "      <td>0.000293</td>\n",
       "      <td>0.000301</td>\n",
       "      <td>0.000306</td>\n",
       "      <td>0.000306</td>\n",
       "      <td>0.000303</td>\n",
       "      <td>0.000295</td>\n",
       "      <td>0.000283</td>\n",
       "      <td>0.000266</td>\n",
       "      <td>0.000245</td>\n",
       "      <td>0.000219</td>\n",
       "      <td>0.000190</td>\n",
       "      <td>0.000158</td>\n",
       "      <td>0.000123</td>\n",
       "      <td>0.000086</td>\n",
       "      <td>0.000048</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>-0.000031</td>\n",
       "      <td>-0.000071</td>\n",
       "      <td>-0.000110</td>\n",
       "      <td>-0.000148</td>\n",
       "      <td>-0.000184</td>\n",
       "      <td>-0.000218</td>\n",
       "      <td>-0.000248</td>\n",
       "      <td>-0.000274</td>\n",
       "      <td>-0.000294</td>\n",
       "      <td>-0.000309</td>\n",
       "      <td>-0.000317</td>\n",
       "      <td>-0.000319</td>\n",
       "      <td>-0.000314</td>\n",
       "      <td>-0.000303</td>\n",
       "      <td>-0.000286</td>\n",
       "      <td>-0.000264</td>\n",
       "      <td>-0.000235</td>\n",
       "      <td>-0.000202</td>\n",
       "      <td>-0.000164</td>\n",
       "      <td>-0.000122</td>\n",
       "      <td>-0.000074</td>\n",
       "      <td>-0.000022</td>\n",
       "      <td>0.000033</td>\n",
       "      <td>0.000092</td>\n",
       "      <td>0.000153</td>\n",
       "      <td>0.000215</td>\n",
       "      <td>0.000276</td>\n",
       "      <td>0.000336</td>\n",
       "      <td>0.000394</td>\n",
       "      <td>0.000450</td>\n",
       "      <td>0.000502</td>\n",
       "      <td>0.000551</td>\n",
       "      <td>0.000596</td>\n",
       "      <td>0.000639</td>\n",
       "      <td>0.000676</td>\n",
       "      <td>0.000706</td>\n",
       "      <td>0.000727</td>\n",
       "      <td>0.000734</td>\n",
       "      <td>0.000726</td>\n",
       "      <td>0.000697</td>\n",
       "      <td>0.000645</td>\n",
       "      <td>0.000568</td>\n",
       "      <td>0.000465</td>\n",
       "      <td>0.000337</td>\n",
       "      <td>0.000186</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>-0.000164</td>\n",
       "      <td>-0.000352</td>\n",
       "      <td>-0.000541</td>\n",
       "      <td>-0.000725</td>\n",
       "      <td>-0.000899</td>\n",
       "      <td>-0.001058</td>\n",
       "      <td>-0.001200</td>\n",
       "      <td>-0.001321</td>\n",
       "      <td>-0.001419</td>\n",
       "      <td>-0.001494</td>\n",
       "      <td>-0.001545</td>\n",
       "      <td>-0.001570</td>\n",
       "      <td>-0.001570</td>\n",
       "      <td>-0.001544</td>\n",
       "      <td>-0.001492</td>\n",
       "      <td>-0.001416</td>\n",
       "      <td>-0.001316</td>\n",
       "      <td>-0.001195</td>\n",
       "      <td>-0.001054</td>\n",
       "      <td>-0.000897</td>\n",
       "      <td>-0.000725</td>\n",
       "      <td>-0.000544</td>\n",
       "      <td>-0.000357</td>\n",
       "      <td>-0.000168</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>0.000195</td>\n",
       "      <td>0.000360</td>\n",
       "      <td>0.000506</td>\n",
       "      <td>0.000631</td>\n",
       "      <td>0.000729</td>\n",
       "      <td>0.000800</td>\n",
       "      <td>0.000842</td>\n",
       "      <td>0.000854</td>\n",
       "      <td>0.000838</td>\n",
       "      <td>0.000796</td>\n",
       "      <td>0.000733</td>\n",
       "      <td>0.000654</td>\n",
       "      <td>0.000564</td>\n",
       "      <td>0.000467</td>\n",
       "      <td>0.000369</td>\n",
       "      <td>0.000271</td>\n",
       "      <td>0.000178</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>-0.000072</td>\n",
       "      <td>-0.000144</td>\n",
       "      <td>-0.000210</td>\n",
       "      <td>-0.000271</td>\n",
       "      <td>-0.000326</td>\n",
       "      <td>-0.000374</td>\n",
       "      <td>-0.000415</td>\n",
       "      <td>-0.000447</td>\n",
       "      <td>-0.000471</td>\n",
       "      <td>-0.000487</td>\n",
       "      <td>-0.000494</td>\n",
       "      <td>-0.000493</td>\n",
       "      <td>-0.000485</td>\n",
       "      <td>-0.000472</td>\n",
       "      <td>-0.000455</td>\n",
       "      <td>-0.000434</td>\n",
       "      <td>-0.000412</td>\n",
       "      <td>-0.000388</td>\n",
       "      <td>-0.000364</td>\n",
       "      <td>-0.000339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000085</td>\n",
       "      <td>0.000099</td>\n",
       "      <td>0.000118</td>\n",
       "      <td>0.000140</td>\n",
       "      <td>0.000165</td>\n",
       "      <td>0.000195</td>\n",
       "      <td>0.000227</td>\n",
       "      <td>0.000261</td>\n",
       "      <td>0.000297</td>\n",
       "      <td>0.000333</td>\n",
       "      <td>0.000367</td>\n",
       "      <td>0.000399</td>\n",
       "      <td>0.000427</td>\n",
       "      <td>0.000451</td>\n",
       "      <td>0.000468</td>\n",
       "      <td>0.000480</td>\n",
       "      <td>0.000485</td>\n",
       "      <td>0.000483</td>\n",
       "      <td>0.000477</td>\n",
       "      <td>0.000465</td>\n",
       "      <td>0.000449</td>\n",
       "      <td>0.000430</td>\n",
       "      <td>0.000409</td>\n",
       "      <td>0.000386</td>\n",
       "      <td>0.000361</td>\n",
       "      <td>0.000336</td>\n",
       "      <td>0.000310</td>\n",
       "      <td>0.000284</td>\n",
       "      <td>0.000258</td>\n",
       "      <td>0.000233</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000080</td>\n",
       "      <td>0.000083</td>\n",
       "      <td>0.000086</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.000091</td>\n",
       "      <td>0.000093</td>\n",
       "      <td>0.000094</td>\n",
       "      <td>0.000095</td>\n",
       "      <td>0.000096</td>\n",
       "      <td>0.000096</td>\n",
       "      <td>0.000096</td>\n",
       "      <td>0.000096</td>\n",
       "      <td>0.000096</td>\n",
       "      <td>0.000096</td>\n",
       "      <td>0.000096</td>\n",
       "      <td>0.000096</td>\n",
       "      <td>0.000096</td>\n",
       "      <td>0.000096</td>\n",
       "      <td>0.000096</td>\n",
       "      <td>0.000096</td>\n",
       "      <td>0.000096</td>\n",
       "      <td>0.000096</td>\n",
       "      <td>0.000096</td>\n",
       "      <td>0.000095</td>\n",
       "      <td>0.000094</td>\n",
       "      <td>0.000094</td>\n",
       "      <td>0.000093</td>\n",
       "      <td>0.000092</td>\n",
       "      <td>0.000090</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.000088</td>\n",
       "      <td>0.000086</td>\n",
       "      <td>0.000085</td>\n",
       "      <td>0.000083</td>\n",
       "      <td>0.000082</td>\n",
       "      <td>0.000081</td>\n",
       "      <td>0.000079</td>\n",
       "      <td>0.000078</td>\n",
       "      <td>0.000077</td>\n",
       "      <td>0.000076</td>\n",
       "      <td>0.000075</td>\n",
       "      <td>0.000075</td>\n",
       "      <td>0.000074</td>\n",
       "      <td>0.000073</td>\n",
       "      <td>0.000072</td>\n",
       "      <td>0.000072</td>\n",
       "      <td>0.000071</td>\n",
       "      <td>0.000070</td>\n",
       "      <td>0.000070</td>\n",
       "      <td>0.000069</td>\n",
       "      <td>0.000068</td>\n",
       "      <td>0.000068</td>\n",
       "      <td>0.000067</td>\n",
       "      <td>0.000067</td>\n",
       "      <td>0.000066</td>\n",
       "      <td>0.000066</td>\n",
       "      <td>0.000065</td>\n",
       "      <td>0.000065</td>\n",
       "      <td>0.000064</td>\n",
       "      <td>0.000064</td>\n",
       "      <td>0.000063</td>\n",
       "      <td>0.000063</td>\n",
       "      <td>0.000063</td>\n",
       "      <td>0.000062</td>\n",
       "      <td>0.000061</td>\n",
       "      <td>0.000061</td>\n",
       "      <td>0.000060</td>\n",
       "      <td>0.000060</td>\n",
       "      <td>0.000059</td>\n",
       "      <td>0.000058</td>\n",
       "      <td>0.000057</td>\n",
       "      <td>0.000056</td>\n",
       "      <td>0.000055</td>\n",
       "      <td>0.000054</td>\n",
       "      <td>0.000053</td>\n",
       "      <td>0.000053</td>\n",
       "      <td>0.000052</td>\n",
       "      <td>0.000051</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>0.000049</td>\n",
       "      <td>0.000049</td>\n",
       "      <td>0.000048</td>\n",
       "      <td>0.000048</td>\n",
       "      <td>0.000048</td>\n",
       "      <td>0.000047</td>\n",
       "      <td>0.000047</td>\n",
       "      <td>0.000047</td>\n",
       "      <td>0.000048</td>\n",
       "      <td>0.000048</td>\n",
       "      <td>0.000048</td>\n",
       "      <td>0.000048</td>\n",
       "      <td>0.000049</td>\n",
       "      <td>0.000049</td>\n",
       "      <td>0.000049</td>\n",
       "      <td>0.000049</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>0.000049</td>\n",
       "      <td>0.000049</td>\n",
       "      <td>0.000048</td>\n",
       "      <td>0.000047</td>\n",
       "      <td>0.000046</td>\n",
       "      <td>0.000045</td>\n",
       "      <td>0.000043</td>\n",
       "      <td>0.000041</td>\n",
       "      <td>0.000038</td>\n",
       "      <td>0.000036</td>\n",
       "      <td>0.000033</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>0.000028</td>\n",
       "      <td>0.000025</td>\n",
       "      <td>0.000022</td>\n",
       "      <td>0.00002</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>0.000021</td>\n",
       "      <td>0.000025</td>\n",
       "      <td>0.000029</td>\n",
       "      <td>0.000034</td>\n",
       "      <td>0.000039</td>\n",
       "      <td>0.000046</td>\n",
       "      <td>0.000053</td>\n",
       "      <td>0.000061</td>\n",
       "      <td>0.000069</td>\n",
       "      <td>0.000078</td>\n",
       "      <td>0.000088</td>\n",
       "      <td>0.000098</td>\n",
       "      <td>0.000108</td>\n",
       "      <td>0.000118</td>\n",
       "      <td>0.000127</td>\n",
       "      <td>0.000136</td>\n",
       "      <td>0.000144</td>\n",
       "      <td>0.000151</td>\n",
       "      <td>0.000157</td>\n",
       "      <td>0.000161</td>\n",
       "      <td>0.000162</td>\n",
       "      <td>0.000162</td>\n",
       "      <td>0.000160</td>\n",
       "      <td>0.000155</td>\n",
       "      <td>0.000148</td>\n",
       "      <td>0.000139</td>\n",
       "      <td>0.000128</td>\n",
       "      <td>0.000116</td>\n",
       "      <td>0.000102</td>\n",
       "      <td>0.000087</td>\n",
       "      <td>0.000072</td>\n",
       "      <td>0.000056</td>\n",
       "      <td>0.000041</td>\n",
       "      <td>0.000026</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>-0.000007</td>\n",
       "      <td>-0.000014</td>\n",
       "      <td>-0.000017</td>\n",
       "      <td>-0.000017</td>\n",
       "      <td>-0.000015</td>\n",
       "      <td>-0.000009</td>\n",
       "      <td>-9.316766e-07</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.000022</td>\n",
       "      <td>0.000036</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>0.000064</td>\n",
       "      <td>0.000077</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.000099</td>\n",
       "      <td>0.000106</td>\n",
       "      <td>0.000110</td>\n",
       "      <td>0.000111</td>\n",
       "      <td>0.000108</td>\n",
       "      <td>0.000103</td>\n",
       "      <td>0.000094</td>\n",
       "      <td>0.000082</td>\n",
       "      <td>0.000068</td>\n",
       "      <td>0.000051</td>\n",
       "      <td>0.000033</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>-0.000008</td>\n",
       "      <td>-0.000029</td>\n",
       "      <td>-0.000050</td>\n",
       "      <td>-0.000071</td>\n",
       "      <td>-0.000091</td>\n",
       "      <td>-0.000110</td>\n",
       "      <td>-0.000128</td>\n",
       "      <td>-0.000144</td>\n",
       "      <td>-0.000158</td>\n",
       "      <td>-0.000170</td>\n",
       "      <td>-0.000179</td>\n",
       "      <td>-0.000186</td>\n",
       "      <td>-0.000190</td>\n",
       "      <td>-0.000191</td>\n",
       "      <td>-0.000189</td>\n",
       "      <td>-0.000184</td>\n",
       "      <td>-0.000177</td>\n",
       "      <td>-0.000167</td>\n",
       "      <td>-0.000155</td>\n",
       "      <td>-0.000141</td>\n",
       "      <td>-0.000125</td>\n",
       "      <td>-0.000109</td>\n",
       "      <td>-0.000092</td>\n",
       "      <td>-0.000074</td>\n",
       "      <td>-0.000057</td>\n",
       "      <td>-0.000039</td>\n",
       "      <td>-0.000022</td>\n",
       "      <td>-0.000006</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.000024</td>\n",
       "      <td>0.000037</td>\n",
       "      <td>0.000049</td>\n",
       "      <td>0.000059</td>\n",
       "      <td>0.000067</td>\n",
       "      <td>0.000072</td>\n",
       "      <td>0.000075</td>\n",
       "      <td>0.000076</td>\n",
       "      <td>0.000073</td>\n",
       "      <td>0.000068</td>\n",
       "      <td>0.000059</td>\n",
       "      <td>0.000048</td>\n",
       "      <td>0.000034</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>-8.584478e-07</td>\n",
       "      <td>-0.000021</td>\n",
       "      <td>-0.000041</td>\n",
       "      <td>-0.000062</td>\n",
       "      <td>-0.000083</td>\n",
       "      <td>-0.000102</td>\n",
       "      <td>-0.000119</td>\n",
       "      <td>-0.000134</td>\n",
       "      <td>-0.000146</td>\n",
       "      <td>-0.000155</td>\n",
       "      <td>-0.000160</td>\n",
       "      <td>-0.000162</td>\n",
       "      <td>-0.000160</td>\n",
       "      <td>-0.000154</td>\n",
       "      <td>-0.000144</td>\n",
       "      <td>-0.000132</td>\n",
       "      <td>-0.000117</td>\n",
       "      <td>-0.000099</td>\n",
       "      <td>-0.000080</td>\n",
       "      <td>-0.000059</td>\n",
       "      <td>-0.000038</td>\n",
       "      <td>-0.000017</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000024</td>\n",
       "      <td>0.000043</td>\n",
       "      <td>0.000059</td>\n",
       "      <td>0.000074</td>\n",
       "      <td>0.000086</td>\n",
       "      <td>0.000096</td>\n",
       "      <td>0.000102</td>\n",
       "      <td>0.000106</td>\n",
       "      <td>0.000108</td>\n",
       "      <td>0.000106</td>\n",
       "      <td>0.000103</td>\n",
       "      <td>0.000097</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.000080</td>\n",
       "      <td>0.000070</td>\n",
       "      <td>0.000059</td>\n",
       "      <td>0.000048</td>\n",
       "      <td>0.000037</td>\n",
       "      <td>0.000026</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>-0.000002</td>\n",
       "      <td>-0.000010</td>\n",
       "      <td>-0.000017</td>\n",
       "      <td>-0.000023</td>\n",
       "      <td>-0.000027</td>\n",
       "      <td>-0.000031</td>\n",
       "      <td>-0.000034</td>\n",
       "      <td>-0.000036</td>\n",
       "      <td>-0.000037</td>\n",
       "      <td>-0.000038</td>\n",
       "      <td>-0.000037</td>\n",
       "      <td>-0.000036</td>\n",
       "      <td>-0.000035</td>\n",
       "      <td>-0.000033</td>\n",
       "      <td>-0.000031</td>\n",
       "      <td>-0.000028</td>\n",
       "      <td>-0.000026</td>\n",
       "      <td>-0.000023</td>\n",
       "      <td>-0.000019</td>\n",
       "      <td>-0.000016</td>\n",
       "      <td>-0.000012</td>\n",
       "      <td>-0.000008</td>\n",
       "      <td>-4.246329e-06</td>\n",
       "      <td>-2.224495e-07</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>0.000020</td>\n",
       "      <td>0.000024</td>\n",
       "      <td>0.000028</td>\n",
       "      <td>0.000032</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>0.000038</td>\n",
       "      <td>0.000041</td>\n",
       "      <td>0.000043</td>\n",
       "      <td>0.000046</td>\n",
       "      <td>0.000048</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>0.000051</td>\n",
       "      <td>0.000053</td>\n",
       "      <td>0.000055</td>\n",
       "      <td>0.000057</td>\n",
       "      <td>0.000059</td>\n",
       "      <td>0.000060</td>\n",
       "      <td>0.000062</td>\n",
       "      <td>0.000063</td>\n",
       "      <td>0.000065</td>\n",
       "      <td>0.000066</td>\n",
       "      <td>0.000066</td>\n",
       "      <td>0.000067</td>\n",
       "      <td>0.000068</td>\n",
       "      <td>0.000069</td>\n",
       "      <td>0.000070</td>\n",
       "      <td>0.000072</td>\n",
       "      <td>0.000075</td>\n",
       "      <td>0.000079</td>\n",
       "      <td>0.000084</td>\n",
       "      <td>0.000090</td>\n",
       "      <td>0.000098</td>\n",
       "      <td>0.000107</td>\n",
       "      <td>0.000118</td>\n",
       "      <td>0.000131</td>\n",
       "      <td>0.000144</td>\n",
       "      <td>0.000160</td>\n",
       "      <td>0.000177</td>\n",
       "      <td>0.000196</td>\n",
       "      <td>0.000217</td>\n",
       "      <td>0.000241</td>\n",
       "      <td>0.000267</td>\n",
       "      <td>0.000295</td>\n",
       "      <td>0.000327</td>\n",
       "      <td>0.000361</td>\n",
       "      <td>0.000399</td>\n",
       "      <td>0.000441</td>\n",
       "      <td>0.000485</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000155</td>\n",
       "      <td>0.000151</td>\n",
       "      <td>0.000146</td>\n",
       "      <td>0.000141</td>\n",
       "      <td>0.000135</td>\n",
       "      <td>0.000130</td>\n",
       "      <td>0.000125</td>\n",
       "      <td>0.000120</td>\n",
       "      <td>0.000115</td>\n",
       "      <td>0.000111</td>\n",
       "      <td>0.000108</td>\n",
       "      <td>0.000106</td>\n",
       "      <td>0.000105</td>\n",
       "      <td>0.000105</td>\n",
       "      <td>0.000106</td>\n",
       "      <td>0.000109</td>\n",
       "      <td>0.000112</td>\n",
       "      <td>0.000116</td>\n",
       "      <td>0.000122</td>\n",
       "      <td>0.000129</td>\n",
       "      <td>0.000136</td>\n",
       "      <td>0.000144</td>\n",
       "      <td>0.000153</td>\n",
       "      <td>0.000162</td>\n",
       "      <td>0.000171</td>\n",
       "      <td>0.000180</td>\n",
       "      <td>0.000189</td>\n",
       "      <td>0.000198</td>\n",
       "      <td>0.000206</td>\n",
       "      <td>0.000214</td>\n",
       "      <td>0.000221</td>\n",
       "      <td>0.000227</td>\n",
       "      <td>0.000232</td>\n",
       "      <td>0.000236</td>\n",
       "      <td>0.000238</td>\n",
       "      <td>0.000239</td>\n",
       "      <td>0.000239</td>\n",
       "      <td>0.000237</td>\n",
       "      <td>0.000234</td>\n",
       "      <td>0.000229</td>\n",
       "      <td>0.000223</td>\n",
       "      <td>0.000215</td>\n",
       "      <td>0.000205</td>\n",
       "      <td>0.000194</td>\n",
       "      <td>0.000182</td>\n",
       "      <td>0.000169</td>\n",
       "      <td>0.000154</td>\n",
       "      <td>0.000139</td>\n",
       "      <td>0.000123</td>\n",
       "      <td>0.000107</td>\n",
       "      <td>0.000090</td>\n",
       "      <td>0.000074</td>\n",
       "      <td>0.000057</td>\n",
       "      <td>0.000042</td>\n",
       "      <td>0.000027</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>-6.847299e-07</td>\n",
       "      <td>-0.000013</td>\n",
       "      <td>-0.000023</td>\n",
       "      <td>-0.000032</td>\n",
       "      <td>-0.000040</td>\n",
       "      <td>-0.000047</td>\n",
       "      <td>-0.000051</td>\n",
       "      <td>-0.000054</td>\n",
       "      <td>-0.000056</td>\n",
       "      <td>-0.000056</td>\n",
       "      <td>-0.000055</td>\n",
       "      <td>-0.000053</td>\n",
       "      <td>-0.000050</td>\n",
       "      <td>-0.000046</td>\n",
       "      <td>-0.000042</td>\n",
       "      <td>-0.000037</td>\n",
       "      <td>-0.000031</td>\n",
       "      <td>-0.000026</td>\n",
       "      <td>-0.000020</td>\n",
       "      <td>-0.000014</td>\n",
       "      <td>-0.000009</td>\n",
       "      <td>-0.000003</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>0.000020</td>\n",
       "      <td>0.000024</td>\n",
       "      <td>0.000027</td>\n",
       "      <td>0.000031</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>0.000039</td>\n",
       "      <td>0.000043</td>\n",
       "      <td>0.000047</td>\n",
       "      <td>0.000051</td>\n",
       "      <td>0.000056</td>\n",
       "      <td>0.000060</td>\n",
       "      <td>0.000065</td>\n",
       "      <td>0.000070</td>\n",
       "      <td>0.000075</td>\n",
       "      <td>0.000080</td>\n",
       "      <td>0.000085</td>\n",
       "      <td>0.000091</td>\n",
       "      <td>0.000096</td>\n",
       "      <td>0.000102</td>\n",
       "      <td>0.000108</td>\n",
       "      <td>0.000114</td>\n",
       "      <td>0.000120</td>\n",
       "      <td>0.000127</td>\n",
       "      <td>0.000134</td>\n",
       "      <td>0.000141</td>\n",
       "      <td>0.000149</td>\n",
       "      <td>0.000157</td>\n",
       "      <td>0.000165</td>\n",
       "      <td>0.000174</td>\n",
       "      <td>0.000184</td>\n",
       "      <td>0.000194</td>\n",
       "      <td>0.000204</td>\n",
       "      <td>0.000216</td>\n",
       "      <td>0.000227</td>\n",
       "      <td>0.000239</td>\n",
       "      <td>0.000251</td>\n",
       "      <td>0.000263</td>\n",
       "      <td>0.000276</td>\n",
       "      <td>0.000288</td>\n",
       "      <td>0.000299</td>\n",
       "      <td>0.000310</td>\n",
       "      <td>0.000320</td>\n",
       "      <td>0.000329</td>\n",
       "      <td>0.000337</td>\n",
       "      <td>0.000343</td>\n",
       "      <td>0.000347</td>\n",
       "      <td>0.000349</td>\n",
       "      <td>0.000349</td>\n",
       "      <td>0.000345</td>\n",
       "      <td>0.000339</td>\n",
       "      <td>0.000330</td>\n",
       "      <td>0.000318</td>\n",
       "      <td>0.000303</td>\n",
       "      <td>0.000285</td>\n",
       "      <td>0.000264</td>\n",
       "      <td>0.000241</td>\n",
       "      <td>0.000214</td>\n",
       "      <td>0.000186</td>\n",
       "      <td>0.000155</td>\n",
       "      <td>0.000123</td>\n",
       "      <td>0.000090</td>\n",
       "      <td>0.000057</td>\n",
       "      <td>0.000023</td>\n",
       "      <td>-0.000011</td>\n",
       "      <td>-0.000044</td>\n",
       "      <td>-0.000076</td>\n",
       "      <td>-0.000105</td>\n",
       "      <td>-0.000132</td>\n",
       "      <td>-0.000157</td>\n",
       "      <td>-0.000178</td>\n",
       "      <td>-0.000195</td>\n",
       "      <td>-0.000209</td>\n",
       "      <td>-0.000218</td>\n",
       "      <td>-0.000224</td>\n",
       "      <td>-0.000225</td>\n",
       "      <td>-0.000221</td>\n",
       "      <td>-0.000214</td>\n",
       "      <td>-0.000202</td>\n",
       "      <td>-0.000186</td>\n",
       "      <td>-0.000167</td>\n",
       "      <td>-0.000145</td>\n",
       "      <td>-0.000120</td>\n",
       "      <td>-0.000094</td>\n",
       "      <td>-0.000065</td>\n",
       "      <td>-0.000036</td>\n",
       "      <td>-7.091757e-06</td>\n",
       "      <td>0.000022</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>0.000076</td>\n",
       "      <td>0.000101</td>\n",
       "      <td>0.000123</td>\n",
       "      <td>0.000142</td>\n",
       "      <td>0.000158</td>\n",
       "      <td>0.000170</td>\n",
       "      <td>0.000178</td>\n",
       "      <td>0.000183</td>\n",
       "      <td>0.000185</td>\n",
       "      <td>0.000182</td>\n",
       "      <td>0.000176</td>\n",
       "      <td>0.000167</td>\n",
       "      <td>0.000154</td>\n",
       "      <td>0.000139</td>\n",
       "      <td>0.000121</td>\n",
       "      <td>0.000102</td>\n",
       "      <td>0.000080</td>\n",
       "      <td>0.000058</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>-0.000011</td>\n",
       "      <td>-0.000033</td>\n",
       "      <td>-0.000054</td>\n",
       "      <td>-0.000074</td>\n",
       "      <td>-0.000092</td>\n",
       "      <td>-0.000107</td>\n",
       "      <td>-0.000121</td>\n",
       "      <td>-0.000132</td>\n",
       "      <td>-0.000140</td>\n",
       "      <td>-0.000146</td>\n",
       "      <td>-0.000148</td>\n",
       "      <td>-0.000148</td>\n",
       "      <td>-0.000146</td>\n",
       "      <td>-0.000140</td>\n",
       "      <td>-0.000133</td>\n",
       "      <td>-0.000122</td>\n",
       "      <td>-0.000110</td>\n",
       "      <td>-0.000097</td>\n",
       "      <td>-0.000082</td>\n",
       "      <td>-0.000066</td>\n",
       "      <td>-0.000049</td>\n",
       "      <td>-0.000032</td>\n",
       "      <td>-1.410418e-05</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.000020</td>\n",
       "      <td>0.000036</td>\n",
       "      <td>0.000052</td>\n",
       "      <td>0.000066</td>\n",
       "      <td>0.000078</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.000099</td>\n",
       "      <td>0.000106</td>\n",
       "      <td>0.000112</td>\n",
       "      <td>0.000117</td>\n",
       "      <td>0.000119</td>\n",
       "      <td>0.000120</td>\n",
       "      <td>0.000120</td>\n",
       "      <td>0.000118</td>\n",
       "      <td>0.000115</td>\n",
       "      <td>0.000111</td>\n",
       "      <td>0.000107</td>\n",
       "      <td>0.000101</td>\n",
       "      <td>0.000095</td>\n",
       "      <td>0.000088</td>\n",
       "      <td>0.000082</td>\n",
       "      <td>0.000074</td>\n",
       "      <td>0.000067</td>\n",
       "      <td>0.000060</td>\n",
       "      <td>0.000054</td>\n",
       "      <td>0.000047</td>\n",
       "      <td>0.000041</td>\n",
       "      <td>0.000036</td>\n",
       "      <td>0.000031</td>\n",
       "      <td>0.000026</td>\n",
       "      <td>0.000023</td>\n",
       "      <td>0.000020</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>0.000020</td>\n",
       "      <td>0.000023</td>\n",
       "      <td>0.000026</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>0.000041</td>\n",
       "      <td>0.000048</td>\n",
       "      <td>0.000055</td>\n",
       "      <td>0.000064</td>\n",
       "      <td>0.000072</td>\n",
       "      <td>0.000081</td>\n",
       "      <td>0.000091</td>\n",
       "      <td>0.000101</td>\n",
       "      <td>0.000111</td>\n",
       "      <td>0.000122</td>\n",
       "      <td>0.000132</td>\n",
       "      <td>0.000142</td>\n",
       "      <td>0.000152</td>\n",
       "      <td>0.000162</td>\n",
       "      <td>0.000171</td>\n",
       "      <td>0.000178</td>\n",
       "      <td>0.000185</td>\n",
       "      <td>0.000191</td>\n",
       "      <td>0.000196</td>\n",
       "      <td>0.000199</td>\n",
       "      <td>0.000201</td>\n",
       "      <td>0.000201</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>0.000198</td>\n",
       "      <td>0.000194</td>\n",
       "      <td>0.000189</td>\n",
       "      <td>0.000182</td>\n",
       "      <td>0.000173</td>\n",
       "      <td>0.000164</td>\n",
       "      <td>0.000152</td>\n",
       "      <td>0.000140</td>\n",
       "      <td>0.000127</td>\n",
       "      <td>0.000113</td>\n",
       "      <td>0.000099</td>\n",
       "      <td>0.000084</td>\n",
       "      <td>0.000070</td>\n",
       "      <td>0.000056</td>\n",
       "      <td>0.000042</td>\n",
       "      <td>0.000028</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>-0.000008</td>\n",
       "      <td>-0.000018</td>\n",
       "      <td>-0.000027</td>\n",
       "      <td>-0.000034</td>\n",
       "      <td>-0.000039</td>\n",
       "      <td>-0.000042</td>\n",
       "      <td>-0.000043</td>\n",
       "      <td>-0.000042</td>\n",
       "      <td>-0.000039</td>\n",
       "      <td>-0.000034</td>\n",
       "      <td>-0.000026</td>\n",
       "      <td>-0.000017</td>\n",
       "      <td>-0.000006</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.000022</td>\n",
       "      <td>0.000038</td>\n",
       "      <td>0.000057</td>\n",
       "      <td>0.000076</td>\n",
       "      <td>0.000097</td>\n",
       "      <td>0.000119</td>\n",
       "      <td>0.000141</td>\n",
       "      <td>0.000162</td>\n",
       "      <td>0.000184</td>\n",
       "      <td>0.000204</td>\n",
       "      <td>0.000222</td>\n",
       "      <td>0.000239</td>\n",
       "      <td>0.000255</td>\n",
       "      <td>0.000269</td>\n",
       "      <td>0.000280</td>\n",
       "      <td>0.000289</td>\n",
       "      <td>0.000295</td>\n",
       "      <td>0.000298</td>\n",
       "      <td>0.000297</td>\n",
       "      <td>0.000292</td>\n",
       "      <td>0.000283</td>\n",
       "      <td>0.000269</td>\n",
       "      <td>0.000252</td>\n",
       "      <td>0.000230</td>\n",
       "      <td>0.000204</td>\n",
       "      <td>0.000176</td>\n",
       "      <td>0.000144</td>\n",
       "      <td>0.000110</td>\n",
       "      <td>0.000074</td>\n",
       "      <td>0.000037</td>\n",
       "      <td>-0.000002</td>\n",
       "      <td>-0.000042</td>\n",
       "      <td>-0.000081</td>\n",
       "      <td>-0.000121</td>\n",
       "      <td>-0.000160</td>\n",
       "      <td>-0.000197</td>\n",
       "      <td>-0.000231</td>\n",
       "      <td>-0.000262</td>\n",
       "      <td>-0.000287</td>\n",
       "      <td>-0.000308</td>\n",
       "      <td>-0.000322</td>\n",
       "      <td>-0.000331</td>\n",
       "      <td>-0.000333</td>\n",
       "      <td>-0.000328</td>\n",
       "      <td>-0.000318</td>\n",
       "      <td>-0.000301</td>\n",
       "      <td>-0.000278</td>\n",
       "      <td>-0.000249</td>\n",
       "      <td>-0.000214</td>\n",
       "      <td>-0.000173</td>\n",
       "      <td>-0.000125</td>\n",
       "      <td>-0.000071</td>\n",
       "      <td>-0.000012</td>\n",
       "      <td>0.000053</td>\n",
       "      <td>0.000122</td>\n",
       "      <td>0.000193</td>\n",
       "      <td>0.000264</td>\n",
       "      <td>0.000335</td>\n",
       "      <td>0.000404</td>\n",
       "      <td>0.000470</td>\n",
       "      <td>0.000531</td>\n",
       "      <td>0.000589</td>\n",
       "      <td>0.000641</td>\n",
       "      <td>0.000689</td>\n",
       "      <td>0.000729</td>\n",
       "      <td>0.000760</td>\n",
       "      <td>0.000779</td>\n",
       "      <td>0.000785</td>\n",
       "      <td>0.000773</td>\n",
       "      <td>0.000739</td>\n",
       "      <td>0.000682</td>\n",
       "      <td>0.000599</td>\n",
       "      <td>0.000490</td>\n",
       "      <td>0.000357</td>\n",
       "      <td>0.000201</td>\n",
       "      <td>0.000027</td>\n",
       "      <td>-0.000160</td>\n",
       "      <td>-0.000353</td>\n",
       "      <td>-0.000546</td>\n",
       "      <td>-0.000733</td>\n",
       "      <td>-0.000910</td>\n",
       "      <td>-0.001072</td>\n",
       "      <td>-0.001214</td>\n",
       "      <td>-0.001335</td>\n",
       "      <td>-0.001431</td>\n",
       "      <td>-0.001502</td>\n",
       "      <td>-0.001548</td>\n",
       "      <td>-0.001567</td>\n",
       "      <td>-0.001561</td>\n",
       "      <td>-0.001529</td>\n",
       "      <td>-0.001473</td>\n",
       "      <td>-0.001393</td>\n",
       "      <td>-0.001291</td>\n",
       "      <td>-0.001170</td>\n",
       "      <td>-0.001034</td>\n",
       "      <td>-0.000885</td>\n",
       "      <td>-0.000725</td>\n",
       "      <td>-0.000558</td>\n",
       "      <td>-0.000386</td>\n",
       "      <td>-0.000213</td>\n",
       "      <td>-0.000043</td>\n",
       "      <td>0.000121</td>\n",
       "      <td>0.000274</td>\n",
       "      <td>0.000413</td>\n",
       "      <td>0.000534</td>\n",
       "      <td>0.000634</td>\n",
       "      <td>0.000712</td>\n",
       "      <td>0.000765</td>\n",
       "      <td>0.000791</td>\n",
       "      <td>0.000791</td>\n",
       "      <td>0.000767</td>\n",
       "      <td>0.000721</td>\n",
       "      <td>0.000656</td>\n",
       "      <td>0.000578</td>\n",
       "      <td>0.000491</td>\n",
       "      <td>0.000398</td>\n",
       "      <td>0.000303</td>\n",
       "      <td>0.000208</td>\n",
       "      <td>0.000116</td>\n",
       "      <td>0.000029</td>\n",
       "      <td>-0.000052</td>\n",
       "      <td>-0.000127</td>\n",
       "      <td>-0.000195</td>\n",
       "      <td>-0.000258</td>\n",
       "      <td>-0.000315</td>\n",
       "      <td>-0.000366</td>\n",
       "      <td>-0.000411</td>\n",
       "      <td>-0.000449</td>\n",
       "      <td>-0.000480</td>\n",
       "      <td>-0.000503</td>\n",
       "      <td>-0.000519</td>\n",
       "      <td>-0.000527</td>\n",
       "      <td>-0.000528</td>\n",
       "      <td>-0.000523</td>\n",
       "      <td>-0.000512</td>\n",
       "      <td>-0.000495</td>\n",
       "      <td>-0.000474</td>\n",
       "      <td>-0.000447</td>\n",
       "      <td>-0.000416</td>\n",
       "      <td>-0.000381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000084</td>\n",
       "      <td>0.000099</td>\n",
       "      <td>0.000117</td>\n",
       "      <td>0.000139</td>\n",
       "      <td>0.000165</td>\n",
       "      <td>0.000194</td>\n",
       "      <td>0.000227</td>\n",
       "      <td>0.000261</td>\n",
       "      <td>0.000297</td>\n",
       "      <td>0.000333</td>\n",
       "      <td>0.000367</td>\n",
       "      <td>0.000399</td>\n",
       "      <td>0.000427</td>\n",
       "      <td>0.000451</td>\n",
       "      <td>0.000468</td>\n",
       "      <td>0.000480</td>\n",
       "      <td>0.000485</td>\n",
       "      <td>0.000484</td>\n",
       "      <td>0.000477</td>\n",
       "      <td>0.000465</td>\n",
       "      <td>0.000449</td>\n",
       "      <td>0.000431</td>\n",
       "      <td>0.000409</td>\n",
       "      <td>0.000386</td>\n",
       "      <td>0.000362</td>\n",
       "      <td>0.000336</td>\n",
       "      <td>0.000311</td>\n",
       "      <td>0.000285</td>\n",
       "      <td>0.000259</td>\n",
       "      <td>0.000234</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000080</td>\n",
       "      <td>0.000084</td>\n",
       "      <td>0.000087</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.000091</td>\n",
       "      <td>0.000093</td>\n",
       "      <td>0.000094</td>\n",
       "      <td>0.000095</td>\n",
       "      <td>0.000096</td>\n",
       "      <td>0.000096</td>\n",
       "      <td>0.000097</td>\n",
       "      <td>0.000097</td>\n",
       "      <td>0.000097</td>\n",
       "      <td>0.000096</td>\n",
       "      <td>0.000096</td>\n",
       "      <td>0.000096</td>\n",
       "      <td>0.000096</td>\n",
       "      <td>0.000096</td>\n",
       "      <td>0.000096</td>\n",
       "      <td>0.000096</td>\n",
       "      <td>0.000096</td>\n",
       "      <td>0.000096</td>\n",
       "      <td>0.000095</td>\n",
       "      <td>0.000095</td>\n",
       "      <td>0.000094</td>\n",
       "      <td>0.000093</td>\n",
       "      <td>0.000092</td>\n",
       "      <td>0.000091</td>\n",
       "      <td>0.000090</td>\n",
       "      <td>0.000088</td>\n",
       "      <td>0.000087</td>\n",
       "      <td>0.000086</td>\n",
       "      <td>0.000084</td>\n",
       "      <td>0.000083</td>\n",
       "      <td>0.000082</td>\n",
       "      <td>0.000080</td>\n",
       "      <td>0.000079</td>\n",
       "      <td>0.000078</td>\n",
       "      <td>0.000077</td>\n",
       "      <td>0.000077</td>\n",
       "      <td>0.000076</td>\n",
       "      <td>0.000075</td>\n",
       "      <td>0.000074</td>\n",
       "      <td>0.000074</td>\n",
       "      <td>0.000073</td>\n",
       "      <td>0.000072</td>\n",
       "      <td>0.000072</td>\n",
       "      <td>0.000071</td>\n",
       "      <td>0.000070</td>\n",
       "      <td>0.000070</td>\n",
       "      <td>0.000069</td>\n",
       "      <td>0.000068</td>\n",
       "      <td>0.000067</td>\n",
       "      <td>0.000067</td>\n",
       "      <td>0.000066</td>\n",
       "      <td>0.000065</td>\n",
       "      <td>0.000065</td>\n",
       "      <td>0.000064</td>\n",
       "      <td>0.000064</td>\n",
       "      <td>0.000063</td>\n",
       "      <td>0.000063</td>\n",
       "      <td>0.000063</td>\n",
       "      <td>0.000062</td>\n",
       "      <td>0.000062</td>\n",
       "      <td>0.000061</td>\n",
       "      <td>0.000061</td>\n",
       "      <td>0.000060</td>\n",
       "      <td>0.000059</td>\n",
       "      <td>0.000059</td>\n",
       "      <td>0.000058</td>\n",
       "      <td>0.000057</td>\n",
       "      <td>0.000056</td>\n",
       "      <td>0.000055</td>\n",
       "      <td>0.000054</td>\n",
       "      <td>0.000054</td>\n",
       "      <td>0.000053</td>\n",
       "      <td>0.000052</td>\n",
       "      <td>0.000051</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>0.000049</td>\n",
       "      <td>0.000048</td>\n",
       "      <td>0.000048</td>\n",
       "      <td>0.000048</td>\n",
       "      <td>0.000048</td>\n",
       "      <td>0.000047</td>\n",
       "      <td>0.000047</td>\n",
       "      <td>0.000048</td>\n",
       "      <td>0.000048</td>\n",
       "      <td>0.000048</td>\n",
       "      <td>0.000048</td>\n",
       "      <td>0.000048</td>\n",
       "      <td>0.000048</td>\n",
       "      <td>0.000049</td>\n",
       "      <td>0.000049</td>\n",
       "      <td>0.000049</td>\n",
       "      <td>0.000049</td>\n",
       "      <td>0.000049</td>\n",
       "      <td>0.000049</td>\n",
       "      <td>0.000049</td>\n",
       "      <td>0.000048</td>\n",
       "      <td>0.000047</td>\n",
       "      <td>0.000046</td>\n",
       "      <td>0.000045</td>\n",
       "      <td>0.000043</td>\n",
       "      <td>0.000041</td>\n",
       "      <td>0.000038</td>\n",
       "      <td>0.000036</td>\n",
       "      <td>0.000033</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>0.000028</td>\n",
       "      <td>0.000025</td>\n",
       "      <td>0.000022</td>\n",
       "      <td>0.00002</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>0.000021</td>\n",
       "      <td>0.000025</td>\n",
       "      <td>0.000029</td>\n",
       "      <td>0.000034</td>\n",
       "      <td>0.000039</td>\n",
       "      <td>0.000046</td>\n",
       "      <td>0.000053</td>\n",
       "      <td>0.000061</td>\n",
       "      <td>0.000069</td>\n",
       "      <td>0.000078</td>\n",
       "      <td>0.000088</td>\n",
       "      <td>0.000098</td>\n",
       "      <td>0.000108</td>\n",
       "      <td>0.000118</td>\n",
       "      <td>0.000128</td>\n",
       "      <td>0.000137</td>\n",
       "      <td>0.000145</td>\n",
       "      <td>0.000152</td>\n",
       "      <td>0.000157</td>\n",
       "      <td>0.000161</td>\n",
       "      <td>0.000163</td>\n",
       "      <td>0.000162</td>\n",
       "      <td>0.000160</td>\n",
       "      <td>0.000155</td>\n",
       "      <td>0.000148</td>\n",
       "      <td>0.000139</td>\n",
       "      <td>0.000128</td>\n",
       "      <td>0.000116</td>\n",
       "      <td>0.000102</td>\n",
       "      <td>0.000087</td>\n",
       "      <td>0.000071</td>\n",
       "      <td>0.000056</td>\n",
       "      <td>0.000040</td>\n",
       "      <td>0.000026</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>-0.000007</td>\n",
       "      <td>-0.000014</td>\n",
       "      <td>-0.000017</td>\n",
       "      <td>-0.000018</td>\n",
       "      <td>-0.000015</td>\n",
       "      <td>-0.000009</td>\n",
       "      <td>-9.151183e-07</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.000022</td>\n",
       "      <td>0.000036</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>0.000064</td>\n",
       "      <td>0.000077</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.000099</td>\n",
       "      <td>0.000106</td>\n",
       "      <td>0.000110</td>\n",
       "      <td>0.000111</td>\n",
       "      <td>0.000109</td>\n",
       "      <td>0.000103</td>\n",
       "      <td>0.000094</td>\n",
       "      <td>0.000082</td>\n",
       "      <td>0.000068</td>\n",
       "      <td>0.000051</td>\n",
       "      <td>0.000033</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>-0.000008</td>\n",
       "      <td>-0.000029</td>\n",
       "      <td>-0.000050</td>\n",
       "      <td>-0.000071</td>\n",
       "      <td>-0.000091</td>\n",
       "      <td>-0.000110</td>\n",
       "      <td>-0.000128</td>\n",
       "      <td>-0.000144</td>\n",
       "      <td>-0.000158</td>\n",
       "      <td>-0.000170</td>\n",
       "      <td>-0.000179</td>\n",
       "      <td>-0.000186</td>\n",
       "      <td>-0.000190</td>\n",
       "      <td>-0.000191</td>\n",
       "      <td>-0.000189</td>\n",
       "      <td>-0.000184</td>\n",
       "      <td>-0.000177</td>\n",
       "      <td>-0.000167</td>\n",
       "      <td>-0.000155</td>\n",
       "      <td>-0.000141</td>\n",
       "      <td>-0.000126</td>\n",
       "      <td>-0.000109</td>\n",
       "      <td>-0.000092</td>\n",
       "      <td>-0.000074</td>\n",
       "      <td>-0.000057</td>\n",
       "      <td>-0.000039</td>\n",
       "      <td>-0.000022</td>\n",
       "      <td>-0.000006</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.000024</td>\n",
       "      <td>0.000038</td>\n",
       "      <td>0.000049</td>\n",
       "      <td>0.000059</td>\n",
       "      <td>0.000067</td>\n",
       "      <td>0.000072</td>\n",
       "      <td>0.000075</td>\n",
       "      <td>0.000076</td>\n",
       "      <td>0.000073</td>\n",
       "      <td>0.000068</td>\n",
       "      <td>0.000059</td>\n",
       "      <td>0.000048</td>\n",
       "      <td>0.000034</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>-9.320814e-07</td>\n",
       "      <td>-0.000021</td>\n",
       "      <td>-0.000041</td>\n",
       "      <td>-0.000062</td>\n",
       "      <td>-0.000083</td>\n",
       "      <td>-0.000102</td>\n",
       "      <td>-0.000119</td>\n",
       "      <td>-0.000134</td>\n",
       "      <td>-0.000146</td>\n",
       "      <td>-0.000155</td>\n",
       "      <td>-0.000160</td>\n",
       "      <td>-0.000162</td>\n",
       "      <td>-0.000160</td>\n",
       "      <td>-0.000154</td>\n",
       "      <td>-0.000145</td>\n",
       "      <td>-0.000132</td>\n",
       "      <td>-0.000117</td>\n",
       "      <td>-0.000099</td>\n",
       "      <td>-0.000080</td>\n",
       "      <td>-0.000059</td>\n",
       "      <td>-0.000038</td>\n",
       "      <td>-0.000017</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000024</td>\n",
       "      <td>0.000043</td>\n",
       "      <td>0.000060</td>\n",
       "      <td>0.000074</td>\n",
       "      <td>0.000086</td>\n",
       "      <td>0.000096</td>\n",
       "      <td>0.000103</td>\n",
       "      <td>0.000107</td>\n",
       "      <td>0.000108</td>\n",
       "      <td>0.000107</td>\n",
       "      <td>0.000103</td>\n",
       "      <td>0.000097</td>\n",
       "      <td>0.000090</td>\n",
       "      <td>0.000080</td>\n",
       "      <td>0.000070</td>\n",
       "      <td>0.000060</td>\n",
       "      <td>0.000048</td>\n",
       "      <td>0.000037</td>\n",
       "      <td>0.000026</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>-0.000002</td>\n",
       "      <td>-0.000010</td>\n",
       "      <td>-0.000017</td>\n",
       "      <td>-0.000023</td>\n",
       "      <td>-0.000027</td>\n",
       "      <td>-0.000031</td>\n",
       "      <td>-0.000034</td>\n",
       "      <td>-0.000036</td>\n",
       "      <td>-0.000037</td>\n",
       "      <td>-0.000038</td>\n",
       "      <td>-0.000037</td>\n",
       "      <td>-0.000037</td>\n",
       "      <td>-0.000035</td>\n",
       "      <td>-0.000033</td>\n",
       "      <td>-0.000031</td>\n",
       "      <td>-0.000028</td>\n",
       "      <td>-0.000026</td>\n",
       "      <td>-0.000022</td>\n",
       "      <td>-0.000019</td>\n",
       "      <td>-0.000015</td>\n",
       "      <td>-0.000012</td>\n",
       "      <td>-0.000008</td>\n",
       "      <td>-3.979811e-06</td>\n",
       "      <td>2.885685e-08</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>0.000020</td>\n",
       "      <td>0.000024</td>\n",
       "      <td>0.000028</td>\n",
       "      <td>0.000031</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>0.000038</td>\n",
       "      <td>0.000040</td>\n",
       "      <td>0.000043</td>\n",
       "      <td>0.000045</td>\n",
       "      <td>0.000047</td>\n",
       "      <td>0.000049</td>\n",
       "      <td>0.000051</td>\n",
       "      <td>0.000053</td>\n",
       "      <td>0.000055</td>\n",
       "      <td>0.000057</td>\n",
       "      <td>0.000059</td>\n",
       "      <td>0.000060</td>\n",
       "      <td>0.000062</td>\n",
       "      <td>0.000063</td>\n",
       "      <td>0.000065</td>\n",
       "      <td>0.000066</td>\n",
       "      <td>0.000067</td>\n",
       "      <td>0.000067</td>\n",
       "      <td>0.000068</td>\n",
       "      <td>0.000069</td>\n",
       "      <td>0.000070</td>\n",
       "      <td>0.000072</td>\n",
       "      <td>0.000075</td>\n",
       "      <td>0.000079</td>\n",
       "      <td>0.000084</td>\n",
       "      <td>0.000091</td>\n",
       "      <td>0.000098</td>\n",
       "      <td>0.000108</td>\n",
       "      <td>0.000119</td>\n",
       "      <td>0.000131</td>\n",
       "      <td>0.000145</td>\n",
       "      <td>0.000160</td>\n",
       "      <td>0.000177</td>\n",
       "      <td>0.000196</td>\n",
       "      <td>0.000218</td>\n",
       "      <td>0.000241</td>\n",
       "      <td>0.000267</td>\n",
       "      <td>0.000296</td>\n",
       "      <td>0.000327</td>\n",
       "      <td>0.000362</td>\n",
       "      <td>0.000400</td>\n",
       "      <td>0.000441</td>\n",
       "      <td>0.000485</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000158</td>\n",
       "      <td>0.000154</td>\n",
       "      <td>0.000149</td>\n",
       "      <td>0.000144</td>\n",
       "      <td>0.000138</td>\n",
       "      <td>0.000132</td>\n",
       "      <td>0.000127</td>\n",
       "      <td>0.000121</td>\n",
       "      <td>0.000116</td>\n",
       "      <td>0.000112</td>\n",
       "      <td>0.000108</td>\n",
       "      <td>0.000105</td>\n",
       "      <td>0.000104</td>\n",
       "      <td>0.000103</td>\n",
       "      <td>0.000104</td>\n",
       "      <td>0.000106</td>\n",
       "      <td>0.000109</td>\n",
       "      <td>0.000113</td>\n",
       "      <td>0.000119</td>\n",
       "      <td>0.000126</td>\n",
       "      <td>0.000133</td>\n",
       "      <td>0.000142</td>\n",
       "      <td>0.000151</td>\n",
       "      <td>0.000160</td>\n",
       "      <td>0.000170</td>\n",
       "      <td>0.000179</td>\n",
       "      <td>0.000189</td>\n",
       "      <td>0.000198</td>\n",
       "      <td>0.000207</td>\n",
       "      <td>0.000215</td>\n",
       "      <td>0.000222</td>\n",
       "      <td>0.000228</td>\n",
       "      <td>0.000233</td>\n",
       "      <td>0.000237</td>\n",
       "      <td>0.000239</td>\n",
       "      <td>0.000241</td>\n",
       "      <td>0.000240</td>\n",
       "      <td>0.000238</td>\n",
       "      <td>0.000235</td>\n",
       "      <td>0.000230</td>\n",
       "      <td>0.000223</td>\n",
       "      <td>0.000215</td>\n",
       "      <td>0.000205</td>\n",
       "      <td>0.000194</td>\n",
       "      <td>0.000182</td>\n",
       "      <td>0.000168</td>\n",
       "      <td>0.000154</td>\n",
       "      <td>0.000138</td>\n",
       "      <td>0.000122</td>\n",
       "      <td>0.000106</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.000073</td>\n",
       "      <td>0.000056</td>\n",
       "      <td>0.000041</td>\n",
       "      <td>0.000026</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>-1.461575e-06</td>\n",
       "      <td>-0.000013</td>\n",
       "      <td>-0.000024</td>\n",
       "      <td>-0.000033</td>\n",
       "      <td>-0.000041</td>\n",
       "      <td>-0.000047</td>\n",
       "      <td>-0.000052</td>\n",
       "      <td>-0.000055</td>\n",
       "      <td>-0.000056</td>\n",
       "      <td>-0.000056</td>\n",
       "      <td>-0.000055</td>\n",
       "      <td>-0.000053</td>\n",
       "      <td>-0.000050</td>\n",
       "      <td>-0.000046</td>\n",
       "      <td>-0.000041</td>\n",
       "      <td>-0.000036</td>\n",
       "      <td>-0.000030</td>\n",
       "      <td>-0.000024</td>\n",
       "      <td>-0.000018</td>\n",
       "      <td>-0.000013</td>\n",
       "      <td>-0.000007</td>\n",
       "      <td>-0.000002</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>0.000020</td>\n",
       "      <td>0.000024</td>\n",
       "      <td>0.000027</td>\n",
       "      <td>0.000031</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>0.000039</td>\n",
       "      <td>0.000043</td>\n",
       "      <td>0.000047</td>\n",
       "      <td>0.000051</td>\n",
       "      <td>0.000055</td>\n",
       "      <td>0.000060</td>\n",
       "      <td>0.000065</td>\n",
       "      <td>0.000070</td>\n",
       "      <td>0.000075</td>\n",
       "      <td>0.000080</td>\n",
       "      <td>0.000085</td>\n",
       "      <td>0.000091</td>\n",
       "      <td>0.000096</td>\n",
       "      <td>0.000102</td>\n",
       "      <td>0.000108</td>\n",
       "      <td>0.000114</td>\n",
       "      <td>0.000120</td>\n",
       "      <td>0.000127</td>\n",
       "      <td>0.000133</td>\n",
       "      <td>0.000141</td>\n",
       "      <td>0.000148</td>\n",
       "      <td>0.000156</td>\n",
       "      <td>0.000165</td>\n",
       "      <td>0.000174</td>\n",
       "      <td>0.000183</td>\n",
       "      <td>0.000193</td>\n",
       "      <td>0.000204</td>\n",
       "      <td>0.000215</td>\n",
       "      <td>0.000226</td>\n",
       "      <td>0.000238</td>\n",
       "      <td>0.000250</td>\n",
       "      <td>0.000262</td>\n",
       "      <td>0.000275</td>\n",
       "      <td>0.000287</td>\n",
       "      <td>0.000298</td>\n",
       "      <td>0.000309</td>\n",
       "      <td>0.000320</td>\n",
       "      <td>0.000329</td>\n",
       "      <td>0.000337</td>\n",
       "      <td>0.000343</td>\n",
       "      <td>0.000348</td>\n",
       "      <td>0.000350</td>\n",
       "      <td>0.000350</td>\n",
       "      <td>0.000347</td>\n",
       "      <td>0.000341</td>\n",
       "      <td>0.000332</td>\n",
       "      <td>0.000320</td>\n",
       "      <td>0.000305</td>\n",
       "      <td>0.000287</td>\n",
       "      <td>0.000266</td>\n",
       "      <td>0.000242</td>\n",
       "      <td>0.000216</td>\n",
       "      <td>0.000187</td>\n",
       "      <td>0.000156</td>\n",
       "      <td>0.000124</td>\n",
       "      <td>0.000091</td>\n",
       "      <td>0.000057</td>\n",
       "      <td>0.000023</td>\n",
       "      <td>-0.000011</td>\n",
       "      <td>-0.000045</td>\n",
       "      <td>-0.000076</td>\n",
       "      <td>-0.000106</td>\n",
       "      <td>-0.000133</td>\n",
       "      <td>-0.000158</td>\n",
       "      <td>-0.000179</td>\n",
       "      <td>-0.000196</td>\n",
       "      <td>-0.000210</td>\n",
       "      <td>-0.000220</td>\n",
       "      <td>-0.000225</td>\n",
       "      <td>-0.000226</td>\n",
       "      <td>-0.000223</td>\n",
       "      <td>-0.000215</td>\n",
       "      <td>-0.000203</td>\n",
       "      <td>-0.000187</td>\n",
       "      <td>-0.000168</td>\n",
       "      <td>-0.000146</td>\n",
       "      <td>-0.000121</td>\n",
       "      <td>-0.000094</td>\n",
       "      <td>-0.000066</td>\n",
       "      <td>-0.000036</td>\n",
       "      <td>-7.056503e-06</td>\n",
       "      <td>0.000022</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>0.000076</td>\n",
       "      <td>0.000101</td>\n",
       "      <td>0.000123</td>\n",
       "      <td>0.000142</td>\n",
       "      <td>0.000158</td>\n",
       "      <td>0.000170</td>\n",
       "      <td>0.000179</td>\n",
       "      <td>0.000184</td>\n",
       "      <td>0.000185</td>\n",
       "      <td>0.000183</td>\n",
       "      <td>0.000177</td>\n",
       "      <td>0.000168</td>\n",
       "      <td>0.000156</td>\n",
       "      <td>0.000140</td>\n",
       "      <td>0.000123</td>\n",
       "      <td>0.000103</td>\n",
       "      <td>0.000082</td>\n",
       "      <td>0.000060</td>\n",
       "      <td>0.000037</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>-0.000010</td>\n",
       "      <td>-0.000032</td>\n",
       "      <td>-0.000054</td>\n",
       "      <td>-0.000073</td>\n",
       "      <td>-0.000091</td>\n",
       "      <td>-0.000107</td>\n",
       "      <td>-0.000121</td>\n",
       "      <td>-0.000132</td>\n",
       "      <td>-0.000140</td>\n",
       "      <td>-0.000145</td>\n",
       "      <td>-0.000148</td>\n",
       "      <td>-0.000148</td>\n",
       "      <td>-0.000145</td>\n",
       "      <td>-0.000140</td>\n",
       "      <td>-0.000132</td>\n",
       "      <td>-0.000122</td>\n",
       "      <td>-0.000110</td>\n",
       "      <td>-0.000097</td>\n",
       "      <td>-0.000082</td>\n",
       "      <td>-0.000066</td>\n",
       "      <td>-0.000049</td>\n",
       "      <td>-0.000032</td>\n",
       "      <td>-1.514253e-05</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>0.000034</td>\n",
       "      <td>0.000049</td>\n",
       "      <td>0.000063</td>\n",
       "      <td>0.000075</td>\n",
       "      <td>0.000086</td>\n",
       "      <td>0.000096</td>\n",
       "      <td>0.000104</td>\n",
       "      <td>0.000110</td>\n",
       "      <td>0.000114</td>\n",
       "      <td>0.000117</td>\n",
       "      <td>0.000119</td>\n",
       "      <td>0.000119</td>\n",
       "      <td>0.000118</td>\n",
       "      <td>0.000116</td>\n",
       "      <td>0.000113</td>\n",
       "      <td>0.000109</td>\n",
       "      <td>0.000104</td>\n",
       "      <td>0.000098</td>\n",
       "      <td>0.000092</td>\n",
       "      <td>0.000085</td>\n",
       "      <td>0.000078</td>\n",
       "      <td>0.000071</td>\n",
       "      <td>0.000064</td>\n",
       "      <td>0.000057</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>0.000043</td>\n",
       "      <td>0.000037</td>\n",
       "      <td>0.000031</td>\n",
       "      <td>0.000026</td>\n",
       "      <td>0.000022</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>0.000019</td>\n",
       "      <td>0.000024</td>\n",
       "      <td>0.000029</td>\n",
       "      <td>0.000034</td>\n",
       "      <td>0.000041</td>\n",
       "      <td>0.000049</td>\n",
       "      <td>0.000057</td>\n",
       "      <td>0.000066</td>\n",
       "      <td>0.000076</td>\n",
       "      <td>0.000086</td>\n",
       "      <td>0.000096</td>\n",
       "      <td>0.000107</td>\n",
       "      <td>0.000117</td>\n",
       "      <td>0.000128</td>\n",
       "      <td>0.000139</td>\n",
       "      <td>0.000149</td>\n",
       "      <td>0.000159</td>\n",
       "      <td>0.000168</td>\n",
       "      <td>0.000177</td>\n",
       "      <td>0.000185</td>\n",
       "      <td>0.000191</td>\n",
       "      <td>0.000196</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>0.000203</td>\n",
       "      <td>0.000204</td>\n",
       "      <td>0.000203</td>\n",
       "      <td>0.000201</td>\n",
       "      <td>0.000197</td>\n",
       "      <td>0.000192</td>\n",
       "      <td>0.000185</td>\n",
       "      <td>0.000177</td>\n",
       "      <td>0.000168</td>\n",
       "      <td>0.000158</td>\n",
       "      <td>0.000146</td>\n",
       "      <td>0.000133</td>\n",
       "      <td>0.000120</td>\n",
       "      <td>0.000106</td>\n",
       "      <td>0.000091</td>\n",
       "      <td>0.000077</td>\n",
       "      <td>0.000062</td>\n",
       "      <td>0.000048</td>\n",
       "      <td>0.000034</td>\n",
       "      <td>0.000021</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>-0.000004</td>\n",
       "      <td>-0.000015</td>\n",
       "      <td>-0.000024</td>\n",
       "      <td>-0.000032</td>\n",
       "      <td>-0.000039</td>\n",
       "      <td>-0.000043</td>\n",
       "      <td>-0.000045</td>\n",
       "      <td>-0.000045</td>\n",
       "      <td>-0.000043</td>\n",
       "      <td>-0.000039</td>\n",
       "      <td>-0.000033</td>\n",
       "      <td>-0.000026</td>\n",
       "      <td>-0.000016</td>\n",
       "      <td>-0.000005</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000023</td>\n",
       "      <td>0.000039</td>\n",
       "      <td>0.000058</td>\n",
       "      <td>0.000077</td>\n",
       "      <td>0.000098</td>\n",
       "      <td>0.000120</td>\n",
       "      <td>0.000142</td>\n",
       "      <td>0.000164</td>\n",
       "      <td>0.000185</td>\n",
       "      <td>0.000206</td>\n",
       "      <td>0.000226</td>\n",
       "      <td>0.000245</td>\n",
       "      <td>0.000262</td>\n",
       "      <td>0.000277</td>\n",
       "      <td>0.000291</td>\n",
       "      <td>0.000302</td>\n",
       "      <td>0.000310</td>\n",
       "      <td>0.000315</td>\n",
       "      <td>0.000315</td>\n",
       "      <td>0.000311</td>\n",
       "      <td>0.000302</td>\n",
       "      <td>0.000287</td>\n",
       "      <td>0.000267</td>\n",
       "      <td>0.000242</td>\n",
       "      <td>0.000211</td>\n",
       "      <td>0.000177</td>\n",
       "      <td>0.000139</td>\n",
       "      <td>0.000099</td>\n",
       "      <td>0.000057</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>-0.000030</td>\n",
       "      <td>-0.000073</td>\n",
       "      <td>-0.000116</td>\n",
       "      <td>-0.000157</td>\n",
       "      <td>-0.000196</td>\n",
       "      <td>-0.000231</td>\n",
       "      <td>-0.000263</td>\n",
       "      <td>-0.000290</td>\n",
       "      <td>-0.000311</td>\n",
       "      <td>-0.000327</td>\n",
       "      <td>-0.000336</td>\n",
       "      <td>-0.000338</td>\n",
       "      <td>-0.000333</td>\n",
       "      <td>-0.000322</td>\n",
       "      <td>-0.000305</td>\n",
       "      <td>-0.000282</td>\n",
       "      <td>-0.000253</td>\n",
       "      <td>-0.000218</td>\n",
       "      <td>-0.000178</td>\n",
       "      <td>-0.000134</td>\n",
       "      <td>-0.000084</td>\n",
       "      <td>-0.000030</td>\n",
       "      <td>0.000027</td>\n",
       "      <td>0.000086</td>\n",
       "      <td>0.000146</td>\n",
       "      <td>0.000206</td>\n",
       "      <td>0.000266</td>\n",
       "      <td>0.000323</td>\n",
       "      <td>0.000378</td>\n",
       "      <td>0.000431</td>\n",
       "      <td>0.000482</td>\n",
       "      <td>0.000531</td>\n",
       "      <td>0.000579</td>\n",
       "      <td>0.000626</td>\n",
       "      <td>0.000671</td>\n",
       "      <td>0.000712</td>\n",
       "      <td>0.000745</td>\n",
       "      <td>0.000767</td>\n",
       "      <td>0.000774</td>\n",
       "      <td>0.000760</td>\n",
       "      <td>0.000721</td>\n",
       "      <td>0.000656</td>\n",
       "      <td>0.000562</td>\n",
       "      <td>0.000442</td>\n",
       "      <td>0.000296</td>\n",
       "      <td>0.000128</td>\n",
       "      <td>-0.000056</td>\n",
       "      <td>-0.000251</td>\n",
       "      <td>-0.000449</td>\n",
       "      <td>-0.000646</td>\n",
       "      <td>-0.000837</td>\n",
       "      <td>-0.001017</td>\n",
       "      <td>-0.001182</td>\n",
       "      <td>-0.001330</td>\n",
       "      <td>-0.001458</td>\n",
       "      <td>-0.001565</td>\n",
       "      <td>-0.001649</td>\n",
       "      <td>-0.001708</td>\n",
       "      <td>-0.001740</td>\n",
       "      <td>-0.001743</td>\n",
       "      <td>-0.001718</td>\n",
       "      <td>-0.001663</td>\n",
       "      <td>-0.001579</td>\n",
       "      <td>-0.001467</td>\n",
       "      <td>-0.001330</td>\n",
       "      <td>-0.001169</td>\n",
       "      <td>-0.000987</td>\n",
       "      <td>-0.000789</td>\n",
       "      <td>-0.000578</td>\n",
       "      <td>-0.000360</td>\n",
       "      <td>-0.000141</td>\n",
       "      <td>0.000076</td>\n",
       "      <td>0.000283</td>\n",
       "      <td>0.000475</td>\n",
       "      <td>0.000646</td>\n",
       "      <td>0.000793</td>\n",
       "      <td>0.000911</td>\n",
       "      <td>0.000999</td>\n",
       "      <td>0.001054</td>\n",
       "      <td>0.001078</td>\n",
       "      <td>0.001072</td>\n",
       "      <td>0.001039</td>\n",
       "      <td>0.000983</td>\n",
       "      <td>0.000910</td>\n",
       "      <td>0.000823</td>\n",
       "      <td>0.000726</td>\n",
       "      <td>0.000623</td>\n",
       "      <td>0.000516</td>\n",
       "      <td>0.000408</td>\n",
       "      <td>0.000298</td>\n",
       "      <td>0.000189</td>\n",
       "      <td>0.000081</td>\n",
       "      <td>-0.000024</td>\n",
       "      <td>-0.000125</td>\n",
       "      <td>-0.000220</td>\n",
       "      <td>-0.000307</td>\n",
       "      <td>-0.000385</td>\n",
       "      <td>-0.000453</td>\n",
       "      <td>-0.000508</td>\n",
       "      <td>-0.000551</td>\n",
       "      <td>-0.000582</td>\n",
       "      <td>-0.000603</td>\n",
       "      <td>-0.000613</td>\n",
       "      <td>-0.000614</td>\n",
       "      <td>-0.000608</td>\n",
       "      <td>-0.000596</td>\n",
       "      <td>-0.000578</td>\n",
       "      <td>-0.000556</td>\n",
       "      <td>-0.000531</td>\n",
       "      <td>-0.000502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000084</td>\n",
       "      <td>0.000099</td>\n",
       "      <td>0.000118</td>\n",
       "      <td>0.000140</td>\n",
       "      <td>0.000165</td>\n",
       "      <td>0.000195</td>\n",
       "      <td>0.000227</td>\n",
       "      <td>0.000262</td>\n",
       "      <td>0.000297</td>\n",
       "      <td>0.000333</td>\n",
       "      <td>0.000368</td>\n",
       "      <td>0.000400</td>\n",
       "      <td>0.000428</td>\n",
       "      <td>0.000451</td>\n",
       "      <td>0.000469</td>\n",
       "      <td>0.000480</td>\n",
       "      <td>0.000485</td>\n",
       "      <td>0.000484</td>\n",
       "      <td>0.000477</td>\n",
       "      <td>0.000465</td>\n",
       "      <td>0.000450</td>\n",
       "      <td>0.000431</td>\n",
       "      <td>0.000410</td>\n",
       "      <td>0.000386</td>\n",
       "      <td>0.000362</td>\n",
       "      <td>0.000337</td>\n",
       "      <td>0.000311</td>\n",
       "      <td>0.000285</td>\n",
       "      <td>0.000259</td>\n",
       "      <td>0.000234</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000080</td>\n",
       "      <td>0.000084</td>\n",
       "      <td>0.000087</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.000091</td>\n",
       "      <td>0.000093</td>\n",
       "      <td>0.000094</td>\n",
       "      <td>0.000095</td>\n",
       "      <td>0.000096</td>\n",
       "      <td>0.000096</td>\n",
       "      <td>0.000096</td>\n",
       "      <td>0.000096</td>\n",
       "      <td>0.000096</td>\n",
       "      <td>0.000096</td>\n",
       "      <td>0.000096</td>\n",
       "      <td>0.000096</td>\n",
       "      <td>0.000096</td>\n",
       "      <td>0.000096</td>\n",
       "      <td>0.000096</td>\n",
       "      <td>0.000096</td>\n",
       "      <td>0.000096</td>\n",
       "      <td>0.000096</td>\n",
       "      <td>0.000095</td>\n",
       "      <td>0.000095</td>\n",
       "      <td>0.000094</td>\n",
       "      <td>0.000093</td>\n",
       "      <td>0.000092</td>\n",
       "      <td>0.000091</td>\n",
       "      <td>0.000090</td>\n",
       "      <td>0.000088</td>\n",
       "      <td>0.000087</td>\n",
       "      <td>0.000086</td>\n",
       "      <td>0.000084</td>\n",
       "      <td>0.000083</td>\n",
       "      <td>0.000082</td>\n",
       "      <td>0.000080</td>\n",
       "      <td>0.000079</td>\n",
       "      <td>0.000078</td>\n",
       "      <td>0.000077</td>\n",
       "      <td>0.000077</td>\n",
       "      <td>0.000076</td>\n",
       "      <td>0.000075</td>\n",
       "      <td>0.000074</td>\n",
       "      <td>0.000073</td>\n",
       "      <td>0.000073</td>\n",
       "      <td>0.000072</td>\n",
       "      <td>0.000071</td>\n",
       "      <td>0.000071</td>\n",
       "      <td>0.000070</td>\n",
       "      <td>0.000069</td>\n",
       "      <td>0.000069</td>\n",
       "      <td>0.000068</td>\n",
       "      <td>0.000067</td>\n",
       "      <td>0.000067</td>\n",
       "      <td>0.000066</td>\n",
       "      <td>0.000066</td>\n",
       "      <td>0.000065</td>\n",
       "      <td>0.000065</td>\n",
       "      <td>0.000064</td>\n",
       "      <td>0.000064</td>\n",
       "      <td>0.000063</td>\n",
       "      <td>0.000063</td>\n",
       "      <td>0.000062</td>\n",
       "      <td>0.000062</td>\n",
       "      <td>0.000061</td>\n",
       "      <td>0.000061</td>\n",
       "      <td>0.000060</td>\n",
       "      <td>0.000059</td>\n",
       "      <td>0.000059</td>\n",
       "      <td>0.000058</td>\n",
       "      <td>0.000057</td>\n",
       "      <td>0.000056</td>\n",
       "      <td>0.000055</td>\n",
       "      <td>0.000054</td>\n",
       "      <td>0.000054</td>\n",
       "      <td>0.000053</td>\n",
       "      <td>0.000052</td>\n",
       "      <td>0.000051</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>0.000049</td>\n",
       "      <td>0.000049</td>\n",
       "      <td>0.000048</td>\n",
       "      <td>0.000048</td>\n",
       "      <td>0.000048</td>\n",
       "      <td>0.000047</td>\n",
       "      <td>0.000047</td>\n",
       "      <td>0.000047</td>\n",
       "      <td>0.000047</td>\n",
       "      <td>0.000047</td>\n",
       "      <td>0.000048</td>\n",
       "      <td>0.000048</td>\n",
       "      <td>0.000048</td>\n",
       "      <td>0.000048</td>\n",
       "      <td>0.000049</td>\n",
       "      <td>0.000049</td>\n",
       "      <td>0.000049</td>\n",
       "      <td>0.000049</td>\n",
       "      <td>0.000049</td>\n",
       "      <td>0.000049</td>\n",
       "      <td>0.000049</td>\n",
       "      <td>0.000048</td>\n",
       "      <td>0.000047</td>\n",
       "      <td>0.000046</td>\n",
       "      <td>0.000045</td>\n",
       "      <td>0.000043</td>\n",
       "      <td>0.000041</td>\n",
       "      <td>0.000038</td>\n",
       "      <td>0.000036</td>\n",
       "      <td>0.000033</td>\n",
       "      <td>0.000031</td>\n",
       "      <td>0.000028</td>\n",
       "      <td>0.000025</td>\n",
       "      <td>0.000022</td>\n",
       "      <td>0.00002</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>0.000021</td>\n",
       "      <td>0.000025</td>\n",
       "      <td>0.000029</td>\n",
       "      <td>0.000034</td>\n",
       "      <td>0.000039</td>\n",
       "      <td>0.000046</td>\n",
       "      <td>0.000053</td>\n",
       "      <td>0.000061</td>\n",
       "      <td>0.000069</td>\n",
       "      <td>0.000078</td>\n",
       "      <td>0.000088</td>\n",
       "      <td>0.000098</td>\n",
       "      <td>0.000108</td>\n",
       "      <td>0.000118</td>\n",
       "      <td>0.000127</td>\n",
       "      <td>0.000136</td>\n",
       "      <td>0.000144</td>\n",
       "      <td>0.000151</td>\n",
       "      <td>0.000157</td>\n",
       "      <td>0.000160</td>\n",
       "      <td>0.000162</td>\n",
       "      <td>0.000162</td>\n",
       "      <td>0.000160</td>\n",
       "      <td>0.000155</td>\n",
       "      <td>0.000148</td>\n",
       "      <td>0.000139</td>\n",
       "      <td>0.000128</td>\n",
       "      <td>0.000116</td>\n",
       "      <td>0.000102</td>\n",
       "      <td>0.000087</td>\n",
       "      <td>0.000071</td>\n",
       "      <td>0.000056</td>\n",
       "      <td>0.000040</td>\n",
       "      <td>0.000026</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>-0.000007</td>\n",
       "      <td>-0.000014</td>\n",
       "      <td>-0.000017</td>\n",
       "      <td>-0.000017</td>\n",
       "      <td>-0.000015</td>\n",
       "      <td>-0.000009</td>\n",
       "      <td>-8.106343e-07</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.000022</td>\n",
       "      <td>0.000036</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>0.000064</td>\n",
       "      <td>0.000077</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.000099</td>\n",
       "      <td>0.000106</td>\n",
       "      <td>0.000110</td>\n",
       "      <td>0.000111</td>\n",
       "      <td>0.000109</td>\n",
       "      <td>0.000103</td>\n",
       "      <td>0.000094</td>\n",
       "      <td>0.000083</td>\n",
       "      <td>0.000068</td>\n",
       "      <td>0.000051</td>\n",
       "      <td>0.000033</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>-0.000008</td>\n",
       "      <td>-0.000029</td>\n",
       "      <td>-0.000050</td>\n",
       "      <td>-0.000071</td>\n",
       "      <td>-0.000092</td>\n",
       "      <td>-0.000111</td>\n",
       "      <td>-0.000129</td>\n",
       "      <td>-0.000145</td>\n",
       "      <td>-0.000159</td>\n",
       "      <td>-0.000171</td>\n",
       "      <td>-0.000180</td>\n",
       "      <td>-0.000186</td>\n",
       "      <td>-0.000190</td>\n",
       "      <td>-0.000191</td>\n",
       "      <td>-0.000189</td>\n",
       "      <td>-0.000184</td>\n",
       "      <td>-0.000177</td>\n",
       "      <td>-0.000167</td>\n",
       "      <td>-0.000155</td>\n",
       "      <td>-0.000141</td>\n",
       "      <td>-0.000126</td>\n",
       "      <td>-0.000109</td>\n",
       "      <td>-0.000092</td>\n",
       "      <td>-0.000074</td>\n",
       "      <td>-0.000057</td>\n",
       "      <td>-0.000039</td>\n",
       "      <td>-0.000022</td>\n",
       "      <td>-0.000005</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.000025</td>\n",
       "      <td>0.000038</td>\n",
       "      <td>0.000049</td>\n",
       "      <td>0.000059</td>\n",
       "      <td>0.000067</td>\n",
       "      <td>0.000072</td>\n",
       "      <td>0.000075</td>\n",
       "      <td>0.000076</td>\n",
       "      <td>0.000073</td>\n",
       "      <td>0.000068</td>\n",
       "      <td>0.000059</td>\n",
       "      <td>0.000048</td>\n",
       "      <td>0.000034</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>-9.590353e-07</td>\n",
       "      <td>-0.000021</td>\n",
       "      <td>-0.000041</td>\n",
       "      <td>-0.000062</td>\n",
       "      <td>-0.000083</td>\n",
       "      <td>-0.000102</td>\n",
       "      <td>-0.000119</td>\n",
       "      <td>-0.000134</td>\n",
       "      <td>-0.000146</td>\n",
       "      <td>-0.000155</td>\n",
       "      <td>-0.000160</td>\n",
       "      <td>-0.000162</td>\n",
       "      <td>-0.000160</td>\n",
       "      <td>-0.000154</td>\n",
       "      <td>-0.000144</td>\n",
       "      <td>-0.000132</td>\n",
       "      <td>-0.000117</td>\n",
       "      <td>-0.000099</td>\n",
       "      <td>-0.000080</td>\n",
       "      <td>-0.000059</td>\n",
       "      <td>-0.000038</td>\n",
       "      <td>-0.000017</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000024</td>\n",
       "      <td>0.000043</td>\n",
       "      <td>0.000060</td>\n",
       "      <td>0.000074</td>\n",
       "      <td>0.000086</td>\n",
       "      <td>0.000096</td>\n",
       "      <td>0.000102</td>\n",
       "      <td>0.000106</td>\n",
       "      <td>0.000108</td>\n",
       "      <td>0.000106</td>\n",
       "      <td>0.000103</td>\n",
       "      <td>0.000097</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.000080</td>\n",
       "      <td>0.000070</td>\n",
       "      <td>0.000059</td>\n",
       "      <td>0.000048</td>\n",
       "      <td>0.000037</td>\n",
       "      <td>0.000026</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>-0.000002</td>\n",
       "      <td>-0.000010</td>\n",
       "      <td>-0.000017</td>\n",
       "      <td>-0.000023</td>\n",
       "      <td>-0.000027</td>\n",
       "      <td>-0.000031</td>\n",
       "      <td>-0.000034</td>\n",
       "      <td>-0.000036</td>\n",
       "      <td>-0.000037</td>\n",
       "      <td>-0.000038</td>\n",
       "      <td>-0.000037</td>\n",
       "      <td>-0.000037</td>\n",
       "      <td>-0.000035</td>\n",
       "      <td>-0.000033</td>\n",
       "      <td>-0.000031</td>\n",
       "      <td>-0.000028</td>\n",
       "      <td>-0.000025</td>\n",
       "      <td>-0.000022</td>\n",
       "      <td>-0.000019</td>\n",
       "      <td>-0.000016</td>\n",
       "      <td>-0.000012</td>\n",
       "      <td>-0.000008</td>\n",
       "      <td>-4.176842e-06</td>\n",
       "      <td>-1.985787e-07</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>0.000020</td>\n",
       "      <td>0.000024</td>\n",
       "      <td>0.000028</td>\n",
       "      <td>0.000031</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>0.000038</td>\n",
       "      <td>0.000041</td>\n",
       "      <td>0.000043</td>\n",
       "      <td>0.000045</td>\n",
       "      <td>0.000048</td>\n",
       "      <td>0.000049</td>\n",
       "      <td>0.000051</td>\n",
       "      <td>0.000053</td>\n",
       "      <td>0.000055</td>\n",
       "      <td>0.000057</td>\n",
       "      <td>0.000059</td>\n",
       "      <td>0.000061</td>\n",
       "      <td>0.000062</td>\n",
       "      <td>0.000064</td>\n",
       "      <td>0.000065</td>\n",
       "      <td>0.000066</td>\n",
       "      <td>0.000067</td>\n",
       "      <td>0.000067</td>\n",
       "      <td>0.000068</td>\n",
       "      <td>0.000069</td>\n",
       "      <td>0.000070</td>\n",
       "      <td>0.000072</td>\n",
       "      <td>0.000075</td>\n",
       "      <td>0.000079</td>\n",
       "      <td>0.000084</td>\n",
       "      <td>0.000090</td>\n",
       "      <td>0.000098</td>\n",
       "      <td>0.000107</td>\n",
       "      <td>0.000118</td>\n",
       "      <td>0.000131</td>\n",
       "      <td>0.000144</td>\n",
       "      <td>0.000160</td>\n",
       "      <td>0.000177</td>\n",
       "      <td>0.000196</td>\n",
       "      <td>0.000217</td>\n",
       "      <td>0.000241</td>\n",
       "      <td>0.000267</td>\n",
       "      <td>0.000296</td>\n",
       "      <td>0.000327</td>\n",
       "      <td>0.000362</td>\n",
       "      <td>0.000400</td>\n",
       "      <td>0.000441</td>\n",
       "      <td>0.000486</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000156</td>\n",
       "      <td>0.000152</td>\n",
       "      <td>0.000147</td>\n",
       "      <td>0.000142</td>\n",
       "      <td>0.000136</td>\n",
       "      <td>0.000131</td>\n",
       "      <td>0.000125</td>\n",
       "      <td>0.000120</td>\n",
       "      <td>0.000115</td>\n",
       "      <td>0.000111</td>\n",
       "      <td>0.000108</td>\n",
       "      <td>0.000106</td>\n",
       "      <td>0.000104</td>\n",
       "      <td>0.000104</td>\n",
       "      <td>0.000105</td>\n",
       "      <td>0.000107</td>\n",
       "      <td>0.000111</td>\n",
       "      <td>0.000115</td>\n",
       "      <td>0.000121</td>\n",
       "      <td>0.000127</td>\n",
       "      <td>0.000135</td>\n",
       "      <td>0.000143</td>\n",
       "      <td>0.000151</td>\n",
       "      <td>0.000161</td>\n",
       "      <td>0.000170</td>\n",
       "      <td>0.000179</td>\n",
       "      <td>0.000189</td>\n",
       "      <td>0.000198</td>\n",
       "      <td>0.000206</td>\n",
       "      <td>0.000214</td>\n",
       "      <td>0.000221</td>\n",
       "      <td>0.000228</td>\n",
       "      <td>0.000233</td>\n",
       "      <td>0.000237</td>\n",
       "      <td>0.000239</td>\n",
       "      <td>0.000240</td>\n",
       "      <td>0.000240</td>\n",
       "      <td>0.000238</td>\n",
       "      <td>0.000235</td>\n",
       "      <td>0.000230</td>\n",
       "      <td>0.000223</td>\n",
       "      <td>0.000215</td>\n",
       "      <td>0.000206</td>\n",
       "      <td>0.000195</td>\n",
       "      <td>0.000183</td>\n",
       "      <td>0.000169</td>\n",
       "      <td>0.000154</td>\n",
       "      <td>0.000139</td>\n",
       "      <td>0.000123</td>\n",
       "      <td>0.000106</td>\n",
       "      <td>0.000090</td>\n",
       "      <td>0.000073</td>\n",
       "      <td>0.000057</td>\n",
       "      <td>0.000041</td>\n",
       "      <td>0.000026</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>-1.656472e-06</td>\n",
       "      <td>-0.000014</td>\n",
       "      <td>-0.000024</td>\n",
       "      <td>-0.000033</td>\n",
       "      <td>-0.000041</td>\n",
       "      <td>-0.000047</td>\n",
       "      <td>-0.000052</td>\n",
       "      <td>-0.000055</td>\n",
       "      <td>-0.000057</td>\n",
       "      <td>-0.000057</td>\n",
       "      <td>-0.000056</td>\n",
       "      <td>-0.000054</td>\n",
       "      <td>-0.000050</td>\n",
       "      <td>-0.000046</td>\n",
       "      <td>-0.000042</td>\n",
       "      <td>-0.000037</td>\n",
       "      <td>-0.000031</td>\n",
       "      <td>-0.000025</td>\n",
       "      <td>-0.000019</td>\n",
       "      <td>-0.000014</td>\n",
       "      <td>-0.000008</td>\n",
       "      <td>-0.000003</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>0.000020</td>\n",
       "      <td>0.000024</td>\n",
       "      <td>0.000028</td>\n",
       "      <td>0.000031</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>0.000039</td>\n",
       "      <td>0.000043</td>\n",
       "      <td>0.000047</td>\n",
       "      <td>0.000051</td>\n",
       "      <td>0.000056</td>\n",
       "      <td>0.000060</td>\n",
       "      <td>0.000065</td>\n",
       "      <td>0.000070</td>\n",
       "      <td>0.000075</td>\n",
       "      <td>0.000080</td>\n",
       "      <td>0.000085</td>\n",
       "      <td>0.000090</td>\n",
       "      <td>0.000096</td>\n",
       "      <td>0.000102</td>\n",
       "      <td>0.000108</td>\n",
       "      <td>0.000114</td>\n",
       "      <td>0.000120</td>\n",
       "      <td>0.000127</td>\n",
       "      <td>0.000134</td>\n",
       "      <td>0.000141</td>\n",
       "      <td>0.000149</td>\n",
       "      <td>0.000157</td>\n",
       "      <td>0.000165</td>\n",
       "      <td>0.000174</td>\n",
       "      <td>0.000184</td>\n",
       "      <td>0.000194</td>\n",
       "      <td>0.000205</td>\n",
       "      <td>0.000216</td>\n",
       "      <td>0.000227</td>\n",
       "      <td>0.000239</td>\n",
       "      <td>0.000251</td>\n",
       "      <td>0.000263</td>\n",
       "      <td>0.000275</td>\n",
       "      <td>0.000287</td>\n",
       "      <td>0.000298</td>\n",
       "      <td>0.000309</td>\n",
       "      <td>0.000319</td>\n",
       "      <td>0.000328</td>\n",
       "      <td>0.000336</td>\n",
       "      <td>0.000342</td>\n",
       "      <td>0.000347</td>\n",
       "      <td>0.000349</td>\n",
       "      <td>0.000349</td>\n",
       "      <td>0.000346</td>\n",
       "      <td>0.000340</td>\n",
       "      <td>0.000331</td>\n",
       "      <td>0.000319</td>\n",
       "      <td>0.000304</td>\n",
       "      <td>0.000286</td>\n",
       "      <td>0.000265</td>\n",
       "      <td>0.000242</td>\n",
       "      <td>0.000215</td>\n",
       "      <td>0.000187</td>\n",
       "      <td>0.000156</td>\n",
       "      <td>0.000124</td>\n",
       "      <td>0.000091</td>\n",
       "      <td>0.000057</td>\n",
       "      <td>0.000023</td>\n",
       "      <td>-0.000011</td>\n",
       "      <td>-0.000044</td>\n",
       "      <td>-0.000076</td>\n",
       "      <td>-0.000106</td>\n",
       "      <td>-0.000133</td>\n",
       "      <td>-0.000158</td>\n",
       "      <td>-0.000179</td>\n",
       "      <td>-0.000197</td>\n",
       "      <td>-0.000210</td>\n",
       "      <td>-0.000220</td>\n",
       "      <td>-0.000226</td>\n",
       "      <td>-0.000227</td>\n",
       "      <td>-0.000223</td>\n",
       "      <td>-0.000215</td>\n",
       "      <td>-0.000204</td>\n",
       "      <td>-0.000188</td>\n",
       "      <td>-0.000169</td>\n",
       "      <td>-0.000146</td>\n",
       "      <td>-0.000121</td>\n",
       "      <td>-0.000094</td>\n",
       "      <td>-0.000065</td>\n",
       "      <td>-0.000036</td>\n",
       "      <td>-6.630909e-06</td>\n",
       "      <td>0.000022</td>\n",
       "      <td>0.000051</td>\n",
       "      <td>0.000077</td>\n",
       "      <td>0.000102</td>\n",
       "      <td>0.000124</td>\n",
       "      <td>0.000143</td>\n",
       "      <td>0.000159</td>\n",
       "      <td>0.000171</td>\n",
       "      <td>0.000180</td>\n",
       "      <td>0.000185</td>\n",
       "      <td>0.000186</td>\n",
       "      <td>0.000184</td>\n",
       "      <td>0.000178</td>\n",
       "      <td>0.000168</td>\n",
       "      <td>0.000155</td>\n",
       "      <td>0.000140</td>\n",
       "      <td>0.000122</td>\n",
       "      <td>0.000103</td>\n",
       "      <td>0.000082</td>\n",
       "      <td>0.000059</td>\n",
       "      <td>0.000036</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>-0.000010</td>\n",
       "      <td>-0.000033</td>\n",
       "      <td>-0.000054</td>\n",
       "      <td>-0.000074</td>\n",
       "      <td>-0.000092</td>\n",
       "      <td>-0.000108</td>\n",
       "      <td>-0.000121</td>\n",
       "      <td>-0.000132</td>\n",
       "      <td>-0.000141</td>\n",
       "      <td>-0.000146</td>\n",
       "      <td>-0.000149</td>\n",
       "      <td>-0.000149</td>\n",
       "      <td>-0.000147</td>\n",
       "      <td>-0.000141</td>\n",
       "      <td>-0.000133</td>\n",
       "      <td>-0.000123</td>\n",
       "      <td>-0.000111</td>\n",
       "      <td>-0.000098</td>\n",
       "      <td>-0.000083</td>\n",
       "      <td>-0.000067</td>\n",
       "      <td>-0.000050</td>\n",
       "      <td>-0.000033</td>\n",
       "      <td>-1.570117e-05</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>0.000034</td>\n",
       "      <td>0.000049</td>\n",
       "      <td>0.000062</td>\n",
       "      <td>0.000075</td>\n",
       "      <td>0.000086</td>\n",
       "      <td>0.000095</td>\n",
       "      <td>0.000103</td>\n",
       "      <td>0.000110</td>\n",
       "      <td>0.000115</td>\n",
       "      <td>0.000118</td>\n",
       "      <td>0.000119</td>\n",
       "      <td>0.000120</td>\n",
       "      <td>0.000119</td>\n",
       "      <td>0.000116</td>\n",
       "      <td>0.000113</td>\n",
       "      <td>0.000109</td>\n",
       "      <td>0.000104</td>\n",
       "      <td>0.000098</td>\n",
       "      <td>0.000092</td>\n",
       "      <td>0.000085</td>\n",
       "      <td>0.000079</td>\n",
       "      <td>0.000072</td>\n",
       "      <td>0.000065</td>\n",
       "      <td>0.000058</td>\n",
       "      <td>0.000052</td>\n",
       "      <td>0.000046</td>\n",
       "      <td>0.000040</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>0.000026</td>\n",
       "      <td>0.000022</td>\n",
       "      <td>0.000020</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>0.000021</td>\n",
       "      <td>0.000024</td>\n",
       "      <td>0.000028</td>\n",
       "      <td>0.000033</td>\n",
       "      <td>0.000039</td>\n",
       "      <td>0.000046</td>\n",
       "      <td>0.000054</td>\n",
       "      <td>0.000062</td>\n",
       "      <td>0.000071</td>\n",
       "      <td>0.000081</td>\n",
       "      <td>0.000091</td>\n",
       "      <td>0.000101</td>\n",
       "      <td>0.000112</td>\n",
       "      <td>0.000122</td>\n",
       "      <td>0.000132</td>\n",
       "      <td>0.000143</td>\n",
       "      <td>0.000152</td>\n",
       "      <td>0.000162</td>\n",
       "      <td>0.000170</td>\n",
       "      <td>0.000178</td>\n",
       "      <td>0.000184</td>\n",
       "      <td>0.000190</td>\n",
       "      <td>0.000194</td>\n",
       "      <td>0.000197</td>\n",
       "      <td>0.000199</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>0.000199</td>\n",
       "      <td>0.000197</td>\n",
       "      <td>0.000193</td>\n",
       "      <td>0.000188</td>\n",
       "      <td>0.000181</td>\n",
       "      <td>0.000173</td>\n",
       "      <td>0.000163</td>\n",
       "      <td>0.000152</td>\n",
       "      <td>0.000140</td>\n",
       "      <td>0.000127</td>\n",
       "      <td>0.000113</td>\n",
       "      <td>0.000099</td>\n",
       "      <td>0.000084</td>\n",
       "      <td>0.000069</td>\n",
       "      <td>0.000054</td>\n",
       "      <td>0.000040</td>\n",
       "      <td>0.000025</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>-0.000003</td>\n",
       "      <td>-0.000016</td>\n",
       "      <td>-0.000028</td>\n",
       "      <td>-0.000038</td>\n",
       "      <td>-0.000046</td>\n",
       "      <td>-0.000053</td>\n",
       "      <td>-0.000057</td>\n",
       "      <td>-0.000058</td>\n",
       "      <td>-0.000057</td>\n",
       "      <td>-0.000053</td>\n",
       "      <td>-0.000046</td>\n",
       "      <td>-0.000037</td>\n",
       "      <td>-0.000026</td>\n",
       "      <td>-0.000012</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.000021</td>\n",
       "      <td>0.000041</td>\n",
       "      <td>0.000062</td>\n",
       "      <td>0.000085</td>\n",
       "      <td>0.000109</td>\n",
       "      <td>0.000134</td>\n",
       "      <td>0.000159</td>\n",
       "      <td>0.000184</td>\n",
       "      <td>0.000208</td>\n",
       "      <td>0.000231</td>\n",
       "      <td>0.000252</td>\n",
       "      <td>0.000272</td>\n",
       "      <td>0.000290</td>\n",
       "      <td>0.000305</td>\n",
       "      <td>0.000319</td>\n",
       "      <td>0.000329</td>\n",
       "      <td>0.000336</td>\n",
       "      <td>0.000338</td>\n",
       "      <td>0.000337</td>\n",
       "      <td>0.000330</td>\n",
       "      <td>0.000317</td>\n",
       "      <td>0.000299</td>\n",
       "      <td>0.000275</td>\n",
       "      <td>0.000245</td>\n",
       "      <td>0.000211</td>\n",
       "      <td>0.000173</td>\n",
       "      <td>0.000131</td>\n",
       "      <td>0.000087</td>\n",
       "      <td>0.000042</td>\n",
       "      <td>-0.000005</td>\n",
       "      <td>-0.000051</td>\n",
       "      <td>-0.000097</td>\n",
       "      <td>-0.000142</td>\n",
       "      <td>-0.000186</td>\n",
       "      <td>-0.000228</td>\n",
       "      <td>-0.000266</td>\n",
       "      <td>-0.000300</td>\n",
       "      <td>-0.000329</td>\n",
       "      <td>-0.000352</td>\n",
       "      <td>-0.000368</td>\n",
       "      <td>-0.000377</td>\n",
       "      <td>-0.000378</td>\n",
       "      <td>-0.000372</td>\n",
       "      <td>-0.000357</td>\n",
       "      <td>-0.000336</td>\n",
       "      <td>-0.000307</td>\n",
       "      <td>-0.000271</td>\n",
       "      <td>-0.000230</td>\n",
       "      <td>-0.000182</td>\n",
       "      <td>-0.000130</td>\n",
       "      <td>-0.000072</td>\n",
       "      <td>-0.000010</td>\n",
       "      <td>0.000056</td>\n",
       "      <td>0.000124</td>\n",
       "      <td>0.000193</td>\n",
       "      <td>0.000261</td>\n",
       "      <td>0.000328</td>\n",
       "      <td>0.000391</td>\n",
       "      <td>0.000451</td>\n",
       "      <td>0.000507</td>\n",
       "      <td>0.000558</td>\n",
       "      <td>0.000604</td>\n",
       "      <td>0.000646</td>\n",
       "      <td>0.000684</td>\n",
       "      <td>0.000717</td>\n",
       "      <td>0.000742</td>\n",
       "      <td>0.000758</td>\n",
       "      <td>0.000761</td>\n",
       "      <td>0.000749</td>\n",
       "      <td>0.000718</td>\n",
       "      <td>0.000665</td>\n",
       "      <td>0.000587</td>\n",
       "      <td>0.000484</td>\n",
       "      <td>0.000359</td>\n",
       "      <td>0.000212</td>\n",
       "      <td>0.000048</td>\n",
       "      <td>-0.000128</td>\n",
       "      <td>-0.000313</td>\n",
       "      <td>-0.000499</td>\n",
       "      <td>-0.000682</td>\n",
       "      <td>-0.000857</td>\n",
       "      <td>-0.001022</td>\n",
       "      <td>-0.001172</td>\n",
       "      <td>-0.001305</td>\n",
       "      <td>-0.001420</td>\n",
       "      <td>-0.001515</td>\n",
       "      <td>-0.001588</td>\n",
       "      <td>-0.001637</td>\n",
       "      <td>-0.001660</td>\n",
       "      <td>-0.001657</td>\n",
       "      <td>-0.001626</td>\n",
       "      <td>-0.001566</td>\n",
       "      <td>-0.001480</td>\n",
       "      <td>-0.001368</td>\n",
       "      <td>-0.001231</td>\n",
       "      <td>-0.001074</td>\n",
       "      <td>-0.000898</td>\n",
       "      <td>-0.000708</td>\n",
       "      <td>-0.000510</td>\n",
       "      <td>-0.000307</td>\n",
       "      <td>-0.000105</td>\n",
       "      <td>0.000091</td>\n",
       "      <td>0.000275</td>\n",
       "      <td>0.000442</td>\n",
       "      <td>0.000587</td>\n",
       "      <td>0.000707</td>\n",
       "      <td>0.000799</td>\n",
       "      <td>0.000862</td>\n",
       "      <td>0.000894</td>\n",
       "      <td>0.000898</td>\n",
       "      <td>0.000876</td>\n",
       "      <td>0.000831</td>\n",
       "      <td>0.000768</td>\n",
       "      <td>0.000692</td>\n",
       "      <td>0.000606</td>\n",
       "      <td>0.000515</td>\n",
       "      <td>0.000421</td>\n",
       "      <td>0.000326</td>\n",
       "      <td>0.000233</td>\n",
       "      <td>0.000141</td>\n",
       "      <td>0.000051</td>\n",
       "      <td>-0.000037</td>\n",
       "      <td>-0.000122</td>\n",
       "      <td>-0.000201</td>\n",
       "      <td>-0.000274</td>\n",
       "      <td>-0.000338</td>\n",
       "      <td>-0.000393</td>\n",
       "      <td>-0.000436</td>\n",
       "      <td>-0.000468</td>\n",
       "      <td>-0.000489</td>\n",
       "      <td>-0.000498</td>\n",
       "      <td>-0.000498</td>\n",
       "      <td>-0.000490</td>\n",
       "      <td>-0.000475</td>\n",
       "      <td>-0.000455</td>\n",
       "      <td>-0.000433</td>\n",
       "      <td>-0.000409</td>\n",
       "      <td>-0.000385</td>\n",
       "      <td>-0.000362</td>\n",
       "      <td>-0.000340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000084</td>\n",
       "      <td>0.000099</td>\n",
       "      <td>0.000118</td>\n",
       "      <td>0.000140</td>\n",
       "      <td>0.000165</td>\n",
       "      <td>0.000195</td>\n",
       "      <td>0.000227</td>\n",
       "      <td>0.000262</td>\n",
       "      <td>0.000297</td>\n",
       "      <td>0.000333</td>\n",
       "      <td>0.000368</td>\n",
       "      <td>0.000400</td>\n",
       "      <td>0.000428</td>\n",
       "      <td>0.000451</td>\n",
       "      <td>0.000469</td>\n",
       "      <td>0.000480</td>\n",
       "      <td>0.000485</td>\n",
       "      <td>0.000484</td>\n",
       "      <td>0.000477</td>\n",
       "      <td>0.000466</td>\n",
       "      <td>0.000450</td>\n",
       "      <td>0.000431</td>\n",
       "      <td>0.000410</td>\n",
       "      <td>0.000386</td>\n",
       "      <td>0.000362</td>\n",
       "      <td>0.000337</td>\n",
       "      <td>0.000311</td>\n",
       "      <td>0.000285</td>\n",
       "      <td>0.000259</td>\n",
       "      <td>0.000234</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000080</td>\n",
       "      <td>0.000084</td>\n",
       "      <td>0.000086</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.000091</td>\n",
       "      <td>0.000093</td>\n",
       "      <td>0.000094</td>\n",
       "      <td>0.000095</td>\n",
       "      <td>0.000096</td>\n",
       "      <td>0.000096</td>\n",
       "      <td>0.000096</td>\n",
       "      <td>0.000097</td>\n",
       "      <td>0.000097</td>\n",
       "      <td>0.000097</td>\n",
       "      <td>0.000097</td>\n",
       "      <td>0.000097</td>\n",
       "      <td>0.000096</td>\n",
       "      <td>0.000096</td>\n",
       "      <td>0.000096</td>\n",
       "      <td>0.000096</td>\n",
       "      <td>0.000096</td>\n",
       "      <td>0.000096</td>\n",
       "      <td>0.000095</td>\n",
       "      <td>0.000095</td>\n",
       "      <td>0.000094</td>\n",
       "      <td>0.000093</td>\n",
       "      <td>0.000092</td>\n",
       "      <td>0.000091</td>\n",
       "      <td>0.000090</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.000087</td>\n",
       "      <td>0.000086</td>\n",
       "      <td>0.000084</td>\n",
       "      <td>0.000083</td>\n",
       "      <td>0.000081</td>\n",
       "      <td>0.000080</td>\n",
       "      <td>0.000079</td>\n",
       "      <td>0.000078</td>\n",
       "      <td>0.000077</td>\n",
       "      <td>0.000076</td>\n",
       "      <td>0.000075</td>\n",
       "      <td>0.000075</td>\n",
       "      <td>0.000074</td>\n",
       "      <td>0.000073</td>\n",
       "      <td>0.000073</td>\n",
       "      <td>0.000072</td>\n",
       "      <td>0.000071</td>\n",
       "      <td>0.000071</td>\n",
       "      <td>0.000070</td>\n",
       "      <td>0.000069</td>\n",
       "      <td>0.000069</td>\n",
       "      <td>0.000068</td>\n",
       "      <td>0.000067</td>\n",
       "      <td>0.000067</td>\n",
       "      <td>0.000066</td>\n",
       "      <td>0.000066</td>\n",
       "      <td>0.000065</td>\n",
       "      <td>0.000065</td>\n",
       "      <td>0.000064</td>\n",
       "      <td>0.000064</td>\n",
       "      <td>0.000063</td>\n",
       "      <td>0.000063</td>\n",
       "      <td>0.000062</td>\n",
       "      <td>0.000062</td>\n",
       "      <td>0.000061</td>\n",
       "      <td>0.000061</td>\n",
       "      <td>0.000060</td>\n",
       "      <td>0.000059</td>\n",
       "      <td>0.000058</td>\n",
       "      <td>0.000058</td>\n",
       "      <td>0.000057</td>\n",
       "      <td>0.000056</td>\n",
       "      <td>0.000055</td>\n",
       "      <td>0.000054</td>\n",
       "      <td>0.000053</td>\n",
       "      <td>0.000052</td>\n",
       "      <td>0.000052</td>\n",
       "      <td>0.000051</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>0.000049</td>\n",
       "      <td>0.000049</td>\n",
       "      <td>0.000048</td>\n",
       "      <td>0.000048</td>\n",
       "      <td>0.000048</td>\n",
       "      <td>0.000047</td>\n",
       "      <td>0.000047</td>\n",
       "      <td>0.000047</td>\n",
       "      <td>0.000047</td>\n",
       "      <td>0.000048</td>\n",
       "      <td>0.000048</td>\n",
       "      <td>0.000048</td>\n",
       "      <td>0.000048</td>\n",
       "      <td>0.000049</td>\n",
       "      <td>0.000049</td>\n",
       "      <td>0.000049</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>0.000049</td>\n",
       "      <td>0.000049</td>\n",
       "      <td>0.000048</td>\n",
       "      <td>0.000046</td>\n",
       "      <td>0.000045</td>\n",
       "      <td>0.000043</td>\n",
       "      <td>0.000041</td>\n",
       "      <td>0.000038</td>\n",
       "      <td>0.000036</td>\n",
       "      <td>0.000033</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>0.000028</td>\n",
       "      <td>0.000025</td>\n",
       "      <td>0.000022</td>\n",
       "      <td>0.00002</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>0.000021</td>\n",
       "      <td>0.000025</td>\n",
       "      <td>0.000029</td>\n",
       "      <td>0.000034</td>\n",
       "      <td>0.000039</td>\n",
       "      <td>0.000046</td>\n",
       "      <td>0.000053</td>\n",
       "      <td>0.000060</td>\n",
       "      <td>0.000069</td>\n",
       "      <td>0.000078</td>\n",
       "      <td>0.000087</td>\n",
       "      <td>0.000097</td>\n",
       "      <td>0.000107</td>\n",
       "      <td>0.000117</td>\n",
       "      <td>0.000127</td>\n",
       "      <td>0.000136</td>\n",
       "      <td>0.000144</td>\n",
       "      <td>0.000151</td>\n",
       "      <td>0.000157</td>\n",
       "      <td>0.000160</td>\n",
       "      <td>0.000162</td>\n",
       "      <td>0.000162</td>\n",
       "      <td>0.000160</td>\n",
       "      <td>0.000155</td>\n",
       "      <td>0.000148</td>\n",
       "      <td>0.000139</td>\n",
       "      <td>0.000129</td>\n",
       "      <td>0.000116</td>\n",
       "      <td>0.000102</td>\n",
       "      <td>0.000087</td>\n",
       "      <td>0.000072</td>\n",
       "      <td>0.000056</td>\n",
       "      <td>0.000041</td>\n",
       "      <td>0.000027</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>-0.000007</td>\n",
       "      <td>-0.000013</td>\n",
       "      <td>-0.000017</td>\n",
       "      <td>-0.000017</td>\n",
       "      <td>-0.000015</td>\n",
       "      <td>-0.000009</td>\n",
       "      <td>-9.350197e-07</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.000022</td>\n",
       "      <td>0.000036</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>0.000064</td>\n",
       "      <td>0.000077</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.000098</td>\n",
       "      <td>0.000105</td>\n",
       "      <td>0.000110</td>\n",
       "      <td>0.000111</td>\n",
       "      <td>0.000108</td>\n",
       "      <td>0.000103</td>\n",
       "      <td>0.000094</td>\n",
       "      <td>0.000082</td>\n",
       "      <td>0.000068</td>\n",
       "      <td>0.000051</td>\n",
       "      <td>0.000033</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>-0.000008</td>\n",
       "      <td>-0.000029</td>\n",
       "      <td>-0.000050</td>\n",
       "      <td>-0.000071</td>\n",
       "      <td>-0.000091</td>\n",
       "      <td>-0.000111</td>\n",
       "      <td>-0.000128</td>\n",
       "      <td>-0.000144</td>\n",
       "      <td>-0.000158</td>\n",
       "      <td>-0.000170</td>\n",
       "      <td>-0.000180</td>\n",
       "      <td>-0.000186</td>\n",
       "      <td>-0.000190</td>\n",
       "      <td>-0.000191</td>\n",
       "      <td>-0.000189</td>\n",
       "      <td>-0.000184</td>\n",
       "      <td>-0.000177</td>\n",
       "      <td>-0.000167</td>\n",
       "      <td>-0.000155</td>\n",
       "      <td>-0.000141</td>\n",
       "      <td>-0.000126</td>\n",
       "      <td>-0.000109</td>\n",
       "      <td>-0.000092</td>\n",
       "      <td>-0.000074</td>\n",
       "      <td>-0.000057</td>\n",
       "      <td>-0.000039</td>\n",
       "      <td>-0.000022</td>\n",
       "      <td>-0.000005</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.000025</td>\n",
       "      <td>0.000038</td>\n",
       "      <td>0.000049</td>\n",
       "      <td>0.000059</td>\n",
       "      <td>0.000067</td>\n",
       "      <td>0.000072</td>\n",
       "      <td>0.000075</td>\n",
       "      <td>0.000076</td>\n",
       "      <td>0.000073</td>\n",
       "      <td>0.000068</td>\n",
       "      <td>0.000059</td>\n",
       "      <td>0.000048</td>\n",
       "      <td>0.000034</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>-9.655635e-07</td>\n",
       "      <td>-0.000021</td>\n",
       "      <td>-0.000041</td>\n",
       "      <td>-0.000062</td>\n",
       "      <td>-0.000083</td>\n",
       "      <td>-0.000102</td>\n",
       "      <td>-0.000119</td>\n",
       "      <td>-0.000134</td>\n",
       "      <td>-0.000146</td>\n",
       "      <td>-0.000155</td>\n",
       "      <td>-0.000160</td>\n",
       "      <td>-0.000162</td>\n",
       "      <td>-0.000160</td>\n",
       "      <td>-0.000154</td>\n",
       "      <td>-0.000145</td>\n",
       "      <td>-0.000132</td>\n",
       "      <td>-0.000117</td>\n",
       "      <td>-0.000099</td>\n",
       "      <td>-0.000080</td>\n",
       "      <td>-0.000060</td>\n",
       "      <td>-0.000038</td>\n",
       "      <td>-0.000017</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000024</td>\n",
       "      <td>0.000043</td>\n",
       "      <td>0.000059</td>\n",
       "      <td>0.000074</td>\n",
       "      <td>0.000086</td>\n",
       "      <td>0.000096</td>\n",
       "      <td>0.000102</td>\n",
       "      <td>0.000106</td>\n",
       "      <td>0.000108</td>\n",
       "      <td>0.000106</td>\n",
       "      <td>0.000103</td>\n",
       "      <td>0.000097</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.000080</td>\n",
       "      <td>0.000070</td>\n",
       "      <td>0.000059</td>\n",
       "      <td>0.000048</td>\n",
       "      <td>0.000037</td>\n",
       "      <td>0.000026</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>-0.000002</td>\n",
       "      <td>-0.000010</td>\n",
       "      <td>-0.000017</td>\n",
       "      <td>-0.000022</td>\n",
       "      <td>-0.000027</td>\n",
       "      <td>-0.000031</td>\n",
       "      <td>-0.000034</td>\n",
       "      <td>-0.000036</td>\n",
       "      <td>-0.000037</td>\n",
       "      <td>-0.000038</td>\n",
       "      <td>-0.000038</td>\n",
       "      <td>-0.000037</td>\n",
       "      <td>-0.000035</td>\n",
       "      <td>-0.000034</td>\n",
       "      <td>-0.000031</td>\n",
       "      <td>-0.000029</td>\n",
       "      <td>-0.000026</td>\n",
       "      <td>-0.000023</td>\n",
       "      <td>-0.000019</td>\n",
       "      <td>-0.000016</td>\n",
       "      <td>-0.000012</td>\n",
       "      <td>-0.000008</td>\n",
       "      <td>-4.222580e-06</td>\n",
       "      <td>-1.869656e-07</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>0.000020</td>\n",
       "      <td>0.000024</td>\n",
       "      <td>0.000028</td>\n",
       "      <td>0.000032</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>0.000038</td>\n",
       "      <td>0.000041</td>\n",
       "      <td>0.000043</td>\n",
       "      <td>0.000046</td>\n",
       "      <td>0.000048</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>0.000051</td>\n",
       "      <td>0.000053</td>\n",
       "      <td>0.000055</td>\n",
       "      <td>0.000057</td>\n",
       "      <td>0.000059</td>\n",
       "      <td>0.000060</td>\n",
       "      <td>0.000062</td>\n",
       "      <td>0.000063</td>\n",
       "      <td>0.000065</td>\n",
       "      <td>0.000066</td>\n",
       "      <td>0.000066</td>\n",
       "      <td>0.000067</td>\n",
       "      <td>0.000068</td>\n",
       "      <td>0.000069</td>\n",
       "      <td>0.000070</td>\n",
       "      <td>0.000072</td>\n",
       "      <td>0.000075</td>\n",
       "      <td>0.000079</td>\n",
       "      <td>0.000084</td>\n",
       "      <td>0.000090</td>\n",
       "      <td>0.000098</td>\n",
       "      <td>0.000108</td>\n",
       "      <td>0.000118</td>\n",
       "      <td>0.000131</td>\n",
       "      <td>0.000145</td>\n",
       "      <td>0.000160</td>\n",
       "      <td>0.000177</td>\n",
       "      <td>0.000196</td>\n",
       "      <td>0.000218</td>\n",
       "      <td>0.000241</td>\n",
       "      <td>0.000267</td>\n",
       "      <td>0.000296</td>\n",
       "      <td>0.000327</td>\n",
       "      <td>0.000362</td>\n",
       "      <td>0.000400</td>\n",
       "      <td>0.000441</td>\n",
       "      <td>0.000486</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000158</td>\n",
       "      <td>0.000153</td>\n",
       "      <td>0.000149</td>\n",
       "      <td>0.000143</td>\n",
       "      <td>0.000138</td>\n",
       "      <td>0.000132</td>\n",
       "      <td>0.000127</td>\n",
       "      <td>0.000121</td>\n",
       "      <td>0.000117</td>\n",
       "      <td>0.000112</td>\n",
       "      <td>0.000109</td>\n",
       "      <td>0.000106</td>\n",
       "      <td>0.000105</td>\n",
       "      <td>0.000104</td>\n",
       "      <td>0.000105</td>\n",
       "      <td>0.000107</td>\n",
       "      <td>0.000110</td>\n",
       "      <td>0.000114</td>\n",
       "      <td>0.000120</td>\n",
       "      <td>0.000126</td>\n",
       "      <td>0.000134</td>\n",
       "      <td>0.000142</td>\n",
       "      <td>0.000151</td>\n",
       "      <td>0.000160</td>\n",
       "      <td>0.000170</td>\n",
       "      <td>0.000179</td>\n",
       "      <td>0.000189</td>\n",
       "      <td>0.000198</td>\n",
       "      <td>0.000207</td>\n",
       "      <td>0.000215</td>\n",
       "      <td>0.000222</td>\n",
       "      <td>0.000229</td>\n",
       "      <td>0.000234</td>\n",
       "      <td>0.000238</td>\n",
       "      <td>0.000240</td>\n",
       "      <td>0.000241</td>\n",
       "      <td>0.000241</td>\n",
       "      <td>0.000239</td>\n",
       "      <td>0.000235</td>\n",
       "      <td>0.000230</td>\n",
       "      <td>0.000224</td>\n",
       "      <td>0.000215</td>\n",
       "      <td>0.000206</td>\n",
       "      <td>0.000194</td>\n",
       "      <td>0.000182</td>\n",
       "      <td>0.000168</td>\n",
       "      <td>0.000154</td>\n",
       "      <td>0.000138</td>\n",
       "      <td>0.000122</td>\n",
       "      <td>0.000106</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.000072</td>\n",
       "      <td>0.000056</td>\n",
       "      <td>0.000040</td>\n",
       "      <td>0.000025</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>-1.969533e-06</td>\n",
       "      <td>-0.000014</td>\n",
       "      <td>-0.000024</td>\n",
       "      <td>-0.000033</td>\n",
       "      <td>-0.000041</td>\n",
       "      <td>-0.000047</td>\n",
       "      <td>-0.000052</td>\n",
       "      <td>-0.000055</td>\n",
       "      <td>-0.000056</td>\n",
       "      <td>-0.000056</td>\n",
       "      <td>-0.000055</td>\n",
       "      <td>-0.000053</td>\n",
       "      <td>-0.000050</td>\n",
       "      <td>-0.000046</td>\n",
       "      <td>-0.000041</td>\n",
       "      <td>-0.000036</td>\n",
       "      <td>-0.000031</td>\n",
       "      <td>-0.000025</td>\n",
       "      <td>-0.000019</td>\n",
       "      <td>-0.000013</td>\n",
       "      <td>-0.000008</td>\n",
       "      <td>-0.000003</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>0.000020</td>\n",
       "      <td>0.000024</td>\n",
       "      <td>0.000028</td>\n",
       "      <td>0.000031</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>0.000039</td>\n",
       "      <td>0.000043</td>\n",
       "      <td>0.000047</td>\n",
       "      <td>0.000052</td>\n",
       "      <td>0.000056</td>\n",
       "      <td>0.000061</td>\n",
       "      <td>0.000065</td>\n",
       "      <td>0.000070</td>\n",
       "      <td>0.000075</td>\n",
       "      <td>0.000080</td>\n",
       "      <td>0.000085</td>\n",
       "      <td>0.000091</td>\n",
       "      <td>0.000096</td>\n",
       "      <td>0.000102</td>\n",
       "      <td>0.000108</td>\n",
       "      <td>0.000114</td>\n",
       "      <td>0.000120</td>\n",
       "      <td>0.000127</td>\n",
       "      <td>0.000134</td>\n",
       "      <td>0.000141</td>\n",
       "      <td>0.000148</td>\n",
       "      <td>0.000156</td>\n",
       "      <td>0.000165</td>\n",
       "      <td>0.000174</td>\n",
       "      <td>0.000183</td>\n",
       "      <td>0.000193</td>\n",
       "      <td>0.000204</td>\n",
       "      <td>0.000215</td>\n",
       "      <td>0.000226</td>\n",
       "      <td>0.000238</td>\n",
       "      <td>0.000250</td>\n",
       "      <td>0.000262</td>\n",
       "      <td>0.000275</td>\n",
       "      <td>0.000287</td>\n",
       "      <td>0.000298</td>\n",
       "      <td>0.000309</td>\n",
       "      <td>0.000320</td>\n",
       "      <td>0.000329</td>\n",
       "      <td>0.000337</td>\n",
       "      <td>0.000343</td>\n",
       "      <td>0.000347</td>\n",
       "      <td>0.000349</td>\n",
       "      <td>0.000349</td>\n",
       "      <td>0.000346</td>\n",
       "      <td>0.000340</td>\n",
       "      <td>0.000331</td>\n",
       "      <td>0.000319</td>\n",
       "      <td>0.000304</td>\n",
       "      <td>0.000286</td>\n",
       "      <td>0.000265</td>\n",
       "      <td>0.000241</td>\n",
       "      <td>0.000214</td>\n",
       "      <td>0.000186</td>\n",
       "      <td>0.000155</td>\n",
       "      <td>0.000123</td>\n",
       "      <td>0.000090</td>\n",
       "      <td>0.000056</td>\n",
       "      <td>0.000022</td>\n",
       "      <td>-0.000012</td>\n",
       "      <td>-0.000045</td>\n",
       "      <td>-0.000076</td>\n",
       "      <td>-0.000106</td>\n",
       "      <td>-0.000133</td>\n",
       "      <td>-0.000157</td>\n",
       "      <td>-0.000178</td>\n",
       "      <td>-0.000195</td>\n",
       "      <td>-0.000208</td>\n",
       "      <td>-0.000218</td>\n",
       "      <td>-0.000223</td>\n",
       "      <td>-0.000224</td>\n",
       "      <td>-0.000220</td>\n",
       "      <td>-0.000213</td>\n",
       "      <td>-0.000201</td>\n",
       "      <td>-0.000185</td>\n",
       "      <td>-0.000166</td>\n",
       "      <td>-0.000144</td>\n",
       "      <td>-0.000120</td>\n",
       "      <td>-0.000093</td>\n",
       "      <td>-0.000065</td>\n",
       "      <td>-0.000036</td>\n",
       "      <td>-7.287763e-06</td>\n",
       "      <td>0.000021</td>\n",
       "      <td>0.000049</td>\n",
       "      <td>0.000075</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000121</td>\n",
       "      <td>0.000140</td>\n",
       "      <td>0.000156</td>\n",
       "      <td>0.000168</td>\n",
       "      <td>0.000177</td>\n",
       "      <td>0.000182</td>\n",
       "      <td>0.000184</td>\n",
       "      <td>0.000182</td>\n",
       "      <td>0.000176</td>\n",
       "      <td>0.000167</td>\n",
       "      <td>0.000155</td>\n",
       "      <td>0.000140</td>\n",
       "      <td>0.000122</td>\n",
       "      <td>0.000103</td>\n",
       "      <td>0.000082</td>\n",
       "      <td>0.000060</td>\n",
       "      <td>0.000037</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>-0.000009</td>\n",
       "      <td>-0.000032</td>\n",
       "      <td>-0.000053</td>\n",
       "      <td>-0.000073</td>\n",
       "      <td>-0.000091</td>\n",
       "      <td>-0.000107</td>\n",
       "      <td>-0.000120</td>\n",
       "      <td>-0.000132</td>\n",
       "      <td>-0.000140</td>\n",
       "      <td>-0.000146</td>\n",
       "      <td>-0.000149</td>\n",
       "      <td>-0.000149</td>\n",
       "      <td>-0.000146</td>\n",
       "      <td>-0.000141</td>\n",
       "      <td>-0.000133</td>\n",
       "      <td>-0.000123</td>\n",
       "      <td>-0.000111</td>\n",
       "      <td>-0.000097</td>\n",
       "      <td>-0.000082</td>\n",
       "      <td>-0.000066</td>\n",
       "      <td>-0.000049</td>\n",
       "      <td>-0.000032</td>\n",
       "      <td>-1.468232e-05</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000019</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>0.000064</td>\n",
       "      <td>0.000077</td>\n",
       "      <td>0.000088</td>\n",
       "      <td>0.000097</td>\n",
       "      <td>0.000105</td>\n",
       "      <td>0.000111</td>\n",
       "      <td>0.000115</td>\n",
       "      <td>0.000118</td>\n",
       "      <td>0.000119</td>\n",
       "      <td>0.000119</td>\n",
       "      <td>0.000117</td>\n",
       "      <td>0.000115</td>\n",
       "      <td>0.000111</td>\n",
       "      <td>0.000107</td>\n",
       "      <td>0.000101</td>\n",
       "      <td>0.000095</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.000082</td>\n",
       "      <td>0.000075</td>\n",
       "      <td>0.000068</td>\n",
       "      <td>0.000061</td>\n",
       "      <td>0.000054</td>\n",
       "      <td>0.000048</td>\n",
       "      <td>0.000042</td>\n",
       "      <td>0.000037</td>\n",
       "      <td>0.000032</td>\n",
       "      <td>0.000028</td>\n",
       "      <td>0.000024</td>\n",
       "      <td>0.000021</td>\n",
       "      <td>0.000019</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>0.000020</td>\n",
       "      <td>0.000022</td>\n",
       "      <td>0.000026</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>0.000041</td>\n",
       "      <td>0.000048</td>\n",
       "      <td>0.000055</td>\n",
       "      <td>0.000064</td>\n",
       "      <td>0.000073</td>\n",
       "      <td>0.000082</td>\n",
       "      <td>0.000092</td>\n",
       "      <td>0.000102</td>\n",
       "      <td>0.000112</td>\n",
       "      <td>0.000122</td>\n",
       "      <td>0.000133</td>\n",
       "      <td>0.000143</td>\n",
       "      <td>0.000153</td>\n",
       "      <td>0.000162</td>\n",
       "      <td>0.000171</td>\n",
       "      <td>0.000179</td>\n",
       "      <td>0.000185</td>\n",
       "      <td>0.000191</td>\n",
       "      <td>0.000196</td>\n",
       "      <td>0.000199</td>\n",
       "      <td>0.000201</td>\n",
       "      <td>0.000202</td>\n",
       "      <td>0.000201</td>\n",
       "      <td>0.000198</td>\n",
       "      <td>0.000194</td>\n",
       "      <td>0.000189</td>\n",
       "      <td>0.000182</td>\n",
       "      <td>0.000173</td>\n",
       "      <td>0.000163</td>\n",
       "      <td>0.000151</td>\n",
       "      <td>0.000139</td>\n",
       "      <td>0.000125</td>\n",
       "      <td>0.000111</td>\n",
       "      <td>0.000096</td>\n",
       "      <td>0.000081</td>\n",
       "      <td>0.000066</td>\n",
       "      <td>0.000051</td>\n",
       "      <td>0.000037</td>\n",
       "      <td>0.000023</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>-0.000002</td>\n",
       "      <td>-0.000014</td>\n",
       "      <td>-0.000024</td>\n",
       "      <td>-0.000032</td>\n",
       "      <td>-0.000039</td>\n",
       "      <td>-0.000043</td>\n",
       "      <td>-0.000046</td>\n",
       "      <td>-0.000046</td>\n",
       "      <td>-0.000044</td>\n",
       "      <td>-0.000040</td>\n",
       "      <td>-0.000033</td>\n",
       "      <td>-0.000025</td>\n",
       "      <td>-0.000015</td>\n",
       "      <td>-0.000002</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.000027</td>\n",
       "      <td>0.000044</td>\n",
       "      <td>0.000063</td>\n",
       "      <td>0.000083</td>\n",
       "      <td>0.000103</td>\n",
       "      <td>0.000125</td>\n",
       "      <td>0.000147</td>\n",
       "      <td>0.000168</td>\n",
       "      <td>0.000189</td>\n",
       "      <td>0.000208</td>\n",
       "      <td>0.000226</td>\n",
       "      <td>0.000243</td>\n",
       "      <td>0.000258</td>\n",
       "      <td>0.000271</td>\n",
       "      <td>0.000283</td>\n",
       "      <td>0.000291</td>\n",
       "      <td>0.000297</td>\n",
       "      <td>0.000299</td>\n",
       "      <td>0.000298</td>\n",
       "      <td>0.000292</td>\n",
       "      <td>0.000281</td>\n",
       "      <td>0.000265</td>\n",
       "      <td>0.000245</td>\n",
       "      <td>0.000219</td>\n",
       "      <td>0.000189</td>\n",
       "      <td>0.000156</td>\n",
       "      <td>0.000119</td>\n",
       "      <td>0.000081</td>\n",
       "      <td>0.000041</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>-0.000039</td>\n",
       "      <td>-0.000079</td>\n",
       "      <td>-0.000117</td>\n",
       "      <td>-0.000154</td>\n",
       "      <td>-0.000188</td>\n",
       "      <td>-0.000219</td>\n",
       "      <td>-0.000247</td>\n",
       "      <td>-0.000270</td>\n",
       "      <td>-0.000288</td>\n",
       "      <td>-0.000301</td>\n",
       "      <td>-0.000308</td>\n",
       "      <td>-0.000310</td>\n",
       "      <td>-0.000306</td>\n",
       "      <td>-0.000296</td>\n",
       "      <td>-0.000281</td>\n",
       "      <td>-0.000261</td>\n",
       "      <td>-0.000236</td>\n",
       "      <td>-0.000206</td>\n",
       "      <td>-0.000170</td>\n",
       "      <td>-0.000130</td>\n",
       "      <td>-0.000085</td>\n",
       "      <td>-0.000035</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>0.000075</td>\n",
       "      <td>0.000134</td>\n",
       "      <td>0.000193</td>\n",
       "      <td>0.000252</td>\n",
       "      <td>0.000309</td>\n",
       "      <td>0.000365</td>\n",
       "      <td>0.000419</td>\n",
       "      <td>0.000471</td>\n",
       "      <td>0.000521</td>\n",
       "      <td>0.000570</td>\n",
       "      <td>0.000618</td>\n",
       "      <td>0.000663</td>\n",
       "      <td>0.000704</td>\n",
       "      <td>0.000737</td>\n",
       "      <td>0.000759</td>\n",
       "      <td>0.000766</td>\n",
       "      <td>0.000752</td>\n",
       "      <td>0.000715</td>\n",
       "      <td>0.000652</td>\n",
       "      <td>0.000561</td>\n",
       "      <td>0.000443</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>0.000136</td>\n",
       "      <td>-0.000045</td>\n",
       "      <td>-0.000236</td>\n",
       "      <td>-0.000431</td>\n",
       "      <td>-0.000624</td>\n",
       "      <td>-0.000810</td>\n",
       "      <td>-0.000984</td>\n",
       "      <td>-0.001143</td>\n",
       "      <td>-0.001284</td>\n",
       "      <td>-0.001404</td>\n",
       "      <td>-0.001502</td>\n",
       "      <td>-0.001576</td>\n",
       "      <td>-0.001625</td>\n",
       "      <td>-0.001647</td>\n",
       "      <td>-0.001642</td>\n",
       "      <td>-0.001610</td>\n",
       "      <td>-0.001549</td>\n",
       "      <td>-0.001461</td>\n",
       "      <td>-0.001348</td>\n",
       "      <td>-0.001211</td>\n",
       "      <td>-0.001053</td>\n",
       "      <td>-0.000876</td>\n",
       "      <td>-0.000686</td>\n",
       "      <td>-0.000486</td>\n",
       "      <td>-0.000282</td>\n",
       "      <td>-0.000079</td>\n",
       "      <td>0.000117</td>\n",
       "      <td>0.000301</td>\n",
       "      <td>0.000467</td>\n",
       "      <td>0.000609</td>\n",
       "      <td>0.000724</td>\n",
       "      <td>0.000808</td>\n",
       "      <td>0.000860</td>\n",
       "      <td>0.000880</td>\n",
       "      <td>0.000869</td>\n",
       "      <td>0.000830</td>\n",
       "      <td>0.000767</td>\n",
       "      <td>0.000685</td>\n",
       "      <td>0.000589</td>\n",
       "      <td>0.000484</td>\n",
       "      <td>0.000376</td>\n",
       "      <td>0.000267</td>\n",
       "      <td>0.000161</td>\n",
       "      <td>0.000059</td>\n",
       "      <td>-0.000037</td>\n",
       "      <td>-0.000126</td>\n",
       "      <td>-0.000208</td>\n",
       "      <td>-0.000282</td>\n",
       "      <td>-0.000349</td>\n",
       "      <td>-0.000406</td>\n",
       "      <td>-0.000454</td>\n",
       "      <td>-0.000492</td>\n",
       "      <td>-0.000518</td>\n",
       "      <td>-0.000533</td>\n",
       "      <td>-0.000536</td>\n",
       "      <td>-0.000529</td>\n",
       "      <td>-0.000513</td>\n",
       "      <td>-0.000489</td>\n",
       "      <td>-0.000459</td>\n",
       "      <td>-0.000424</td>\n",
       "      <td>-0.000388</td>\n",
       "      <td>-0.000351</td>\n",
       "      <td>-0.000315</td>\n",
       "      <td>-0.000281</td>\n",
       "      <td>-0.000249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9095</th>\n",
       "      <td>0.000080</td>\n",
       "      <td>0.000091</td>\n",
       "      <td>0.000104</td>\n",
       "      <td>0.000121</td>\n",
       "      <td>0.000141</td>\n",
       "      <td>0.000164</td>\n",
       "      <td>0.000191</td>\n",
       "      <td>0.000220</td>\n",
       "      <td>0.000250</td>\n",
       "      <td>0.000281</td>\n",
       "      <td>0.000312</td>\n",
       "      <td>0.000341</td>\n",
       "      <td>0.000368</td>\n",
       "      <td>0.000391</td>\n",
       "      <td>0.000409</td>\n",
       "      <td>0.000423</td>\n",
       "      <td>0.000431</td>\n",
       "      <td>0.000434</td>\n",
       "      <td>0.000432</td>\n",
       "      <td>0.000427</td>\n",
       "      <td>0.000418</td>\n",
       "      <td>0.000407</td>\n",
       "      <td>0.000394</td>\n",
       "      <td>0.000380</td>\n",
       "      <td>0.000364</td>\n",
       "      <td>0.000348</td>\n",
       "      <td>0.000331</td>\n",
       "      <td>0.000313</td>\n",
       "      <td>0.000295</td>\n",
       "      <td>0.000278</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000082</td>\n",
       "      <td>0.000085</td>\n",
       "      <td>0.000087</td>\n",
       "      <td>0.000088</td>\n",
       "      <td>0.000090</td>\n",
       "      <td>0.000091</td>\n",
       "      <td>0.000091</td>\n",
       "      <td>0.000092</td>\n",
       "      <td>0.000093</td>\n",
       "      <td>0.000093</td>\n",
       "      <td>0.000094</td>\n",
       "      <td>0.000095</td>\n",
       "      <td>0.000095</td>\n",
       "      <td>0.000096</td>\n",
       "      <td>0.000097</td>\n",
       "      <td>0.000097</td>\n",
       "      <td>0.000098</td>\n",
       "      <td>0.000098</td>\n",
       "      <td>0.000098</td>\n",
       "      <td>0.000098</td>\n",
       "      <td>0.000098</td>\n",
       "      <td>0.000098</td>\n",
       "      <td>0.000097</td>\n",
       "      <td>0.000096</td>\n",
       "      <td>0.000096</td>\n",
       "      <td>0.000095</td>\n",
       "      <td>0.000094</td>\n",
       "      <td>0.000093</td>\n",
       "      <td>0.000092</td>\n",
       "      <td>0.000091</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.000088</td>\n",
       "      <td>0.000087</td>\n",
       "      <td>0.000086</td>\n",
       "      <td>0.000085</td>\n",
       "      <td>0.000084</td>\n",
       "      <td>0.000082</td>\n",
       "      <td>0.000081</td>\n",
       "      <td>0.000080</td>\n",
       "      <td>0.000078</td>\n",
       "      <td>0.000077</td>\n",
       "      <td>0.000075</td>\n",
       "      <td>0.000074</td>\n",
       "      <td>0.000072</td>\n",
       "      <td>0.000071</td>\n",
       "      <td>0.000070</td>\n",
       "      <td>0.000069</td>\n",
       "      <td>0.000068</td>\n",
       "      <td>0.000067</td>\n",
       "      <td>0.000067</td>\n",
       "      <td>0.000066</td>\n",
       "      <td>0.000065</td>\n",
       "      <td>0.000065</td>\n",
       "      <td>0.000065</td>\n",
       "      <td>0.000064</td>\n",
       "      <td>0.000064</td>\n",
       "      <td>0.000064</td>\n",
       "      <td>0.000064</td>\n",
       "      <td>0.000063</td>\n",
       "      <td>0.000063</td>\n",
       "      <td>0.000063</td>\n",
       "      <td>0.000062</td>\n",
       "      <td>0.000062</td>\n",
       "      <td>0.000061</td>\n",
       "      <td>0.000061</td>\n",
       "      <td>0.000060</td>\n",
       "      <td>0.000060</td>\n",
       "      <td>0.000059</td>\n",
       "      <td>0.000058</td>\n",
       "      <td>0.000058</td>\n",
       "      <td>0.000057</td>\n",
       "      <td>0.000056</td>\n",
       "      <td>0.000055</td>\n",
       "      <td>0.000054</td>\n",
       "      <td>0.000052</td>\n",
       "      <td>0.000051</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>0.000049</td>\n",
       "      <td>0.000048</td>\n",
       "      <td>0.000047</td>\n",
       "      <td>0.000046</td>\n",
       "      <td>0.000046</td>\n",
       "      <td>0.000045</td>\n",
       "      <td>0.000045</td>\n",
       "      <td>0.000045</td>\n",
       "      <td>0.000045</td>\n",
       "      <td>0.000045</td>\n",
       "      <td>0.000045</td>\n",
       "      <td>0.000046</td>\n",
       "      <td>0.000046</td>\n",
       "      <td>0.000046</td>\n",
       "      <td>0.000047</td>\n",
       "      <td>0.000047</td>\n",
       "      <td>0.000048</td>\n",
       "      <td>0.000048</td>\n",
       "      <td>0.000048</td>\n",
       "      <td>0.000048</td>\n",
       "      <td>0.000048</td>\n",
       "      <td>0.000048</td>\n",
       "      <td>0.000048</td>\n",
       "      <td>0.000047</td>\n",
       "      <td>0.000046</td>\n",
       "      <td>0.000044</td>\n",
       "      <td>0.000043</td>\n",
       "      <td>0.000041</td>\n",
       "      <td>0.000039</td>\n",
       "      <td>0.000036</td>\n",
       "      <td>0.000034</td>\n",
       "      <td>0.000031</td>\n",
       "      <td>0.000029</td>\n",
       "      <td>0.000026</td>\n",
       "      <td>0.000024</td>\n",
       "      <td>0.000022</td>\n",
       "      <td>0.00002</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>0.000020</td>\n",
       "      <td>0.000022</td>\n",
       "      <td>0.000024</td>\n",
       "      <td>0.000028</td>\n",
       "      <td>0.000031</td>\n",
       "      <td>0.000036</td>\n",
       "      <td>0.000040</td>\n",
       "      <td>0.000046</td>\n",
       "      <td>0.000052</td>\n",
       "      <td>0.000059</td>\n",
       "      <td>0.000067</td>\n",
       "      <td>0.000075</td>\n",
       "      <td>0.000083</td>\n",
       "      <td>0.000092</td>\n",
       "      <td>0.000101</td>\n",
       "      <td>0.000110</td>\n",
       "      <td>0.000119</td>\n",
       "      <td>0.000127</td>\n",
       "      <td>0.000134</td>\n",
       "      <td>0.000141</td>\n",
       "      <td>0.000146</td>\n",
       "      <td>0.000149</td>\n",
       "      <td>0.000151</td>\n",
       "      <td>0.000151</td>\n",
       "      <td>0.000148</td>\n",
       "      <td>0.000144</td>\n",
       "      <td>0.000138</td>\n",
       "      <td>0.000130</td>\n",
       "      <td>0.000120</td>\n",
       "      <td>0.000108</td>\n",
       "      <td>0.000095</td>\n",
       "      <td>0.000082</td>\n",
       "      <td>0.000068</td>\n",
       "      <td>0.000054</td>\n",
       "      <td>0.000040</td>\n",
       "      <td>0.000027</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>-0.000003</td>\n",
       "      <td>-0.000009</td>\n",
       "      <td>-0.000012</td>\n",
       "      <td>-0.000012</td>\n",
       "      <td>-0.000010</td>\n",
       "      <td>-0.000005</td>\n",
       "      <td>2.530169e-06</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>0.000023</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>0.000047</td>\n",
       "      <td>0.000060</td>\n",
       "      <td>0.000071</td>\n",
       "      <td>0.000081</td>\n",
       "      <td>0.000090</td>\n",
       "      <td>0.000096</td>\n",
       "      <td>0.000099</td>\n",
       "      <td>0.000099</td>\n",
       "      <td>0.000097</td>\n",
       "      <td>0.000092</td>\n",
       "      <td>0.000083</td>\n",
       "      <td>0.000072</td>\n",
       "      <td>0.000059</td>\n",
       "      <td>0.000044</td>\n",
       "      <td>0.000027</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>-0.000010</td>\n",
       "      <td>-0.000029</td>\n",
       "      <td>-0.000049</td>\n",
       "      <td>-0.000068</td>\n",
       "      <td>-0.000086</td>\n",
       "      <td>-0.000103</td>\n",
       "      <td>-0.000119</td>\n",
       "      <td>-0.000133</td>\n",
       "      <td>-0.000146</td>\n",
       "      <td>-0.000156</td>\n",
       "      <td>-0.000164</td>\n",
       "      <td>-0.000169</td>\n",
       "      <td>-0.000172</td>\n",
       "      <td>-0.000172</td>\n",
       "      <td>-0.000170</td>\n",
       "      <td>-0.000164</td>\n",
       "      <td>-0.000157</td>\n",
       "      <td>-0.000147</td>\n",
       "      <td>-0.000136</td>\n",
       "      <td>-0.000123</td>\n",
       "      <td>-0.000108</td>\n",
       "      <td>-0.000093</td>\n",
       "      <td>-0.000077</td>\n",
       "      <td>-0.000061</td>\n",
       "      <td>-0.000045</td>\n",
       "      <td>-0.000029</td>\n",
       "      <td>-0.000013</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>0.000028</td>\n",
       "      <td>0.000040</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>0.000059</td>\n",
       "      <td>0.000065</td>\n",
       "      <td>0.000070</td>\n",
       "      <td>0.000072</td>\n",
       "      <td>0.000072</td>\n",
       "      <td>0.000070</td>\n",
       "      <td>0.000064</td>\n",
       "      <td>0.000057</td>\n",
       "      <td>0.000046</td>\n",
       "      <td>0.000034</td>\n",
       "      <td>0.000019</td>\n",
       "      <td>2.772098e-06</td>\n",
       "      <td>-0.000015</td>\n",
       "      <td>-0.000033</td>\n",
       "      <td>-0.000052</td>\n",
       "      <td>-0.000070</td>\n",
       "      <td>-0.000087</td>\n",
       "      <td>-0.000102</td>\n",
       "      <td>-0.000116</td>\n",
       "      <td>-0.000127</td>\n",
       "      <td>-0.000135</td>\n",
       "      <td>-0.000141</td>\n",
       "      <td>-0.000143</td>\n",
       "      <td>-0.000141</td>\n",
       "      <td>-0.000137</td>\n",
       "      <td>-0.000130</td>\n",
       "      <td>-0.000119</td>\n",
       "      <td>-0.000107</td>\n",
       "      <td>-0.000092</td>\n",
       "      <td>-0.000075</td>\n",
       "      <td>-0.000057</td>\n",
       "      <td>-0.000039</td>\n",
       "      <td>-0.000020</td>\n",
       "      <td>-0.000002</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>0.000033</td>\n",
       "      <td>0.000048</td>\n",
       "      <td>0.000061</td>\n",
       "      <td>0.000072</td>\n",
       "      <td>0.000081</td>\n",
       "      <td>0.000088</td>\n",
       "      <td>0.000092</td>\n",
       "      <td>0.000094</td>\n",
       "      <td>0.000094</td>\n",
       "      <td>0.000091</td>\n",
       "      <td>0.000086</td>\n",
       "      <td>0.000080</td>\n",
       "      <td>0.000072</td>\n",
       "      <td>0.000063</td>\n",
       "      <td>0.000054</td>\n",
       "      <td>0.000044</td>\n",
       "      <td>0.000034</td>\n",
       "      <td>0.000023</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>-0.000004</td>\n",
       "      <td>-0.000012</td>\n",
       "      <td>-0.000019</td>\n",
       "      <td>-0.000025</td>\n",
       "      <td>-0.000030</td>\n",
       "      <td>-0.000034</td>\n",
       "      <td>-0.000037</td>\n",
       "      <td>-0.000039</td>\n",
       "      <td>-0.000040</td>\n",
       "      <td>-0.000040</td>\n",
       "      <td>-0.000040</td>\n",
       "      <td>-0.000039</td>\n",
       "      <td>-0.000037</td>\n",
       "      <td>-0.000035</td>\n",
       "      <td>-0.000032</td>\n",
       "      <td>-0.000029</td>\n",
       "      <td>-0.000025</td>\n",
       "      <td>-0.000021</td>\n",
       "      <td>-0.000017</td>\n",
       "      <td>-0.000013</td>\n",
       "      <td>-0.000009</td>\n",
       "      <td>-0.000004</td>\n",
       "      <td>1.430718e-07</td>\n",
       "      <td>4.443608e-06</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>0.000021</td>\n",
       "      <td>0.000025</td>\n",
       "      <td>0.000028</td>\n",
       "      <td>0.000031</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>0.000037</td>\n",
       "      <td>0.000040</td>\n",
       "      <td>0.000042</td>\n",
       "      <td>0.000044</td>\n",
       "      <td>0.000046</td>\n",
       "      <td>0.000048</td>\n",
       "      <td>0.000049</td>\n",
       "      <td>0.000051</td>\n",
       "      <td>0.000053</td>\n",
       "      <td>0.000055</td>\n",
       "      <td>0.000057</td>\n",
       "      <td>0.000059</td>\n",
       "      <td>0.000061</td>\n",
       "      <td>0.000063</td>\n",
       "      <td>0.000064</td>\n",
       "      <td>0.000065</td>\n",
       "      <td>0.000066</td>\n",
       "      <td>0.000067</td>\n",
       "      <td>0.000067</td>\n",
       "      <td>0.000067</td>\n",
       "      <td>0.000066</td>\n",
       "      <td>0.000067</td>\n",
       "      <td>0.000067</td>\n",
       "      <td>0.000069</td>\n",
       "      <td>0.000071</td>\n",
       "      <td>0.000075</td>\n",
       "      <td>0.000080</td>\n",
       "      <td>0.000086</td>\n",
       "      <td>0.000094</td>\n",
       "      <td>0.000104</td>\n",
       "      <td>0.000116</td>\n",
       "      <td>0.000129</td>\n",
       "      <td>0.000144</td>\n",
       "      <td>0.000160</td>\n",
       "      <td>0.000179</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>0.000224</td>\n",
       "      <td>0.000250</td>\n",
       "      <td>0.000280</td>\n",
       "      <td>0.000313</td>\n",
       "      <td>0.000349</td>\n",
       "      <td>0.000389</td>\n",
       "      <td>0.000432</td>\n",
       "      <td>0.000479</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000164</td>\n",
       "      <td>0.000161</td>\n",
       "      <td>0.000158</td>\n",
       "      <td>0.000154</td>\n",
       "      <td>0.000149</td>\n",
       "      <td>0.000144</td>\n",
       "      <td>0.000139</td>\n",
       "      <td>0.000133</td>\n",
       "      <td>0.000128</td>\n",
       "      <td>0.000122</td>\n",
       "      <td>0.000117</td>\n",
       "      <td>0.000113</td>\n",
       "      <td>0.000110</td>\n",
       "      <td>0.000107</td>\n",
       "      <td>0.000105</td>\n",
       "      <td>0.000105</td>\n",
       "      <td>0.000105</td>\n",
       "      <td>0.000107</td>\n",
       "      <td>0.000110</td>\n",
       "      <td>0.000114</td>\n",
       "      <td>0.000119</td>\n",
       "      <td>0.000125</td>\n",
       "      <td>0.000132</td>\n",
       "      <td>0.000139</td>\n",
       "      <td>0.000147</td>\n",
       "      <td>0.000156</td>\n",
       "      <td>0.000164</td>\n",
       "      <td>0.000173</td>\n",
       "      <td>0.000181</td>\n",
       "      <td>0.000190</td>\n",
       "      <td>0.000197</td>\n",
       "      <td>0.000205</td>\n",
       "      <td>0.000211</td>\n",
       "      <td>0.000216</td>\n",
       "      <td>0.000221</td>\n",
       "      <td>0.000224</td>\n",
       "      <td>0.000225</td>\n",
       "      <td>0.000225</td>\n",
       "      <td>0.000224</td>\n",
       "      <td>0.000221</td>\n",
       "      <td>0.000216</td>\n",
       "      <td>0.000210</td>\n",
       "      <td>0.000202</td>\n",
       "      <td>0.000192</td>\n",
       "      <td>0.000181</td>\n",
       "      <td>0.000168</td>\n",
       "      <td>0.000154</td>\n",
       "      <td>0.000140</td>\n",
       "      <td>0.000124</td>\n",
       "      <td>0.000108</td>\n",
       "      <td>0.000091</td>\n",
       "      <td>0.000075</td>\n",
       "      <td>0.000059</td>\n",
       "      <td>0.000043</td>\n",
       "      <td>0.000028</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>1.144641e-06</td>\n",
       "      <td>-0.000011</td>\n",
       "      <td>-0.000021</td>\n",
       "      <td>-0.000030</td>\n",
       "      <td>-0.000037</td>\n",
       "      <td>-0.000043</td>\n",
       "      <td>-0.000047</td>\n",
       "      <td>-0.000050</td>\n",
       "      <td>-0.000051</td>\n",
       "      <td>-0.000051</td>\n",
       "      <td>-0.000049</td>\n",
       "      <td>-0.000046</td>\n",
       "      <td>-0.000042</td>\n",
       "      <td>-0.000037</td>\n",
       "      <td>-0.000031</td>\n",
       "      <td>-0.000025</td>\n",
       "      <td>-0.000018</td>\n",
       "      <td>-0.000011</td>\n",
       "      <td>-0.000005</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000020</td>\n",
       "      <td>0.000025</td>\n",
       "      <td>0.000029</td>\n",
       "      <td>0.000033</td>\n",
       "      <td>0.000037</td>\n",
       "      <td>0.000040</td>\n",
       "      <td>0.000043</td>\n",
       "      <td>0.000045</td>\n",
       "      <td>0.000048</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>0.000052</td>\n",
       "      <td>0.000055</td>\n",
       "      <td>0.000057</td>\n",
       "      <td>0.000060</td>\n",
       "      <td>0.000062</td>\n",
       "      <td>0.000064</td>\n",
       "      <td>0.000067</td>\n",
       "      <td>0.000069</td>\n",
       "      <td>0.000072</td>\n",
       "      <td>0.000075</td>\n",
       "      <td>0.000078</td>\n",
       "      <td>0.000081</td>\n",
       "      <td>0.000084</td>\n",
       "      <td>0.000088</td>\n",
       "      <td>0.000093</td>\n",
       "      <td>0.000098</td>\n",
       "      <td>0.000103</td>\n",
       "      <td>0.000109</td>\n",
       "      <td>0.000116</td>\n",
       "      <td>0.000123</td>\n",
       "      <td>0.000131</td>\n",
       "      <td>0.000140</td>\n",
       "      <td>0.000149</td>\n",
       "      <td>0.000160</td>\n",
       "      <td>0.000170</td>\n",
       "      <td>0.000182</td>\n",
       "      <td>0.000193</td>\n",
       "      <td>0.000206</td>\n",
       "      <td>0.000218</td>\n",
       "      <td>0.000231</td>\n",
       "      <td>0.000243</td>\n",
       "      <td>0.000256</td>\n",
       "      <td>0.000268</td>\n",
       "      <td>0.000280</td>\n",
       "      <td>0.000290</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>0.000309</td>\n",
       "      <td>0.000316</td>\n",
       "      <td>0.000322</td>\n",
       "      <td>0.000326</td>\n",
       "      <td>0.000328</td>\n",
       "      <td>0.000327</td>\n",
       "      <td>0.000323</td>\n",
       "      <td>0.000317</td>\n",
       "      <td>0.000308</td>\n",
       "      <td>0.000297</td>\n",
       "      <td>0.000283</td>\n",
       "      <td>0.000266</td>\n",
       "      <td>0.000246</td>\n",
       "      <td>0.000224</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>0.000173</td>\n",
       "      <td>0.000145</td>\n",
       "      <td>0.000116</td>\n",
       "      <td>0.000086</td>\n",
       "      <td>0.000055</td>\n",
       "      <td>0.000024</td>\n",
       "      <td>-0.000006</td>\n",
       "      <td>-0.000036</td>\n",
       "      <td>-0.000065</td>\n",
       "      <td>-0.000092</td>\n",
       "      <td>-0.000116</td>\n",
       "      <td>-0.000138</td>\n",
       "      <td>-0.000157</td>\n",
       "      <td>-0.000173</td>\n",
       "      <td>-0.000186</td>\n",
       "      <td>-0.000194</td>\n",
       "      <td>-0.000199</td>\n",
       "      <td>-0.000200</td>\n",
       "      <td>-0.000197</td>\n",
       "      <td>-0.000190</td>\n",
       "      <td>-0.000179</td>\n",
       "      <td>-0.000165</td>\n",
       "      <td>-0.000148</td>\n",
       "      <td>-0.000127</td>\n",
       "      <td>-0.000104</td>\n",
       "      <td>-0.000080</td>\n",
       "      <td>-0.000053</td>\n",
       "      <td>-0.000026</td>\n",
       "      <td>9.177744e-07</td>\n",
       "      <td>0.000028</td>\n",
       "      <td>0.000054</td>\n",
       "      <td>0.000079</td>\n",
       "      <td>0.000103</td>\n",
       "      <td>0.000124</td>\n",
       "      <td>0.000142</td>\n",
       "      <td>0.000157</td>\n",
       "      <td>0.000169</td>\n",
       "      <td>0.000178</td>\n",
       "      <td>0.000183</td>\n",
       "      <td>0.000185</td>\n",
       "      <td>0.000183</td>\n",
       "      <td>0.000178</td>\n",
       "      <td>0.000170</td>\n",
       "      <td>0.000159</td>\n",
       "      <td>0.000145</td>\n",
       "      <td>0.000129</td>\n",
       "      <td>0.000111</td>\n",
       "      <td>0.000091</td>\n",
       "      <td>0.000071</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>0.000028</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>-0.000013</td>\n",
       "      <td>-0.000033</td>\n",
       "      <td>-0.000051</td>\n",
       "      <td>-0.000067</td>\n",
       "      <td>-0.000081</td>\n",
       "      <td>-0.000093</td>\n",
       "      <td>-0.000103</td>\n",
       "      <td>-0.000110</td>\n",
       "      <td>-0.000115</td>\n",
       "      <td>-0.000117</td>\n",
       "      <td>-0.000116</td>\n",
       "      <td>-0.000114</td>\n",
       "      <td>-0.000108</td>\n",
       "      <td>-0.000101</td>\n",
       "      <td>-0.000092</td>\n",
       "      <td>-0.000081</td>\n",
       "      <td>-0.000069</td>\n",
       "      <td>-0.000056</td>\n",
       "      <td>-0.000042</td>\n",
       "      <td>-0.000028</td>\n",
       "      <td>-0.000013</td>\n",
       "      <td>6.117171e-07</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000027</td>\n",
       "      <td>0.000040</td>\n",
       "      <td>0.000051</td>\n",
       "      <td>0.000061</td>\n",
       "      <td>0.000070</td>\n",
       "      <td>0.000078</td>\n",
       "      <td>0.000085</td>\n",
       "      <td>0.000090</td>\n",
       "      <td>0.000094</td>\n",
       "      <td>0.000096</td>\n",
       "      <td>0.000098</td>\n",
       "      <td>0.000098</td>\n",
       "      <td>0.000097</td>\n",
       "      <td>0.000095</td>\n",
       "      <td>0.000092</td>\n",
       "      <td>0.000088</td>\n",
       "      <td>0.000084</td>\n",
       "      <td>0.000079</td>\n",
       "      <td>0.000074</td>\n",
       "      <td>0.000068</td>\n",
       "      <td>0.000063</td>\n",
       "      <td>0.000057</td>\n",
       "      <td>0.000052</td>\n",
       "      <td>0.000047</td>\n",
       "      <td>0.000043</td>\n",
       "      <td>0.000040</td>\n",
       "      <td>0.000037</td>\n",
       "      <td>0.000034</td>\n",
       "      <td>0.000033</td>\n",
       "      <td>0.000032</td>\n",
       "      <td>0.000031</td>\n",
       "      <td>0.000032</td>\n",
       "      <td>0.000033</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>0.000037</td>\n",
       "      <td>0.000041</td>\n",
       "      <td>0.000044</td>\n",
       "      <td>0.000049</td>\n",
       "      <td>0.000053</td>\n",
       "      <td>0.000059</td>\n",
       "      <td>0.000064</td>\n",
       "      <td>0.000070</td>\n",
       "      <td>0.000077</td>\n",
       "      <td>0.000083</td>\n",
       "      <td>0.000090</td>\n",
       "      <td>0.000097</td>\n",
       "      <td>0.000103</td>\n",
       "      <td>0.000110</td>\n",
       "      <td>0.000117</td>\n",
       "      <td>0.000123</td>\n",
       "      <td>0.000129</td>\n",
       "      <td>0.000135</td>\n",
       "      <td>0.000141</td>\n",
       "      <td>0.000147</td>\n",
       "      <td>0.000152</td>\n",
       "      <td>0.000157</td>\n",
       "      <td>0.000161</td>\n",
       "      <td>0.000165</td>\n",
       "      <td>0.000168</td>\n",
       "      <td>0.000171</td>\n",
       "      <td>0.000172</td>\n",
       "      <td>0.000173</td>\n",
       "      <td>0.000173</td>\n",
       "      <td>0.000171</td>\n",
       "      <td>0.000169</td>\n",
       "      <td>0.000166</td>\n",
       "      <td>0.000162</td>\n",
       "      <td>0.000158</td>\n",
       "      <td>0.000152</td>\n",
       "      <td>0.000146</td>\n",
       "      <td>0.000139</td>\n",
       "      <td>0.000132</td>\n",
       "      <td>0.000124</td>\n",
       "      <td>0.000116</td>\n",
       "      <td>0.000107</td>\n",
       "      <td>0.000099</td>\n",
       "      <td>0.000090</td>\n",
       "      <td>0.000082</td>\n",
       "      <td>0.000074</td>\n",
       "      <td>0.000067</td>\n",
       "      <td>0.000060</td>\n",
       "      <td>0.000054</td>\n",
       "      <td>0.000049</td>\n",
       "      <td>0.000044</td>\n",
       "      <td>0.000040</td>\n",
       "      <td>0.000037</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>0.000034</td>\n",
       "      <td>0.000034</td>\n",
       "      <td>0.000034</td>\n",
       "      <td>0.000036</td>\n",
       "      <td>0.000038</td>\n",
       "      <td>0.000040</td>\n",
       "      <td>0.000043</td>\n",
       "      <td>0.000047</td>\n",
       "      <td>0.000051</td>\n",
       "      <td>0.000056</td>\n",
       "      <td>0.000062</td>\n",
       "      <td>0.000068</td>\n",
       "      <td>0.000074</td>\n",
       "      <td>0.000082</td>\n",
       "      <td>0.000090</td>\n",
       "      <td>0.000098</td>\n",
       "      <td>0.000107</td>\n",
       "      <td>0.000117</td>\n",
       "      <td>0.000127</td>\n",
       "      <td>0.000138</td>\n",
       "      <td>0.000148</td>\n",
       "      <td>0.000159</td>\n",
       "      <td>0.000171</td>\n",
       "      <td>0.000183</td>\n",
       "      <td>0.000195</td>\n",
       "      <td>0.000207</td>\n",
       "      <td>0.000218</td>\n",
       "      <td>0.000228</td>\n",
       "      <td>0.000237</td>\n",
       "      <td>0.000244</td>\n",
       "      <td>0.000249</td>\n",
       "      <td>0.000251</td>\n",
       "      <td>0.000250</td>\n",
       "      <td>0.000247</td>\n",
       "      <td>0.000241</td>\n",
       "      <td>0.000232</td>\n",
       "      <td>0.000220</td>\n",
       "      <td>0.000207</td>\n",
       "      <td>0.000191</td>\n",
       "      <td>0.000173</td>\n",
       "      <td>0.000154</td>\n",
       "      <td>0.000134</td>\n",
       "      <td>0.000112</td>\n",
       "      <td>0.000090</td>\n",
       "      <td>0.000068</td>\n",
       "      <td>0.000047</td>\n",
       "      <td>0.000026</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>-0.000011</td>\n",
       "      <td>-0.000027</td>\n",
       "      <td>-0.000040</td>\n",
       "      <td>-0.000050</td>\n",
       "      <td>-0.000057</td>\n",
       "      <td>-0.000062</td>\n",
       "      <td>-0.000062</td>\n",
       "      <td>-0.000060</td>\n",
       "      <td>-0.000054</td>\n",
       "      <td>-0.000045</td>\n",
       "      <td>-0.000032</td>\n",
       "      <td>-0.000017</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000023</td>\n",
       "      <td>0.000046</td>\n",
       "      <td>0.000070</td>\n",
       "      <td>0.000096</td>\n",
       "      <td>0.000122</td>\n",
       "      <td>0.000148</td>\n",
       "      <td>0.000176</td>\n",
       "      <td>0.000206</td>\n",
       "      <td>0.000238</td>\n",
       "      <td>0.000272</td>\n",
       "      <td>0.000310</td>\n",
       "      <td>0.000354</td>\n",
       "      <td>0.000403</td>\n",
       "      <td>0.000457</td>\n",
       "      <td>0.000515</td>\n",
       "      <td>0.000575</td>\n",
       "      <td>0.000633</td>\n",
       "      <td>0.000685</td>\n",
       "      <td>0.000729</td>\n",
       "      <td>0.000761</td>\n",
       "      <td>0.000778</td>\n",
       "      <td>0.000777</td>\n",
       "      <td>0.000757</td>\n",
       "      <td>0.000717</td>\n",
       "      <td>0.000657</td>\n",
       "      <td>0.000578</td>\n",
       "      <td>0.000483</td>\n",
       "      <td>0.000372</td>\n",
       "      <td>0.000249</td>\n",
       "      <td>0.000114</td>\n",
       "      <td>-0.000031</td>\n",
       "      <td>-0.000184</td>\n",
       "      <td>-0.000343</td>\n",
       "      <td>-0.000504</td>\n",
       "      <td>-0.000666</td>\n",
       "      <td>-0.000825</td>\n",
       "      <td>-0.000978</td>\n",
       "      <td>-0.001121</td>\n",
       "      <td>-0.001252</td>\n",
       "      <td>-0.001366</td>\n",
       "      <td>-0.001462</td>\n",
       "      <td>-0.001536</td>\n",
       "      <td>-0.001587</td>\n",
       "      <td>-0.001614</td>\n",
       "      <td>-0.001615</td>\n",
       "      <td>-0.001592</td>\n",
       "      <td>-0.001543</td>\n",
       "      <td>-0.001469</td>\n",
       "      <td>-0.001372</td>\n",
       "      <td>-0.001253</td>\n",
       "      <td>-0.001115</td>\n",
       "      <td>-0.000961</td>\n",
       "      <td>-0.000796</td>\n",
       "      <td>-0.000625</td>\n",
       "      <td>-0.000455</td>\n",
       "      <td>-0.000289</td>\n",
       "      <td>-0.000131</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000144</td>\n",
       "      <td>0.000256</td>\n",
       "      <td>0.000350</td>\n",
       "      <td>0.000427</td>\n",
       "      <td>0.000488</td>\n",
       "      <td>0.000535</td>\n",
       "      <td>0.000567</td>\n",
       "      <td>0.000587</td>\n",
       "      <td>0.000594</td>\n",
       "      <td>0.000587</td>\n",
       "      <td>0.000567</td>\n",
       "      <td>0.000526</td>\n",
       "      <td>0.000468</td>\n",
       "      <td>0.000397</td>\n",
       "      <td>0.000316</td>\n",
       "      <td>0.000227</td>\n",
       "      <td>0.000135</td>\n",
       "      <td>0.000041</td>\n",
       "      <td>-0.000053</td>\n",
       "      <td>-0.000143</td>\n",
       "      <td>-0.000228</td>\n",
       "      <td>-0.000308</td>\n",
       "      <td>-0.000381</td>\n",
       "      <td>-0.000447</td>\n",
       "      <td>-0.000506</td>\n",
       "      <td>-0.000556</td>\n",
       "      <td>-0.000598</td>\n",
       "      <td>-0.000630</td>\n",
       "      <td>-0.000652</td>\n",
       "      <td>-0.000664</td>\n",
       "      <td>-0.000665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9096</th>\n",
       "      <td>0.000080</td>\n",
       "      <td>0.000091</td>\n",
       "      <td>0.000104</td>\n",
       "      <td>0.000121</td>\n",
       "      <td>0.000141</td>\n",
       "      <td>0.000164</td>\n",
       "      <td>0.000191</td>\n",
       "      <td>0.000220</td>\n",
       "      <td>0.000250</td>\n",
       "      <td>0.000281</td>\n",
       "      <td>0.000312</td>\n",
       "      <td>0.000342</td>\n",
       "      <td>0.000368</td>\n",
       "      <td>0.000391</td>\n",
       "      <td>0.000409</td>\n",
       "      <td>0.000423</td>\n",
       "      <td>0.000431</td>\n",
       "      <td>0.000434</td>\n",
       "      <td>0.000433</td>\n",
       "      <td>0.000427</td>\n",
       "      <td>0.000418</td>\n",
       "      <td>0.000407</td>\n",
       "      <td>0.000394</td>\n",
       "      <td>0.000380</td>\n",
       "      <td>0.000364</td>\n",
       "      <td>0.000348</td>\n",
       "      <td>0.000331</td>\n",
       "      <td>0.000313</td>\n",
       "      <td>0.000296</td>\n",
       "      <td>0.000278</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000082</td>\n",
       "      <td>0.000085</td>\n",
       "      <td>0.000087</td>\n",
       "      <td>0.000088</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.000090</td>\n",
       "      <td>0.000091</td>\n",
       "      <td>0.000092</td>\n",
       "      <td>0.000093</td>\n",
       "      <td>0.000093</td>\n",
       "      <td>0.000094</td>\n",
       "      <td>0.000094</td>\n",
       "      <td>0.000095</td>\n",
       "      <td>0.000096</td>\n",
       "      <td>0.000096</td>\n",
       "      <td>0.000097</td>\n",
       "      <td>0.000097</td>\n",
       "      <td>0.000097</td>\n",
       "      <td>0.000097</td>\n",
       "      <td>0.000097</td>\n",
       "      <td>0.000097</td>\n",
       "      <td>0.000097</td>\n",
       "      <td>0.000097</td>\n",
       "      <td>0.000096</td>\n",
       "      <td>0.000095</td>\n",
       "      <td>0.000095</td>\n",
       "      <td>0.000094</td>\n",
       "      <td>0.000093</td>\n",
       "      <td>0.000092</td>\n",
       "      <td>0.000091</td>\n",
       "      <td>0.000090</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.000087</td>\n",
       "      <td>0.000086</td>\n",
       "      <td>0.000085</td>\n",
       "      <td>0.000084</td>\n",
       "      <td>0.000083</td>\n",
       "      <td>0.000081</td>\n",
       "      <td>0.000080</td>\n",
       "      <td>0.000079</td>\n",
       "      <td>0.000077</td>\n",
       "      <td>0.000076</td>\n",
       "      <td>0.000074</td>\n",
       "      <td>0.000073</td>\n",
       "      <td>0.000072</td>\n",
       "      <td>0.000070</td>\n",
       "      <td>0.000069</td>\n",
       "      <td>0.000068</td>\n",
       "      <td>0.000067</td>\n",
       "      <td>0.000067</td>\n",
       "      <td>0.000066</td>\n",
       "      <td>0.000066</td>\n",
       "      <td>0.000065</td>\n",
       "      <td>0.000065</td>\n",
       "      <td>0.000065</td>\n",
       "      <td>0.000064</td>\n",
       "      <td>0.000064</td>\n",
       "      <td>0.000064</td>\n",
       "      <td>0.000063</td>\n",
       "      <td>0.000063</td>\n",
       "      <td>0.000063</td>\n",
       "      <td>0.000062</td>\n",
       "      <td>0.000062</td>\n",
       "      <td>0.000061</td>\n",
       "      <td>0.000061</td>\n",
       "      <td>0.000060</td>\n",
       "      <td>0.000059</td>\n",
       "      <td>0.000058</td>\n",
       "      <td>0.000058</td>\n",
       "      <td>0.000057</td>\n",
       "      <td>0.000056</td>\n",
       "      <td>0.000055</td>\n",
       "      <td>0.000054</td>\n",
       "      <td>0.000053</td>\n",
       "      <td>0.000052</td>\n",
       "      <td>0.000051</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>0.000049</td>\n",
       "      <td>0.000048</td>\n",
       "      <td>0.000047</td>\n",
       "      <td>0.000047</td>\n",
       "      <td>0.000046</td>\n",
       "      <td>0.000046</td>\n",
       "      <td>0.000045</td>\n",
       "      <td>0.000045</td>\n",
       "      <td>0.000045</td>\n",
       "      <td>0.000045</td>\n",
       "      <td>0.000046</td>\n",
       "      <td>0.000046</td>\n",
       "      <td>0.000046</td>\n",
       "      <td>0.000047</td>\n",
       "      <td>0.000047</td>\n",
       "      <td>0.000048</td>\n",
       "      <td>0.000048</td>\n",
       "      <td>0.000048</td>\n",
       "      <td>0.000049</td>\n",
       "      <td>0.000049</td>\n",
       "      <td>0.000049</td>\n",
       "      <td>0.000048</td>\n",
       "      <td>0.000048</td>\n",
       "      <td>0.000047</td>\n",
       "      <td>0.000046</td>\n",
       "      <td>0.000044</td>\n",
       "      <td>0.000043</td>\n",
       "      <td>0.000041</td>\n",
       "      <td>0.000038</td>\n",
       "      <td>0.000036</td>\n",
       "      <td>0.000034</td>\n",
       "      <td>0.000031</td>\n",
       "      <td>0.000028</td>\n",
       "      <td>0.000026</td>\n",
       "      <td>0.000024</td>\n",
       "      <td>0.000022</td>\n",
       "      <td>0.00002</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>0.000020</td>\n",
       "      <td>0.000022</td>\n",
       "      <td>0.000025</td>\n",
       "      <td>0.000028</td>\n",
       "      <td>0.000031</td>\n",
       "      <td>0.000036</td>\n",
       "      <td>0.000041</td>\n",
       "      <td>0.000046</td>\n",
       "      <td>0.000052</td>\n",
       "      <td>0.000059</td>\n",
       "      <td>0.000067</td>\n",
       "      <td>0.000075</td>\n",
       "      <td>0.000084</td>\n",
       "      <td>0.000092</td>\n",
       "      <td>0.000101</td>\n",
       "      <td>0.000110</td>\n",
       "      <td>0.000119</td>\n",
       "      <td>0.000127</td>\n",
       "      <td>0.000135</td>\n",
       "      <td>0.000141</td>\n",
       "      <td>0.000146</td>\n",
       "      <td>0.000149</td>\n",
       "      <td>0.000151</td>\n",
       "      <td>0.000150</td>\n",
       "      <td>0.000148</td>\n",
       "      <td>0.000144</td>\n",
       "      <td>0.000137</td>\n",
       "      <td>0.000129</td>\n",
       "      <td>0.000119</td>\n",
       "      <td>0.000108</td>\n",
       "      <td>0.000095</td>\n",
       "      <td>0.000082</td>\n",
       "      <td>0.000068</td>\n",
       "      <td>0.000054</td>\n",
       "      <td>0.000040</td>\n",
       "      <td>0.000027</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>-0.000003</td>\n",
       "      <td>-0.000009</td>\n",
       "      <td>-0.000012</td>\n",
       "      <td>-0.000012</td>\n",
       "      <td>-0.000010</td>\n",
       "      <td>-0.000005</td>\n",
       "      <td>2.694662e-06</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>0.000023</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>0.000048</td>\n",
       "      <td>0.000060</td>\n",
       "      <td>0.000071</td>\n",
       "      <td>0.000082</td>\n",
       "      <td>0.000090</td>\n",
       "      <td>0.000096</td>\n",
       "      <td>0.000099</td>\n",
       "      <td>0.000099</td>\n",
       "      <td>0.000097</td>\n",
       "      <td>0.000091</td>\n",
       "      <td>0.000083</td>\n",
       "      <td>0.000072</td>\n",
       "      <td>0.000059</td>\n",
       "      <td>0.000044</td>\n",
       "      <td>0.000027</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>-0.000010</td>\n",
       "      <td>-0.000029</td>\n",
       "      <td>-0.000049</td>\n",
       "      <td>-0.000068</td>\n",
       "      <td>-0.000086</td>\n",
       "      <td>-0.000103</td>\n",
       "      <td>-0.000119</td>\n",
       "      <td>-0.000133</td>\n",
       "      <td>-0.000146</td>\n",
       "      <td>-0.000156</td>\n",
       "      <td>-0.000164</td>\n",
       "      <td>-0.000169</td>\n",
       "      <td>-0.000172</td>\n",
       "      <td>-0.000172</td>\n",
       "      <td>-0.000170</td>\n",
       "      <td>-0.000165</td>\n",
       "      <td>-0.000157</td>\n",
       "      <td>-0.000147</td>\n",
       "      <td>-0.000136</td>\n",
       "      <td>-0.000123</td>\n",
       "      <td>-0.000108</td>\n",
       "      <td>-0.000093</td>\n",
       "      <td>-0.000077</td>\n",
       "      <td>-0.000061</td>\n",
       "      <td>-0.000044</td>\n",
       "      <td>-0.000029</td>\n",
       "      <td>-0.000013</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>0.000028</td>\n",
       "      <td>0.000040</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>0.000059</td>\n",
       "      <td>0.000065</td>\n",
       "      <td>0.000070</td>\n",
       "      <td>0.000072</td>\n",
       "      <td>0.000072</td>\n",
       "      <td>0.000070</td>\n",
       "      <td>0.000065</td>\n",
       "      <td>0.000057</td>\n",
       "      <td>0.000046</td>\n",
       "      <td>0.000034</td>\n",
       "      <td>0.000019</td>\n",
       "      <td>2.891610e-06</td>\n",
       "      <td>-0.000015</td>\n",
       "      <td>-0.000033</td>\n",
       "      <td>-0.000051</td>\n",
       "      <td>-0.000070</td>\n",
       "      <td>-0.000087</td>\n",
       "      <td>-0.000102</td>\n",
       "      <td>-0.000116</td>\n",
       "      <td>-0.000127</td>\n",
       "      <td>-0.000135</td>\n",
       "      <td>-0.000141</td>\n",
       "      <td>-0.000143</td>\n",
       "      <td>-0.000142</td>\n",
       "      <td>-0.000137</td>\n",
       "      <td>-0.000130</td>\n",
       "      <td>-0.000120</td>\n",
       "      <td>-0.000107</td>\n",
       "      <td>-0.000092</td>\n",
       "      <td>-0.000076</td>\n",
       "      <td>-0.000058</td>\n",
       "      <td>-0.000039</td>\n",
       "      <td>-0.000021</td>\n",
       "      <td>-0.000002</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>0.000032</td>\n",
       "      <td>0.000048</td>\n",
       "      <td>0.000061</td>\n",
       "      <td>0.000072</td>\n",
       "      <td>0.000081</td>\n",
       "      <td>0.000088</td>\n",
       "      <td>0.000092</td>\n",
       "      <td>0.000094</td>\n",
       "      <td>0.000094</td>\n",
       "      <td>0.000091</td>\n",
       "      <td>0.000086</td>\n",
       "      <td>0.000080</td>\n",
       "      <td>0.000072</td>\n",
       "      <td>0.000064</td>\n",
       "      <td>0.000054</td>\n",
       "      <td>0.000044</td>\n",
       "      <td>0.000034</td>\n",
       "      <td>0.000023</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>-0.000004</td>\n",
       "      <td>-0.000012</td>\n",
       "      <td>-0.000019</td>\n",
       "      <td>-0.000025</td>\n",
       "      <td>-0.000030</td>\n",
       "      <td>-0.000034</td>\n",
       "      <td>-0.000037</td>\n",
       "      <td>-0.000039</td>\n",
       "      <td>-0.000040</td>\n",
       "      <td>-0.000040</td>\n",
       "      <td>-0.000040</td>\n",
       "      <td>-0.000039</td>\n",
       "      <td>-0.000037</td>\n",
       "      <td>-0.000035</td>\n",
       "      <td>-0.000032</td>\n",
       "      <td>-0.000028</td>\n",
       "      <td>-0.000025</td>\n",
       "      <td>-0.000021</td>\n",
       "      <td>-0.000017</td>\n",
       "      <td>-0.000013</td>\n",
       "      <td>-0.000008</td>\n",
       "      <td>-0.000004</td>\n",
       "      <td>3.069076e-07</td>\n",
       "      <td>4.595399e-06</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>0.000021</td>\n",
       "      <td>0.000025</td>\n",
       "      <td>0.000028</td>\n",
       "      <td>0.000032</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>0.000038</td>\n",
       "      <td>0.000040</td>\n",
       "      <td>0.000042</td>\n",
       "      <td>0.000044</td>\n",
       "      <td>0.000046</td>\n",
       "      <td>0.000048</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>0.000051</td>\n",
       "      <td>0.000053</td>\n",
       "      <td>0.000055</td>\n",
       "      <td>0.000057</td>\n",
       "      <td>0.000059</td>\n",
       "      <td>0.000061</td>\n",
       "      <td>0.000062</td>\n",
       "      <td>0.000064</td>\n",
       "      <td>0.000065</td>\n",
       "      <td>0.000066</td>\n",
       "      <td>0.000066</td>\n",
       "      <td>0.000066</td>\n",
       "      <td>0.000066</td>\n",
       "      <td>0.000066</td>\n",
       "      <td>0.000066</td>\n",
       "      <td>0.000067</td>\n",
       "      <td>0.000068</td>\n",
       "      <td>0.000071</td>\n",
       "      <td>0.000074</td>\n",
       "      <td>0.000080</td>\n",
       "      <td>0.000086</td>\n",
       "      <td>0.000094</td>\n",
       "      <td>0.000104</td>\n",
       "      <td>0.000116</td>\n",
       "      <td>0.000129</td>\n",
       "      <td>0.000144</td>\n",
       "      <td>0.000161</td>\n",
       "      <td>0.000179</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>0.000224</td>\n",
       "      <td>0.000250</td>\n",
       "      <td>0.000280</td>\n",
       "      <td>0.000313</td>\n",
       "      <td>0.000349</td>\n",
       "      <td>0.000389</td>\n",
       "      <td>0.000432</td>\n",
       "      <td>0.000479</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000165</td>\n",
       "      <td>0.000162</td>\n",
       "      <td>0.000159</td>\n",
       "      <td>0.000155</td>\n",
       "      <td>0.000150</td>\n",
       "      <td>0.000145</td>\n",
       "      <td>0.000139</td>\n",
       "      <td>0.000133</td>\n",
       "      <td>0.000128</td>\n",
       "      <td>0.000123</td>\n",
       "      <td>0.000118</td>\n",
       "      <td>0.000114</td>\n",
       "      <td>0.000110</td>\n",
       "      <td>0.000108</td>\n",
       "      <td>0.000106</td>\n",
       "      <td>0.000106</td>\n",
       "      <td>0.000106</td>\n",
       "      <td>0.000107</td>\n",
       "      <td>0.000110</td>\n",
       "      <td>0.000114</td>\n",
       "      <td>0.000118</td>\n",
       "      <td>0.000124</td>\n",
       "      <td>0.000130</td>\n",
       "      <td>0.000138</td>\n",
       "      <td>0.000145</td>\n",
       "      <td>0.000154</td>\n",
       "      <td>0.000162</td>\n",
       "      <td>0.000171</td>\n",
       "      <td>0.000179</td>\n",
       "      <td>0.000188</td>\n",
       "      <td>0.000196</td>\n",
       "      <td>0.000203</td>\n",
       "      <td>0.000210</td>\n",
       "      <td>0.000215</td>\n",
       "      <td>0.000220</td>\n",
       "      <td>0.000223</td>\n",
       "      <td>0.000225</td>\n",
       "      <td>0.000225</td>\n",
       "      <td>0.000224</td>\n",
       "      <td>0.000221</td>\n",
       "      <td>0.000216</td>\n",
       "      <td>0.000210</td>\n",
       "      <td>0.000202</td>\n",
       "      <td>0.000192</td>\n",
       "      <td>0.000181</td>\n",
       "      <td>0.000169</td>\n",
       "      <td>0.000155</td>\n",
       "      <td>0.000140</td>\n",
       "      <td>0.000125</td>\n",
       "      <td>0.000109</td>\n",
       "      <td>0.000092</td>\n",
       "      <td>0.000076</td>\n",
       "      <td>0.000059</td>\n",
       "      <td>0.000044</td>\n",
       "      <td>0.000028</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>1.253275e-06</td>\n",
       "      <td>-0.000011</td>\n",
       "      <td>-0.000021</td>\n",
       "      <td>-0.000030</td>\n",
       "      <td>-0.000037</td>\n",
       "      <td>-0.000043</td>\n",
       "      <td>-0.000048</td>\n",
       "      <td>-0.000050</td>\n",
       "      <td>-0.000051</td>\n",
       "      <td>-0.000051</td>\n",
       "      <td>-0.000049</td>\n",
       "      <td>-0.000046</td>\n",
       "      <td>-0.000042</td>\n",
       "      <td>-0.000037</td>\n",
       "      <td>-0.000031</td>\n",
       "      <td>-0.000024</td>\n",
       "      <td>-0.000018</td>\n",
       "      <td>-0.000011</td>\n",
       "      <td>-0.000004</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>0.000021</td>\n",
       "      <td>0.000026</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>0.000034</td>\n",
       "      <td>0.000038</td>\n",
       "      <td>0.000041</td>\n",
       "      <td>0.000043</td>\n",
       "      <td>0.000046</td>\n",
       "      <td>0.000048</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>0.000052</td>\n",
       "      <td>0.000055</td>\n",
       "      <td>0.000057</td>\n",
       "      <td>0.000059</td>\n",
       "      <td>0.000061</td>\n",
       "      <td>0.000064</td>\n",
       "      <td>0.000066</td>\n",
       "      <td>0.000069</td>\n",
       "      <td>0.000071</td>\n",
       "      <td>0.000074</td>\n",
       "      <td>0.000077</td>\n",
       "      <td>0.000080</td>\n",
       "      <td>0.000084</td>\n",
       "      <td>0.000088</td>\n",
       "      <td>0.000092</td>\n",
       "      <td>0.000097</td>\n",
       "      <td>0.000103</td>\n",
       "      <td>0.000109</td>\n",
       "      <td>0.000116</td>\n",
       "      <td>0.000123</td>\n",
       "      <td>0.000131</td>\n",
       "      <td>0.000140</td>\n",
       "      <td>0.000149</td>\n",
       "      <td>0.000160</td>\n",
       "      <td>0.000170</td>\n",
       "      <td>0.000182</td>\n",
       "      <td>0.000194</td>\n",
       "      <td>0.000206</td>\n",
       "      <td>0.000218</td>\n",
       "      <td>0.000231</td>\n",
       "      <td>0.000243</td>\n",
       "      <td>0.000256</td>\n",
       "      <td>0.000268</td>\n",
       "      <td>0.000280</td>\n",
       "      <td>0.000290</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>0.000309</td>\n",
       "      <td>0.000316</td>\n",
       "      <td>0.000322</td>\n",
       "      <td>0.000326</td>\n",
       "      <td>0.000328</td>\n",
       "      <td>0.000327</td>\n",
       "      <td>0.000323</td>\n",
       "      <td>0.000317</td>\n",
       "      <td>0.000309</td>\n",
       "      <td>0.000297</td>\n",
       "      <td>0.000283</td>\n",
       "      <td>0.000266</td>\n",
       "      <td>0.000246</td>\n",
       "      <td>0.000224</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>0.000173</td>\n",
       "      <td>0.000145</td>\n",
       "      <td>0.000116</td>\n",
       "      <td>0.000086</td>\n",
       "      <td>0.000055</td>\n",
       "      <td>0.000023</td>\n",
       "      <td>-0.000007</td>\n",
       "      <td>-0.000037</td>\n",
       "      <td>-0.000066</td>\n",
       "      <td>-0.000093</td>\n",
       "      <td>-0.000117</td>\n",
       "      <td>-0.000139</td>\n",
       "      <td>-0.000158</td>\n",
       "      <td>-0.000173</td>\n",
       "      <td>-0.000185</td>\n",
       "      <td>-0.000194</td>\n",
       "      <td>-0.000198</td>\n",
       "      <td>-0.000199</td>\n",
       "      <td>-0.000195</td>\n",
       "      <td>-0.000188</td>\n",
       "      <td>-0.000177</td>\n",
       "      <td>-0.000163</td>\n",
       "      <td>-0.000145</td>\n",
       "      <td>-0.000125</td>\n",
       "      <td>-0.000102</td>\n",
       "      <td>-0.000077</td>\n",
       "      <td>-0.000051</td>\n",
       "      <td>-0.000025</td>\n",
       "      <td>2.282651e-06</td>\n",
       "      <td>0.000029</td>\n",
       "      <td>0.000055</td>\n",
       "      <td>0.000079</td>\n",
       "      <td>0.000102</td>\n",
       "      <td>0.000123</td>\n",
       "      <td>0.000141</td>\n",
       "      <td>0.000156</td>\n",
       "      <td>0.000167</td>\n",
       "      <td>0.000176</td>\n",
       "      <td>0.000181</td>\n",
       "      <td>0.000182</td>\n",
       "      <td>0.000180</td>\n",
       "      <td>0.000175</td>\n",
       "      <td>0.000167</td>\n",
       "      <td>0.000155</td>\n",
       "      <td>0.000142</td>\n",
       "      <td>0.000126</td>\n",
       "      <td>0.000108</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.000069</td>\n",
       "      <td>0.000048</td>\n",
       "      <td>0.000027</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>-0.000013</td>\n",
       "      <td>-0.000032</td>\n",
       "      <td>-0.000049</td>\n",
       "      <td>-0.000065</td>\n",
       "      <td>-0.000079</td>\n",
       "      <td>-0.000091</td>\n",
       "      <td>-0.000100</td>\n",
       "      <td>-0.000107</td>\n",
       "      <td>-0.000112</td>\n",
       "      <td>-0.000114</td>\n",
       "      <td>-0.000113</td>\n",
       "      <td>-0.000110</td>\n",
       "      <td>-0.000106</td>\n",
       "      <td>-0.000098</td>\n",
       "      <td>-0.000090</td>\n",
       "      <td>-0.000079</td>\n",
       "      <td>-0.000067</td>\n",
       "      <td>-0.000054</td>\n",
       "      <td>-0.000041</td>\n",
       "      <td>-0.000027</td>\n",
       "      <td>-0.000013</td>\n",
       "      <td>1.197801e-06</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>0.000028</td>\n",
       "      <td>0.000040</td>\n",
       "      <td>0.000051</td>\n",
       "      <td>0.000061</td>\n",
       "      <td>0.000070</td>\n",
       "      <td>0.000078</td>\n",
       "      <td>0.000084</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.000093</td>\n",
       "      <td>0.000095</td>\n",
       "      <td>0.000096</td>\n",
       "      <td>0.000096</td>\n",
       "      <td>0.000094</td>\n",
       "      <td>0.000092</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.000086</td>\n",
       "      <td>0.000082</td>\n",
       "      <td>0.000077</td>\n",
       "      <td>0.000073</td>\n",
       "      <td>0.000068</td>\n",
       "      <td>0.000063</td>\n",
       "      <td>0.000058</td>\n",
       "      <td>0.000053</td>\n",
       "      <td>0.000048</td>\n",
       "      <td>0.000044</td>\n",
       "      <td>0.000041</td>\n",
       "      <td>0.000038</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>0.000033</td>\n",
       "      <td>0.000032</td>\n",
       "      <td>0.000032</td>\n",
       "      <td>0.000032</td>\n",
       "      <td>0.000033</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>0.000038</td>\n",
       "      <td>0.000042</td>\n",
       "      <td>0.000046</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>0.000056</td>\n",
       "      <td>0.000061</td>\n",
       "      <td>0.000067</td>\n",
       "      <td>0.000073</td>\n",
       "      <td>0.000079</td>\n",
       "      <td>0.000086</td>\n",
       "      <td>0.000092</td>\n",
       "      <td>0.000098</td>\n",
       "      <td>0.000105</td>\n",
       "      <td>0.000111</td>\n",
       "      <td>0.000117</td>\n",
       "      <td>0.000122</td>\n",
       "      <td>0.000128</td>\n",
       "      <td>0.000133</td>\n",
       "      <td>0.000138</td>\n",
       "      <td>0.000143</td>\n",
       "      <td>0.000148</td>\n",
       "      <td>0.000152</td>\n",
       "      <td>0.000155</td>\n",
       "      <td>0.000158</td>\n",
       "      <td>0.000161</td>\n",
       "      <td>0.000163</td>\n",
       "      <td>0.000164</td>\n",
       "      <td>0.000164</td>\n",
       "      <td>0.000164</td>\n",
       "      <td>0.000163</td>\n",
       "      <td>0.000161</td>\n",
       "      <td>0.000159</td>\n",
       "      <td>0.000155</td>\n",
       "      <td>0.000152</td>\n",
       "      <td>0.000147</td>\n",
       "      <td>0.000142</td>\n",
       "      <td>0.000137</td>\n",
       "      <td>0.000131</td>\n",
       "      <td>0.000125</td>\n",
       "      <td>0.000118</td>\n",
       "      <td>0.000111</td>\n",
       "      <td>0.000104</td>\n",
       "      <td>0.000096</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.000082</td>\n",
       "      <td>0.000074</td>\n",
       "      <td>0.000067</td>\n",
       "      <td>0.000060</td>\n",
       "      <td>0.000053</td>\n",
       "      <td>0.000047</td>\n",
       "      <td>0.000041</td>\n",
       "      <td>0.000037</td>\n",
       "      <td>0.000033</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>0.000028</td>\n",
       "      <td>0.000028</td>\n",
       "      <td>0.000028</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>0.000033</td>\n",
       "      <td>0.000036</td>\n",
       "      <td>0.000041</td>\n",
       "      <td>0.000047</td>\n",
       "      <td>0.000053</td>\n",
       "      <td>0.000061</td>\n",
       "      <td>0.000069</td>\n",
       "      <td>0.000078</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000112</td>\n",
       "      <td>0.000124</td>\n",
       "      <td>0.000137</td>\n",
       "      <td>0.000149</td>\n",
       "      <td>0.000162</td>\n",
       "      <td>0.000174</td>\n",
       "      <td>0.000185</td>\n",
       "      <td>0.000196</td>\n",
       "      <td>0.000206</td>\n",
       "      <td>0.000215</td>\n",
       "      <td>0.000223</td>\n",
       "      <td>0.000229</td>\n",
       "      <td>0.000234</td>\n",
       "      <td>0.000238</td>\n",
       "      <td>0.000239</td>\n",
       "      <td>0.000239</td>\n",
       "      <td>0.000236</td>\n",
       "      <td>0.000232</td>\n",
       "      <td>0.000225</td>\n",
       "      <td>0.000217</td>\n",
       "      <td>0.000208</td>\n",
       "      <td>0.000196</td>\n",
       "      <td>0.000184</td>\n",
       "      <td>0.000169</td>\n",
       "      <td>0.000154</td>\n",
       "      <td>0.000137</td>\n",
       "      <td>0.000119</td>\n",
       "      <td>0.000099</td>\n",
       "      <td>0.000079</td>\n",
       "      <td>0.000058</td>\n",
       "      <td>0.000037</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>-0.000003</td>\n",
       "      <td>-0.000020</td>\n",
       "      <td>-0.000035</td>\n",
       "      <td>-0.000047</td>\n",
       "      <td>-0.000056</td>\n",
       "      <td>-0.000062</td>\n",
       "      <td>-0.000064</td>\n",
       "      <td>-0.000062</td>\n",
       "      <td>-0.000057</td>\n",
       "      <td>-0.000049</td>\n",
       "      <td>-0.000038</td>\n",
       "      <td>-0.000024</td>\n",
       "      <td>-0.000008</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.000029</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>0.000072</td>\n",
       "      <td>0.000093</td>\n",
       "      <td>0.000116</td>\n",
       "      <td>0.000140</td>\n",
       "      <td>0.000165</td>\n",
       "      <td>0.000193</td>\n",
       "      <td>0.000225</td>\n",
       "      <td>0.000260</td>\n",
       "      <td>0.000302</td>\n",
       "      <td>0.000350</td>\n",
       "      <td>0.000406</td>\n",
       "      <td>0.000469</td>\n",
       "      <td>0.000537</td>\n",
       "      <td>0.000607</td>\n",
       "      <td>0.000676</td>\n",
       "      <td>0.000740</td>\n",
       "      <td>0.000794</td>\n",
       "      <td>0.000835</td>\n",
       "      <td>0.000859</td>\n",
       "      <td>0.000863</td>\n",
       "      <td>0.000847</td>\n",
       "      <td>0.000809</td>\n",
       "      <td>0.000749</td>\n",
       "      <td>0.000669</td>\n",
       "      <td>0.000570</td>\n",
       "      <td>0.000454</td>\n",
       "      <td>0.000324</td>\n",
       "      <td>0.000180</td>\n",
       "      <td>0.000024</td>\n",
       "      <td>-0.000142</td>\n",
       "      <td>-0.000316</td>\n",
       "      <td>-0.000495</td>\n",
       "      <td>-0.000677</td>\n",
       "      <td>-0.000858</td>\n",
       "      <td>-0.001033</td>\n",
       "      <td>-0.001199</td>\n",
       "      <td>-0.001349</td>\n",
       "      <td>-0.001481</td>\n",
       "      <td>-0.001591</td>\n",
       "      <td>-0.001675</td>\n",
       "      <td>-0.001730</td>\n",
       "      <td>-0.001756</td>\n",
       "      <td>-0.001750</td>\n",
       "      <td>-0.001712</td>\n",
       "      <td>-0.001643</td>\n",
       "      <td>-0.001546</td>\n",
       "      <td>-0.001421</td>\n",
       "      <td>-0.001272</td>\n",
       "      <td>-0.001103</td>\n",
       "      <td>-0.000917</td>\n",
       "      <td>-0.000721</td>\n",
       "      <td>-0.000520</td>\n",
       "      <td>-0.000321</td>\n",
       "      <td>-0.000130</td>\n",
       "      <td>0.000049</td>\n",
       "      <td>0.000209</td>\n",
       "      <td>0.000346</td>\n",
       "      <td>0.000456</td>\n",
       "      <td>0.000538</td>\n",
       "      <td>0.000589</td>\n",
       "      <td>0.000610</td>\n",
       "      <td>0.000600</td>\n",
       "      <td>0.000564</td>\n",
       "      <td>0.000502</td>\n",
       "      <td>0.000420</td>\n",
       "      <td>0.000320</td>\n",
       "      <td>0.000206</td>\n",
       "      <td>0.000087</td>\n",
       "      <td>-0.000035</td>\n",
       "      <td>-0.000156</td>\n",
       "      <td>-0.000272</td>\n",
       "      <td>-0.000382</td>\n",
       "      <td>-0.000482</td>\n",
       "      <td>-0.000569</td>\n",
       "      <td>-0.000642</td>\n",
       "      <td>-0.000699</td>\n",
       "      <td>-0.000740</td>\n",
       "      <td>-0.000765</td>\n",
       "      <td>-0.000775</td>\n",
       "      <td>-0.000771</td>\n",
       "      <td>-0.000754</td>\n",
       "      <td>-0.000726</td>\n",
       "      <td>-0.000687</td>\n",
       "      <td>-0.000639</td>\n",
       "      <td>-0.000583</td>\n",
       "      <td>-0.000520</td>\n",
       "      <td>-0.000451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9097</th>\n",
       "      <td>0.000080</td>\n",
       "      <td>0.000091</td>\n",
       "      <td>0.000104</td>\n",
       "      <td>0.000121</td>\n",
       "      <td>0.000141</td>\n",
       "      <td>0.000164</td>\n",
       "      <td>0.000191</td>\n",
       "      <td>0.000220</td>\n",
       "      <td>0.000250</td>\n",
       "      <td>0.000282</td>\n",
       "      <td>0.000313</td>\n",
       "      <td>0.000342</td>\n",
       "      <td>0.000368</td>\n",
       "      <td>0.000391</td>\n",
       "      <td>0.000410</td>\n",
       "      <td>0.000423</td>\n",
       "      <td>0.000431</td>\n",
       "      <td>0.000434</td>\n",
       "      <td>0.000433</td>\n",
       "      <td>0.000427</td>\n",
       "      <td>0.000418</td>\n",
       "      <td>0.000407</td>\n",
       "      <td>0.000394</td>\n",
       "      <td>0.000380</td>\n",
       "      <td>0.000364</td>\n",
       "      <td>0.000348</td>\n",
       "      <td>0.000331</td>\n",
       "      <td>0.000313</td>\n",
       "      <td>0.000295</td>\n",
       "      <td>0.000277</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000082</td>\n",
       "      <td>0.000085</td>\n",
       "      <td>0.000087</td>\n",
       "      <td>0.000088</td>\n",
       "      <td>0.000090</td>\n",
       "      <td>0.000091</td>\n",
       "      <td>0.000091</td>\n",
       "      <td>0.000092</td>\n",
       "      <td>0.000093</td>\n",
       "      <td>0.000093</td>\n",
       "      <td>0.000094</td>\n",
       "      <td>0.000095</td>\n",
       "      <td>0.000095</td>\n",
       "      <td>0.000096</td>\n",
       "      <td>0.000097</td>\n",
       "      <td>0.000097</td>\n",
       "      <td>0.000098</td>\n",
       "      <td>0.000098</td>\n",
       "      <td>0.000098</td>\n",
       "      <td>0.000098</td>\n",
       "      <td>0.000098</td>\n",
       "      <td>0.000097</td>\n",
       "      <td>0.000097</td>\n",
       "      <td>0.000096</td>\n",
       "      <td>0.000096</td>\n",
       "      <td>0.000095</td>\n",
       "      <td>0.000094</td>\n",
       "      <td>0.000093</td>\n",
       "      <td>0.000091</td>\n",
       "      <td>0.000090</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.000088</td>\n",
       "      <td>0.000087</td>\n",
       "      <td>0.000086</td>\n",
       "      <td>0.000085</td>\n",
       "      <td>0.000084</td>\n",
       "      <td>0.000082</td>\n",
       "      <td>0.000081</td>\n",
       "      <td>0.000080</td>\n",
       "      <td>0.000078</td>\n",
       "      <td>0.000077</td>\n",
       "      <td>0.000075</td>\n",
       "      <td>0.000074</td>\n",
       "      <td>0.000073</td>\n",
       "      <td>0.000071</td>\n",
       "      <td>0.000070</td>\n",
       "      <td>0.000069</td>\n",
       "      <td>0.000068</td>\n",
       "      <td>0.000067</td>\n",
       "      <td>0.000067</td>\n",
       "      <td>0.000066</td>\n",
       "      <td>0.000065</td>\n",
       "      <td>0.000065</td>\n",
       "      <td>0.000065</td>\n",
       "      <td>0.000064</td>\n",
       "      <td>0.000064</td>\n",
       "      <td>0.000064</td>\n",
       "      <td>0.000064</td>\n",
       "      <td>0.000063</td>\n",
       "      <td>0.000063</td>\n",
       "      <td>0.000063</td>\n",
       "      <td>0.000063</td>\n",
       "      <td>0.000062</td>\n",
       "      <td>0.000062</td>\n",
       "      <td>0.000061</td>\n",
       "      <td>0.000060</td>\n",
       "      <td>0.000060</td>\n",
       "      <td>0.000059</td>\n",
       "      <td>0.000058</td>\n",
       "      <td>0.000057</td>\n",
       "      <td>0.000056</td>\n",
       "      <td>0.000055</td>\n",
       "      <td>0.000054</td>\n",
       "      <td>0.000053</td>\n",
       "      <td>0.000052</td>\n",
       "      <td>0.000051</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>0.000049</td>\n",
       "      <td>0.000048</td>\n",
       "      <td>0.000047</td>\n",
       "      <td>0.000046</td>\n",
       "      <td>0.000046</td>\n",
       "      <td>0.000045</td>\n",
       "      <td>0.000045</td>\n",
       "      <td>0.000045</td>\n",
       "      <td>0.000045</td>\n",
       "      <td>0.000045</td>\n",
       "      <td>0.000045</td>\n",
       "      <td>0.000046</td>\n",
       "      <td>0.000046</td>\n",
       "      <td>0.000046</td>\n",
       "      <td>0.000047</td>\n",
       "      <td>0.000047</td>\n",
       "      <td>0.000048</td>\n",
       "      <td>0.000048</td>\n",
       "      <td>0.000048</td>\n",
       "      <td>0.000049</td>\n",
       "      <td>0.000049</td>\n",
       "      <td>0.000048</td>\n",
       "      <td>0.000048</td>\n",
       "      <td>0.000047</td>\n",
       "      <td>0.000046</td>\n",
       "      <td>0.000045</td>\n",
       "      <td>0.000043</td>\n",
       "      <td>0.000041</td>\n",
       "      <td>0.000039</td>\n",
       "      <td>0.000036</td>\n",
       "      <td>0.000034</td>\n",
       "      <td>0.000031</td>\n",
       "      <td>0.000029</td>\n",
       "      <td>0.000026</td>\n",
       "      <td>0.000024</td>\n",
       "      <td>0.000022</td>\n",
       "      <td>0.00002</td>\n",
       "      <td>0.000019</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>0.000019</td>\n",
       "      <td>0.000022</td>\n",
       "      <td>0.000024</td>\n",
       "      <td>0.000027</td>\n",
       "      <td>0.000031</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>0.000040</td>\n",
       "      <td>0.000046</td>\n",
       "      <td>0.000052</td>\n",
       "      <td>0.000059</td>\n",
       "      <td>0.000066</td>\n",
       "      <td>0.000075</td>\n",
       "      <td>0.000083</td>\n",
       "      <td>0.000092</td>\n",
       "      <td>0.000101</td>\n",
       "      <td>0.000110</td>\n",
       "      <td>0.000119</td>\n",
       "      <td>0.000127</td>\n",
       "      <td>0.000135</td>\n",
       "      <td>0.000141</td>\n",
       "      <td>0.000146</td>\n",
       "      <td>0.000149</td>\n",
       "      <td>0.000151</td>\n",
       "      <td>0.000151</td>\n",
       "      <td>0.000149</td>\n",
       "      <td>0.000144</td>\n",
       "      <td>0.000138</td>\n",
       "      <td>0.000130</td>\n",
       "      <td>0.000120</td>\n",
       "      <td>0.000108</td>\n",
       "      <td>0.000096</td>\n",
       "      <td>0.000082</td>\n",
       "      <td>0.000068</td>\n",
       "      <td>0.000054</td>\n",
       "      <td>0.000040</td>\n",
       "      <td>0.000027</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>-0.000003</td>\n",
       "      <td>-0.000009</td>\n",
       "      <td>-0.000012</td>\n",
       "      <td>-0.000012</td>\n",
       "      <td>-0.000010</td>\n",
       "      <td>-0.000005</td>\n",
       "      <td>2.403662e-06</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>0.000023</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>0.000047</td>\n",
       "      <td>0.000060</td>\n",
       "      <td>0.000071</td>\n",
       "      <td>0.000082</td>\n",
       "      <td>0.000090</td>\n",
       "      <td>0.000096</td>\n",
       "      <td>0.000099</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000097</td>\n",
       "      <td>0.000092</td>\n",
       "      <td>0.000083</td>\n",
       "      <td>0.000072</td>\n",
       "      <td>0.000059</td>\n",
       "      <td>0.000044</td>\n",
       "      <td>0.000027</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>-0.000010</td>\n",
       "      <td>-0.000029</td>\n",
       "      <td>-0.000049</td>\n",
       "      <td>-0.000068</td>\n",
       "      <td>-0.000086</td>\n",
       "      <td>-0.000103</td>\n",
       "      <td>-0.000119</td>\n",
       "      <td>-0.000133</td>\n",
       "      <td>-0.000146</td>\n",
       "      <td>-0.000156</td>\n",
       "      <td>-0.000164</td>\n",
       "      <td>-0.000169</td>\n",
       "      <td>-0.000172</td>\n",
       "      <td>-0.000172</td>\n",
       "      <td>-0.000170</td>\n",
       "      <td>-0.000165</td>\n",
       "      <td>-0.000157</td>\n",
       "      <td>-0.000148</td>\n",
       "      <td>-0.000136</td>\n",
       "      <td>-0.000123</td>\n",
       "      <td>-0.000109</td>\n",
       "      <td>-0.000093</td>\n",
       "      <td>-0.000077</td>\n",
       "      <td>-0.000061</td>\n",
       "      <td>-0.000045</td>\n",
       "      <td>-0.000029</td>\n",
       "      <td>-0.000013</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>0.000029</td>\n",
       "      <td>0.000040</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>0.000059</td>\n",
       "      <td>0.000066</td>\n",
       "      <td>0.000070</td>\n",
       "      <td>0.000073</td>\n",
       "      <td>0.000073</td>\n",
       "      <td>0.000070</td>\n",
       "      <td>0.000065</td>\n",
       "      <td>0.000057</td>\n",
       "      <td>0.000047</td>\n",
       "      <td>0.000034</td>\n",
       "      <td>0.000019</td>\n",
       "      <td>3.104169e-06</td>\n",
       "      <td>-0.000015</td>\n",
       "      <td>-0.000033</td>\n",
       "      <td>-0.000051</td>\n",
       "      <td>-0.000070</td>\n",
       "      <td>-0.000087</td>\n",
       "      <td>-0.000102</td>\n",
       "      <td>-0.000116</td>\n",
       "      <td>-0.000127</td>\n",
       "      <td>-0.000136</td>\n",
       "      <td>-0.000141</td>\n",
       "      <td>-0.000143</td>\n",
       "      <td>-0.000142</td>\n",
       "      <td>-0.000137</td>\n",
       "      <td>-0.000130</td>\n",
       "      <td>-0.000120</td>\n",
       "      <td>-0.000107</td>\n",
       "      <td>-0.000092</td>\n",
       "      <td>-0.000075</td>\n",
       "      <td>-0.000058</td>\n",
       "      <td>-0.000039</td>\n",
       "      <td>-0.000020</td>\n",
       "      <td>-0.000002</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>0.000032</td>\n",
       "      <td>0.000048</td>\n",
       "      <td>0.000061</td>\n",
       "      <td>0.000072</td>\n",
       "      <td>0.000081</td>\n",
       "      <td>0.000088</td>\n",
       "      <td>0.000092</td>\n",
       "      <td>0.000094</td>\n",
       "      <td>0.000094</td>\n",
       "      <td>0.000091</td>\n",
       "      <td>0.000086</td>\n",
       "      <td>0.000080</td>\n",
       "      <td>0.000072</td>\n",
       "      <td>0.000063</td>\n",
       "      <td>0.000054</td>\n",
       "      <td>0.000044</td>\n",
       "      <td>0.000034</td>\n",
       "      <td>0.000023</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>-0.000004</td>\n",
       "      <td>-0.000012</td>\n",
       "      <td>-0.000019</td>\n",
       "      <td>-0.000025</td>\n",
       "      <td>-0.000030</td>\n",
       "      <td>-0.000034</td>\n",
       "      <td>-0.000037</td>\n",
       "      <td>-0.000039</td>\n",
       "      <td>-0.000040</td>\n",
       "      <td>-0.000040</td>\n",
       "      <td>-0.000040</td>\n",
       "      <td>-0.000039</td>\n",
       "      <td>-0.000037</td>\n",
       "      <td>-0.000035</td>\n",
       "      <td>-0.000032</td>\n",
       "      <td>-0.000029</td>\n",
       "      <td>-0.000025</td>\n",
       "      <td>-0.000021</td>\n",
       "      <td>-0.000017</td>\n",
       "      <td>-0.000013</td>\n",
       "      <td>-0.000009</td>\n",
       "      <td>-0.000004</td>\n",
       "      <td>9.076379e-08</td>\n",
       "      <td>4.408955e-06</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>0.000021</td>\n",
       "      <td>0.000025</td>\n",
       "      <td>0.000028</td>\n",
       "      <td>0.000032</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>0.000038</td>\n",
       "      <td>0.000040</td>\n",
       "      <td>0.000042</td>\n",
       "      <td>0.000044</td>\n",
       "      <td>0.000046</td>\n",
       "      <td>0.000048</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>0.000051</td>\n",
       "      <td>0.000053</td>\n",
       "      <td>0.000055</td>\n",
       "      <td>0.000057</td>\n",
       "      <td>0.000059</td>\n",
       "      <td>0.000061</td>\n",
       "      <td>0.000062</td>\n",
       "      <td>0.000064</td>\n",
       "      <td>0.000065</td>\n",
       "      <td>0.000066</td>\n",
       "      <td>0.000066</td>\n",
       "      <td>0.000067</td>\n",
       "      <td>0.000066</td>\n",
       "      <td>0.000066</td>\n",
       "      <td>0.000066</td>\n",
       "      <td>0.000067</td>\n",
       "      <td>0.000069</td>\n",
       "      <td>0.000071</td>\n",
       "      <td>0.000075</td>\n",
       "      <td>0.000080</td>\n",
       "      <td>0.000086</td>\n",
       "      <td>0.000094</td>\n",
       "      <td>0.000104</td>\n",
       "      <td>0.000116</td>\n",
       "      <td>0.000129</td>\n",
       "      <td>0.000144</td>\n",
       "      <td>0.000161</td>\n",
       "      <td>0.000179</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>0.000224</td>\n",
       "      <td>0.000250</td>\n",
       "      <td>0.000280</td>\n",
       "      <td>0.000313</td>\n",
       "      <td>0.000349</td>\n",
       "      <td>0.000389</td>\n",
       "      <td>0.000432</td>\n",
       "      <td>0.000479</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000164</td>\n",
       "      <td>0.000162</td>\n",
       "      <td>0.000158</td>\n",
       "      <td>0.000154</td>\n",
       "      <td>0.000149</td>\n",
       "      <td>0.000144</td>\n",
       "      <td>0.000139</td>\n",
       "      <td>0.000133</td>\n",
       "      <td>0.000127</td>\n",
       "      <td>0.000122</td>\n",
       "      <td>0.000117</td>\n",
       "      <td>0.000113</td>\n",
       "      <td>0.000109</td>\n",
       "      <td>0.000107</td>\n",
       "      <td>0.000105</td>\n",
       "      <td>0.000104</td>\n",
       "      <td>0.000105</td>\n",
       "      <td>0.000107</td>\n",
       "      <td>0.000109</td>\n",
       "      <td>0.000113</td>\n",
       "      <td>0.000118</td>\n",
       "      <td>0.000124</td>\n",
       "      <td>0.000131</td>\n",
       "      <td>0.000138</td>\n",
       "      <td>0.000146</td>\n",
       "      <td>0.000154</td>\n",
       "      <td>0.000163</td>\n",
       "      <td>0.000172</td>\n",
       "      <td>0.000180</td>\n",
       "      <td>0.000189</td>\n",
       "      <td>0.000196</td>\n",
       "      <td>0.000204</td>\n",
       "      <td>0.000210</td>\n",
       "      <td>0.000216</td>\n",
       "      <td>0.000220</td>\n",
       "      <td>0.000223</td>\n",
       "      <td>0.000225</td>\n",
       "      <td>0.000225</td>\n",
       "      <td>0.000224</td>\n",
       "      <td>0.000221</td>\n",
       "      <td>0.000216</td>\n",
       "      <td>0.000210</td>\n",
       "      <td>0.000202</td>\n",
       "      <td>0.000193</td>\n",
       "      <td>0.000182</td>\n",
       "      <td>0.000170</td>\n",
       "      <td>0.000156</td>\n",
       "      <td>0.000141</td>\n",
       "      <td>0.000126</td>\n",
       "      <td>0.000110</td>\n",
       "      <td>0.000093</td>\n",
       "      <td>0.000077</td>\n",
       "      <td>0.000061</td>\n",
       "      <td>0.000045</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>1.968223e-06</td>\n",
       "      <td>-0.000010</td>\n",
       "      <td>-0.000021</td>\n",
       "      <td>-0.000030</td>\n",
       "      <td>-0.000038</td>\n",
       "      <td>-0.000044</td>\n",
       "      <td>-0.000048</td>\n",
       "      <td>-0.000051</td>\n",
       "      <td>-0.000052</td>\n",
       "      <td>-0.000052</td>\n",
       "      <td>-0.000050</td>\n",
       "      <td>-0.000047</td>\n",
       "      <td>-0.000043</td>\n",
       "      <td>-0.000038</td>\n",
       "      <td>-0.000032</td>\n",
       "      <td>-0.000026</td>\n",
       "      <td>-0.000019</td>\n",
       "      <td>-0.000012</td>\n",
       "      <td>-0.000005</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000020</td>\n",
       "      <td>0.000025</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>0.000034</td>\n",
       "      <td>0.000038</td>\n",
       "      <td>0.000041</td>\n",
       "      <td>0.000044</td>\n",
       "      <td>0.000046</td>\n",
       "      <td>0.000048</td>\n",
       "      <td>0.000051</td>\n",
       "      <td>0.000053</td>\n",
       "      <td>0.000055</td>\n",
       "      <td>0.000057</td>\n",
       "      <td>0.000059</td>\n",
       "      <td>0.000061</td>\n",
       "      <td>0.000064</td>\n",
       "      <td>0.000066</td>\n",
       "      <td>0.000068</td>\n",
       "      <td>0.000071</td>\n",
       "      <td>0.000073</td>\n",
       "      <td>0.000076</td>\n",
       "      <td>0.000080</td>\n",
       "      <td>0.000083</td>\n",
       "      <td>0.000087</td>\n",
       "      <td>0.000092</td>\n",
       "      <td>0.000097</td>\n",
       "      <td>0.000103</td>\n",
       "      <td>0.000109</td>\n",
       "      <td>0.000116</td>\n",
       "      <td>0.000123</td>\n",
       "      <td>0.000131</td>\n",
       "      <td>0.000140</td>\n",
       "      <td>0.000149</td>\n",
       "      <td>0.000160</td>\n",
       "      <td>0.000170</td>\n",
       "      <td>0.000182</td>\n",
       "      <td>0.000194</td>\n",
       "      <td>0.000206</td>\n",
       "      <td>0.000219</td>\n",
       "      <td>0.000231</td>\n",
       "      <td>0.000244</td>\n",
       "      <td>0.000257</td>\n",
       "      <td>0.000269</td>\n",
       "      <td>0.000281</td>\n",
       "      <td>0.000292</td>\n",
       "      <td>0.000302</td>\n",
       "      <td>0.000310</td>\n",
       "      <td>0.000318</td>\n",
       "      <td>0.000323</td>\n",
       "      <td>0.000327</td>\n",
       "      <td>0.000329</td>\n",
       "      <td>0.000328</td>\n",
       "      <td>0.000324</td>\n",
       "      <td>0.000318</td>\n",
       "      <td>0.000309</td>\n",
       "      <td>0.000297</td>\n",
       "      <td>0.000283</td>\n",
       "      <td>0.000266</td>\n",
       "      <td>0.000246</td>\n",
       "      <td>0.000224</td>\n",
       "      <td>0.000199</td>\n",
       "      <td>0.000172</td>\n",
       "      <td>0.000144</td>\n",
       "      <td>0.000115</td>\n",
       "      <td>0.000085</td>\n",
       "      <td>0.000054</td>\n",
       "      <td>0.000022</td>\n",
       "      <td>-0.000008</td>\n",
       "      <td>-0.000038</td>\n",
       "      <td>-0.000067</td>\n",
       "      <td>-0.000094</td>\n",
       "      <td>-0.000118</td>\n",
       "      <td>-0.000140</td>\n",
       "      <td>-0.000159</td>\n",
       "      <td>-0.000174</td>\n",
       "      <td>-0.000186</td>\n",
       "      <td>-0.000195</td>\n",
       "      <td>-0.000199</td>\n",
       "      <td>-0.000199</td>\n",
       "      <td>-0.000196</td>\n",
       "      <td>-0.000188</td>\n",
       "      <td>-0.000177</td>\n",
       "      <td>-0.000163</td>\n",
       "      <td>-0.000145</td>\n",
       "      <td>-0.000124</td>\n",
       "      <td>-0.000102</td>\n",
       "      <td>-0.000077</td>\n",
       "      <td>-0.000051</td>\n",
       "      <td>-0.000024</td>\n",
       "      <td>2.959693e-06</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>0.000055</td>\n",
       "      <td>0.000080</td>\n",
       "      <td>0.000103</td>\n",
       "      <td>0.000123</td>\n",
       "      <td>0.000141</td>\n",
       "      <td>0.000156</td>\n",
       "      <td>0.000168</td>\n",
       "      <td>0.000176</td>\n",
       "      <td>0.000181</td>\n",
       "      <td>0.000182</td>\n",
       "      <td>0.000180</td>\n",
       "      <td>0.000175</td>\n",
       "      <td>0.000167</td>\n",
       "      <td>0.000156</td>\n",
       "      <td>0.000142</td>\n",
       "      <td>0.000126</td>\n",
       "      <td>0.000109</td>\n",
       "      <td>0.000090</td>\n",
       "      <td>0.000070</td>\n",
       "      <td>0.000049</td>\n",
       "      <td>0.000028</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>-0.000013</td>\n",
       "      <td>-0.000032</td>\n",
       "      <td>-0.000049</td>\n",
       "      <td>-0.000065</td>\n",
       "      <td>-0.000079</td>\n",
       "      <td>-0.000090</td>\n",
       "      <td>-0.000100</td>\n",
       "      <td>-0.000107</td>\n",
       "      <td>-0.000111</td>\n",
       "      <td>-0.000113</td>\n",
       "      <td>-0.000113</td>\n",
       "      <td>-0.000110</td>\n",
       "      <td>-0.000105</td>\n",
       "      <td>-0.000098</td>\n",
       "      <td>-0.000090</td>\n",
       "      <td>-0.000079</td>\n",
       "      <td>-0.000067</td>\n",
       "      <td>-0.000055</td>\n",
       "      <td>-0.000041</td>\n",
       "      <td>-0.000028</td>\n",
       "      <td>-0.000014</td>\n",
       "      <td>1.368512e-07</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000026</td>\n",
       "      <td>0.000039</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>0.000060</td>\n",
       "      <td>0.000069</td>\n",
       "      <td>0.000076</td>\n",
       "      <td>0.000083</td>\n",
       "      <td>0.000088</td>\n",
       "      <td>0.000091</td>\n",
       "      <td>0.000093</td>\n",
       "      <td>0.000095</td>\n",
       "      <td>0.000094</td>\n",
       "      <td>0.000093</td>\n",
       "      <td>0.000091</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.000085</td>\n",
       "      <td>0.000081</td>\n",
       "      <td>0.000077</td>\n",
       "      <td>0.000072</td>\n",
       "      <td>0.000068</td>\n",
       "      <td>0.000063</td>\n",
       "      <td>0.000058</td>\n",
       "      <td>0.000053</td>\n",
       "      <td>0.000049</td>\n",
       "      <td>0.000045</td>\n",
       "      <td>0.000041</td>\n",
       "      <td>0.000038</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>0.000033</td>\n",
       "      <td>0.000032</td>\n",
       "      <td>0.000032</td>\n",
       "      <td>0.000032</td>\n",
       "      <td>0.000033</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>0.000037</td>\n",
       "      <td>0.000041</td>\n",
       "      <td>0.000044</td>\n",
       "      <td>0.000049</td>\n",
       "      <td>0.000054</td>\n",
       "      <td>0.000060</td>\n",
       "      <td>0.000066</td>\n",
       "      <td>0.000072</td>\n",
       "      <td>0.000078</td>\n",
       "      <td>0.000085</td>\n",
       "      <td>0.000092</td>\n",
       "      <td>0.000099</td>\n",
       "      <td>0.000105</td>\n",
       "      <td>0.000112</td>\n",
       "      <td>0.000119</td>\n",
       "      <td>0.000125</td>\n",
       "      <td>0.000131</td>\n",
       "      <td>0.000138</td>\n",
       "      <td>0.000143</td>\n",
       "      <td>0.000149</td>\n",
       "      <td>0.000154</td>\n",
       "      <td>0.000159</td>\n",
       "      <td>0.000164</td>\n",
       "      <td>0.000167</td>\n",
       "      <td>0.000170</td>\n",
       "      <td>0.000172</td>\n",
       "      <td>0.000173</td>\n",
       "      <td>0.000173</td>\n",
       "      <td>0.000172</td>\n",
       "      <td>0.000170</td>\n",
       "      <td>0.000167</td>\n",
       "      <td>0.000164</td>\n",
       "      <td>0.000159</td>\n",
       "      <td>0.000154</td>\n",
       "      <td>0.000149</td>\n",
       "      <td>0.000143</td>\n",
       "      <td>0.000136</td>\n",
       "      <td>0.000129</td>\n",
       "      <td>0.000122</td>\n",
       "      <td>0.000114</td>\n",
       "      <td>0.000106</td>\n",
       "      <td>0.000098</td>\n",
       "      <td>0.000090</td>\n",
       "      <td>0.000082</td>\n",
       "      <td>0.000075</td>\n",
       "      <td>0.000067</td>\n",
       "      <td>0.000060</td>\n",
       "      <td>0.000054</td>\n",
       "      <td>0.000048</td>\n",
       "      <td>0.000043</td>\n",
       "      <td>0.000039</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>0.000032</td>\n",
       "      <td>0.000031</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>0.000031</td>\n",
       "      <td>0.000032</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>0.000038</td>\n",
       "      <td>0.000041</td>\n",
       "      <td>0.000046</td>\n",
       "      <td>0.000051</td>\n",
       "      <td>0.000056</td>\n",
       "      <td>0.000063</td>\n",
       "      <td>0.000070</td>\n",
       "      <td>0.000078</td>\n",
       "      <td>0.000086</td>\n",
       "      <td>0.000096</td>\n",
       "      <td>0.000105</td>\n",
       "      <td>0.000116</td>\n",
       "      <td>0.000126</td>\n",
       "      <td>0.000138</td>\n",
       "      <td>0.000149</td>\n",
       "      <td>0.000161</td>\n",
       "      <td>0.000172</td>\n",
       "      <td>0.000184</td>\n",
       "      <td>0.000196</td>\n",
       "      <td>0.000208</td>\n",
       "      <td>0.000219</td>\n",
       "      <td>0.000229</td>\n",
       "      <td>0.000237</td>\n",
       "      <td>0.000244</td>\n",
       "      <td>0.000248</td>\n",
       "      <td>0.000250</td>\n",
       "      <td>0.000250</td>\n",
       "      <td>0.000247</td>\n",
       "      <td>0.000242</td>\n",
       "      <td>0.000234</td>\n",
       "      <td>0.000224</td>\n",
       "      <td>0.000212</td>\n",
       "      <td>0.000198</td>\n",
       "      <td>0.000182</td>\n",
       "      <td>0.000164</td>\n",
       "      <td>0.000144</td>\n",
       "      <td>0.000124</td>\n",
       "      <td>0.000102</td>\n",
       "      <td>0.000080</td>\n",
       "      <td>0.000058</td>\n",
       "      <td>0.000037</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>-0.000002</td>\n",
       "      <td>-0.000019</td>\n",
       "      <td>-0.000034</td>\n",
       "      <td>-0.000046</td>\n",
       "      <td>-0.000055</td>\n",
       "      <td>-0.000061</td>\n",
       "      <td>-0.000063</td>\n",
       "      <td>-0.000062</td>\n",
       "      <td>-0.000058</td>\n",
       "      <td>-0.000050</td>\n",
       "      <td>-0.000039</td>\n",
       "      <td>-0.000024</td>\n",
       "      <td>-0.000006</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>0.000039</td>\n",
       "      <td>0.000065</td>\n",
       "      <td>0.000093</td>\n",
       "      <td>0.000123</td>\n",
       "      <td>0.000153</td>\n",
       "      <td>0.000186</td>\n",
       "      <td>0.000220</td>\n",
       "      <td>0.000256</td>\n",
       "      <td>0.000295</td>\n",
       "      <td>0.000338</td>\n",
       "      <td>0.000384</td>\n",
       "      <td>0.000436</td>\n",
       "      <td>0.000491</td>\n",
       "      <td>0.000549</td>\n",
       "      <td>0.000607</td>\n",
       "      <td>0.000663</td>\n",
       "      <td>0.000713</td>\n",
       "      <td>0.000754</td>\n",
       "      <td>0.000782</td>\n",
       "      <td>0.000796</td>\n",
       "      <td>0.000792</td>\n",
       "      <td>0.000770</td>\n",
       "      <td>0.000729</td>\n",
       "      <td>0.000668</td>\n",
       "      <td>0.000589</td>\n",
       "      <td>0.000494</td>\n",
       "      <td>0.000383</td>\n",
       "      <td>0.000259</td>\n",
       "      <td>0.000123</td>\n",
       "      <td>-0.000024</td>\n",
       "      <td>-0.000180</td>\n",
       "      <td>-0.000342</td>\n",
       "      <td>-0.000509</td>\n",
       "      <td>-0.000677</td>\n",
       "      <td>-0.000842</td>\n",
       "      <td>-0.001002</td>\n",
       "      <td>-0.001152</td>\n",
       "      <td>-0.001290</td>\n",
       "      <td>-0.001410</td>\n",
       "      <td>-0.001512</td>\n",
       "      <td>-0.001592</td>\n",
       "      <td>-0.001648</td>\n",
       "      <td>-0.001681</td>\n",
       "      <td>-0.001688</td>\n",
       "      <td>-0.001670</td>\n",
       "      <td>-0.001625</td>\n",
       "      <td>-0.001556</td>\n",
       "      <td>-0.001461</td>\n",
       "      <td>-0.001344</td>\n",
       "      <td>-0.001205</td>\n",
       "      <td>-0.001049</td>\n",
       "      <td>-0.000879</td>\n",
       "      <td>-0.000700</td>\n",
       "      <td>-0.000516</td>\n",
       "      <td>-0.000334</td>\n",
       "      <td>-0.000158</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000155</td>\n",
       "      <td>0.000283</td>\n",
       "      <td>0.000389</td>\n",
       "      <td>0.000470</td>\n",
       "      <td>0.000526</td>\n",
       "      <td>0.000557</td>\n",
       "      <td>0.000564</td>\n",
       "      <td>0.000548</td>\n",
       "      <td>0.000511</td>\n",
       "      <td>0.000456</td>\n",
       "      <td>0.000385</td>\n",
       "      <td>0.000303</td>\n",
       "      <td>0.000213</td>\n",
       "      <td>0.000117</td>\n",
       "      <td>0.000019</td>\n",
       "      <td>-0.000079</td>\n",
       "      <td>-0.000174</td>\n",
       "      <td>-0.000263</td>\n",
       "      <td>-0.000345</td>\n",
       "      <td>-0.000418</td>\n",
       "      <td>-0.000481</td>\n",
       "      <td>-0.000534</td>\n",
       "      <td>-0.000577</td>\n",
       "      <td>-0.000608</td>\n",
       "      <td>-0.000629</td>\n",
       "      <td>-0.000640</td>\n",
       "      <td>-0.000639</td>\n",
       "      <td>-0.000626</td>\n",
       "      <td>-0.000603</td>\n",
       "      <td>-0.000569</td>\n",
       "      <td>-0.000525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9098</th>\n",
       "      <td>0.000080</td>\n",
       "      <td>0.000091</td>\n",
       "      <td>0.000104</td>\n",
       "      <td>0.000121</td>\n",
       "      <td>0.000141</td>\n",
       "      <td>0.000164</td>\n",
       "      <td>0.000191</td>\n",
       "      <td>0.000220</td>\n",
       "      <td>0.000250</td>\n",
       "      <td>0.000281</td>\n",
       "      <td>0.000312</td>\n",
       "      <td>0.000342</td>\n",
       "      <td>0.000368</td>\n",
       "      <td>0.000391</td>\n",
       "      <td>0.000409</td>\n",
       "      <td>0.000423</td>\n",
       "      <td>0.000431</td>\n",
       "      <td>0.000434</td>\n",
       "      <td>0.000433</td>\n",
       "      <td>0.000427</td>\n",
       "      <td>0.000419</td>\n",
       "      <td>0.000408</td>\n",
       "      <td>0.000395</td>\n",
       "      <td>0.000380</td>\n",
       "      <td>0.000364</td>\n",
       "      <td>0.000348</td>\n",
       "      <td>0.000331</td>\n",
       "      <td>0.000313</td>\n",
       "      <td>0.000296</td>\n",
       "      <td>0.000278</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000082</td>\n",
       "      <td>0.000084</td>\n",
       "      <td>0.000086</td>\n",
       "      <td>0.000088</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.000090</td>\n",
       "      <td>0.000091</td>\n",
       "      <td>0.000092</td>\n",
       "      <td>0.000093</td>\n",
       "      <td>0.000094</td>\n",
       "      <td>0.000094</td>\n",
       "      <td>0.000095</td>\n",
       "      <td>0.000096</td>\n",
       "      <td>0.000096</td>\n",
       "      <td>0.000097</td>\n",
       "      <td>0.000097</td>\n",
       "      <td>0.000098</td>\n",
       "      <td>0.000098</td>\n",
       "      <td>0.000098</td>\n",
       "      <td>0.000098</td>\n",
       "      <td>0.000098</td>\n",
       "      <td>0.000098</td>\n",
       "      <td>0.000097</td>\n",
       "      <td>0.000096</td>\n",
       "      <td>0.000096</td>\n",
       "      <td>0.000095</td>\n",
       "      <td>0.000094</td>\n",
       "      <td>0.000093</td>\n",
       "      <td>0.000092</td>\n",
       "      <td>0.000091</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.000088</td>\n",
       "      <td>0.000087</td>\n",
       "      <td>0.000086</td>\n",
       "      <td>0.000085</td>\n",
       "      <td>0.000083</td>\n",
       "      <td>0.000082</td>\n",
       "      <td>0.000081</td>\n",
       "      <td>0.000080</td>\n",
       "      <td>0.000078</td>\n",
       "      <td>0.000077</td>\n",
       "      <td>0.000075</td>\n",
       "      <td>0.000074</td>\n",
       "      <td>0.000073</td>\n",
       "      <td>0.000071</td>\n",
       "      <td>0.000070</td>\n",
       "      <td>0.000069</td>\n",
       "      <td>0.000068</td>\n",
       "      <td>0.000067</td>\n",
       "      <td>0.000066</td>\n",
       "      <td>0.000066</td>\n",
       "      <td>0.000065</td>\n",
       "      <td>0.000065</td>\n",
       "      <td>0.000065</td>\n",
       "      <td>0.000064</td>\n",
       "      <td>0.000064</td>\n",
       "      <td>0.000064</td>\n",
       "      <td>0.000064</td>\n",
       "      <td>0.000063</td>\n",
       "      <td>0.000063</td>\n",
       "      <td>0.000063</td>\n",
       "      <td>0.000063</td>\n",
       "      <td>0.000062</td>\n",
       "      <td>0.000062</td>\n",
       "      <td>0.000061</td>\n",
       "      <td>0.000060</td>\n",
       "      <td>0.000060</td>\n",
       "      <td>0.000059</td>\n",
       "      <td>0.000058</td>\n",
       "      <td>0.000057</td>\n",
       "      <td>0.000056</td>\n",
       "      <td>0.000055</td>\n",
       "      <td>0.000054</td>\n",
       "      <td>0.000053</td>\n",
       "      <td>0.000052</td>\n",
       "      <td>0.000051</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>0.000049</td>\n",
       "      <td>0.000048</td>\n",
       "      <td>0.000047</td>\n",
       "      <td>0.000046</td>\n",
       "      <td>0.000046</td>\n",
       "      <td>0.000045</td>\n",
       "      <td>0.000045</td>\n",
       "      <td>0.000045</td>\n",
       "      <td>0.000045</td>\n",
       "      <td>0.000045</td>\n",
       "      <td>0.000045</td>\n",
       "      <td>0.000045</td>\n",
       "      <td>0.000046</td>\n",
       "      <td>0.000046</td>\n",
       "      <td>0.000047</td>\n",
       "      <td>0.000047</td>\n",
       "      <td>0.000048</td>\n",
       "      <td>0.000048</td>\n",
       "      <td>0.000049</td>\n",
       "      <td>0.000049</td>\n",
       "      <td>0.000049</td>\n",
       "      <td>0.000049</td>\n",
       "      <td>0.000048</td>\n",
       "      <td>0.000047</td>\n",
       "      <td>0.000046</td>\n",
       "      <td>0.000045</td>\n",
       "      <td>0.000043</td>\n",
       "      <td>0.000041</td>\n",
       "      <td>0.000039</td>\n",
       "      <td>0.000036</td>\n",
       "      <td>0.000034</td>\n",
       "      <td>0.000031</td>\n",
       "      <td>0.000029</td>\n",
       "      <td>0.000026</td>\n",
       "      <td>0.000024</td>\n",
       "      <td>0.000022</td>\n",
       "      <td>0.00002</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>0.000019</td>\n",
       "      <td>0.000022</td>\n",
       "      <td>0.000024</td>\n",
       "      <td>0.000027</td>\n",
       "      <td>0.000031</td>\n",
       "      <td>0.000036</td>\n",
       "      <td>0.000041</td>\n",
       "      <td>0.000046</td>\n",
       "      <td>0.000052</td>\n",
       "      <td>0.000059</td>\n",
       "      <td>0.000067</td>\n",
       "      <td>0.000075</td>\n",
       "      <td>0.000084</td>\n",
       "      <td>0.000092</td>\n",
       "      <td>0.000101</td>\n",
       "      <td>0.000110</td>\n",
       "      <td>0.000119</td>\n",
       "      <td>0.000127</td>\n",
       "      <td>0.000135</td>\n",
       "      <td>0.000141</td>\n",
       "      <td>0.000146</td>\n",
       "      <td>0.000149</td>\n",
       "      <td>0.000151</td>\n",
       "      <td>0.000151</td>\n",
       "      <td>0.000148</td>\n",
       "      <td>0.000144</td>\n",
       "      <td>0.000138</td>\n",
       "      <td>0.000130</td>\n",
       "      <td>0.000120</td>\n",
       "      <td>0.000108</td>\n",
       "      <td>0.000096</td>\n",
       "      <td>0.000082</td>\n",
       "      <td>0.000068</td>\n",
       "      <td>0.000054</td>\n",
       "      <td>0.000040</td>\n",
       "      <td>0.000027</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>-0.000003</td>\n",
       "      <td>-0.000009</td>\n",
       "      <td>-0.000012</td>\n",
       "      <td>-0.000012</td>\n",
       "      <td>-0.000010</td>\n",
       "      <td>-0.000005</td>\n",
       "      <td>2.401107e-06</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>0.000023</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>0.000047</td>\n",
       "      <td>0.000060</td>\n",
       "      <td>0.000071</td>\n",
       "      <td>0.000081</td>\n",
       "      <td>0.000090</td>\n",
       "      <td>0.000096</td>\n",
       "      <td>0.000099</td>\n",
       "      <td>0.000099</td>\n",
       "      <td>0.000097</td>\n",
       "      <td>0.000091</td>\n",
       "      <td>0.000083</td>\n",
       "      <td>0.000072</td>\n",
       "      <td>0.000059</td>\n",
       "      <td>0.000044</td>\n",
       "      <td>0.000027</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>-0.000010</td>\n",
       "      <td>-0.000029</td>\n",
       "      <td>-0.000048</td>\n",
       "      <td>-0.000067</td>\n",
       "      <td>-0.000086</td>\n",
       "      <td>-0.000103</td>\n",
       "      <td>-0.000119</td>\n",
       "      <td>-0.000133</td>\n",
       "      <td>-0.000146</td>\n",
       "      <td>-0.000156</td>\n",
       "      <td>-0.000164</td>\n",
       "      <td>-0.000169</td>\n",
       "      <td>-0.000172</td>\n",
       "      <td>-0.000172</td>\n",
       "      <td>-0.000170</td>\n",
       "      <td>-0.000165</td>\n",
       "      <td>-0.000157</td>\n",
       "      <td>-0.000148</td>\n",
       "      <td>-0.000136</td>\n",
       "      <td>-0.000123</td>\n",
       "      <td>-0.000108</td>\n",
       "      <td>-0.000093</td>\n",
       "      <td>-0.000077</td>\n",
       "      <td>-0.000061</td>\n",
       "      <td>-0.000045</td>\n",
       "      <td>-0.000029</td>\n",
       "      <td>-0.000013</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>0.000028</td>\n",
       "      <td>0.000040</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>0.000059</td>\n",
       "      <td>0.000066</td>\n",
       "      <td>0.000070</td>\n",
       "      <td>0.000072</td>\n",
       "      <td>0.000072</td>\n",
       "      <td>0.000070</td>\n",
       "      <td>0.000065</td>\n",
       "      <td>0.000057</td>\n",
       "      <td>0.000047</td>\n",
       "      <td>0.000034</td>\n",
       "      <td>0.000019</td>\n",
       "      <td>3.016449e-06</td>\n",
       "      <td>-0.000015</td>\n",
       "      <td>-0.000033</td>\n",
       "      <td>-0.000051</td>\n",
       "      <td>-0.000069</td>\n",
       "      <td>-0.000087</td>\n",
       "      <td>-0.000102</td>\n",
       "      <td>-0.000116</td>\n",
       "      <td>-0.000127</td>\n",
       "      <td>-0.000135</td>\n",
       "      <td>-0.000141</td>\n",
       "      <td>-0.000143</td>\n",
       "      <td>-0.000142</td>\n",
       "      <td>-0.000137</td>\n",
       "      <td>-0.000130</td>\n",
       "      <td>-0.000119</td>\n",
       "      <td>-0.000107</td>\n",
       "      <td>-0.000092</td>\n",
       "      <td>-0.000075</td>\n",
       "      <td>-0.000058</td>\n",
       "      <td>-0.000039</td>\n",
       "      <td>-0.000020</td>\n",
       "      <td>-0.000002</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>0.000032</td>\n",
       "      <td>0.000048</td>\n",
       "      <td>0.000061</td>\n",
       "      <td>0.000072</td>\n",
       "      <td>0.000081</td>\n",
       "      <td>0.000088</td>\n",
       "      <td>0.000092</td>\n",
       "      <td>0.000094</td>\n",
       "      <td>0.000093</td>\n",
       "      <td>0.000091</td>\n",
       "      <td>0.000086</td>\n",
       "      <td>0.000080</td>\n",
       "      <td>0.000072</td>\n",
       "      <td>0.000063</td>\n",
       "      <td>0.000054</td>\n",
       "      <td>0.000044</td>\n",
       "      <td>0.000033</td>\n",
       "      <td>0.000023</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>-0.000004</td>\n",
       "      <td>-0.000012</td>\n",
       "      <td>-0.000019</td>\n",
       "      <td>-0.000025</td>\n",
       "      <td>-0.000029</td>\n",
       "      <td>-0.000033</td>\n",
       "      <td>-0.000036</td>\n",
       "      <td>-0.000038</td>\n",
       "      <td>-0.000040</td>\n",
       "      <td>-0.000040</td>\n",
       "      <td>-0.000040</td>\n",
       "      <td>-0.000039</td>\n",
       "      <td>-0.000037</td>\n",
       "      <td>-0.000035</td>\n",
       "      <td>-0.000032</td>\n",
       "      <td>-0.000028</td>\n",
       "      <td>-0.000025</td>\n",
       "      <td>-0.000021</td>\n",
       "      <td>-0.000017</td>\n",
       "      <td>-0.000013</td>\n",
       "      <td>-0.000009</td>\n",
       "      <td>-0.000004</td>\n",
       "      <td>-6.805936e-08</td>\n",
       "      <td>4.207411e-06</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>0.000021</td>\n",
       "      <td>0.000024</td>\n",
       "      <td>0.000028</td>\n",
       "      <td>0.000031</td>\n",
       "      <td>0.000034</td>\n",
       "      <td>0.000037</td>\n",
       "      <td>0.000040</td>\n",
       "      <td>0.000042</td>\n",
       "      <td>0.000044</td>\n",
       "      <td>0.000046</td>\n",
       "      <td>0.000048</td>\n",
       "      <td>0.000049</td>\n",
       "      <td>0.000051</td>\n",
       "      <td>0.000053</td>\n",
       "      <td>0.000055</td>\n",
       "      <td>0.000057</td>\n",
       "      <td>0.000059</td>\n",
       "      <td>0.000061</td>\n",
       "      <td>0.000063</td>\n",
       "      <td>0.000064</td>\n",
       "      <td>0.000065</td>\n",
       "      <td>0.000066</td>\n",
       "      <td>0.000067</td>\n",
       "      <td>0.000067</td>\n",
       "      <td>0.000067</td>\n",
       "      <td>0.000066</td>\n",
       "      <td>0.000067</td>\n",
       "      <td>0.000067</td>\n",
       "      <td>0.000069</td>\n",
       "      <td>0.000071</td>\n",
       "      <td>0.000075</td>\n",
       "      <td>0.000080</td>\n",
       "      <td>0.000086</td>\n",
       "      <td>0.000095</td>\n",
       "      <td>0.000105</td>\n",
       "      <td>0.000116</td>\n",
       "      <td>0.000129</td>\n",
       "      <td>0.000144</td>\n",
       "      <td>0.000161</td>\n",
       "      <td>0.000179</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>0.000224</td>\n",
       "      <td>0.000250</td>\n",
       "      <td>0.000280</td>\n",
       "      <td>0.000313</td>\n",
       "      <td>0.000349</td>\n",
       "      <td>0.000389</td>\n",
       "      <td>0.000432</td>\n",
       "      <td>0.000479</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000165</td>\n",
       "      <td>0.000162</td>\n",
       "      <td>0.000158</td>\n",
       "      <td>0.000154</td>\n",
       "      <td>0.000149</td>\n",
       "      <td>0.000144</td>\n",
       "      <td>0.000138</td>\n",
       "      <td>0.000132</td>\n",
       "      <td>0.000126</td>\n",
       "      <td>0.000121</td>\n",
       "      <td>0.000116</td>\n",
       "      <td>0.000112</td>\n",
       "      <td>0.000109</td>\n",
       "      <td>0.000106</td>\n",
       "      <td>0.000105</td>\n",
       "      <td>0.000105</td>\n",
       "      <td>0.000105</td>\n",
       "      <td>0.000107</td>\n",
       "      <td>0.000110</td>\n",
       "      <td>0.000114</td>\n",
       "      <td>0.000119</td>\n",
       "      <td>0.000125</td>\n",
       "      <td>0.000132</td>\n",
       "      <td>0.000140</td>\n",
       "      <td>0.000147</td>\n",
       "      <td>0.000156</td>\n",
       "      <td>0.000164</td>\n",
       "      <td>0.000173</td>\n",
       "      <td>0.000181</td>\n",
       "      <td>0.000190</td>\n",
       "      <td>0.000197</td>\n",
       "      <td>0.000205</td>\n",
       "      <td>0.000211</td>\n",
       "      <td>0.000216</td>\n",
       "      <td>0.000220</td>\n",
       "      <td>0.000223</td>\n",
       "      <td>0.000225</td>\n",
       "      <td>0.000225</td>\n",
       "      <td>0.000224</td>\n",
       "      <td>0.000221</td>\n",
       "      <td>0.000216</td>\n",
       "      <td>0.000210</td>\n",
       "      <td>0.000202</td>\n",
       "      <td>0.000192</td>\n",
       "      <td>0.000181</td>\n",
       "      <td>0.000169</td>\n",
       "      <td>0.000155</td>\n",
       "      <td>0.000140</td>\n",
       "      <td>0.000125</td>\n",
       "      <td>0.000109</td>\n",
       "      <td>0.000092</td>\n",
       "      <td>0.000076</td>\n",
       "      <td>0.000060</td>\n",
       "      <td>0.000044</td>\n",
       "      <td>0.000029</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>1.973925e-06</td>\n",
       "      <td>-0.000010</td>\n",
       "      <td>-0.000020</td>\n",
       "      <td>-0.000029</td>\n",
       "      <td>-0.000037</td>\n",
       "      <td>-0.000043</td>\n",
       "      <td>-0.000047</td>\n",
       "      <td>-0.000050</td>\n",
       "      <td>-0.000051</td>\n",
       "      <td>-0.000051</td>\n",
       "      <td>-0.000049</td>\n",
       "      <td>-0.000046</td>\n",
       "      <td>-0.000042</td>\n",
       "      <td>-0.000038</td>\n",
       "      <td>-0.000032</td>\n",
       "      <td>-0.000026</td>\n",
       "      <td>-0.000019</td>\n",
       "      <td>-0.000012</td>\n",
       "      <td>-0.000005</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000020</td>\n",
       "      <td>0.000025</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>0.000034</td>\n",
       "      <td>0.000037</td>\n",
       "      <td>0.000040</td>\n",
       "      <td>0.000043</td>\n",
       "      <td>0.000045</td>\n",
       "      <td>0.000048</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>0.000052</td>\n",
       "      <td>0.000054</td>\n",
       "      <td>0.000056</td>\n",
       "      <td>0.000059</td>\n",
       "      <td>0.000061</td>\n",
       "      <td>0.000063</td>\n",
       "      <td>0.000066</td>\n",
       "      <td>0.000068</td>\n",
       "      <td>0.000071</td>\n",
       "      <td>0.000074</td>\n",
       "      <td>0.000077</td>\n",
       "      <td>0.000080</td>\n",
       "      <td>0.000084</td>\n",
       "      <td>0.000088</td>\n",
       "      <td>0.000093</td>\n",
       "      <td>0.000098</td>\n",
       "      <td>0.000104</td>\n",
       "      <td>0.000110</td>\n",
       "      <td>0.000117</td>\n",
       "      <td>0.000124</td>\n",
       "      <td>0.000133</td>\n",
       "      <td>0.000141</td>\n",
       "      <td>0.000151</td>\n",
       "      <td>0.000161</td>\n",
       "      <td>0.000172</td>\n",
       "      <td>0.000183</td>\n",
       "      <td>0.000195</td>\n",
       "      <td>0.000207</td>\n",
       "      <td>0.000219</td>\n",
       "      <td>0.000232</td>\n",
       "      <td>0.000244</td>\n",
       "      <td>0.000256</td>\n",
       "      <td>0.000268</td>\n",
       "      <td>0.000280</td>\n",
       "      <td>0.000290</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>0.000309</td>\n",
       "      <td>0.000316</td>\n",
       "      <td>0.000321</td>\n",
       "      <td>0.000325</td>\n",
       "      <td>0.000327</td>\n",
       "      <td>0.000326</td>\n",
       "      <td>0.000323</td>\n",
       "      <td>0.000316</td>\n",
       "      <td>0.000308</td>\n",
       "      <td>0.000296</td>\n",
       "      <td>0.000282</td>\n",
       "      <td>0.000265</td>\n",
       "      <td>0.000246</td>\n",
       "      <td>0.000224</td>\n",
       "      <td>0.000199</td>\n",
       "      <td>0.000173</td>\n",
       "      <td>0.000145</td>\n",
       "      <td>0.000116</td>\n",
       "      <td>0.000086</td>\n",
       "      <td>0.000055</td>\n",
       "      <td>0.000024</td>\n",
       "      <td>-0.000007</td>\n",
       "      <td>-0.000037</td>\n",
       "      <td>-0.000066</td>\n",
       "      <td>-0.000092</td>\n",
       "      <td>-0.000117</td>\n",
       "      <td>-0.000139</td>\n",
       "      <td>-0.000158</td>\n",
       "      <td>-0.000174</td>\n",
       "      <td>-0.000186</td>\n",
       "      <td>-0.000195</td>\n",
       "      <td>-0.000200</td>\n",
       "      <td>-0.000200</td>\n",
       "      <td>-0.000197</td>\n",
       "      <td>-0.000190</td>\n",
       "      <td>-0.000179</td>\n",
       "      <td>-0.000164</td>\n",
       "      <td>-0.000147</td>\n",
       "      <td>-0.000126</td>\n",
       "      <td>-0.000103</td>\n",
       "      <td>-0.000078</td>\n",
       "      <td>-0.000052</td>\n",
       "      <td>-0.000025</td>\n",
       "      <td>2.026221e-06</td>\n",
       "      <td>0.000029</td>\n",
       "      <td>0.000055</td>\n",
       "      <td>0.000080</td>\n",
       "      <td>0.000103</td>\n",
       "      <td>0.000124</td>\n",
       "      <td>0.000142</td>\n",
       "      <td>0.000157</td>\n",
       "      <td>0.000169</td>\n",
       "      <td>0.000177</td>\n",
       "      <td>0.000182</td>\n",
       "      <td>0.000184</td>\n",
       "      <td>0.000182</td>\n",
       "      <td>0.000177</td>\n",
       "      <td>0.000168</td>\n",
       "      <td>0.000157</td>\n",
       "      <td>0.000143</td>\n",
       "      <td>0.000127</td>\n",
       "      <td>0.000109</td>\n",
       "      <td>0.000090</td>\n",
       "      <td>0.000070</td>\n",
       "      <td>0.000049</td>\n",
       "      <td>0.000028</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>-0.000013</td>\n",
       "      <td>-0.000032</td>\n",
       "      <td>-0.000049</td>\n",
       "      <td>-0.000065</td>\n",
       "      <td>-0.000079</td>\n",
       "      <td>-0.000091</td>\n",
       "      <td>-0.000101</td>\n",
       "      <td>-0.000108</td>\n",
       "      <td>-0.000113</td>\n",
       "      <td>-0.000115</td>\n",
       "      <td>-0.000115</td>\n",
       "      <td>-0.000112</td>\n",
       "      <td>-0.000107</td>\n",
       "      <td>-0.000100</td>\n",
       "      <td>-0.000092</td>\n",
       "      <td>-0.000081</td>\n",
       "      <td>-0.000069</td>\n",
       "      <td>-0.000057</td>\n",
       "      <td>-0.000043</td>\n",
       "      <td>-0.000029</td>\n",
       "      <td>-0.000015</td>\n",
       "      <td>-9.353716e-07</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.000026</td>\n",
       "      <td>0.000039</td>\n",
       "      <td>0.000051</td>\n",
       "      <td>0.000061</td>\n",
       "      <td>0.000071</td>\n",
       "      <td>0.000079</td>\n",
       "      <td>0.000085</td>\n",
       "      <td>0.000091</td>\n",
       "      <td>0.000095</td>\n",
       "      <td>0.000097</td>\n",
       "      <td>0.000098</td>\n",
       "      <td>0.000098</td>\n",
       "      <td>0.000097</td>\n",
       "      <td>0.000095</td>\n",
       "      <td>0.000092</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.000084</td>\n",
       "      <td>0.000079</td>\n",
       "      <td>0.000074</td>\n",
       "      <td>0.000069</td>\n",
       "      <td>0.000063</td>\n",
       "      <td>0.000058</td>\n",
       "      <td>0.000053</td>\n",
       "      <td>0.000048</td>\n",
       "      <td>0.000043</td>\n",
       "      <td>0.000040</td>\n",
       "      <td>0.000036</td>\n",
       "      <td>0.000034</td>\n",
       "      <td>0.000032</td>\n",
       "      <td>0.000031</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>0.000032</td>\n",
       "      <td>0.000033</td>\n",
       "      <td>0.000036</td>\n",
       "      <td>0.000039</td>\n",
       "      <td>0.000043</td>\n",
       "      <td>0.000048</td>\n",
       "      <td>0.000053</td>\n",
       "      <td>0.000058</td>\n",
       "      <td>0.000064</td>\n",
       "      <td>0.000070</td>\n",
       "      <td>0.000076</td>\n",
       "      <td>0.000082</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.000095</td>\n",
       "      <td>0.000102</td>\n",
       "      <td>0.000108</td>\n",
       "      <td>0.000114</td>\n",
       "      <td>0.000120</td>\n",
       "      <td>0.000126</td>\n",
       "      <td>0.000132</td>\n",
       "      <td>0.000138</td>\n",
       "      <td>0.000144</td>\n",
       "      <td>0.000149</td>\n",
       "      <td>0.000154</td>\n",
       "      <td>0.000159</td>\n",
       "      <td>0.000162</td>\n",
       "      <td>0.000166</td>\n",
       "      <td>0.000168</td>\n",
       "      <td>0.000170</td>\n",
       "      <td>0.000170</td>\n",
       "      <td>0.000170</td>\n",
       "      <td>0.000169</td>\n",
       "      <td>0.000168</td>\n",
       "      <td>0.000165</td>\n",
       "      <td>0.000161</td>\n",
       "      <td>0.000157</td>\n",
       "      <td>0.000153</td>\n",
       "      <td>0.000147</td>\n",
       "      <td>0.000141</td>\n",
       "      <td>0.000135</td>\n",
       "      <td>0.000128</td>\n",
       "      <td>0.000122</td>\n",
       "      <td>0.000114</td>\n",
       "      <td>0.000107</td>\n",
       "      <td>0.000099</td>\n",
       "      <td>0.000092</td>\n",
       "      <td>0.000084</td>\n",
       "      <td>0.000076</td>\n",
       "      <td>0.000068</td>\n",
       "      <td>0.000061</td>\n",
       "      <td>0.000053</td>\n",
       "      <td>0.000047</td>\n",
       "      <td>0.000040</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>0.000027</td>\n",
       "      <td>0.000025</td>\n",
       "      <td>0.000024</td>\n",
       "      <td>0.000025</td>\n",
       "      <td>0.000027</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>0.000034</td>\n",
       "      <td>0.000039</td>\n",
       "      <td>0.000045</td>\n",
       "      <td>0.000053</td>\n",
       "      <td>0.000060</td>\n",
       "      <td>0.000069</td>\n",
       "      <td>0.000079</td>\n",
       "      <td>0.000090</td>\n",
       "      <td>0.000101</td>\n",
       "      <td>0.000113</td>\n",
       "      <td>0.000126</td>\n",
       "      <td>0.000138</td>\n",
       "      <td>0.000151</td>\n",
       "      <td>0.000163</td>\n",
       "      <td>0.000174</td>\n",
       "      <td>0.000184</td>\n",
       "      <td>0.000193</td>\n",
       "      <td>0.000201</td>\n",
       "      <td>0.000208</td>\n",
       "      <td>0.000213</td>\n",
       "      <td>0.000217</td>\n",
       "      <td>0.000219</td>\n",
       "      <td>0.000219</td>\n",
       "      <td>0.000218</td>\n",
       "      <td>0.000215</td>\n",
       "      <td>0.000211</td>\n",
       "      <td>0.000206</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>0.000193</td>\n",
       "      <td>0.000186</td>\n",
       "      <td>0.000177</td>\n",
       "      <td>0.000168</td>\n",
       "      <td>0.000159</td>\n",
       "      <td>0.000148</td>\n",
       "      <td>0.000136</td>\n",
       "      <td>0.000123</td>\n",
       "      <td>0.000109</td>\n",
       "      <td>0.000094</td>\n",
       "      <td>0.000078</td>\n",
       "      <td>0.000062</td>\n",
       "      <td>0.000047</td>\n",
       "      <td>0.000032</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>-0.000003</td>\n",
       "      <td>-0.000010</td>\n",
       "      <td>-0.000014</td>\n",
       "      <td>-0.000016</td>\n",
       "      <td>-0.000014</td>\n",
       "      <td>-0.000010</td>\n",
       "      <td>-0.000003</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>0.000032</td>\n",
       "      <td>0.000047</td>\n",
       "      <td>0.000065</td>\n",
       "      <td>0.000084</td>\n",
       "      <td>0.000106</td>\n",
       "      <td>0.000129</td>\n",
       "      <td>0.000154</td>\n",
       "      <td>0.000179</td>\n",
       "      <td>0.000207</td>\n",
       "      <td>0.000236</td>\n",
       "      <td>0.000267</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>0.000335</td>\n",
       "      <td>0.000373</td>\n",
       "      <td>0.000412</td>\n",
       "      <td>0.000453</td>\n",
       "      <td>0.000494</td>\n",
       "      <td>0.000534</td>\n",
       "      <td>0.000570</td>\n",
       "      <td>0.000602</td>\n",
       "      <td>0.000626</td>\n",
       "      <td>0.000642</td>\n",
       "      <td>0.000646</td>\n",
       "      <td>0.000638</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>0.000584</td>\n",
       "      <td>0.000538</td>\n",
       "      <td>0.000480</td>\n",
       "      <td>0.000408</td>\n",
       "      <td>0.000324</td>\n",
       "      <td>0.000228</td>\n",
       "      <td>0.000119</td>\n",
       "      <td>-0.000001</td>\n",
       "      <td>-0.000132</td>\n",
       "      <td>-0.000274</td>\n",
       "      <td>-0.000423</td>\n",
       "      <td>-0.000578</td>\n",
       "      <td>-0.000734</td>\n",
       "      <td>-0.000888</td>\n",
       "      <td>-0.001035</td>\n",
       "      <td>-0.001170</td>\n",
       "      <td>-0.001289</td>\n",
       "      <td>-0.001389</td>\n",
       "      <td>-0.001467</td>\n",
       "      <td>-0.001521</td>\n",
       "      <td>-0.001549</td>\n",
       "      <td>-0.001551</td>\n",
       "      <td>-0.001527</td>\n",
       "      <td>-0.001478</td>\n",
       "      <td>-0.001405</td>\n",
       "      <td>-0.001310</td>\n",
       "      <td>-0.001193</td>\n",
       "      <td>-0.001059</td>\n",
       "      <td>-0.000909</td>\n",
       "      <td>-0.000748</td>\n",
       "      <td>-0.000578</td>\n",
       "      <td>-0.000403</td>\n",
       "      <td>-0.000229</td>\n",
       "      <td>-0.000060</td>\n",
       "      <td>0.000099</td>\n",
       "      <td>0.000243</td>\n",
       "      <td>0.000367</td>\n",
       "      <td>0.000468</td>\n",
       "      <td>0.000543</td>\n",
       "      <td>0.000589</td>\n",
       "      <td>0.000605</td>\n",
       "      <td>0.000591</td>\n",
       "      <td>0.000549</td>\n",
       "      <td>0.000481</td>\n",
       "      <td>0.000391</td>\n",
       "      <td>0.000282</td>\n",
       "      <td>0.000164</td>\n",
       "      <td>0.000039</td>\n",
       "      <td>-0.000088</td>\n",
       "      <td>-0.000213</td>\n",
       "      <td>-0.000333</td>\n",
       "      <td>-0.000444</td>\n",
       "      <td>-0.000542</td>\n",
       "      <td>-0.000626</td>\n",
       "      <td>-0.000693</td>\n",
       "      <td>-0.000744</td>\n",
       "      <td>-0.000779</td>\n",
       "      <td>-0.000798</td>\n",
       "      <td>-0.000803</td>\n",
       "      <td>-0.000794</td>\n",
       "      <td>-0.000772</td>\n",
       "      <td>-0.000740</td>\n",
       "      <td>-0.000697</td>\n",
       "      <td>-0.000645</td>\n",
       "      <td>-0.000584</td>\n",
       "      <td>-0.000517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9099</th>\n",
       "      <td>0.000080</td>\n",
       "      <td>0.000091</td>\n",
       "      <td>0.000104</td>\n",
       "      <td>0.000121</td>\n",
       "      <td>0.000141</td>\n",
       "      <td>0.000164</td>\n",
       "      <td>0.000191</td>\n",
       "      <td>0.000220</td>\n",
       "      <td>0.000250</td>\n",
       "      <td>0.000282</td>\n",
       "      <td>0.000312</td>\n",
       "      <td>0.000342</td>\n",
       "      <td>0.000368</td>\n",
       "      <td>0.000391</td>\n",
       "      <td>0.000409</td>\n",
       "      <td>0.000423</td>\n",
       "      <td>0.000431</td>\n",
       "      <td>0.000434</td>\n",
       "      <td>0.000432</td>\n",
       "      <td>0.000427</td>\n",
       "      <td>0.000418</td>\n",
       "      <td>0.000407</td>\n",
       "      <td>0.000394</td>\n",
       "      <td>0.000380</td>\n",
       "      <td>0.000364</td>\n",
       "      <td>0.000348</td>\n",
       "      <td>0.000331</td>\n",
       "      <td>0.000313</td>\n",
       "      <td>0.000295</td>\n",
       "      <td>0.000278</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000082</td>\n",
       "      <td>0.000084</td>\n",
       "      <td>0.000086</td>\n",
       "      <td>0.000088</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.000090</td>\n",
       "      <td>0.000091</td>\n",
       "      <td>0.000092</td>\n",
       "      <td>0.000093</td>\n",
       "      <td>0.000093</td>\n",
       "      <td>0.000094</td>\n",
       "      <td>0.000095</td>\n",
       "      <td>0.000095</td>\n",
       "      <td>0.000096</td>\n",
       "      <td>0.000097</td>\n",
       "      <td>0.000097</td>\n",
       "      <td>0.000098</td>\n",
       "      <td>0.000098</td>\n",
       "      <td>0.000098</td>\n",
       "      <td>0.000098</td>\n",
       "      <td>0.000098</td>\n",
       "      <td>0.000098</td>\n",
       "      <td>0.000097</td>\n",
       "      <td>0.000097</td>\n",
       "      <td>0.000096</td>\n",
       "      <td>0.000095</td>\n",
       "      <td>0.000094</td>\n",
       "      <td>0.000093</td>\n",
       "      <td>0.000092</td>\n",
       "      <td>0.000091</td>\n",
       "      <td>0.000090</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.000087</td>\n",
       "      <td>0.000086</td>\n",
       "      <td>0.000085</td>\n",
       "      <td>0.000084</td>\n",
       "      <td>0.000082</td>\n",
       "      <td>0.000081</td>\n",
       "      <td>0.000079</td>\n",
       "      <td>0.000078</td>\n",
       "      <td>0.000076</td>\n",
       "      <td>0.000075</td>\n",
       "      <td>0.000073</td>\n",
       "      <td>0.000072</td>\n",
       "      <td>0.000071</td>\n",
       "      <td>0.000070</td>\n",
       "      <td>0.000069</td>\n",
       "      <td>0.000068</td>\n",
       "      <td>0.000067</td>\n",
       "      <td>0.000066</td>\n",
       "      <td>0.000066</td>\n",
       "      <td>0.000065</td>\n",
       "      <td>0.000065</td>\n",
       "      <td>0.000065</td>\n",
       "      <td>0.000064</td>\n",
       "      <td>0.000064</td>\n",
       "      <td>0.000064</td>\n",
       "      <td>0.000064</td>\n",
       "      <td>0.000063</td>\n",
       "      <td>0.000063</td>\n",
       "      <td>0.000063</td>\n",
       "      <td>0.000063</td>\n",
       "      <td>0.000062</td>\n",
       "      <td>0.000062</td>\n",
       "      <td>0.000061</td>\n",
       "      <td>0.000061</td>\n",
       "      <td>0.000060</td>\n",
       "      <td>0.000059</td>\n",
       "      <td>0.000058</td>\n",
       "      <td>0.000057</td>\n",
       "      <td>0.000056</td>\n",
       "      <td>0.000055</td>\n",
       "      <td>0.000054</td>\n",
       "      <td>0.000053</td>\n",
       "      <td>0.000052</td>\n",
       "      <td>0.000051</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>0.000049</td>\n",
       "      <td>0.000048</td>\n",
       "      <td>0.000047</td>\n",
       "      <td>0.000046</td>\n",
       "      <td>0.000046</td>\n",
       "      <td>0.000045</td>\n",
       "      <td>0.000045</td>\n",
       "      <td>0.000045</td>\n",
       "      <td>0.000045</td>\n",
       "      <td>0.000045</td>\n",
       "      <td>0.000045</td>\n",
       "      <td>0.000045</td>\n",
       "      <td>0.000046</td>\n",
       "      <td>0.000046</td>\n",
       "      <td>0.000047</td>\n",
       "      <td>0.000047</td>\n",
       "      <td>0.000048</td>\n",
       "      <td>0.000048</td>\n",
       "      <td>0.000048</td>\n",
       "      <td>0.000049</td>\n",
       "      <td>0.000049</td>\n",
       "      <td>0.000049</td>\n",
       "      <td>0.000048</td>\n",
       "      <td>0.000047</td>\n",
       "      <td>0.000046</td>\n",
       "      <td>0.000045</td>\n",
       "      <td>0.000043</td>\n",
       "      <td>0.000041</td>\n",
       "      <td>0.000039</td>\n",
       "      <td>0.000037</td>\n",
       "      <td>0.000034</td>\n",
       "      <td>0.000032</td>\n",
       "      <td>0.000029</td>\n",
       "      <td>0.000027</td>\n",
       "      <td>0.000024</td>\n",
       "      <td>0.000022</td>\n",
       "      <td>0.00002</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>0.000019</td>\n",
       "      <td>0.000021</td>\n",
       "      <td>0.000024</td>\n",
       "      <td>0.000027</td>\n",
       "      <td>0.000031</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>0.000040</td>\n",
       "      <td>0.000046</td>\n",
       "      <td>0.000052</td>\n",
       "      <td>0.000059</td>\n",
       "      <td>0.000067</td>\n",
       "      <td>0.000075</td>\n",
       "      <td>0.000083</td>\n",
       "      <td>0.000092</td>\n",
       "      <td>0.000101</td>\n",
       "      <td>0.000110</td>\n",
       "      <td>0.000119</td>\n",
       "      <td>0.000127</td>\n",
       "      <td>0.000135</td>\n",
       "      <td>0.000141</td>\n",
       "      <td>0.000146</td>\n",
       "      <td>0.000149</td>\n",
       "      <td>0.000151</td>\n",
       "      <td>0.000151</td>\n",
       "      <td>0.000148</td>\n",
       "      <td>0.000144</td>\n",
       "      <td>0.000138</td>\n",
       "      <td>0.000130</td>\n",
       "      <td>0.000120</td>\n",
       "      <td>0.000108</td>\n",
       "      <td>0.000095</td>\n",
       "      <td>0.000082</td>\n",
       "      <td>0.000068</td>\n",
       "      <td>0.000053</td>\n",
       "      <td>0.000040</td>\n",
       "      <td>0.000027</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>-0.000003</td>\n",
       "      <td>-0.000009</td>\n",
       "      <td>-0.000012</td>\n",
       "      <td>-0.000012</td>\n",
       "      <td>-0.000010</td>\n",
       "      <td>-0.000005</td>\n",
       "      <td>2.471923e-06</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>0.000023</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>0.000047</td>\n",
       "      <td>0.000060</td>\n",
       "      <td>0.000071</td>\n",
       "      <td>0.000082</td>\n",
       "      <td>0.000090</td>\n",
       "      <td>0.000096</td>\n",
       "      <td>0.000099</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000097</td>\n",
       "      <td>0.000092</td>\n",
       "      <td>0.000083</td>\n",
       "      <td>0.000073</td>\n",
       "      <td>0.000059</td>\n",
       "      <td>0.000044</td>\n",
       "      <td>0.000027</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>-0.000010</td>\n",
       "      <td>-0.000029</td>\n",
       "      <td>-0.000049</td>\n",
       "      <td>-0.000068</td>\n",
       "      <td>-0.000086</td>\n",
       "      <td>-0.000103</td>\n",
       "      <td>-0.000119</td>\n",
       "      <td>-0.000134</td>\n",
       "      <td>-0.000146</td>\n",
       "      <td>-0.000156</td>\n",
       "      <td>-0.000164</td>\n",
       "      <td>-0.000170</td>\n",
       "      <td>-0.000172</td>\n",
       "      <td>-0.000172</td>\n",
       "      <td>-0.000170</td>\n",
       "      <td>-0.000165</td>\n",
       "      <td>-0.000157</td>\n",
       "      <td>-0.000147</td>\n",
       "      <td>-0.000136</td>\n",
       "      <td>-0.000123</td>\n",
       "      <td>-0.000108</td>\n",
       "      <td>-0.000093</td>\n",
       "      <td>-0.000077</td>\n",
       "      <td>-0.000061</td>\n",
       "      <td>-0.000044</td>\n",
       "      <td>-0.000028</td>\n",
       "      <td>-0.000013</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>0.000029</td>\n",
       "      <td>0.000040</td>\n",
       "      <td>0.000051</td>\n",
       "      <td>0.000059</td>\n",
       "      <td>0.000066</td>\n",
       "      <td>0.000070</td>\n",
       "      <td>0.000072</td>\n",
       "      <td>0.000072</td>\n",
       "      <td>0.000070</td>\n",
       "      <td>0.000065</td>\n",
       "      <td>0.000057</td>\n",
       "      <td>0.000046</td>\n",
       "      <td>0.000034</td>\n",
       "      <td>0.000019</td>\n",
       "      <td>2.712188e-06</td>\n",
       "      <td>-0.000015</td>\n",
       "      <td>-0.000033</td>\n",
       "      <td>-0.000052</td>\n",
       "      <td>-0.000070</td>\n",
       "      <td>-0.000087</td>\n",
       "      <td>-0.000102</td>\n",
       "      <td>-0.000116</td>\n",
       "      <td>-0.000127</td>\n",
       "      <td>-0.000135</td>\n",
       "      <td>-0.000141</td>\n",
       "      <td>-0.000143</td>\n",
       "      <td>-0.000141</td>\n",
       "      <td>-0.000137</td>\n",
       "      <td>-0.000130</td>\n",
       "      <td>-0.000119</td>\n",
       "      <td>-0.000107</td>\n",
       "      <td>-0.000092</td>\n",
       "      <td>-0.000075</td>\n",
       "      <td>-0.000057</td>\n",
       "      <td>-0.000039</td>\n",
       "      <td>-0.000020</td>\n",
       "      <td>-0.000002</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>0.000033</td>\n",
       "      <td>0.000048</td>\n",
       "      <td>0.000061</td>\n",
       "      <td>0.000072</td>\n",
       "      <td>0.000081</td>\n",
       "      <td>0.000088</td>\n",
       "      <td>0.000092</td>\n",
       "      <td>0.000094</td>\n",
       "      <td>0.000094</td>\n",
       "      <td>0.000091</td>\n",
       "      <td>0.000086</td>\n",
       "      <td>0.000080</td>\n",
       "      <td>0.000072</td>\n",
       "      <td>0.000063</td>\n",
       "      <td>0.000054</td>\n",
       "      <td>0.000044</td>\n",
       "      <td>0.000033</td>\n",
       "      <td>0.000023</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>-0.000004</td>\n",
       "      <td>-0.000012</td>\n",
       "      <td>-0.000019</td>\n",
       "      <td>-0.000025</td>\n",
       "      <td>-0.000030</td>\n",
       "      <td>-0.000034</td>\n",
       "      <td>-0.000037</td>\n",
       "      <td>-0.000039</td>\n",
       "      <td>-0.000040</td>\n",
       "      <td>-0.000040</td>\n",
       "      <td>-0.000040</td>\n",
       "      <td>-0.000039</td>\n",
       "      <td>-0.000037</td>\n",
       "      <td>-0.000035</td>\n",
       "      <td>-0.000032</td>\n",
       "      <td>-0.000029</td>\n",
       "      <td>-0.000025</td>\n",
       "      <td>-0.000021</td>\n",
       "      <td>-0.000017</td>\n",
       "      <td>-0.000013</td>\n",
       "      <td>-0.000009</td>\n",
       "      <td>-0.000004</td>\n",
       "      <td>3.775710e-08</td>\n",
       "      <td>4.375332e-06</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>0.000021</td>\n",
       "      <td>0.000025</td>\n",
       "      <td>0.000028</td>\n",
       "      <td>0.000032</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>0.000038</td>\n",
       "      <td>0.000040</td>\n",
       "      <td>0.000042</td>\n",
       "      <td>0.000044</td>\n",
       "      <td>0.000046</td>\n",
       "      <td>0.000048</td>\n",
       "      <td>0.000049</td>\n",
       "      <td>0.000051</td>\n",
       "      <td>0.000053</td>\n",
       "      <td>0.000055</td>\n",
       "      <td>0.000057</td>\n",
       "      <td>0.000059</td>\n",
       "      <td>0.000061</td>\n",
       "      <td>0.000063</td>\n",
       "      <td>0.000064</td>\n",
       "      <td>0.000065</td>\n",
       "      <td>0.000066</td>\n",
       "      <td>0.000067</td>\n",
       "      <td>0.000067</td>\n",
       "      <td>0.000067</td>\n",
       "      <td>0.000066</td>\n",
       "      <td>0.000066</td>\n",
       "      <td>0.000067</td>\n",
       "      <td>0.000068</td>\n",
       "      <td>0.000071</td>\n",
       "      <td>0.000075</td>\n",
       "      <td>0.000080</td>\n",
       "      <td>0.000086</td>\n",
       "      <td>0.000094</td>\n",
       "      <td>0.000104</td>\n",
       "      <td>0.000116</td>\n",
       "      <td>0.000129</td>\n",
       "      <td>0.000144</td>\n",
       "      <td>0.000160</td>\n",
       "      <td>0.000179</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>0.000224</td>\n",
       "      <td>0.000250</td>\n",
       "      <td>0.000280</td>\n",
       "      <td>0.000313</td>\n",
       "      <td>0.000349</td>\n",
       "      <td>0.000389</td>\n",
       "      <td>0.000432</td>\n",
       "      <td>0.000479</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000163</td>\n",
       "      <td>0.000160</td>\n",
       "      <td>0.000157</td>\n",
       "      <td>0.000153</td>\n",
       "      <td>0.000149</td>\n",
       "      <td>0.000144</td>\n",
       "      <td>0.000139</td>\n",
       "      <td>0.000134</td>\n",
       "      <td>0.000128</td>\n",
       "      <td>0.000123</td>\n",
       "      <td>0.000118</td>\n",
       "      <td>0.000114</td>\n",
       "      <td>0.000111</td>\n",
       "      <td>0.000108</td>\n",
       "      <td>0.000106</td>\n",
       "      <td>0.000106</td>\n",
       "      <td>0.000106</td>\n",
       "      <td>0.000108</td>\n",
       "      <td>0.000110</td>\n",
       "      <td>0.000114</td>\n",
       "      <td>0.000119</td>\n",
       "      <td>0.000124</td>\n",
       "      <td>0.000131</td>\n",
       "      <td>0.000138</td>\n",
       "      <td>0.000146</td>\n",
       "      <td>0.000154</td>\n",
       "      <td>0.000163</td>\n",
       "      <td>0.000171</td>\n",
       "      <td>0.000180</td>\n",
       "      <td>0.000188</td>\n",
       "      <td>0.000196</td>\n",
       "      <td>0.000203</td>\n",
       "      <td>0.000210</td>\n",
       "      <td>0.000215</td>\n",
       "      <td>0.000219</td>\n",
       "      <td>0.000223</td>\n",
       "      <td>0.000224</td>\n",
       "      <td>0.000225</td>\n",
       "      <td>0.000223</td>\n",
       "      <td>0.000220</td>\n",
       "      <td>0.000216</td>\n",
       "      <td>0.000210</td>\n",
       "      <td>0.000202</td>\n",
       "      <td>0.000192</td>\n",
       "      <td>0.000181</td>\n",
       "      <td>0.000169</td>\n",
       "      <td>0.000155</td>\n",
       "      <td>0.000140</td>\n",
       "      <td>0.000125</td>\n",
       "      <td>0.000109</td>\n",
       "      <td>0.000092</td>\n",
       "      <td>0.000076</td>\n",
       "      <td>0.000060</td>\n",
       "      <td>0.000044</td>\n",
       "      <td>0.000029</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>1.923702e-06</td>\n",
       "      <td>-0.000010</td>\n",
       "      <td>-0.000020</td>\n",
       "      <td>-0.000029</td>\n",
       "      <td>-0.000037</td>\n",
       "      <td>-0.000043</td>\n",
       "      <td>-0.000047</td>\n",
       "      <td>-0.000050</td>\n",
       "      <td>-0.000051</td>\n",
       "      <td>-0.000051</td>\n",
       "      <td>-0.000049</td>\n",
       "      <td>-0.000046</td>\n",
       "      <td>-0.000042</td>\n",
       "      <td>-0.000037</td>\n",
       "      <td>-0.000031</td>\n",
       "      <td>-0.000025</td>\n",
       "      <td>-0.000019</td>\n",
       "      <td>-0.000012</td>\n",
       "      <td>-0.000005</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000020</td>\n",
       "      <td>0.000025</td>\n",
       "      <td>0.000029</td>\n",
       "      <td>0.000033</td>\n",
       "      <td>0.000037</td>\n",
       "      <td>0.000040</td>\n",
       "      <td>0.000043</td>\n",
       "      <td>0.000045</td>\n",
       "      <td>0.000048</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>0.000052</td>\n",
       "      <td>0.000054</td>\n",
       "      <td>0.000057</td>\n",
       "      <td>0.000059</td>\n",
       "      <td>0.000061</td>\n",
       "      <td>0.000064</td>\n",
       "      <td>0.000066</td>\n",
       "      <td>0.000069</td>\n",
       "      <td>0.000071</td>\n",
       "      <td>0.000074</td>\n",
       "      <td>0.000077</td>\n",
       "      <td>0.000080</td>\n",
       "      <td>0.000084</td>\n",
       "      <td>0.000088</td>\n",
       "      <td>0.000092</td>\n",
       "      <td>0.000097</td>\n",
       "      <td>0.000103</td>\n",
       "      <td>0.000109</td>\n",
       "      <td>0.000116</td>\n",
       "      <td>0.000123</td>\n",
       "      <td>0.000132</td>\n",
       "      <td>0.000140</td>\n",
       "      <td>0.000150</td>\n",
       "      <td>0.000160</td>\n",
       "      <td>0.000171</td>\n",
       "      <td>0.000182</td>\n",
       "      <td>0.000194</td>\n",
       "      <td>0.000206</td>\n",
       "      <td>0.000219</td>\n",
       "      <td>0.000231</td>\n",
       "      <td>0.000244</td>\n",
       "      <td>0.000257</td>\n",
       "      <td>0.000269</td>\n",
       "      <td>0.000280</td>\n",
       "      <td>0.000291</td>\n",
       "      <td>0.000301</td>\n",
       "      <td>0.000310</td>\n",
       "      <td>0.000317</td>\n",
       "      <td>0.000322</td>\n",
       "      <td>0.000326</td>\n",
       "      <td>0.000328</td>\n",
       "      <td>0.000327</td>\n",
       "      <td>0.000323</td>\n",
       "      <td>0.000317</td>\n",
       "      <td>0.000308</td>\n",
       "      <td>0.000296</td>\n",
       "      <td>0.000282</td>\n",
       "      <td>0.000265</td>\n",
       "      <td>0.000245</td>\n",
       "      <td>0.000223</td>\n",
       "      <td>0.000199</td>\n",
       "      <td>0.000173</td>\n",
       "      <td>0.000145</td>\n",
       "      <td>0.000115</td>\n",
       "      <td>0.000085</td>\n",
       "      <td>0.000054</td>\n",
       "      <td>0.000023</td>\n",
       "      <td>-0.000007</td>\n",
       "      <td>-0.000037</td>\n",
       "      <td>-0.000066</td>\n",
       "      <td>-0.000092</td>\n",
       "      <td>-0.000117</td>\n",
       "      <td>-0.000139</td>\n",
       "      <td>-0.000158</td>\n",
       "      <td>-0.000173</td>\n",
       "      <td>-0.000185</td>\n",
       "      <td>-0.000194</td>\n",
       "      <td>-0.000199</td>\n",
       "      <td>-0.000199</td>\n",
       "      <td>-0.000196</td>\n",
       "      <td>-0.000189</td>\n",
       "      <td>-0.000178</td>\n",
       "      <td>-0.000164</td>\n",
       "      <td>-0.000146</td>\n",
       "      <td>-0.000126</td>\n",
       "      <td>-0.000103</td>\n",
       "      <td>-0.000078</td>\n",
       "      <td>-0.000052</td>\n",
       "      <td>-0.000025</td>\n",
       "      <td>1.829063e-06</td>\n",
       "      <td>0.000029</td>\n",
       "      <td>0.000055</td>\n",
       "      <td>0.000079</td>\n",
       "      <td>0.000102</td>\n",
       "      <td>0.000123</td>\n",
       "      <td>0.000141</td>\n",
       "      <td>0.000156</td>\n",
       "      <td>0.000168</td>\n",
       "      <td>0.000177</td>\n",
       "      <td>0.000182</td>\n",
       "      <td>0.000184</td>\n",
       "      <td>0.000182</td>\n",
       "      <td>0.000177</td>\n",
       "      <td>0.000168</td>\n",
       "      <td>0.000157</td>\n",
       "      <td>0.000143</td>\n",
       "      <td>0.000127</td>\n",
       "      <td>0.000109</td>\n",
       "      <td>0.000090</td>\n",
       "      <td>0.000070</td>\n",
       "      <td>0.000049</td>\n",
       "      <td>0.000027</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>-0.000014</td>\n",
       "      <td>-0.000033</td>\n",
       "      <td>-0.000051</td>\n",
       "      <td>-0.000066</td>\n",
       "      <td>-0.000080</td>\n",
       "      <td>-0.000092</td>\n",
       "      <td>-0.000101</td>\n",
       "      <td>-0.000108</td>\n",
       "      <td>-0.000113</td>\n",
       "      <td>-0.000115</td>\n",
       "      <td>-0.000114</td>\n",
       "      <td>-0.000111</td>\n",
       "      <td>-0.000106</td>\n",
       "      <td>-0.000099</td>\n",
       "      <td>-0.000090</td>\n",
       "      <td>-0.000079</td>\n",
       "      <td>-0.000067</td>\n",
       "      <td>-0.000054</td>\n",
       "      <td>-0.000040</td>\n",
       "      <td>-0.000026</td>\n",
       "      <td>-0.000012</td>\n",
       "      <td>1.579923e-06</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>0.000028</td>\n",
       "      <td>0.000040</td>\n",
       "      <td>0.000052</td>\n",
       "      <td>0.000062</td>\n",
       "      <td>0.000070</td>\n",
       "      <td>0.000078</td>\n",
       "      <td>0.000084</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.000092</td>\n",
       "      <td>0.000094</td>\n",
       "      <td>0.000095</td>\n",
       "      <td>0.000094</td>\n",
       "      <td>0.000093</td>\n",
       "      <td>0.000091</td>\n",
       "      <td>0.000088</td>\n",
       "      <td>0.000084</td>\n",
       "      <td>0.000081</td>\n",
       "      <td>0.000076</td>\n",
       "      <td>0.000072</td>\n",
       "      <td>0.000067</td>\n",
       "      <td>0.000063</td>\n",
       "      <td>0.000058</td>\n",
       "      <td>0.000054</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>0.000046</td>\n",
       "      <td>0.000043</td>\n",
       "      <td>0.000040</td>\n",
       "      <td>0.000038</td>\n",
       "      <td>0.000036</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>0.000036</td>\n",
       "      <td>0.000037</td>\n",
       "      <td>0.000039</td>\n",
       "      <td>0.000042</td>\n",
       "      <td>0.000045</td>\n",
       "      <td>0.000049</td>\n",
       "      <td>0.000053</td>\n",
       "      <td>0.000058</td>\n",
       "      <td>0.000063</td>\n",
       "      <td>0.000068</td>\n",
       "      <td>0.000074</td>\n",
       "      <td>0.000080</td>\n",
       "      <td>0.000087</td>\n",
       "      <td>0.000093</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000107</td>\n",
       "      <td>0.000114</td>\n",
       "      <td>0.000121</td>\n",
       "      <td>0.000127</td>\n",
       "      <td>0.000134</td>\n",
       "      <td>0.000140</td>\n",
       "      <td>0.000146</td>\n",
       "      <td>0.000152</td>\n",
       "      <td>0.000157</td>\n",
       "      <td>0.000161</td>\n",
       "      <td>0.000165</td>\n",
       "      <td>0.000168</td>\n",
       "      <td>0.000171</td>\n",
       "      <td>0.000172</td>\n",
       "      <td>0.000172</td>\n",
       "      <td>0.000172</td>\n",
       "      <td>0.000170</td>\n",
       "      <td>0.000168</td>\n",
       "      <td>0.000164</td>\n",
       "      <td>0.000160</td>\n",
       "      <td>0.000156</td>\n",
       "      <td>0.000150</td>\n",
       "      <td>0.000144</td>\n",
       "      <td>0.000138</td>\n",
       "      <td>0.000131</td>\n",
       "      <td>0.000124</td>\n",
       "      <td>0.000117</td>\n",
       "      <td>0.000110</td>\n",
       "      <td>0.000102</td>\n",
       "      <td>0.000095</td>\n",
       "      <td>0.000087</td>\n",
       "      <td>0.000080</td>\n",
       "      <td>0.000073</td>\n",
       "      <td>0.000066</td>\n",
       "      <td>0.000060</td>\n",
       "      <td>0.000053</td>\n",
       "      <td>0.000047</td>\n",
       "      <td>0.000042</td>\n",
       "      <td>0.000037</td>\n",
       "      <td>0.000033</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>0.000028</td>\n",
       "      <td>0.000027</td>\n",
       "      <td>0.000028</td>\n",
       "      <td>0.000029</td>\n",
       "      <td>0.000032</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>0.000040</td>\n",
       "      <td>0.000045</td>\n",
       "      <td>0.000051</td>\n",
       "      <td>0.000058</td>\n",
       "      <td>0.000065</td>\n",
       "      <td>0.000074</td>\n",
       "      <td>0.000083</td>\n",
       "      <td>0.000092</td>\n",
       "      <td>0.000103</td>\n",
       "      <td>0.000113</td>\n",
       "      <td>0.000124</td>\n",
       "      <td>0.000135</td>\n",
       "      <td>0.000147</td>\n",
       "      <td>0.000158</td>\n",
       "      <td>0.000169</td>\n",
       "      <td>0.000180</td>\n",
       "      <td>0.000191</td>\n",
       "      <td>0.000201</td>\n",
       "      <td>0.000210</td>\n",
       "      <td>0.000219</td>\n",
       "      <td>0.000226</td>\n",
       "      <td>0.000232</td>\n",
       "      <td>0.000236</td>\n",
       "      <td>0.000239</td>\n",
       "      <td>0.000239</td>\n",
       "      <td>0.000239</td>\n",
       "      <td>0.000236</td>\n",
       "      <td>0.000233</td>\n",
       "      <td>0.000227</td>\n",
       "      <td>0.000221</td>\n",
       "      <td>0.000213</td>\n",
       "      <td>0.000203</td>\n",
       "      <td>0.000192</td>\n",
       "      <td>0.000179</td>\n",
       "      <td>0.000163</td>\n",
       "      <td>0.000146</td>\n",
       "      <td>0.000128</td>\n",
       "      <td>0.000107</td>\n",
       "      <td>0.000086</td>\n",
       "      <td>0.000065</td>\n",
       "      <td>0.000043</td>\n",
       "      <td>0.000023</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>-0.000013</td>\n",
       "      <td>-0.000028</td>\n",
       "      <td>-0.000040</td>\n",
       "      <td>-0.000048</td>\n",
       "      <td>-0.000054</td>\n",
       "      <td>-0.000056</td>\n",
       "      <td>-0.000055</td>\n",
       "      <td>-0.000050</td>\n",
       "      <td>-0.000041</td>\n",
       "      <td>-0.000028</td>\n",
       "      <td>-0.000011</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>0.000064</td>\n",
       "      <td>0.000096</td>\n",
       "      <td>0.000131</td>\n",
       "      <td>0.000169</td>\n",
       "      <td>0.000207</td>\n",
       "      <td>0.000247</td>\n",
       "      <td>0.000287</td>\n",
       "      <td>0.000327</td>\n",
       "      <td>0.000366</td>\n",
       "      <td>0.000404</td>\n",
       "      <td>0.000441</td>\n",
       "      <td>0.000475</td>\n",
       "      <td>0.000506</td>\n",
       "      <td>0.000534</td>\n",
       "      <td>0.000557</td>\n",
       "      <td>0.000575</td>\n",
       "      <td>0.000588</td>\n",
       "      <td>0.000593</td>\n",
       "      <td>0.000589</td>\n",
       "      <td>0.000578</td>\n",
       "      <td>0.000560</td>\n",
       "      <td>0.000535</td>\n",
       "      <td>0.000502</td>\n",
       "      <td>0.000460</td>\n",
       "      <td>0.000408</td>\n",
       "      <td>0.000345</td>\n",
       "      <td>0.000269</td>\n",
       "      <td>0.000180</td>\n",
       "      <td>0.000076</td>\n",
       "      <td>-0.000041</td>\n",
       "      <td>-0.000172</td>\n",
       "      <td>-0.000314</td>\n",
       "      <td>-0.000465</td>\n",
       "      <td>-0.000622</td>\n",
       "      <td>-0.000779</td>\n",
       "      <td>-0.000931</td>\n",
       "      <td>-0.001073</td>\n",
       "      <td>-0.001201</td>\n",
       "      <td>-0.001312</td>\n",
       "      <td>-0.001403</td>\n",
       "      <td>-0.001472</td>\n",
       "      <td>-0.001520</td>\n",
       "      <td>-0.001546</td>\n",
       "      <td>-0.001548</td>\n",
       "      <td>-0.001527</td>\n",
       "      <td>-0.001483</td>\n",
       "      <td>-0.001417</td>\n",
       "      <td>-0.001329</td>\n",
       "      <td>-0.001219</td>\n",
       "      <td>-0.001089</td>\n",
       "      <td>-0.000940</td>\n",
       "      <td>-0.000773</td>\n",
       "      <td>-0.000590</td>\n",
       "      <td>-0.000396</td>\n",
       "      <td>-0.000196</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.000193</td>\n",
       "      <td>0.000366</td>\n",
       "      <td>0.000516</td>\n",
       "      <td>0.000638</td>\n",
       "      <td>0.000726</td>\n",
       "      <td>0.000775</td>\n",
       "      <td>0.000783</td>\n",
       "      <td>0.000752</td>\n",
       "      <td>0.000683</td>\n",
       "      <td>0.000581</td>\n",
       "      <td>0.000453</td>\n",
       "      <td>0.000312</td>\n",
       "      <td>0.000161</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>-0.000148</td>\n",
       "      <td>-0.000298</td>\n",
       "      <td>-0.000438</td>\n",
       "      <td>-0.000565</td>\n",
       "      <td>-0.000675</td>\n",
       "      <td>-0.000767</td>\n",
       "      <td>-0.000841</td>\n",
       "      <td>-0.000896</td>\n",
       "      <td>-0.000934</td>\n",
       "      <td>-0.000954</td>\n",
       "      <td>-0.000957</td>\n",
       "      <td>-0.000944</td>\n",
       "      <td>-0.000915</td>\n",
       "      <td>-0.000871</td>\n",
       "      <td>-0.000811</td>\n",
       "      <td>-0.000738</td>\n",
       "      <td>-0.000652</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9100 rows × 1151 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        1350.0    1351.0    1352.0    1353.0    1354.0    1355.0    1356.0  \\\n",
       "0     0.000084  0.000099  0.000117  0.000139  0.000165  0.000194  0.000227   \n",
       "1     0.000085  0.000099  0.000118  0.000140  0.000165  0.000195  0.000227   \n",
       "2     0.000084  0.000099  0.000117  0.000139  0.000165  0.000194  0.000227   \n",
       "3     0.000084  0.000099  0.000118  0.000140  0.000165  0.000195  0.000227   \n",
       "4     0.000084  0.000099  0.000118  0.000140  0.000165  0.000195  0.000227   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "9095  0.000080  0.000091  0.000104  0.000121  0.000141  0.000164  0.000191   \n",
       "9096  0.000080  0.000091  0.000104  0.000121  0.000141  0.000164  0.000191   \n",
       "9097  0.000080  0.000091  0.000104  0.000121  0.000141  0.000164  0.000191   \n",
       "9098  0.000080  0.000091  0.000104  0.000121  0.000141  0.000164  0.000191   \n",
       "9099  0.000080  0.000091  0.000104  0.000121  0.000141  0.000164  0.000191   \n",
       "\n",
       "        1357.0    1358.0    1359.0    1360.0    1361.0    1362.0    1363.0  \\\n",
       "0     0.000261  0.000297  0.000333  0.000367  0.000399  0.000427  0.000450   \n",
       "1     0.000261  0.000297  0.000333  0.000367  0.000399  0.000427  0.000451   \n",
       "2     0.000261  0.000297  0.000333  0.000367  0.000399  0.000427  0.000451   \n",
       "3     0.000262  0.000297  0.000333  0.000368  0.000400  0.000428  0.000451   \n",
       "4     0.000262  0.000297  0.000333  0.000368  0.000400  0.000428  0.000451   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "9095  0.000220  0.000250  0.000281  0.000312  0.000341  0.000368  0.000391   \n",
       "9096  0.000220  0.000250  0.000281  0.000312  0.000342  0.000368  0.000391   \n",
       "9097  0.000220  0.000250  0.000282  0.000313  0.000342  0.000368  0.000391   \n",
       "9098  0.000220  0.000250  0.000281  0.000312  0.000342  0.000368  0.000391   \n",
       "9099  0.000220  0.000250  0.000282  0.000312  0.000342  0.000368  0.000391   \n",
       "\n",
       "        1364.0    1365.0    1366.0    1367.0    1368.0    1369.0    1370.0  \\\n",
       "0     0.000468  0.000479  0.000484  0.000483  0.000477  0.000465  0.000449   \n",
       "1     0.000468  0.000480  0.000485  0.000483  0.000477  0.000465  0.000449   \n",
       "2     0.000468  0.000480  0.000485  0.000484  0.000477  0.000465  0.000449   \n",
       "3     0.000469  0.000480  0.000485  0.000484  0.000477  0.000465  0.000450   \n",
       "4     0.000469  0.000480  0.000485  0.000484  0.000477  0.000466  0.000450   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "9095  0.000409  0.000423  0.000431  0.000434  0.000432  0.000427  0.000418   \n",
       "9096  0.000409  0.000423  0.000431  0.000434  0.000433  0.000427  0.000418   \n",
       "9097  0.000410  0.000423  0.000431  0.000434  0.000433  0.000427  0.000418   \n",
       "9098  0.000409  0.000423  0.000431  0.000434  0.000433  0.000427  0.000419   \n",
       "9099  0.000409  0.000423  0.000431  0.000434  0.000432  0.000427  0.000418   \n",
       "\n",
       "        1371.0    1372.0    1373.0    1374.0    1375.0    1376.0    1377.0  \\\n",
       "0     0.000430  0.000409  0.000386  0.000361  0.000336  0.000311  0.000285   \n",
       "1     0.000430  0.000409  0.000386  0.000361  0.000336  0.000310  0.000284   \n",
       "2     0.000431  0.000409  0.000386  0.000362  0.000336  0.000311  0.000285   \n",
       "3     0.000431  0.000410  0.000386  0.000362  0.000337  0.000311  0.000285   \n",
       "4     0.000431  0.000410  0.000386  0.000362  0.000337  0.000311  0.000285   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "9095  0.000407  0.000394  0.000380  0.000364  0.000348  0.000331  0.000313   \n",
       "9096  0.000407  0.000394  0.000380  0.000364  0.000348  0.000331  0.000313   \n",
       "9097  0.000407  0.000394  0.000380  0.000364  0.000348  0.000331  0.000313   \n",
       "9098  0.000408  0.000395  0.000380  0.000364  0.000348  0.000331  0.000313   \n",
       "9099  0.000407  0.000394  0.000380  0.000364  0.000348  0.000331  0.000313   \n",
       "\n",
       "        1378.0    1379.0  1380.0  1381.0  1382.0  1383.0  1384.0  1385.0  \\\n",
       "0     0.000259  0.000233     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "1     0.000258  0.000233     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "2     0.000259  0.000234     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "3     0.000259  0.000234     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "4     0.000259  0.000234     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "...        ...       ...     ...     ...     ...     ...     ...     ...   \n",
       "9095  0.000295  0.000278     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "9096  0.000296  0.000278     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "9097  0.000295  0.000277     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "9098  0.000296  0.000278     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "9099  0.000295  0.000278     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "\n",
       "      1386.0  1387.0  1388.0  1389.0  1390.0  1391.0  1392.0  1393.0  1394.0  \\\n",
       "0        0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "1        0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "2        0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "3        0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "4        0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "...      ...     ...     ...     ...     ...     ...     ...     ...     ...   \n",
       "9095     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "9096     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "9097     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "9098     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "9099     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "\n",
       "      1395.0  1396.0  1397.0  1398.0  1399.0  1400.0  1401.0  1402.0  1403.0  \\\n",
       "0        0.0     0.0     0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0   \n",
       "1        0.0     0.0     0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0   \n",
       "2        0.0     0.0     0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0   \n",
       "3        0.0     0.0     0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0   \n",
       "4        0.0     0.0     0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0   \n",
       "...      ...     ...     ...     ...     ...     ...     ...     ...     ...   \n",
       "9095     0.0     0.0     0.0     0.0     0.0    -0.0    -0.0    -0.0    -0.0   \n",
       "9096     0.0     0.0     0.0     0.0     0.0    -0.0    -0.0    -0.0    -0.0   \n",
       "9097     0.0     0.0     0.0     0.0     0.0    -0.0    -0.0    -0.0    -0.0   \n",
       "9098     0.0     0.0     0.0     0.0     0.0    -0.0    -0.0    -0.0    -0.0   \n",
       "9099     0.0     0.0     0.0     0.0     0.0    -0.0    -0.0    -0.0    -0.0   \n",
       "\n",
       "      1404.0  1405.0  1406.0  1407.0  1408.0  1409.0  1410.0  1411.0  1412.0  \\\n",
       "0       -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0   \n",
       "1       -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0   \n",
       "2       -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0   \n",
       "3       -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0   \n",
       "4       -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0   \n",
       "...      ...     ...     ...     ...     ...     ...     ...     ...     ...   \n",
       "9095    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0   \n",
       "9096    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0   \n",
       "9097    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0   \n",
       "9098    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0   \n",
       "9099    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0   \n",
       "\n",
       "      1413.0  1414.0  1415.0  1416.0  1417.0  1418.0  1419.0  1420.0  1421.0  \\\n",
       "0       -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0   \n",
       "1       -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0   \n",
       "2       -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0   \n",
       "3       -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0   \n",
       "4       -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0   \n",
       "...      ...     ...     ...     ...     ...     ...     ...     ...     ...   \n",
       "9095    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0   \n",
       "9096    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0   \n",
       "9097    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0   \n",
       "9098    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0   \n",
       "9099    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0   \n",
       "\n",
       "      1422.0  1423.0  1424.0  1425.0  1426.0  1427.0  1428.0  1429.0  1430.0  \\\n",
       "0       -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0   \n",
       "1       -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0   \n",
       "2       -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0   \n",
       "3       -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0   \n",
       "4       -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0   \n",
       "...      ...     ...     ...     ...     ...     ...     ...     ...     ...   \n",
       "9095    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0   \n",
       "9096    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0   \n",
       "9097    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0   \n",
       "9098    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0   \n",
       "9099    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0   \n",
       "\n",
       "      1431.0  1432.0  1433.0  1434.0  1435.0  1436.0  1437.0  1438.0  1439.0  \\\n",
       "0       -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0   \n",
       "1       -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0   \n",
       "2       -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0   \n",
       "3       -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0   \n",
       "4       -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0   \n",
       "...      ...     ...     ...     ...     ...     ...     ...     ...     ...   \n",
       "9095    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0   \n",
       "9096    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0   \n",
       "9097    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0   \n",
       "9098    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0   \n",
       "9099    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0   \n",
       "\n",
       "      1440.0  1441.0  1442.0  1443.0  1444.0  1445.0  1446.0  1447.0  1448.0  \\\n",
       "0       -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0   \n",
       "1       -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0   \n",
       "2       -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0   \n",
       "3       -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0   \n",
       "4       -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0   \n",
       "...      ...     ...     ...     ...     ...     ...     ...     ...     ...   \n",
       "9095    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0   \n",
       "9096    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0   \n",
       "9097    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0   \n",
       "9098    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0   \n",
       "9099    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0   \n",
       "\n",
       "      1449.0  1450.0  1451.0  1452.0  1453.0  1454.0  1455.0  1456.0  1457.0  \\\n",
       "0       -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0   \n",
       "1       -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0   \n",
       "2       -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0   \n",
       "3       -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0   \n",
       "4       -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0   \n",
       "...      ...     ...     ...     ...     ...     ...     ...     ...     ...   \n",
       "9095    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0   \n",
       "9096    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0   \n",
       "9097    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0   \n",
       "9098    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0   \n",
       "9099    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0   \n",
       "\n",
       "      1458.0  1459.0  1460.0  1461.0  1462.0  1463.0  1464.0  1465.0  1466.0  \\\n",
       "0       -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0   \n",
       "1       -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0   \n",
       "2       -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0   \n",
       "3       -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0   \n",
       "4       -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0   \n",
       "...      ...     ...     ...     ...     ...     ...     ...     ...     ...   \n",
       "9095    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0   \n",
       "9096    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0   \n",
       "9097    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0   \n",
       "9098    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0   \n",
       "9099    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0   \n",
       "\n",
       "      1467.0  1468.0  1469.0  1470.0  1471.0  1472.0  1473.0  1474.0  1475.0  \\\n",
       "0       -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0   \n",
       "1       -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0   \n",
       "2       -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0   \n",
       "3       -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0   \n",
       "4       -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0   \n",
       "...      ...     ...     ...     ...     ...     ...     ...     ...     ...   \n",
       "9095    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0   \n",
       "9096    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0   \n",
       "9097    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0   \n",
       "9098    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0   \n",
       "9099    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0   \n",
       "\n",
       "      1476.0  1477.0  1478.0  1479.0  1480.0  1481.0  1482.0  1483.0  1484.0  \\\n",
       "0       -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0   \n",
       "1       -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0   \n",
       "2       -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0   \n",
       "3       -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0   \n",
       "4       -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0   \n",
       "...      ...     ...     ...     ...     ...     ...     ...     ...     ...   \n",
       "9095    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0   \n",
       "9096    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0   \n",
       "9097    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0   \n",
       "9098    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0   \n",
       "9099    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0   \n",
       "\n",
       "      1485.0  1486.0  1487.0  1488.0  1489.0  1490.0  1491.0  1492.0  1493.0  \\\n",
       "0       -0.0    -0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "1       -0.0    -0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "2       -0.0    -0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "3       -0.0    -0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "4       -0.0    -0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "...      ...     ...     ...     ...     ...     ...     ...     ...     ...   \n",
       "9095    -0.0    -0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "9096    -0.0    -0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "9097    -0.0    -0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "9098    -0.0    -0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "9099    -0.0    -0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "\n",
       "      1494.0  1495.0  1496.0  1497.0  1498.0  1499.0    1500.0    1501.0  \\\n",
       "0        0.0     0.0     0.0     0.0     0.0     0.0  0.000080  0.000084   \n",
       "1        0.0     0.0     0.0     0.0     0.0     0.0  0.000080  0.000083   \n",
       "2        0.0     0.0     0.0     0.0     0.0     0.0  0.000080  0.000084   \n",
       "3        0.0     0.0     0.0     0.0     0.0     0.0  0.000080  0.000084   \n",
       "4        0.0     0.0     0.0     0.0     0.0     0.0  0.000080  0.000084   \n",
       "...      ...     ...     ...     ...     ...     ...       ...       ...   \n",
       "9095     0.0     0.0     0.0     0.0     0.0     0.0  0.000082  0.000085   \n",
       "9096     0.0     0.0     0.0     0.0     0.0     0.0  0.000082  0.000085   \n",
       "9097     0.0     0.0     0.0     0.0     0.0     0.0  0.000082  0.000085   \n",
       "9098     0.0     0.0     0.0     0.0     0.0     0.0  0.000082  0.000084   \n",
       "9099     0.0     0.0     0.0     0.0     0.0     0.0  0.000082  0.000084   \n",
       "\n",
       "        1502.0    1503.0    1504.0    1505.0    1506.0    1507.0    1508.0  \\\n",
       "0     0.000087  0.000089  0.000091  0.000093  0.000094  0.000095  0.000096   \n",
       "1     0.000086  0.000089  0.000091  0.000093  0.000094  0.000095  0.000096   \n",
       "2     0.000087  0.000089  0.000091  0.000093  0.000094  0.000095  0.000096   \n",
       "3     0.000087  0.000089  0.000091  0.000093  0.000094  0.000095  0.000096   \n",
       "4     0.000086  0.000089  0.000091  0.000093  0.000094  0.000095  0.000096   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "9095  0.000087  0.000088  0.000090  0.000091  0.000091  0.000092  0.000093   \n",
       "9096  0.000087  0.000088  0.000089  0.000090  0.000091  0.000092  0.000093   \n",
       "9097  0.000087  0.000088  0.000090  0.000091  0.000091  0.000092  0.000093   \n",
       "9098  0.000086  0.000088  0.000089  0.000090  0.000091  0.000092  0.000093   \n",
       "9099  0.000086  0.000088  0.000089  0.000090  0.000091  0.000092  0.000093   \n",
       "\n",
       "        1509.0    1510.0    1511.0    1512.0    1513.0    1514.0    1515.0  \\\n",
       "0     0.000096  0.000096  0.000096  0.000096  0.000096  0.000096  0.000096   \n",
       "1     0.000096  0.000096  0.000096  0.000096  0.000096  0.000096  0.000096   \n",
       "2     0.000096  0.000097  0.000097  0.000097  0.000096  0.000096  0.000096   \n",
       "3     0.000096  0.000096  0.000096  0.000096  0.000096  0.000096  0.000096   \n",
       "4     0.000096  0.000096  0.000097  0.000097  0.000097  0.000097  0.000097   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "9095  0.000093  0.000094  0.000095  0.000095  0.000096  0.000097  0.000097   \n",
       "9096  0.000093  0.000094  0.000094  0.000095  0.000096  0.000096  0.000097   \n",
       "9097  0.000093  0.000094  0.000095  0.000095  0.000096  0.000097  0.000097   \n",
       "9098  0.000094  0.000094  0.000095  0.000096  0.000096  0.000097  0.000097   \n",
       "9099  0.000093  0.000094  0.000095  0.000095  0.000096  0.000097  0.000097   \n",
       "\n",
       "        1516.0    1517.0    1518.0    1519.0    1520.0    1521.0    1522.0  \\\n",
       "0     0.000096  0.000096  0.000096  0.000096  0.000096  0.000096  0.000095   \n",
       "1     0.000096  0.000096  0.000096  0.000096  0.000096  0.000096  0.000096   \n",
       "2     0.000096  0.000096  0.000096  0.000096  0.000096  0.000096  0.000095   \n",
       "3     0.000096  0.000096  0.000096  0.000096  0.000096  0.000096  0.000095   \n",
       "4     0.000096  0.000096  0.000096  0.000096  0.000096  0.000096  0.000095   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "9095  0.000098  0.000098  0.000098  0.000098  0.000098  0.000098  0.000097   \n",
       "9096  0.000097  0.000097  0.000097  0.000097  0.000097  0.000097  0.000097   \n",
       "9097  0.000098  0.000098  0.000098  0.000098  0.000098  0.000097  0.000097   \n",
       "9098  0.000098  0.000098  0.000098  0.000098  0.000098  0.000098  0.000097   \n",
       "9099  0.000098  0.000098  0.000098  0.000098  0.000098  0.000098  0.000097   \n",
       "\n",
       "        1523.0    1524.0    1525.0    1526.0    1527.0    1528.0    1529.0  \\\n",
       "0     0.000095  0.000094  0.000093  0.000092  0.000091  0.000090  0.000089   \n",
       "1     0.000095  0.000094  0.000094  0.000093  0.000092  0.000090  0.000089   \n",
       "2     0.000095  0.000094  0.000093  0.000092  0.000091  0.000090  0.000088   \n",
       "3     0.000095  0.000094  0.000093  0.000092  0.000091  0.000090  0.000088   \n",
       "4     0.000095  0.000094  0.000093  0.000092  0.000091  0.000090  0.000089   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "9095  0.000096  0.000096  0.000095  0.000094  0.000093  0.000092  0.000091   \n",
       "9096  0.000096  0.000095  0.000095  0.000094  0.000093  0.000092  0.000091   \n",
       "9097  0.000096  0.000096  0.000095  0.000094  0.000093  0.000091  0.000090   \n",
       "9098  0.000096  0.000096  0.000095  0.000094  0.000093  0.000092  0.000091   \n",
       "9099  0.000097  0.000096  0.000095  0.000094  0.000093  0.000092  0.000091   \n",
       "\n",
       "        1530.0    1531.0    1532.0    1533.0    1534.0    1535.0    1536.0  \\\n",
       "0     0.000087  0.000086  0.000084  0.000083  0.000082  0.000080  0.000079   \n",
       "1     0.000088  0.000086  0.000085  0.000083  0.000082  0.000081  0.000079   \n",
       "2     0.000087  0.000086  0.000084  0.000083  0.000082  0.000080  0.000079   \n",
       "3     0.000087  0.000086  0.000084  0.000083  0.000082  0.000080  0.000079   \n",
       "4     0.000087  0.000086  0.000084  0.000083  0.000081  0.000080  0.000079   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "9095  0.000089  0.000088  0.000087  0.000086  0.000085  0.000084  0.000082   \n",
       "9096  0.000090  0.000089  0.000087  0.000086  0.000085  0.000084  0.000083   \n",
       "9097  0.000089  0.000088  0.000087  0.000086  0.000085  0.000084  0.000082   \n",
       "9098  0.000089  0.000088  0.000087  0.000086  0.000085  0.000083  0.000082   \n",
       "9099  0.000090  0.000089  0.000087  0.000086  0.000085  0.000084  0.000082   \n",
       "\n",
       "        1537.0    1538.0    1539.0    1540.0    1541.0    1542.0    1543.0  \\\n",
       "0     0.000078  0.000077  0.000076  0.000075  0.000075  0.000074  0.000073   \n",
       "1     0.000078  0.000077  0.000076  0.000075  0.000075  0.000074  0.000073   \n",
       "2     0.000078  0.000077  0.000077  0.000076  0.000075  0.000074  0.000074   \n",
       "3     0.000078  0.000077  0.000077  0.000076  0.000075  0.000074  0.000073   \n",
       "4     0.000078  0.000077  0.000076  0.000075  0.000075  0.000074  0.000073   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "9095  0.000081  0.000080  0.000078  0.000077  0.000075  0.000074  0.000072   \n",
       "9096  0.000081  0.000080  0.000079  0.000077  0.000076  0.000074  0.000073   \n",
       "9097  0.000081  0.000080  0.000078  0.000077  0.000075  0.000074  0.000073   \n",
       "9098  0.000081  0.000080  0.000078  0.000077  0.000075  0.000074  0.000073   \n",
       "9099  0.000081  0.000079  0.000078  0.000076  0.000075  0.000073  0.000072   \n",
       "\n",
       "        1544.0    1545.0    1546.0    1547.0    1548.0    1549.0    1550.0  \\\n",
       "0     0.000073  0.000072  0.000071  0.000071  0.000070  0.000069  0.000069   \n",
       "1     0.000072  0.000072  0.000071  0.000070  0.000070  0.000069  0.000068   \n",
       "2     0.000073  0.000072  0.000072  0.000071  0.000070  0.000070  0.000069   \n",
       "3     0.000073  0.000072  0.000071  0.000071  0.000070  0.000069  0.000069   \n",
       "4     0.000073  0.000072  0.000071  0.000071  0.000070  0.000069  0.000069   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "9095  0.000071  0.000070  0.000069  0.000068  0.000067  0.000067  0.000066   \n",
       "9096  0.000072  0.000070  0.000069  0.000068  0.000067  0.000067  0.000066   \n",
       "9097  0.000071  0.000070  0.000069  0.000068  0.000067  0.000067  0.000066   \n",
       "9098  0.000071  0.000070  0.000069  0.000068  0.000067  0.000066  0.000066   \n",
       "9099  0.000071  0.000070  0.000069  0.000068  0.000067  0.000066  0.000066   \n",
       "\n",
       "        1551.0    1552.0    1553.0    1554.0    1555.0    1556.0    1557.0  \\\n",
       "0     0.000068  0.000067  0.000067  0.000066  0.000066  0.000065  0.000065   \n",
       "1     0.000068  0.000067  0.000067  0.000066  0.000066  0.000065  0.000065   \n",
       "2     0.000068  0.000067  0.000067  0.000066  0.000065  0.000065  0.000064   \n",
       "3     0.000068  0.000067  0.000067  0.000066  0.000066  0.000065  0.000065   \n",
       "4     0.000068  0.000067  0.000067  0.000066  0.000066  0.000065  0.000065   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "9095  0.000065  0.000065  0.000065  0.000064  0.000064  0.000064  0.000064   \n",
       "9096  0.000066  0.000065  0.000065  0.000065  0.000064  0.000064  0.000064   \n",
       "9097  0.000065  0.000065  0.000065  0.000064  0.000064  0.000064  0.000064   \n",
       "9098  0.000065  0.000065  0.000065  0.000064  0.000064  0.000064  0.000064   \n",
       "9099  0.000065  0.000065  0.000065  0.000064  0.000064  0.000064  0.000064   \n",
       "\n",
       "        1558.0    1559.0    1560.0    1561.0    1562.0    1563.0    1564.0  \\\n",
       "0     0.000064  0.000064  0.000064  0.000063  0.000063  0.000062  0.000062   \n",
       "1     0.000064  0.000064  0.000063  0.000063  0.000063  0.000062  0.000061   \n",
       "2     0.000064  0.000063  0.000063  0.000063  0.000062  0.000062  0.000061   \n",
       "3     0.000064  0.000064  0.000063  0.000063  0.000062  0.000062  0.000061   \n",
       "4     0.000064  0.000064  0.000063  0.000063  0.000062  0.000062  0.000061   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "9095  0.000063  0.000063  0.000063  0.000062  0.000062  0.000061  0.000061   \n",
       "9096  0.000063  0.000063  0.000063  0.000062  0.000062  0.000061  0.000061   \n",
       "9097  0.000063  0.000063  0.000063  0.000063  0.000062  0.000062  0.000061   \n",
       "9098  0.000063  0.000063  0.000063  0.000063  0.000062  0.000062  0.000061   \n",
       "9099  0.000063  0.000063  0.000063  0.000063  0.000062  0.000062  0.000061   \n",
       "\n",
       "        1565.0    1566.0    1567.0    1568.0    1569.0    1570.0    1571.0  \\\n",
       "0     0.000061  0.000060  0.000060  0.000059  0.000058  0.000057  0.000056   \n",
       "1     0.000061  0.000060  0.000060  0.000059  0.000058  0.000057  0.000056   \n",
       "2     0.000061  0.000060  0.000059  0.000059  0.000058  0.000057  0.000056   \n",
       "3     0.000061  0.000060  0.000059  0.000059  0.000058  0.000057  0.000056   \n",
       "4     0.000061  0.000060  0.000059  0.000058  0.000058  0.000057  0.000056   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "9095  0.000060  0.000060  0.000059  0.000058  0.000058  0.000057  0.000056   \n",
       "9096  0.000060  0.000059  0.000058  0.000058  0.000057  0.000056  0.000055   \n",
       "9097  0.000060  0.000060  0.000059  0.000058  0.000057  0.000056  0.000055   \n",
       "9098  0.000060  0.000060  0.000059  0.000058  0.000057  0.000056  0.000055   \n",
       "9099  0.000061  0.000060  0.000059  0.000058  0.000057  0.000056  0.000055   \n",
       "\n",
       "        1572.0    1573.0    1574.0    1575.0    1576.0    1577.0    1578.0  \\\n",
       "0     0.000055  0.000054  0.000054  0.000053  0.000052  0.000051  0.000050   \n",
       "1     0.000055  0.000054  0.000053  0.000053  0.000052  0.000051  0.000050   \n",
       "2     0.000055  0.000054  0.000054  0.000053  0.000052  0.000051  0.000050   \n",
       "3     0.000055  0.000054  0.000054  0.000053  0.000052  0.000051  0.000050   \n",
       "4     0.000055  0.000054  0.000053  0.000052  0.000052  0.000051  0.000050   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "9095  0.000055  0.000054  0.000052  0.000051  0.000050  0.000049  0.000048   \n",
       "9096  0.000054  0.000053  0.000052  0.000051  0.000050  0.000049  0.000048   \n",
       "9097  0.000054  0.000053  0.000052  0.000051  0.000050  0.000049  0.000048   \n",
       "9098  0.000054  0.000053  0.000052  0.000051  0.000050  0.000049  0.000048   \n",
       "9099  0.000054  0.000053  0.000052  0.000051  0.000050  0.000049  0.000048   \n",
       "\n",
       "        1579.0    1580.0    1581.0    1582.0    1583.0    1584.0    1585.0  \\\n",
       "0     0.000049  0.000049  0.000048  0.000048  0.000048  0.000047  0.000047   \n",
       "1     0.000049  0.000049  0.000048  0.000048  0.000048  0.000047  0.000047   \n",
       "2     0.000050  0.000049  0.000048  0.000048  0.000048  0.000048  0.000047   \n",
       "3     0.000049  0.000049  0.000048  0.000048  0.000048  0.000047  0.000047   \n",
       "4     0.000049  0.000049  0.000048  0.000048  0.000048  0.000047  0.000047   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "9095  0.000047  0.000046  0.000046  0.000045  0.000045  0.000045  0.000045   \n",
       "9096  0.000047  0.000047  0.000046  0.000046  0.000045  0.000045  0.000045   \n",
       "9097  0.000047  0.000046  0.000046  0.000045  0.000045  0.000045  0.000045   \n",
       "9098  0.000047  0.000046  0.000046  0.000045  0.000045  0.000045  0.000045   \n",
       "9099  0.000047  0.000046  0.000046  0.000045  0.000045  0.000045  0.000045   \n",
       "\n",
       "        1586.0    1587.0    1588.0    1589.0    1590.0    1591.0    1592.0  \\\n",
       "0     0.000047  0.000047  0.000048  0.000048  0.000048  0.000048  0.000049   \n",
       "1     0.000047  0.000048  0.000048  0.000048  0.000048  0.000049  0.000049   \n",
       "2     0.000047  0.000048  0.000048  0.000048  0.000048  0.000048  0.000048   \n",
       "3     0.000047  0.000047  0.000047  0.000048  0.000048  0.000048  0.000048   \n",
       "4     0.000047  0.000047  0.000048  0.000048  0.000048  0.000048  0.000049   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "9095  0.000045  0.000045  0.000046  0.000046  0.000046  0.000047  0.000047   \n",
       "9096  0.000045  0.000046  0.000046  0.000046  0.000047  0.000047  0.000048   \n",
       "9097  0.000045  0.000045  0.000046  0.000046  0.000046  0.000047  0.000047   \n",
       "9098  0.000045  0.000045  0.000045  0.000046  0.000046  0.000047  0.000047   \n",
       "9099  0.000045  0.000045  0.000045  0.000046  0.000046  0.000047  0.000047   \n",
       "\n",
       "        1593.0    1594.0    1595.0    1596.0    1597.0    1598.0    1599.0  \\\n",
       "0     0.000049  0.000049  0.000049  0.000049  0.000049  0.000049  0.000049   \n",
       "1     0.000049  0.000049  0.000050  0.000050  0.000050  0.000049  0.000049   \n",
       "2     0.000049  0.000049  0.000049  0.000049  0.000049  0.000049  0.000049   \n",
       "3     0.000049  0.000049  0.000049  0.000049  0.000049  0.000049  0.000049   \n",
       "4     0.000049  0.000049  0.000050  0.000050  0.000050  0.000050  0.000049   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "9095  0.000048  0.000048  0.000048  0.000048  0.000048  0.000048  0.000048   \n",
       "9096  0.000048  0.000048  0.000049  0.000049  0.000049  0.000048  0.000048   \n",
       "9097  0.000048  0.000048  0.000048  0.000049  0.000049  0.000048  0.000048   \n",
       "9098  0.000048  0.000048  0.000049  0.000049  0.000049  0.000049  0.000048   \n",
       "9099  0.000048  0.000048  0.000048  0.000049  0.000049  0.000049  0.000048   \n",
       "\n",
       "        1600.0    1601.0    1602.0    1603.0    1604.0    1605.0    1606.0  \\\n",
       "0     0.000048  0.000047  0.000046  0.000045  0.000043  0.000041  0.000038   \n",
       "1     0.000048  0.000047  0.000046  0.000045  0.000043  0.000041  0.000038   \n",
       "2     0.000048  0.000047  0.000046  0.000045  0.000043  0.000041  0.000038   \n",
       "3     0.000048  0.000047  0.000046  0.000045  0.000043  0.000041  0.000038   \n",
       "4     0.000049  0.000048  0.000046  0.000045  0.000043  0.000041  0.000038   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "9095  0.000047  0.000046  0.000044  0.000043  0.000041  0.000039  0.000036   \n",
       "9096  0.000047  0.000046  0.000044  0.000043  0.000041  0.000038  0.000036   \n",
       "9097  0.000047  0.000046  0.000045  0.000043  0.000041  0.000039  0.000036   \n",
       "9098  0.000047  0.000046  0.000045  0.000043  0.000041  0.000039  0.000036   \n",
       "9099  0.000047  0.000046  0.000045  0.000043  0.000041  0.000039  0.000037   \n",
       "\n",
       "        1607.0    1608.0    1609.0    1610.0    1611.0    1612.0   1613.0  \\\n",
       "0     0.000036  0.000033  0.000030  0.000028  0.000025  0.000022  0.00002   \n",
       "1     0.000036  0.000033  0.000030  0.000028  0.000025  0.000022  0.00002   \n",
       "2     0.000036  0.000033  0.000030  0.000028  0.000025  0.000022  0.00002   \n",
       "3     0.000036  0.000033  0.000031  0.000028  0.000025  0.000022  0.00002   \n",
       "4     0.000036  0.000033  0.000030  0.000028  0.000025  0.000022  0.00002   \n",
       "...        ...       ...       ...       ...       ...       ...      ...   \n",
       "9095  0.000034  0.000031  0.000029  0.000026  0.000024  0.000022  0.00002   \n",
       "9096  0.000034  0.000031  0.000028  0.000026  0.000024  0.000022  0.00002   \n",
       "9097  0.000034  0.000031  0.000029  0.000026  0.000024  0.000022  0.00002   \n",
       "9098  0.000034  0.000031  0.000029  0.000026  0.000024  0.000022  0.00002   \n",
       "9099  0.000034  0.000032  0.000029  0.000027  0.000024  0.000022  0.00002   \n",
       "\n",
       "        1614.0    1615.0    1616.0    1617.0    1618.0    1619.0    1620.0  \\\n",
       "0     0.000018  0.000016  0.000014  0.000013  0.000012  0.000012  0.000012   \n",
       "1     0.000018  0.000016  0.000014  0.000013  0.000012  0.000012  0.000012   \n",
       "2     0.000018  0.000016  0.000014  0.000013  0.000012  0.000012  0.000012   \n",
       "3     0.000018  0.000016  0.000014  0.000013  0.000013  0.000012  0.000012   \n",
       "4     0.000018  0.000016  0.000014  0.000013  0.000012  0.000012  0.000012   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "9095  0.000018  0.000017  0.000016  0.000015  0.000015  0.000015  0.000016   \n",
       "9096  0.000018  0.000017  0.000016  0.000015  0.000015  0.000015  0.000016   \n",
       "9097  0.000019  0.000017  0.000016  0.000015  0.000015  0.000015  0.000016   \n",
       "9098  0.000018  0.000017  0.000016  0.000015  0.000015  0.000015  0.000016   \n",
       "9099  0.000018  0.000017  0.000016  0.000015  0.000015  0.000015  0.000015   \n",
       "\n",
       "        1621.0    1622.0    1623.0    1624.0    1625.0    1626.0    1627.0  \\\n",
       "0     0.000013  0.000014  0.000016  0.000018  0.000021  0.000025  0.000029   \n",
       "1     0.000013  0.000014  0.000016  0.000018  0.000021  0.000025  0.000029   \n",
       "2     0.000013  0.000014  0.000016  0.000018  0.000021  0.000025  0.000029   \n",
       "3     0.000013  0.000014  0.000016  0.000018  0.000021  0.000025  0.000029   \n",
       "4     0.000013  0.000014  0.000016  0.000018  0.000021  0.000025  0.000029   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "9095  0.000017  0.000018  0.000020  0.000022  0.000024  0.000028  0.000031   \n",
       "9096  0.000017  0.000018  0.000020  0.000022  0.000025  0.000028  0.000031   \n",
       "9097  0.000016  0.000018  0.000019  0.000022  0.000024  0.000027  0.000031   \n",
       "9098  0.000016  0.000018  0.000019  0.000022  0.000024  0.000027  0.000031   \n",
       "9099  0.000016  0.000018  0.000019  0.000021  0.000024  0.000027  0.000031   \n",
       "\n",
       "        1628.0    1629.0    1630.0    1631.0    1632.0    1633.0    1634.0  \\\n",
       "0     0.000034  0.000040  0.000046  0.000053  0.000061  0.000069  0.000078   \n",
       "1     0.000034  0.000039  0.000046  0.000053  0.000061  0.000069  0.000078   \n",
       "2     0.000034  0.000039  0.000046  0.000053  0.000061  0.000069  0.000078   \n",
       "3     0.000034  0.000039  0.000046  0.000053  0.000061  0.000069  0.000078   \n",
       "4     0.000034  0.000039  0.000046  0.000053  0.000060  0.000069  0.000078   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "9095  0.000036  0.000040  0.000046  0.000052  0.000059  0.000067  0.000075   \n",
       "9096  0.000036  0.000041  0.000046  0.000052  0.000059  0.000067  0.000075   \n",
       "9097  0.000035  0.000040  0.000046  0.000052  0.000059  0.000066  0.000075   \n",
       "9098  0.000036  0.000041  0.000046  0.000052  0.000059  0.000067  0.000075   \n",
       "9099  0.000035  0.000040  0.000046  0.000052  0.000059  0.000067  0.000075   \n",
       "\n",
       "        1635.0    1636.0    1637.0    1638.0    1639.0    1640.0    1641.0  \\\n",
       "0     0.000088  0.000098  0.000108  0.000118  0.000127  0.000136  0.000144   \n",
       "1     0.000088  0.000098  0.000108  0.000118  0.000127  0.000136  0.000144   \n",
       "2     0.000088  0.000098  0.000108  0.000118  0.000128  0.000137  0.000145   \n",
       "3     0.000088  0.000098  0.000108  0.000118  0.000127  0.000136  0.000144   \n",
       "4     0.000087  0.000097  0.000107  0.000117  0.000127  0.000136  0.000144   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "9095  0.000083  0.000092  0.000101  0.000110  0.000119  0.000127  0.000134   \n",
       "9096  0.000084  0.000092  0.000101  0.000110  0.000119  0.000127  0.000135   \n",
       "9097  0.000083  0.000092  0.000101  0.000110  0.000119  0.000127  0.000135   \n",
       "9098  0.000084  0.000092  0.000101  0.000110  0.000119  0.000127  0.000135   \n",
       "9099  0.000083  0.000092  0.000101  0.000110  0.000119  0.000127  0.000135   \n",
       "\n",
       "        1642.0    1643.0    1644.0    1645.0    1646.0    1647.0    1648.0  \\\n",
       "0     0.000151  0.000157  0.000161  0.000162  0.000162  0.000160  0.000155   \n",
       "1     0.000151  0.000157  0.000161  0.000162  0.000162  0.000160  0.000155   \n",
       "2     0.000152  0.000157  0.000161  0.000163  0.000162  0.000160  0.000155   \n",
       "3     0.000151  0.000157  0.000160  0.000162  0.000162  0.000160  0.000155   \n",
       "4     0.000151  0.000157  0.000160  0.000162  0.000162  0.000160  0.000155   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "9095  0.000141  0.000146  0.000149  0.000151  0.000151  0.000148  0.000144   \n",
       "9096  0.000141  0.000146  0.000149  0.000151  0.000150  0.000148  0.000144   \n",
       "9097  0.000141  0.000146  0.000149  0.000151  0.000151  0.000149  0.000144   \n",
       "9098  0.000141  0.000146  0.000149  0.000151  0.000151  0.000148  0.000144   \n",
       "9099  0.000141  0.000146  0.000149  0.000151  0.000151  0.000148  0.000144   \n",
       "\n",
       "        1649.0    1650.0    1651.0    1652.0    1653.0    1654.0    1655.0  \\\n",
       "0     0.000148  0.000139  0.000128  0.000116  0.000102  0.000087  0.000071   \n",
       "1     0.000148  0.000139  0.000128  0.000116  0.000102  0.000087  0.000072   \n",
       "2     0.000148  0.000139  0.000128  0.000116  0.000102  0.000087  0.000071   \n",
       "3     0.000148  0.000139  0.000128  0.000116  0.000102  0.000087  0.000071   \n",
       "4     0.000148  0.000139  0.000129  0.000116  0.000102  0.000087  0.000072   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "9095  0.000138  0.000130  0.000120  0.000108  0.000095  0.000082  0.000068   \n",
       "9096  0.000137  0.000129  0.000119  0.000108  0.000095  0.000082  0.000068   \n",
       "9097  0.000138  0.000130  0.000120  0.000108  0.000096  0.000082  0.000068   \n",
       "9098  0.000138  0.000130  0.000120  0.000108  0.000096  0.000082  0.000068   \n",
       "9099  0.000138  0.000130  0.000120  0.000108  0.000095  0.000082  0.000068   \n",
       "\n",
       "        1656.0    1657.0    1658.0    1659.0    1660.0    1661.0    1662.0  \\\n",
       "0     0.000056  0.000040  0.000026  0.000013  0.000002 -0.000007 -0.000014   \n",
       "1     0.000056  0.000041  0.000026  0.000013  0.000002 -0.000007 -0.000014   \n",
       "2     0.000056  0.000040  0.000026  0.000013  0.000002 -0.000007 -0.000014   \n",
       "3     0.000056  0.000040  0.000026  0.000013  0.000002 -0.000007 -0.000014   \n",
       "4     0.000056  0.000041  0.000027  0.000013  0.000002 -0.000007 -0.000013   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "9095  0.000054  0.000040  0.000027  0.000015  0.000005 -0.000003 -0.000009   \n",
       "9096  0.000054  0.000040  0.000027  0.000015  0.000005 -0.000003 -0.000009   \n",
       "9097  0.000054  0.000040  0.000027  0.000015  0.000005 -0.000003 -0.000009   \n",
       "9098  0.000054  0.000040  0.000027  0.000015  0.000005 -0.000003 -0.000009   \n",
       "9099  0.000053  0.000040  0.000027  0.000015  0.000005 -0.000003 -0.000009   \n",
       "\n",
       "        1663.0    1664.0    1665.0    1666.0        1667.0    1668.0  \\\n",
       "0    -0.000017 -0.000018 -0.000015 -0.000009 -9.116918e-07  0.000010   \n",
       "1    -0.000017 -0.000017 -0.000015 -0.000009 -9.316766e-07  0.000010   \n",
       "2    -0.000017 -0.000018 -0.000015 -0.000009 -9.151183e-07  0.000010   \n",
       "3    -0.000017 -0.000017 -0.000015 -0.000009 -8.106343e-07  0.000010   \n",
       "4    -0.000017 -0.000017 -0.000015 -0.000009 -9.350197e-07  0.000010   \n",
       "...        ...       ...       ...       ...           ...       ...   \n",
       "9095 -0.000012 -0.000012 -0.000010 -0.000005  2.530169e-06  0.000012   \n",
       "9096 -0.000012 -0.000012 -0.000010 -0.000005  2.694662e-06  0.000012   \n",
       "9097 -0.000012 -0.000012 -0.000010 -0.000005  2.403662e-06  0.000012   \n",
       "9098 -0.000012 -0.000012 -0.000010 -0.000005  2.401107e-06  0.000012   \n",
       "9099 -0.000012 -0.000012 -0.000010 -0.000005  2.471923e-06  0.000012   \n",
       "\n",
       "        1669.0    1670.0    1671.0    1672.0    1673.0    1674.0    1675.0  \\\n",
       "0     0.000022  0.000036  0.000050  0.000064  0.000077  0.000089  0.000099   \n",
       "1     0.000022  0.000036  0.000050  0.000064  0.000077  0.000089  0.000099   \n",
       "2     0.000022  0.000036  0.000050  0.000064  0.000077  0.000089  0.000099   \n",
       "3     0.000022  0.000036  0.000050  0.000064  0.000077  0.000089  0.000099   \n",
       "4     0.000022  0.000036  0.000050  0.000064  0.000077  0.000089  0.000098   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "9095  0.000023  0.000035  0.000047  0.000060  0.000071  0.000081  0.000090   \n",
       "9096  0.000023  0.000035  0.000048  0.000060  0.000071  0.000082  0.000090   \n",
       "9097  0.000023  0.000035  0.000047  0.000060  0.000071  0.000082  0.000090   \n",
       "9098  0.000023  0.000035  0.000047  0.000060  0.000071  0.000081  0.000090   \n",
       "9099  0.000023  0.000035  0.000047  0.000060  0.000071  0.000082  0.000090   \n",
       "\n",
       "        1676.0    1677.0    1678.0    1679.0    1680.0    1681.0    1682.0  \\\n",
       "0     0.000106  0.000110  0.000111  0.000109  0.000103  0.000094  0.000082   \n",
       "1     0.000106  0.000110  0.000111  0.000108  0.000103  0.000094  0.000082   \n",
       "2     0.000106  0.000110  0.000111  0.000109  0.000103  0.000094  0.000082   \n",
       "3     0.000106  0.000110  0.000111  0.000109  0.000103  0.000094  0.000083   \n",
       "4     0.000105  0.000110  0.000111  0.000108  0.000103  0.000094  0.000082   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "9095  0.000096  0.000099  0.000099  0.000097  0.000092  0.000083  0.000072   \n",
       "9096  0.000096  0.000099  0.000099  0.000097  0.000091  0.000083  0.000072   \n",
       "9097  0.000096  0.000099  0.000100  0.000097  0.000092  0.000083  0.000072   \n",
       "9098  0.000096  0.000099  0.000099  0.000097  0.000091  0.000083  0.000072   \n",
       "9099  0.000096  0.000099  0.000100  0.000097  0.000092  0.000083  0.000073   \n",
       "\n",
       "        1683.0    1684.0    1685.0    1686.0    1687.0    1688.0    1689.0  \\\n",
       "0     0.000068  0.000051  0.000033  0.000013 -0.000008 -0.000029 -0.000050   \n",
       "1     0.000068  0.000051  0.000033  0.000013 -0.000008 -0.000029 -0.000050   \n",
       "2     0.000068  0.000051  0.000033  0.000013 -0.000008 -0.000029 -0.000050   \n",
       "3     0.000068  0.000051  0.000033  0.000013 -0.000008 -0.000029 -0.000050   \n",
       "4     0.000068  0.000051  0.000033  0.000013 -0.000008 -0.000029 -0.000050   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "9095  0.000059  0.000044  0.000027  0.000009 -0.000010 -0.000029 -0.000049   \n",
       "9096  0.000059  0.000044  0.000027  0.000009 -0.000010 -0.000029 -0.000049   \n",
       "9097  0.000059  0.000044  0.000027  0.000009 -0.000010 -0.000029 -0.000049   \n",
       "9098  0.000059  0.000044  0.000027  0.000009 -0.000010 -0.000029 -0.000048   \n",
       "9099  0.000059  0.000044  0.000027  0.000009 -0.000010 -0.000029 -0.000049   \n",
       "\n",
       "        1690.0    1691.0    1692.0    1693.0    1694.0    1695.0    1696.0  \\\n",
       "0    -0.000071 -0.000092 -0.000111 -0.000129 -0.000145 -0.000159 -0.000170   \n",
       "1    -0.000071 -0.000091 -0.000110 -0.000128 -0.000144 -0.000158 -0.000170   \n",
       "2    -0.000071 -0.000091 -0.000110 -0.000128 -0.000144 -0.000158 -0.000170   \n",
       "3    -0.000071 -0.000092 -0.000111 -0.000129 -0.000145 -0.000159 -0.000171   \n",
       "4    -0.000071 -0.000091 -0.000111 -0.000128 -0.000144 -0.000158 -0.000170   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "9095 -0.000068 -0.000086 -0.000103 -0.000119 -0.000133 -0.000146 -0.000156   \n",
       "9096 -0.000068 -0.000086 -0.000103 -0.000119 -0.000133 -0.000146 -0.000156   \n",
       "9097 -0.000068 -0.000086 -0.000103 -0.000119 -0.000133 -0.000146 -0.000156   \n",
       "9098 -0.000067 -0.000086 -0.000103 -0.000119 -0.000133 -0.000146 -0.000156   \n",
       "9099 -0.000068 -0.000086 -0.000103 -0.000119 -0.000134 -0.000146 -0.000156   \n",
       "\n",
       "        1697.0    1698.0    1699.0    1700.0    1701.0    1702.0    1703.0  \\\n",
       "0    -0.000180 -0.000186 -0.000190 -0.000191 -0.000189 -0.000184 -0.000177   \n",
       "1    -0.000179 -0.000186 -0.000190 -0.000191 -0.000189 -0.000184 -0.000177   \n",
       "2    -0.000179 -0.000186 -0.000190 -0.000191 -0.000189 -0.000184 -0.000177   \n",
       "3    -0.000180 -0.000186 -0.000190 -0.000191 -0.000189 -0.000184 -0.000177   \n",
       "4    -0.000180 -0.000186 -0.000190 -0.000191 -0.000189 -0.000184 -0.000177   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "9095 -0.000164 -0.000169 -0.000172 -0.000172 -0.000170 -0.000164 -0.000157   \n",
       "9096 -0.000164 -0.000169 -0.000172 -0.000172 -0.000170 -0.000165 -0.000157   \n",
       "9097 -0.000164 -0.000169 -0.000172 -0.000172 -0.000170 -0.000165 -0.000157   \n",
       "9098 -0.000164 -0.000169 -0.000172 -0.000172 -0.000170 -0.000165 -0.000157   \n",
       "9099 -0.000164 -0.000170 -0.000172 -0.000172 -0.000170 -0.000165 -0.000157   \n",
       "\n",
       "        1704.0    1705.0    1706.0    1707.0    1708.0    1709.0    1710.0  \\\n",
       "0    -0.000167 -0.000155 -0.000141 -0.000126 -0.000109 -0.000092 -0.000074   \n",
       "1    -0.000167 -0.000155 -0.000141 -0.000125 -0.000109 -0.000092 -0.000074   \n",
       "2    -0.000167 -0.000155 -0.000141 -0.000126 -0.000109 -0.000092 -0.000074   \n",
       "3    -0.000167 -0.000155 -0.000141 -0.000126 -0.000109 -0.000092 -0.000074   \n",
       "4    -0.000167 -0.000155 -0.000141 -0.000126 -0.000109 -0.000092 -0.000074   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "9095 -0.000147 -0.000136 -0.000123 -0.000108 -0.000093 -0.000077 -0.000061   \n",
       "9096 -0.000147 -0.000136 -0.000123 -0.000108 -0.000093 -0.000077 -0.000061   \n",
       "9097 -0.000148 -0.000136 -0.000123 -0.000109 -0.000093 -0.000077 -0.000061   \n",
       "9098 -0.000148 -0.000136 -0.000123 -0.000108 -0.000093 -0.000077 -0.000061   \n",
       "9099 -0.000147 -0.000136 -0.000123 -0.000108 -0.000093 -0.000077 -0.000061   \n",
       "\n",
       "        1711.0    1712.0    1713.0    1714.0    1715.0    1716.0    1717.0  \\\n",
       "0    -0.000057 -0.000039 -0.000022 -0.000006  0.000010  0.000024  0.000038   \n",
       "1    -0.000057 -0.000039 -0.000022 -0.000006  0.000010  0.000024  0.000037   \n",
       "2    -0.000057 -0.000039 -0.000022 -0.000006  0.000010  0.000024  0.000038   \n",
       "3    -0.000057 -0.000039 -0.000022 -0.000005  0.000010  0.000025  0.000038   \n",
       "4    -0.000057 -0.000039 -0.000022 -0.000005  0.000010  0.000025  0.000038   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "9095 -0.000045 -0.000029 -0.000013  0.000002  0.000016  0.000028  0.000040   \n",
       "9096 -0.000044 -0.000029 -0.000013  0.000002  0.000016  0.000028  0.000040   \n",
       "9097 -0.000045 -0.000029 -0.000013  0.000002  0.000016  0.000029  0.000040   \n",
       "9098 -0.000045 -0.000029 -0.000013  0.000002  0.000016  0.000028  0.000040   \n",
       "9099 -0.000044 -0.000028 -0.000013  0.000002  0.000016  0.000029  0.000040   \n",
       "\n",
       "        1718.0    1719.0    1720.0    1721.0    1722.0    1723.0    1724.0  \\\n",
       "0     0.000049  0.000059  0.000067  0.000072  0.000076  0.000076  0.000073   \n",
       "1     0.000049  0.000059  0.000067  0.000072  0.000075  0.000076  0.000073   \n",
       "2     0.000049  0.000059  0.000067  0.000072  0.000075  0.000076  0.000073   \n",
       "3     0.000049  0.000059  0.000067  0.000072  0.000075  0.000076  0.000073   \n",
       "4     0.000049  0.000059  0.000067  0.000072  0.000075  0.000076  0.000073   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "9095  0.000050  0.000059  0.000065  0.000070  0.000072  0.000072  0.000070   \n",
       "9096  0.000050  0.000059  0.000065  0.000070  0.000072  0.000072  0.000070   \n",
       "9097  0.000050  0.000059  0.000066  0.000070  0.000073  0.000073  0.000070   \n",
       "9098  0.000050  0.000059  0.000066  0.000070  0.000072  0.000072  0.000070   \n",
       "9099  0.000051  0.000059  0.000066  0.000070  0.000072  0.000072  0.000070   \n",
       "\n",
       "        1725.0    1726.0    1727.0    1728.0    1729.0        1730.0  \\\n",
       "0     0.000068  0.000059  0.000048  0.000034  0.000018 -6.216049e-07   \n",
       "1     0.000068  0.000059  0.000048  0.000034  0.000017 -8.584478e-07   \n",
       "2     0.000068  0.000059  0.000048  0.000034  0.000017 -9.320814e-07   \n",
       "3     0.000068  0.000059  0.000048  0.000034  0.000017 -9.590353e-07   \n",
       "4     0.000068  0.000059  0.000048  0.000034  0.000017 -9.655635e-07   \n",
       "...        ...       ...       ...       ...       ...           ...   \n",
       "9095  0.000064  0.000057  0.000046  0.000034  0.000019  2.772098e-06   \n",
       "9096  0.000065  0.000057  0.000046  0.000034  0.000019  2.891610e-06   \n",
       "9097  0.000065  0.000057  0.000047  0.000034  0.000019  3.104169e-06   \n",
       "9098  0.000065  0.000057  0.000047  0.000034  0.000019  3.016449e-06   \n",
       "9099  0.000065  0.000057  0.000046  0.000034  0.000019  2.712188e-06   \n",
       "\n",
       "        1731.0    1732.0    1733.0    1734.0    1735.0    1736.0    1737.0  \\\n",
       "0    -0.000020 -0.000041 -0.000062 -0.000082 -0.000102 -0.000119 -0.000134   \n",
       "1    -0.000021 -0.000041 -0.000062 -0.000083 -0.000102 -0.000119 -0.000134   \n",
       "2    -0.000021 -0.000041 -0.000062 -0.000083 -0.000102 -0.000119 -0.000134   \n",
       "3    -0.000021 -0.000041 -0.000062 -0.000083 -0.000102 -0.000119 -0.000134   \n",
       "4    -0.000021 -0.000041 -0.000062 -0.000083 -0.000102 -0.000119 -0.000134   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "9095 -0.000015 -0.000033 -0.000052 -0.000070 -0.000087 -0.000102 -0.000116   \n",
       "9096 -0.000015 -0.000033 -0.000051 -0.000070 -0.000087 -0.000102 -0.000116   \n",
       "9097 -0.000015 -0.000033 -0.000051 -0.000070 -0.000087 -0.000102 -0.000116   \n",
       "9098 -0.000015 -0.000033 -0.000051 -0.000069 -0.000087 -0.000102 -0.000116   \n",
       "9099 -0.000015 -0.000033 -0.000052 -0.000070 -0.000087 -0.000102 -0.000116   \n",
       "\n",
       "        1738.0    1739.0    1740.0    1741.0    1742.0    1743.0    1744.0  \\\n",
       "0    -0.000146 -0.000155 -0.000160 -0.000162 -0.000160 -0.000154 -0.000145   \n",
       "1    -0.000146 -0.000155 -0.000160 -0.000162 -0.000160 -0.000154 -0.000144   \n",
       "2    -0.000146 -0.000155 -0.000160 -0.000162 -0.000160 -0.000154 -0.000145   \n",
       "3    -0.000146 -0.000155 -0.000160 -0.000162 -0.000160 -0.000154 -0.000144   \n",
       "4    -0.000146 -0.000155 -0.000160 -0.000162 -0.000160 -0.000154 -0.000145   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "9095 -0.000127 -0.000135 -0.000141 -0.000143 -0.000141 -0.000137 -0.000130   \n",
       "9096 -0.000127 -0.000135 -0.000141 -0.000143 -0.000142 -0.000137 -0.000130   \n",
       "9097 -0.000127 -0.000136 -0.000141 -0.000143 -0.000142 -0.000137 -0.000130   \n",
       "9098 -0.000127 -0.000135 -0.000141 -0.000143 -0.000142 -0.000137 -0.000130   \n",
       "9099 -0.000127 -0.000135 -0.000141 -0.000143 -0.000141 -0.000137 -0.000130   \n",
       "\n",
       "        1745.0    1746.0    1747.0    1748.0    1749.0    1750.0    1751.0  \\\n",
       "0    -0.000132 -0.000117 -0.000100 -0.000080 -0.000060 -0.000038 -0.000017   \n",
       "1    -0.000132 -0.000117 -0.000099 -0.000080 -0.000059 -0.000038 -0.000017   \n",
       "2    -0.000132 -0.000117 -0.000099 -0.000080 -0.000059 -0.000038 -0.000017   \n",
       "3    -0.000132 -0.000117 -0.000099 -0.000080 -0.000059 -0.000038 -0.000017   \n",
       "4    -0.000132 -0.000117 -0.000099 -0.000080 -0.000060 -0.000038 -0.000017   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "9095 -0.000119 -0.000107 -0.000092 -0.000075 -0.000057 -0.000039 -0.000020   \n",
       "9096 -0.000120 -0.000107 -0.000092 -0.000076 -0.000058 -0.000039 -0.000021   \n",
       "9097 -0.000120 -0.000107 -0.000092 -0.000075 -0.000058 -0.000039 -0.000020   \n",
       "9098 -0.000119 -0.000107 -0.000092 -0.000075 -0.000058 -0.000039 -0.000020   \n",
       "9099 -0.000119 -0.000107 -0.000092 -0.000075 -0.000057 -0.000039 -0.000020   \n",
       "\n",
       "        1752.0    1753.0    1754.0    1755.0    1756.0    1757.0    1758.0  \\\n",
       "0     0.000004  0.000024  0.000042  0.000059  0.000074  0.000086  0.000096   \n",
       "1     0.000004  0.000024  0.000043  0.000059  0.000074  0.000086  0.000096   \n",
       "2     0.000004  0.000024  0.000043  0.000060  0.000074  0.000086  0.000096   \n",
       "3     0.000004  0.000024  0.000043  0.000060  0.000074  0.000086  0.000096   \n",
       "4     0.000004  0.000024  0.000043  0.000059  0.000074  0.000086  0.000096   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "9095 -0.000002  0.000016  0.000033  0.000048  0.000061  0.000072  0.000081   \n",
       "9096 -0.000002  0.000016  0.000032  0.000048  0.000061  0.000072  0.000081   \n",
       "9097 -0.000002  0.000016  0.000032  0.000048  0.000061  0.000072  0.000081   \n",
       "9098 -0.000002  0.000016  0.000032  0.000048  0.000061  0.000072  0.000081   \n",
       "9099 -0.000002  0.000016  0.000033  0.000048  0.000061  0.000072  0.000081   \n",
       "\n",
       "        1759.0    1760.0    1761.0    1762.0    1763.0    1764.0    1765.0  \\\n",
       "0     0.000102  0.000106  0.000108  0.000106  0.000103  0.000097  0.000089   \n",
       "1     0.000102  0.000106  0.000108  0.000106  0.000103  0.000097  0.000089   \n",
       "2     0.000103  0.000107  0.000108  0.000107  0.000103  0.000097  0.000090   \n",
       "3     0.000102  0.000106  0.000108  0.000106  0.000103  0.000097  0.000089   \n",
       "4     0.000102  0.000106  0.000108  0.000106  0.000103  0.000097  0.000089   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "9095  0.000088  0.000092  0.000094  0.000094  0.000091  0.000086  0.000080   \n",
       "9096  0.000088  0.000092  0.000094  0.000094  0.000091  0.000086  0.000080   \n",
       "9097  0.000088  0.000092  0.000094  0.000094  0.000091  0.000086  0.000080   \n",
       "9098  0.000088  0.000092  0.000094  0.000093  0.000091  0.000086  0.000080   \n",
       "9099  0.000088  0.000092  0.000094  0.000094  0.000091  0.000086  0.000080   \n",
       "\n",
       "        1766.0    1767.0    1768.0    1769.0    1770.0    1771.0    1772.0  \\\n",
       "0     0.000080  0.000070  0.000060  0.000048  0.000037  0.000026  0.000016   \n",
       "1     0.000080  0.000070  0.000059  0.000048  0.000037  0.000026  0.000016   \n",
       "2     0.000080  0.000070  0.000060  0.000048  0.000037  0.000026  0.000016   \n",
       "3     0.000080  0.000070  0.000059  0.000048  0.000037  0.000026  0.000016   \n",
       "4     0.000080  0.000070  0.000059  0.000048  0.000037  0.000026  0.000016   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "9095  0.000072  0.000063  0.000054  0.000044  0.000034  0.000023  0.000014   \n",
       "9096  0.000072  0.000064  0.000054  0.000044  0.000034  0.000023  0.000014   \n",
       "9097  0.000072  0.000063  0.000054  0.000044  0.000034  0.000023  0.000014   \n",
       "9098  0.000072  0.000063  0.000054  0.000044  0.000033  0.000023  0.000014   \n",
       "9099  0.000072  0.000063  0.000054  0.000044  0.000033  0.000023  0.000013   \n",
       "\n",
       "        1773.0    1774.0    1775.0    1776.0    1777.0    1778.0    1779.0  \\\n",
       "0     0.000007 -0.000002 -0.000010 -0.000017 -0.000023 -0.000027 -0.000031   \n",
       "1     0.000006 -0.000002 -0.000010 -0.000017 -0.000023 -0.000027 -0.000031   \n",
       "2     0.000006 -0.000002 -0.000010 -0.000017 -0.000023 -0.000027 -0.000031   \n",
       "3     0.000006 -0.000002 -0.000010 -0.000017 -0.000023 -0.000027 -0.000031   \n",
       "4     0.000007 -0.000002 -0.000010 -0.000017 -0.000022 -0.000027 -0.000031   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "9095  0.000004 -0.000004 -0.000012 -0.000019 -0.000025 -0.000030 -0.000034   \n",
       "9096  0.000004 -0.000004 -0.000012 -0.000019 -0.000025 -0.000030 -0.000034   \n",
       "9097  0.000004 -0.000004 -0.000012 -0.000019 -0.000025 -0.000030 -0.000034   \n",
       "9098  0.000004 -0.000004 -0.000012 -0.000019 -0.000025 -0.000029 -0.000033   \n",
       "9099  0.000004 -0.000004 -0.000012 -0.000019 -0.000025 -0.000030 -0.000034   \n",
       "\n",
       "        1780.0    1781.0    1782.0    1783.0    1784.0    1785.0    1786.0  \\\n",
       "0    -0.000034 -0.000036 -0.000037 -0.000038 -0.000038 -0.000037 -0.000035   \n",
       "1    -0.000034 -0.000036 -0.000037 -0.000038 -0.000037 -0.000036 -0.000035   \n",
       "2    -0.000034 -0.000036 -0.000037 -0.000038 -0.000037 -0.000037 -0.000035   \n",
       "3    -0.000034 -0.000036 -0.000037 -0.000038 -0.000037 -0.000037 -0.000035   \n",
       "4    -0.000034 -0.000036 -0.000037 -0.000038 -0.000038 -0.000037 -0.000035   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "9095 -0.000037 -0.000039 -0.000040 -0.000040 -0.000040 -0.000039 -0.000037   \n",
       "9096 -0.000037 -0.000039 -0.000040 -0.000040 -0.000040 -0.000039 -0.000037   \n",
       "9097 -0.000037 -0.000039 -0.000040 -0.000040 -0.000040 -0.000039 -0.000037   \n",
       "9098 -0.000036 -0.000038 -0.000040 -0.000040 -0.000040 -0.000039 -0.000037   \n",
       "9099 -0.000037 -0.000039 -0.000040 -0.000040 -0.000040 -0.000039 -0.000037   \n",
       "\n",
       "        1787.0    1788.0    1789.0    1790.0    1791.0    1792.0    1793.0  \\\n",
       "0    -0.000034 -0.000031 -0.000029 -0.000026 -0.000023 -0.000019 -0.000016   \n",
       "1    -0.000033 -0.000031 -0.000028 -0.000026 -0.000023 -0.000019 -0.000016   \n",
       "2    -0.000033 -0.000031 -0.000028 -0.000026 -0.000022 -0.000019 -0.000015   \n",
       "3    -0.000033 -0.000031 -0.000028 -0.000025 -0.000022 -0.000019 -0.000016   \n",
       "4    -0.000034 -0.000031 -0.000029 -0.000026 -0.000023 -0.000019 -0.000016   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "9095 -0.000035 -0.000032 -0.000029 -0.000025 -0.000021 -0.000017 -0.000013   \n",
       "9096 -0.000035 -0.000032 -0.000028 -0.000025 -0.000021 -0.000017 -0.000013   \n",
       "9097 -0.000035 -0.000032 -0.000029 -0.000025 -0.000021 -0.000017 -0.000013   \n",
       "9098 -0.000035 -0.000032 -0.000028 -0.000025 -0.000021 -0.000017 -0.000013   \n",
       "9099 -0.000035 -0.000032 -0.000029 -0.000025 -0.000021 -0.000017 -0.000013   \n",
       "\n",
       "        1794.0    1795.0        1796.0        1797.0    1798.0    1799.0  \\\n",
       "0    -0.000012 -0.000008 -4.116338e-06 -1.047570e-07  0.000004  0.000008   \n",
       "1    -0.000012 -0.000008 -4.246329e-06 -2.224495e-07  0.000004  0.000008   \n",
       "2    -0.000012 -0.000008 -3.979811e-06  2.885685e-08  0.000004  0.000008   \n",
       "3    -0.000012 -0.000008 -4.176842e-06 -1.985787e-07  0.000004  0.000008   \n",
       "4    -0.000012 -0.000008 -4.222580e-06 -1.869656e-07  0.000004  0.000008   \n",
       "...        ...       ...           ...           ...       ...       ...   \n",
       "9095 -0.000009 -0.000004  1.430718e-07  4.443608e-06  0.000009  0.000013   \n",
       "9096 -0.000008 -0.000004  3.069076e-07  4.595399e-06  0.000009  0.000013   \n",
       "9097 -0.000009 -0.000004  9.076379e-08  4.408955e-06  0.000009  0.000013   \n",
       "9098 -0.000009 -0.000004 -6.805936e-08  4.207411e-06  0.000008  0.000013   \n",
       "9099 -0.000009 -0.000004  3.775710e-08  4.375332e-06  0.000009  0.000013   \n",
       "\n",
       "        1800.0    1801.0    1802.0    1803.0    1804.0    1805.0    1806.0  \\\n",
       "0     0.000012  0.000016  0.000020  0.000024  0.000028  0.000032  0.000035   \n",
       "1     0.000012  0.000016  0.000020  0.000024  0.000028  0.000032  0.000035   \n",
       "2     0.000012  0.000016  0.000020  0.000024  0.000028  0.000031  0.000035   \n",
       "3     0.000012  0.000016  0.000020  0.000024  0.000028  0.000031  0.000035   \n",
       "4     0.000012  0.000016  0.000020  0.000024  0.000028  0.000032  0.000035   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "9095  0.000017  0.000021  0.000025  0.000028  0.000031  0.000035  0.000037   \n",
       "9096  0.000017  0.000021  0.000025  0.000028  0.000032  0.000035  0.000038   \n",
       "9097  0.000017  0.000021  0.000025  0.000028  0.000032  0.000035  0.000038   \n",
       "9098  0.000017  0.000021  0.000024  0.000028  0.000031  0.000034  0.000037   \n",
       "9099  0.000017  0.000021  0.000025  0.000028  0.000032  0.000035  0.000038   \n",
       "\n",
       "        1807.0    1808.0    1809.0    1810.0    1811.0    1812.0    1813.0  \\\n",
       "0     0.000038  0.000041  0.000043  0.000046  0.000048  0.000050  0.000051   \n",
       "1     0.000038  0.000041  0.000043  0.000046  0.000048  0.000050  0.000051   \n",
       "2     0.000038  0.000040  0.000043  0.000045  0.000047  0.000049  0.000051   \n",
       "3     0.000038  0.000041  0.000043  0.000045  0.000048  0.000049  0.000051   \n",
       "4     0.000038  0.000041  0.000043  0.000046  0.000048  0.000050  0.000051   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "9095  0.000040  0.000042  0.000044  0.000046  0.000048  0.000049  0.000051   \n",
       "9096  0.000040  0.000042  0.000044  0.000046  0.000048  0.000050  0.000051   \n",
       "9097  0.000040  0.000042  0.000044  0.000046  0.000048  0.000050  0.000051   \n",
       "9098  0.000040  0.000042  0.000044  0.000046  0.000048  0.000049  0.000051   \n",
       "9099  0.000040  0.000042  0.000044  0.000046  0.000048  0.000049  0.000051   \n",
       "\n",
       "        1814.0    1815.0    1816.0    1817.0    1818.0    1819.0    1820.0  \\\n",
       "0     0.000053  0.000055  0.000057  0.000059  0.000060  0.000062  0.000063   \n",
       "1     0.000053  0.000055  0.000057  0.000059  0.000060  0.000062  0.000063   \n",
       "2     0.000053  0.000055  0.000057  0.000059  0.000060  0.000062  0.000063   \n",
       "3     0.000053  0.000055  0.000057  0.000059  0.000061  0.000062  0.000064   \n",
       "4     0.000053  0.000055  0.000057  0.000059  0.000060  0.000062  0.000063   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "9095  0.000053  0.000055  0.000057  0.000059  0.000061  0.000063  0.000064   \n",
       "9096  0.000053  0.000055  0.000057  0.000059  0.000061  0.000062  0.000064   \n",
       "9097  0.000053  0.000055  0.000057  0.000059  0.000061  0.000062  0.000064   \n",
       "9098  0.000053  0.000055  0.000057  0.000059  0.000061  0.000063  0.000064   \n",
       "9099  0.000053  0.000055  0.000057  0.000059  0.000061  0.000063  0.000064   \n",
       "\n",
       "        1821.0    1822.0    1823.0    1824.0    1825.0    1826.0    1827.0  \\\n",
       "0     0.000065  0.000066  0.000066  0.000067  0.000068  0.000069  0.000070   \n",
       "1     0.000065  0.000066  0.000066  0.000067  0.000068  0.000069  0.000070   \n",
       "2     0.000065  0.000066  0.000067  0.000067  0.000068  0.000069  0.000070   \n",
       "3     0.000065  0.000066  0.000067  0.000067  0.000068  0.000069  0.000070   \n",
       "4     0.000065  0.000066  0.000066  0.000067  0.000068  0.000069  0.000070   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "9095  0.000065  0.000066  0.000067  0.000067  0.000067  0.000066  0.000067   \n",
       "9096  0.000065  0.000066  0.000066  0.000066  0.000066  0.000066  0.000066   \n",
       "9097  0.000065  0.000066  0.000066  0.000067  0.000066  0.000066  0.000066   \n",
       "9098  0.000065  0.000066  0.000067  0.000067  0.000067  0.000066  0.000067   \n",
       "9099  0.000065  0.000066  0.000067  0.000067  0.000067  0.000066  0.000066   \n",
       "\n",
       "        1828.0    1829.0    1830.0    1831.0    1832.0    1833.0    1834.0  \\\n",
       "0     0.000072  0.000075  0.000079  0.000084  0.000090  0.000098  0.000107   \n",
       "1     0.000072  0.000075  0.000079  0.000084  0.000090  0.000098  0.000107   \n",
       "2     0.000072  0.000075  0.000079  0.000084  0.000091  0.000098  0.000108   \n",
       "3     0.000072  0.000075  0.000079  0.000084  0.000090  0.000098  0.000107   \n",
       "4     0.000072  0.000075  0.000079  0.000084  0.000090  0.000098  0.000108   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "9095  0.000067  0.000069  0.000071  0.000075  0.000080  0.000086  0.000094   \n",
       "9096  0.000067  0.000068  0.000071  0.000074  0.000080  0.000086  0.000094   \n",
       "9097  0.000067  0.000069  0.000071  0.000075  0.000080  0.000086  0.000094   \n",
       "9098  0.000067  0.000069  0.000071  0.000075  0.000080  0.000086  0.000095   \n",
       "9099  0.000067  0.000068  0.000071  0.000075  0.000080  0.000086  0.000094   \n",
       "\n",
       "        1835.0    1836.0    1837.0    1838.0    1839.0    1840.0    1841.0  \\\n",
       "0     0.000118  0.000130  0.000144  0.000160  0.000177  0.000196  0.000217   \n",
       "1     0.000118  0.000131  0.000144  0.000160  0.000177  0.000196  0.000217   \n",
       "2     0.000119  0.000131  0.000145  0.000160  0.000177  0.000196  0.000218   \n",
       "3     0.000118  0.000131  0.000144  0.000160  0.000177  0.000196  0.000217   \n",
       "4     0.000118  0.000131  0.000145  0.000160  0.000177  0.000196  0.000218   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "9095  0.000104  0.000116  0.000129  0.000144  0.000160  0.000179  0.000200   \n",
       "9096  0.000104  0.000116  0.000129  0.000144  0.000161  0.000179  0.000200   \n",
       "9097  0.000104  0.000116  0.000129  0.000144  0.000161  0.000179  0.000200   \n",
       "9098  0.000105  0.000116  0.000129  0.000144  0.000161  0.000179  0.000200   \n",
       "9099  0.000104  0.000116  0.000129  0.000144  0.000160  0.000179  0.000200   \n",
       "\n",
       "        1842.0    1843.0    1844.0    1845.0    1846.0    1847.0    1848.0  \\\n",
       "0     0.000241  0.000267  0.000295  0.000327  0.000361  0.000399  0.000441   \n",
       "1     0.000241  0.000267  0.000295  0.000327  0.000361  0.000399  0.000441   \n",
       "2     0.000241  0.000267  0.000296  0.000327  0.000362  0.000400  0.000441   \n",
       "3     0.000241  0.000267  0.000296  0.000327  0.000362  0.000400  0.000441   \n",
       "4     0.000241  0.000267  0.000296  0.000327  0.000362  0.000400  0.000441   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "9095  0.000224  0.000250  0.000280  0.000313  0.000349  0.000389  0.000432   \n",
       "9096  0.000224  0.000250  0.000280  0.000313  0.000349  0.000389  0.000432   \n",
       "9097  0.000224  0.000250  0.000280  0.000313  0.000349  0.000389  0.000432   \n",
       "9098  0.000224  0.000250  0.000280  0.000313  0.000349  0.000389  0.000432   \n",
       "9099  0.000224  0.000250  0.000280  0.000313  0.000349  0.000389  0.000432   \n",
       "\n",
       "        1849.0  1850.0  1851.0  1852.0  1853.0  1854.0  1855.0  1856.0  \\\n",
       "0     0.000485     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "1     0.000485     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "2     0.000485     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "3     0.000486     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "4     0.000486     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "...        ...     ...     ...     ...     ...     ...     ...     ...   \n",
       "9095  0.000479     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "9096  0.000479     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "9097  0.000479     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "9098  0.000479     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "9099  0.000479     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "\n",
       "      1857.0  1858.0  1859.0  1860.0  1861.0  1862.0  1863.0  1864.0  1865.0  \\\n",
       "0        0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "1        0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "2        0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "3        0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "4        0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "...      ...     ...     ...     ...     ...     ...     ...     ...     ...   \n",
       "9095     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "9096     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "9097     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "9098     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "9099     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "\n",
       "      1866.0  1867.0  1868.0  1869.0  1870.0  1871.0  1872.0  1873.0  1874.0  \\\n",
       "0        0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "1        0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "2        0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "3        0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "4        0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "...      ...     ...     ...     ...     ...     ...     ...     ...     ...   \n",
       "9095     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "9096     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "9097     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "9098     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "9099     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "\n",
       "      1875.0  1876.0  1877.0  1878.0  1879.0  1880.0  1881.0  1882.0  1883.0  \\\n",
       "0        0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "1        0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "2        0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "3        0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "4        0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "...      ...     ...     ...     ...     ...     ...     ...     ...     ...   \n",
       "9095     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "9096     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "9097     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "9098     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "9099     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "\n",
       "      1884.0  1885.0  1886.0  1887.0  1888.0  1889.0  1890.0  1891.0  1892.0  \\\n",
       "0        0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "1        0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "2        0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "3        0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "4        0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "...      ...     ...     ...     ...     ...     ...     ...     ...     ...   \n",
       "9095     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "9096     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "9097     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "9098     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "9099     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "\n",
       "      1893.0  1894.0  1895.0  1896.0  1897.0  1898.0  1899.0  1900.0  1901.0  \\\n",
       "0       -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0   \n",
       "1       -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0   \n",
       "2       -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0   \n",
       "3       -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0   \n",
       "4       -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0   \n",
       "...      ...     ...     ...     ...     ...     ...     ...     ...     ...   \n",
       "9095     0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0   \n",
       "9096     0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0   \n",
       "9097     0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0   \n",
       "9098     0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0   \n",
       "9099     0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0   \n",
       "\n",
       "      1902.0  1903.0  1904.0  1905.0  1906.0  1907.0  1908.0  1909.0  1910.0  \\\n",
       "0       -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0   \n",
       "1       -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0   \n",
       "2       -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0   \n",
       "3       -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0   \n",
       "4       -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0   \n",
       "...      ...     ...     ...     ...     ...     ...     ...     ...     ...   \n",
       "9095    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0   \n",
       "9096    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0   \n",
       "9097    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0   \n",
       "9098    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0   \n",
       "9099    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0   \n",
       "\n",
       "      1911.0  1912.0  1913.0  1914.0  1915.0  1916.0  1917.0  1918.0  1919.0  \\\n",
       "0       -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0   \n",
       "1       -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0   \n",
       "2       -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0   \n",
       "3       -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0   \n",
       "4       -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0   \n",
       "...      ...     ...     ...     ...     ...     ...     ...     ...     ...   \n",
       "9095    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0   \n",
       "9096    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0   \n",
       "9097    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0   \n",
       "9098    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0   \n",
       "9099    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0   \n",
       "\n",
       "      1920.0  1921.0  1922.0  1923.0  1924.0  1925.0  1926.0  1927.0  1928.0  \\\n",
       "0       -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0   \n",
       "1       -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0   \n",
       "2       -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0   \n",
       "3       -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0   \n",
       "4       -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0   \n",
       "...      ...     ...     ...     ...     ...     ...     ...     ...     ...   \n",
       "9095    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0   \n",
       "9096    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0   \n",
       "9097    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0   \n",
       "9098    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0   \n",
       "9099    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0   \n",
       "\n",
       "      1929.0  1930.0  1931.0  1932.0  1933.0  1934.0  1935.0  1936.0  1937.0  \\\n",
       "0       -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0     0.0   \n",
       "1       -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0     0.0   \n",
       "2       -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0     0.0   \n",
       "3       -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0     0.0   \n",
       "4       -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0     0.0   \n",
       "...      ...     ...     ...     ...     ...     ...     ...     ...     ...   \n",
       "9095    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0   \n",
       "9096    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0   \n",
       "9097    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0   \n",
       "9098    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0   \n",
       "9099    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0   \n",
       "\n",
       "      1938.0  1939.0  1940.0  1941.0  1942.0  1943.0  1944.0  1945.0  1946.0  \\\n",
       "0        0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "1        0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "2        0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "3        0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "4        0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "...      ...     ...     ...     ...     ...     ...     ...     ...     ...   \n",
       "9095    -0.0    -0.0    -0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "9096    -0.0    -0.0    -0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "9097    -0.0    -0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "9098    -0.0    -0.0    -0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "9099    -0.0    -0.0    -0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "\n",
       "      1947.0  1948.0  1949.0  1950.0  1951.0  1952.0  1953.0  1954.0  1955.0  \\\n",
       "0        0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0   \n",
       "1        0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0   \n",
       "2        0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0   \n",
       "3        0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0   \n",
       "4        0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0   \n",
       "...      ...     ...     ...     ...     ...     ...     ...     ...     ...   \n",
       "9095     0.0     0.0     0.0     0.0     0.0    -0.0    -0.0    -0.0    -0.0   \n",
       "9096     0.0     0.0     0.0     0.0    -0.0    -0.0    -0.0    -0.0    -0.0   \n",
       "9097     0.0     0.0     0.0     0.0    -0.0    -0.0    -0.0    -0.0    -0.0   \n",
       "9098     0.0     0.0     0.0     0.0     0.0    -0.0    -0.0    -0.0    -0.0   \n",
       "9099     0.0     0.0     0.0     0.0    -0.0    -0.0    -0.0    -0.0    -0.0   \n",
       "\n",
       "      1956.0  1957.0  1958.0  1959.0  1960.0  1961.0  1962.0  1963.0  1964.0  \\\n",
       "0       -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0   \n",
       "1       -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0   \n",
       "2       -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0   \n",
       "3       -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0   \n",
       "4       -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0   \n",
       "...      ...     ...     ...     ...     ...     ...     ...     ...     ...   \n",
       "9095    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0   \n",
       "9096    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0   \n",
       "9097    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0   \n",
       "9098    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0   \n",
       "9099    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0   \n",
       "\n",
       "      1965.0  1966.0  1967.0  1968.0  1969.0  1970.0  1971.0  1972.0  1973.0  \\\n",
       "0       -0.0    -0.0    -0.0    -0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "1       -0.0    -0.0    -0.0    -0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "2       -0.0    -0.0    -0.0    -0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "3       -0.0    -0.0    -0.0    -0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "4       -0.0    -0.0    -0.0    -0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "...      ...     ...     ...     ...     ...     ...     ...     ...     ...   \n",
       "9095    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0     0.0     0.0     0.0   \n",
       "9096    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0     0.0     0.0   \n",
       "9097    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0     0.0     0.0     0.0   \n",
       "9098    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0     0.0     0.0   \n",
       "9099    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0    -0.0     0.0     0.0   \n",
       "\n",
       "      1974.0  1975.0  1976.0  1977.0  1978.0  1979.0  1980.0  1981.0  1982.0  \\\n",
       "0        0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "1        0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "2        0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "3        0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "4        0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "...      ...     ...     ...     ...     ...     ...     ...     ...     ...   \n",
       "9095     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "9096     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "9097     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "9098     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "9099     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "\n",
       "      1983.0  1984.0  1985.0  1986.0  1987.0  1988.0  1989.0  1990.0  1991.0  \\\n",
       "0        0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "1        0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "2        0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "3        0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "4        0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "...      ...     ...     ...     ...     ...     ...     ...     ...     ...   \n",
       "9095     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "9096     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "9097     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "9098     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "9099     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "\n",
       "      1992.0  1993.0  1994.0  1995.0  1996.0  1997.0  1998.0  1999.0  2000.0  \\\n",
       "0        0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "1        0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "2        0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "3        0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "4        0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "...      ...     ...     ...     ...     ...     ...     ...     ...     ...   \n",
       "9095     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "9096     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "9097     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "9098     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "9099     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "\n",
       "      2001.0  2002.0  2003.0  2004.0  2005.0  2006.0  2007.0  2008.0  2009.0  \\\n",
       "0        0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "1        0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "2        0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "3        0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "4        0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "...      ...     ...     ...     ...     ...     ...     ...     ...     ...   \n",
       "9095     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "9096     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "9097     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "9098     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "9099     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "\n",
       "      2010.0  2011.0  2012.0  2013.0  2014.0  2015.0  2016.0  2017.0  2018.0  \\\n",
       "0        0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "1        0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "2        0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "3        0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "4        0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "...      ...     ...     ...     ...     ...     ...     ...     ...     ...   \n",
       "9095     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "9096     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "9097     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "9098     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "9099     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "\n",
       "      2019.0  2020.0  2021.0  2022.0  2023.0  2024.0  2025.0  2026.0  2027.0  \\\n",
       "0        0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "1        0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "2        0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "3        0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "4        0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "...      ...     ...     ...     ...     ...     ...     ...     ...     ...   \n",
       "9095     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "9096     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "9097     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "9098     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "9099     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "\n",
       "      2028.0  2029.0  2030.0  2031.0  2032.0  2033.0  2034.0  2035.0  2036.0  \\\n",
       "0        0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "1        0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "2        0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "3        0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "4        0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "...      ...     ...     ...     ...     ...     ...     ...     ...     ...   \n",
       "9095     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "9096     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "9097     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "9098     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "9099     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "\n",
       "      2037.0  2038.0  2039.0  2040.0  2041.0  2042.0  2043.0  2044.0  2045.0  \\\n",
       "0        0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "1        0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "2        0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "3        0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "4        0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "...      ...     ...     ...     ...     ...     ...     ...     ...     ...   \n",
       "9095     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "9096     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "9097     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "9098     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "9099     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "\n",
       "      2046.0  2047.0  2048.0  2049.0    2050.0    2051.0    2052.0    2053.0  \\\n",
       "0        0.0     0.0     0.0     0.0  0.000157  0.000152  0.000148  0.000142   \n",
       "1        0.0     0.0     0.0     0.0  0.000155  0.000151  0.000146  0.000141   \n",
       "2        0.0     0.0     0.0     0.0  0.000158  0.000154  0.000149  0.000144   \n",
       "3        0.0     0.0     0.0     0.0  0.000156  0.000152  0.000147  0.000142   \n",
       "4        0.0     0.0     0.0     0.0  0.000158  0.000153  0.000149  0.000143   \n",
       "...      ...     ...     ...     ...       ...       ...       ...       ...   \n",
       "9095     0.0     0.0     0.0     0.0  0.000164  0.000161  0.000158  0.000154   \n",
       "9096     0.0     0.0     0.0     0.0  0.000165  0.000162  0.000159  0.000155   \n",
       "9097     0.0     0.0     0.0     0.0  0.000164  0.000162  0.000158  0.000154   \n",
       "9098     0.0     0.0     0.0     0.0  0.000165  0.000162  0.000158  0.000154   \n",
       "9099     0.0     0.0     0.0     0.0  0.000163  0.000160  0.000157  0.000153   \n",
       "\n",
       "        2054.0    2055.0    2056.0    2057.0    2058.0    2059.0    2060.0  \\\n",
       "0     0.000137  0.000132  0.000126  0.000121  0.000116  0.000112  0.000108   \n",
       "1     0.000135  0.000130  0.000125  0.000120  0.000115  0.000111  0.000108   \n",
       "2     0.000138  0.000132  0.000127  0.000121  0.000116  0.000112  0.000108   \n",
       "3     0.000136  0.000131  0.000125  0.000120  0.000115  0.000111  0.000108   \n",
       "4     0.000138  0.000132  0.000127  0.000121  0.000117  0.000112  0.000109   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "9095  0.000149  0.000144  0.000139  0.000133  0.000128  0.000122  0.000117   \n",
       "9096  0.000150  0.000145  0.000139  0.000133  0.000128  0.000123  0.000118   \n",
       "9097  0.000149  0.000144  0.000139  0.000133  0.000127  0.000122  0.000117   \n",
       "9098  0.000149  0.000144  0.000138  0.000132  0.000126  0.000121  0.000116   \n",
       "9099  0.000149  0.000144  0.000139  0.000134  0.000128  0.000123  0.000118   \n",
       "\n",
       "        2061.0    2062.0    2063.0    2064.0    2065.0    2066.0    2067.0  \\\n",
       "0     0.000106  0.000104  0.000104  0.000105  0.000107  0.000110  0.000114   \n",
       "1     0.000106  0.000105  0.000105  0.000106  0.000109  0.000112  0.000116   \n",
       "2     0.000105  0.000104  0.000103  0.000104  0.000106  0.000109  0.000113   \n",
       "3     0.000106  0.000104  0.000104  0.000105  0.000107  0.000111  0.000115   \n",
       "4     0.000106  0.000105  0.000104  0.000105  0.000107  0.000110  0.000114   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "9095  0.000113  0.000110  0.000107  0.000105  0.000105  0.000105  0.000107   \n",
       "9096  0.000114  0.000110  0.000108  0.000106  0.000106  0.000106  0.000107   \n",
       "9097  0.000113  0.000109  0.000107  0.000105  0.000104  0.000105  0.000107   \n",
       "9098  0.000112  0.000109  0.000106  0.000105  0.000105  0.000105  0.000107   \n",
       "9099  0.000114  0.000111  0.000108  0.000106  0.000106  0.000106  0.000108   \n",
       "\n",
       "        2068.0    2069.0    2070.0    2071.0    2072.0    2073.0    2074.0  \\\n",
       "0     0.000120  0.000127  0.000134  0.000143  0.000151  0.000161  0.000170   \n",
       "1     0.000122  0.000129  0.000136  0.000144  0.000153  0.000162  0.000171   \n",
       "2     0.000119  0.000126  0.000133  0.000142  0.000151  0.000160  0.000170   \n",
       "3     0.000121  0.000127  0.000135  0.000143  0.000151  0.000161  0.000170   \n",
       "4     0.000120  0.000126  0.000134  0.000142  0.000151  0.000160  0.000170   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "9095  0.000110  0.000114  0.000119  0.000125  0.000132  0.000139  0.000147   \n",
       "9096  0.000110  0.000114  0.000118  0.000124  0.000130  0.000138  0.000145   \n",
       "9097  0.000109  0.000113  0.000118  0.000124  0.000131  0.000138  0.000146   \n",
       "9098  0.000110  0.000114  0.000119  0.000125  0.000132  0.000140  0.000147   \n",
       "9099  0.000110  0.000114  0.000119  0.000124  0.000131  0.000138  0.000146   \n",
       "\n",
       "        2075.0    2076.0    2077.0    2078.0    2079.0    2080.0    2081.0  \\\n",
       "0     0.000180  0.000189  0.000198  0.000207  0.000214  0.000222  0.000228   \n",
       "1     0.000180  0.000189  0.000198  0.000206  0.000214  0.000221  0.000227   \n",
       "2     0.000179  0.000189  0.000198  0.000207  0.000215  0.000222  0.000228   \n",
       "3     0.000179  0.000189  0.000198  0.000206  0.000214  0.000221  0.000228   \n",
       "4     0.000179  0.000189  0.000198  0.000207  0.000215  0.000222  0.000229   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "9095  0.000156  0.000164  0.000173  0.000181  0.000190  0.000197  0.000205   \n",
       "9096  0.000154  0.000162  0.000171  0.000179  0.000188  0.000196  0.000203   \n",
       "9097  0.000154  0.000163  0.000172  0.000180  0.000189  0.000196  0.000204   \n",
       "9098  0.000156  0.000164  0.000173  0.000181  0.000190  0.000197  0.000205   \n",
       "9099  0.000154  0.000163  0.000171  0.000180  0.000188  0.000196  0.000203   \n",
       "\n",
       "        2082.0    2083.0    2084.0    2085.0    2086.0    2087.0    2088.0  \\\n",
       "0     0.000233  0.000236  0.000239  0.000240  0.000240  0.000238  0.000235   \n",
       "1     0.000232  0.000236  0.000238  0.000239  0.000239  0.000237  0.000234   \n",
       "2     0.000233  0.000237  0.000239  0.000241  0.000240  0.000238  0.000235   \n",
       "3     0.000233  0.000237  0.000239  0.000240  0.000240  0.000238  0.000235   \n",
       "4     0.000234  0.000238  0.000240  0.000241  0.000241  0.000239  0.000235   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "9095  0.000211  0.000216  0.000221  0.000224  0.000225  0.000225  0.000224   \n",
       "9096  0.000210  0.000215  0.000220  0.000223  0.000225  0.000225  0.000224   \n",
       "9097  0.000210  0.000216  0.000220  0.000223  0.000225  0.000225  0.000224   \n",
       "9098  0.000211  0.000216  0.000220  0.000223  0.000225  0.000225  0.000224   \n",
       "9099  0.000210  0.000215  0.000219  0.000223  0.000224  0.000225  0.000223   \n",
       "\n",
       "        2089.0    2090.0    2091.0    2092.0    2093.0    2094.0    2095.0  \\\n",
       "0     0.000230  0.000223  0.000215  0.000206  0.000195  0.000183  0.000169   \n",
       "1     0.000229  0.000223  0.000215  0.000205  0.000194  0.000182  0.000169   \n",
       "2     0.000230  0.000223  0.000215  0.000205  0.000194  0.000182  0.000168   \n",
       "3     0.000230  0.000223  0.000215  0.000206  0.000195  0.000183  0.000169   \n",
       "4     0.000230  0.000224  0.000215  0.000206  0.000194  0.000182  0.000168   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "9095  0.000221  0.000216  0.000210  0.000202  0.000192  0.000181  0.000168   \n",
       "9096  0.000221  0.000216  0.000210  0.000202  0.000192  0.000181  0.000169   \n",
       "9097  0.000221  0.000216  0.000210  0.000202  0.000193  0.000182  0.000170   \n",
       "9098  0.000221  0.000216  0.000210  0.000202  0.000192  0.000181  0.000169   \n",
       "9099  0.000220  0.000216  0.000210  0.000202  0.000192  0.000181  0.000169   \n",
       "\n",
       "        2096.0    2097.0    2098.0    2099.0    2100.0    2101.0    2102.0  \\\n",
       "0     0.000154  0.000139  0.000123  0.000107  0.000090  0.000073  0.000057   \n",
       "1     0.000154  0.000139  0.000123  0.000107  0.000090  0.000074  0.000057   \n",
       "2     0.000154  0.000138  0.000122  0.000106  0.000089  0.000073  0.000056   \n",
       "3     0.000154  0.000139  0.000123  0.000106  0.000090  0.000073  0.000057   \n",
       "4     0.000154  0.000138  0.000122  0.000106  0.000089  0.000072  0.000056   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "9095  0.000154  0.000140  0.000124  0.000108  0.000091  0.000075  0.000059   \n",
       "9096  0.000155  0.000140  0.000125  0.000109  0.000092  0.000076  0.000059   \n",
       "9097  0.000156  0.000141  0.000126  0.000110  0.000093  0.000077  0.000061   \n",
       "9098  0.000155  0.000140  0.000125  0.000109  0.000092  0.000076  0.000060   \n",
       "9099  0.000155  0.000140  0.000125  0.000109  0.000092  0.000076  0.000060   \n",
       "\n",
       "        2103.0    2104.0    2105.0        2106.0    2107.0    2108.0  \\\n",
       "0     0.000041  0.000026  0.000012 -8.493508e-07 -0.000013 -0.000023   \n",
       "1     0.000042  0.000027  0.000012 -6.847299e-07 -0.000013 -0.000023   \n",
       "2     0.000041  0.000026  0.000012 -1.461575e-06 -0.000013 -0.000024   \n",
       "3     0.000041  0.000026  0.000012 -1.656472e-06 -0.000014 -0.000024   \n",
       "4     0.000040  0.000025  0.000011 -1.969533e-06 -0.000014 -0.000024   \n",
       "...        ...       ...       ...           ...       ...       ...   \n",
       "9095  0.000043  0.000028  0.000014  1.144641e-06 -0.000011 -0.000021   \n",
       "9096  0.000044  0.000028  0.000014  1.253275e-06 -0.000011 -0.000021   \n",
       "9097  0.000045  0.000030  0.000015  1.968223e-06 -0.000010 -0.000021   \n",
       "9098  0.000044  0.000029  0.000015  1.973925e-06 -0.000010 -0.000020   \n",
       "9099  0.000044  0.000029  0.000015  1.923702e-06 -0.000010 -0.000020   \n",
       "\n",
       "        2109.0    2110.0    2111.0    2112.0    2113.0    2114.0    2115.0  \\\n",
       "0    -0.000033 -0.000040 -0.000047 -0.000051 -0.000054 -0.000056 -0.000056   \n",
       "1    -0.000032 -0.000040 -0.000047 -0.000051 -0.000054 -0.000056 -0.000056   \n",
       "2    -0.000033 -0.000041 -0.000047 -0.000052 -0.000055 -0.000056 -0.000056   \n",
       "3    -0.000033 -0.000041 -0.000047 -0.000052 -0.000055 -0.000057 -0.000057   \n",
       "4    -0.000033 -0.000041 -0.000047 -0.000052 -0.000055 -0.000056 -0.000056   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "9095 -0.000030 -0.000037 -0.000043 -0.000047 -0.000050 -0.000051 -0.000051   \n",
       "9096 -0.000030 -0.000037 -0.000043 -0.000048 -0.000050 -0.000051 -0.000051   \n",
       "9097 -0.000030 -0.000038 -0.000044 -0.000048 -0.000051 -0.000052 -0.000052   \n",
       "9098 -0.000029 -0.000037 -0.000043 -0.000047 -0.000050 -0.000051 -0.000051   \n",
       "9099 -0.000029 -0.000037 -0.000043 -0.000047 -0.000050 -0.000051 -0.000051   \n",
       "\n",
       "        2116.0    2117.0    2118.0    2119.0    2120.0    2121.0    2122.0  \\\n",
       "0    -0.000055 -0.000053 -0.000050 -0.000046 -0.000042 -0.000037 -0.000031   \n",
       "1    -0.000055 -0.000053 -0.000050 -0.000046 -0.000042 -0.000037 -0.000031   \n",
       "2    -0.000055 -0.000053 -0.000050 -0.000046 -0.000041 -0.000036 -0.000030   \n",
       "3    -0.000056 -0.000054 -0.000050 -0.000046 -0.000042 -0.000037 -0.000031   \n",
       "4    -0.000055 -0.000053 -0.000050 -0.000046 -0.000041 -0.000036 -0.000031   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "9095 -0.000049 -0.000046 -0.000042 -0.000037 -0.000031 -0.000025 -0.000018   \n",
       "9096 -0.000049 -0.000046 -0.000042 -0.000037 -0.000031 -0.000024 -0.000018   \n",
       "9097 -0.000050 -0.000047 -0.000043 -0.000038 -0.000032 -0.000026 -0.000019   \n",
       "9098 -0.000049 -0.000046 -0.000042 -0.000038 -0.000032 -0.000026 -0.000019   \n",
       "9099 -0.000049 -0.000046 -0.000042 -0.000037 -0.000031 -0.000025 -0.000019   \n",
       "\n",
       "        2123.0    2124.0    2125.0    2126.0    2127.0    2128.0    2129.0  \\\n",
       "0    -0.000026 -0.000020 -0.000014 -0.000009 -0.000003  0.000002  0.000006   \n",
       "1    -0.000026 -0.000020 -0.000014 -0.000009 -0.000003  0.000002  0.000007   \n",
       "2    -0.000024 -0.000018 -0.000013 -0.000007 -0.000002  0.000003  0.000008   \n",
       "3    -0.000025 -0.000019 -0.000014 -0.000008 -0.000003  0.000003  0.000007   \n",
       "4    -0.000025 -0.000019 -0.000013 -0.000008 -0.000003  0.000002  0.000007   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "9095 -0.000011 -0.000005  0.000002  0.000008  0.000014  0.000020  0.000025   \n",
       "9096 -0.000011 -0.000004  0.000003  0.000009  0.000015  0.000021  0.000026   \n",
       "9097 -0.000012 -0.000005  0.000002  0.000008  0.000014  0.000020  0.000025   \n",
       "9098 -0.000012 -0.000005  0.000002  0.000008  0.000014  0.000020  0.000025   \n",
       "9099 -0.000012 -0.000005  0.000002  0.000008  0.000014  0.000020  0.000025   \n",
       "\n",
       "        2130.0    2131.0    2132.0    2133.0    2134.0    2135.0    2136.0  \\\n",
       "0     0.000011  0.000015  0.000019  0.000023  0.000027  0.000031  0.000034   \n",
       "1     0.000011  0.000016  0.000020  0.000024  0.000027  0.000031  0.000035   \n",
       "2     0.000012  0.000016  0.000020  0.000024  0.000027  0.000031  0.000035   \n",
       "3     0.000012  0.000016  0.000020  0.000024  0.000028  0.000031  0.000035   \n",
       "4     0.000012  0.000016  0.000020  0.000024  0.000028  0.000031  0.000035   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "9095  0.000029  0.000033  0.000037  0.000040  0.000043  0.000045  0.000048   \n",
       "9096  0.000030  0.000034  0.000038  0.000041  0.000043  0.000046  0.000048   \n",
       "9097  0.000030  0.000034  0.000038  0.000041  0.000044  0.000046  0.000048   \n",
       "9098  0.000030  0.000034  0.000037  0.000040  0.000043  0.000045  0.000048   \n",
       "9099  0.000029  0.000033  0.000037  0.000040  0.000043  0.000045  0.000048   \n",
       "\n",
       "        2137.0    2138.0    2139.0    2140.0    2141.0    2142.0    2143.0  \\\n",
       "0     0.000038  0.000043  0.000047  0.000051  0.000056  0.000061  0.000065   \n",
       "1     0.000039  0.000043  0.000047  0.000051  0.000056  0.000060  0.000065   \n",
       "2     0.000039  0.000043  0.000047  0.000051  0.000055  0.000060  0.000065   \n",
       "3     0.000039  0.000043  0.000047  0.000051  0.000056  0.000060  0.000065   \n",
       "4     0.000039  0.000043  0.000047  0.000052  0.000056  0.000061  0.000065   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "9095  0.000050  0.000052  0.000055  0.000057  0.000060  0.000062  0.000064   \n",
       "9096  0.000050  0.000052  0.000055  0.000057  0.000059  0.000061  0.000064   \n",
       "9097  0.000051  0.000053  0.000055  0.000057  0.000059  0.000061  0.000064   \n",
       "9098  0.000050  0.000052  0.000054  0.000056  0.000059  0.000061  0.000063   \n",
       "9099  0.000050  0.000052  0.000054  0.000057  0.000059  0.000061  0.000064   \n",
       "\n",
       "        2144.0    2145.0    2146.0    2147.0    2148.0    2149.0    2150.0  \\\n",
       "0     0.000070  0.000075  0.000081  0.000086  0.000092  0.000097  0.000103   \n",
       "1     0.000070  0.000075  0.000080  0.000085  0.000091  0.000096  0.000102   \n",
       "2     0.000070  0.000075  0.000080  0.000085  0.000091  0.000096  0.000102   \n",
       "3     0.000070  0.000075  0.000080  0.000085  0.000090  0.000096  0.000102   \n",
       "4     0.000070  0.000075  0.000080  0.000085  0.000091  0.000096  0.000102   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "9095  0.000067  0.000069  0.000072  0.000075  0.000078  0.000081  0.000084   \n",
       "9096  0.000066  0.000069  0.000071  0.000074  0.000077  0.000080  0.000084   \n",
       "9097  0.000066  0.000068  0.000071  0.000073  0.000076  0.000080  0.000083   \n",
       "9098  0.000066  0.000068  0.000071  0.000074  0.000077  0.000080  0.000084   \n",
       "9099  0.000066  0.000069  0.000071  0.000074  0.000077  0.000080  0.000084   \n",
       "\n",
       "        2151.0    2152.0    2153.0    2154.0    2155.0    2156.0    2157.0  \\\n",
       "0     0.000109  0.000115  0.000121  0.000128  0.000134  0.000141  0.000149   \n",
       "1     0.000108  0.000114  0.000120  0.000127  0.000134  0.000141  0.000149   \n",
       "2     0.000108  0.000114  0.000120  0.000127  0.000133  0.000141  0.000148   \n",
       "3     0.000108  0.000114  0.000120  0.000127  0.000134  0.000141  0.000149   \n",
       "4     0.000108  0.000114  0.000120  0.000127  0.000134  0.000141  0.000148   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "9095  0.000088  0.000093  0.000098  0.000103  0.000109  0.000116  0.000123   \n",
       "9096  0.000088  0.000092  0.000097  0.000103  0.000109  0.000116  0.000123   \n",
       "9097  0.000087  0.000092  0.000097  0.000103  0.000109  0.000116  0.000123   \n",
       "9098  0.000088  0.000093  0.000098  0.000104  0.000110  0.000117  0.000124   \n",
       "9099  0.000088  0.000092  0.000097  0.000103  0.000109  0.000116  0.000123   \n",
       "\n",
       "        2158.0    2159.0    2160.0    2161.0    2162.0    2163.0    2164.0  \\\n",
       "0     0.000157  0.000165  0.000174  0.000183  0.000193  0.000204  0.000215   \n",
       "1     0.000157  0.000165  0.000174  0.000184  0.000194  0.000204  0.000216   \n",
       "2     0.000156  0.000165  0.000174  0.000183  0.000193  0.000204  0.000215   \n",
       "3     0.000157  0.000165  0.000174  0.000184  0.000194  0.000205  0.000216   \n",
       "4     0.000156  0.000165  0.000174  0.000183  0.000193  0.000204  0.000215   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "9095  0.000131  0.000140  0.000149  0.000160  0.000170  0.000182  0.000193   \n",
       "9096  0.000131  0.000140  0.000149  0.000160  0.000170  0.000182  0.000194   \n",
       "9097  0.000131  0.000140  0.000149  0.000160  0.000170  0.000182  0.000194   \n",
       "9098  0.000133  0.000141  0.000151  0.000161  0.000172  0.000183  0.000195   \n",
       "9099  0.000132  0.000140  0.000150  0.000160  0.000171  0.000182  0.000194   \n",
       "\n",
       "        2165.0    2166.0    2167.0    2168.0    2169.0    2170.0    2171.0  \\\n",
       "0     0.000226  0.000238  0.000250  0.000262  0.000275  0.000287  0.000299   \n",
       "1     0.000227  0.000239  0.000251  0.000263  0.000276  0.000288  0.000299   \n",
       "2     0.000226  0.000238  0.000250  0.000262  0.000275  0.000287  0.000298   \n",
       "3     0.000227  0.000239  0.000251  0.000263  0.000275  0.000287  0.000298   \n",
       "4     0.000226  0.000238  0.000250  0.000262  0.000275  0.000287  0.000298   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "9095  0.000206  0.000218  0.000231  0.000243  0.000256  0.000268  0.000280   \n",
       "9096  0.000206  0.000218  0.000231  0.000243  0.000256  0.000268  0.000280   \n",
       "9097  0.000206  0.000219  0.000231  0.000244  0.000257  0.000269  0.000281   \n",
       "9098  0.000207  0.000219  0.000232  0.000244  0.000256  0.000268  0.000280   \n",
       "9099  0.000206  0.000219  0.000231  0.000244  0.000257  0.000269  0.000280   \n",
       "\n",
       "        2172.0    2173.0    2174.0    2175.0    2176.0    2177.0    2178.0  \\\n",
       "0     0.000310  0.000320  0.000329  0.000337  0.000343  0.000348  0.000350   \n",
       "1     0.000310  0.000320  0.000329  0.000337  0.000343  0.000347  0.000349   \n",
       "2     0.000309  0.000320  0.000329  0.000337  0.000343  0.000348  0.000350   \n",
       "3     0.000309  0.000319  0.000328  0.000336  0.000342  0.000347  0.000349   \n",
       "4     0.000309  0.000320  0.000329  0.000337  0.000343  0.000347  0.000349   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "9095  0.000290  0.000300  0.000309  0.000316  0.000322  0.000326  0.000328   \n",
       "9096  0.000290  0.000300  0.000309  0.000316  0.000322  0.000326  0.000328   \n",
       "9097  0.000292  0.000302  0.000310  0.000318  0.000323  0.000327  0.000329   \n",
       "9098  0.000290  0.000300  0.000309  0.000316  0.000321  0.000325  0.000327   \n",
       "9099  0.000291  0.000301  0.000310  0.000317  0.000322  0.000326  0.000328   \n",
       "\n",
       "        2179.0    2180.0    2181.0    2182.0    2183.0    2184.0    2185.0  \\\n",
       "0     0.000350  0.000346  0.000340  0.000331  0.000319  0.000304  0.000286   \n",
       "1     0.000349  0.000345  0.000339  0.000330  0.000318  0.000303  0.000285   \n",
       "2     0.000350  0.000347  0.000341  0.000332  0.000320  0.000305  0.000287   \n",
       "3     0.000349  0.000346  0.000340  0.000331  0.000319  0.000304  0.000286   \n",
       "4     0.000349  0.000346  0.000340  0.000331  0.000319  0.000304  0.000286   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "9095  0.000327  0.000323  0.000317  0.000308  0.000297  0.000283  0.000266   \n",
       "9096  0.000327  0.000323  0.000317  0.000309  0.000297  0.000283  0.000266   \n",
       "9097  0.000328  0.000324  0.000318  0.000309  0.000297  0.000283  0.000266   \n",
       "9098  0.000326  0.000323  0.000316  0.000308  0.000296  0.000282  0.000265   \n",
       "9099  0.000327  0.000323  0.000317  0.000308  0.000296  0.000282  0.000265   \n",
       "\n",
       "        2186.0    2187.0    2188.0    2189.0    2190.0    2191.0    2192.0  \\\n",
       "0     0.000265  0.000241  0.000214  0.000185  0.000155  0.000123  0.000090   \n",
       "1     0.000264  0.000241  0.000214  0.000186  0.000155  0.000123  0.000090   \n",
       "2     0.000266  0.000242  0.000216  0.000187  0.000156  0.000124  0.000091   \n",
       "3     0.000265  0.000242  0.000215  0.000187  0.000156  0.000124  0.000091   \n",
       "4     0.000265  0.000241  0.000214  0.000186  0.000155  0.000123  0.000090   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "9095  0.000246  0.000224  0.000200  0.000173  0.000145  0.000116  0.000086   \n",
       "9096  0.000246  0.000224  0.000200  0.000173  0.000145  0.000116  0.000086   \n",
       "9097  0.000246  0.000224  0.000199  0.000172  0.000144  0.000115  0.000085   \n",
       "9098  0.000246  0.000224  0.000199  0.000173  0.000145  0.000116  0.000086   \n",
       "9099  0.000245  0.000223  0.000199  0.000173  0.000145  0.000115  0.000085   \n",
       "\n",
       "        2193.0    2194.0    2195.0    2196.0    2197.0    2198.0    2199.0  \\\n",
       "0     0.000056  0.000022 -0.000012 -0.000045 -0.000076 -0.000106 -0.000133   \n",
       "1     0.000057  0.000023 -0.000011 -0.000044 -0.000076 -0.000105 -0.000132   \n",
       "2     0.000057  0.000023 -0.000011 -0.000045 -0.000076 -0.000106 -0.000133   \n",
       "3     0.000057  0.000023 -0.000011 -0.000044 -0.000076 -0.000106 -0.000133   \n",
       "4     0.000056  0.000022 -0.000012 -0.000045 -0.000076 -0.000106 -0.000133   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "9095  0.000055  0.000024 -0.000006 -0.000036 -0.000065 -0.000092 -0.000116   \n",
       "9096  0.000055  0.000023 -0.000007 -0.000037 -0.000066 -0.000093 -0.000117   \n",
       "9097  0.000054  0.000022 -0.000008 -0.000038 -0.000067 -0.000094 -0.000118   \n",
       "9098  0.000055  0.000024 -0.000007 -0.000037 -0.000066 -0.000092 -0.000117   \n",
       "9099  0.000054  0.000023 -0.000007 -0.000037 -0.000066 -0.000092 -0.000117   \n",
       "\n",
       "        2200.0    2201.0    2202.0    2203.0    2204.0    2205.0    2206.0  \\\n",
       "0    -0.000157 -0.000178 -0.000195 -0.000208 -0.000218 -0.000223 -0.000224   \n",
       "1    -0.000157 -0.000178 -0.000195 -0.000209 -0.000218 -0.000224 -0.000225   \n",
       "2    -0.000158 -0.000179 -0.000196 -0.000210 -0.000220 -0.000225 -0.000226   \n",
       "3    -0.000158 -0.000179 -0.000197 -0.000210 -0.000220 -0.000226 -0.000227   \n",
       "4    -0.000157 -0.000178 -0.000195 -0.000208 -0.000218 -0.000223 -0.000224   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "9095 -0.000138 -0.000157 -0.000173 -0.000186 -0.000194 -0.000199 -0.000200   \n",
       "9096 -0.000139 -0.000158 -0.000173 -0.000185 -0.000194 -0.000198 -0.000199   \n",
       "9097 -0.000140 -0.000159 -0.000174 -0.000186 -0.000195 -0.000199 -0.000199   \n",
       "9098 -0.000139 -0.000158 -0.000174 -0.000186 -0.000195 -0.000200 -0.000200   \n",
       "9099 -0.000139 -0.000158 -0.000173 -0.000185 -0.000194 -0.000199 -0.000199   \n",
       "\n",
       "        2207.0    2208.0    2209.0    2210.0    2211.0    2212.0    2213.0  \\\n",
       "0    -0.000220 -0.000212 -0.000201 -0.000185 -0.000166 -0.000144 -0.000119   \n",
       "1    -0.000221 -0.000214 -0.000202 -0.000186 -0.000167 -0.000145 -0.000120   \n",
       "2    -0.000223 -0.000215 -0.000203 -0.000187 -0.000168 -0.000146 -0.000121   \n",
       "3    -0.000223 -0.000215 -0.000204 -0.000188 -0.000169 -0.000146 -0.000121   \n",
       "4    -0.000220 -0.000213 -0.000201 -0.000185 -0.000166 -0.000144 -0.000120   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "9095 -0.000197 -0.000190 -0.000179 -0.000165 -0.000148 -0.000127 -0.000104   \n",
       "9096 -0.000195 -0.000188 -0.000177 -0.000163 -0.000145 -0.000125 -0.000102   \n",
       "9097 -0.000196 -0.000188 -0.000177 -0.000163 -0.000145 -0.000124 -0.000102   \n",
       "9098 -0.000197 -0.000190 -0.000179 -0.000164 -0.000147 -0.000126 -0.000103   \n",
       "9099 -0.000196 -0.000189 -0.000178 -0.000164 -0.000146 -0.000126 -0.000103   \n",
       "\n",
       "        2214.0    2215.0    2216.0        2217.0    2218.0    2219.0  \\\n",
       "0    -0.000092 -0.000064 -0.000036 -6.592421e-06  0.000022  0.000050   \n",
       "1    -0.000094 -0.000065 -0.000036 -7.091757e-06  0.000022  0.000050   \n",
       "2    -0.000094 -0.000066 -0.000036 -7.056503e-06  0.000022  0.000050   \n",
       "3    -0.000094 -0.000065 -0.000036 -6.630909e-06  0.000022  0.000051   \n",
       "4    -0.000093 -0.000065 -0.000036 -7.287763e-06  0.000021  0.000049   \n",
       "...        ...       ...       ...           ...       ...       ...   \n",
       "9095 -0.000080 -0.000053 -0.000026  9.177744e-07  0.000028  0.000054   \n",
       "9096 -0.000077 -0.000051 -0.000025  2.282651e-06  0.000029  0.000055   \n",
       "9097 -0.000077 -0.000051 -0.000024  2.959693e-06  0.000030  0.000055   \n",
       "9098 -0.000078 -0.000052 -0.000025  2.026221e-06  0.000029  0.000055   \n",
       "9099 -0.000078 -0.000052 -0.000025  1.829063e-06  0.000029  0.000055   \n",
       "\n",
       "        2220.0    2221.0    2222.0    2223.0    2224.0    2225.0    2226.0  \\\n",
       "0     0.000076  0.000100  0.000122  0.000141  0.000156  0.000168  0.000177   \n",
       "1     0.000076  0.000101  0.000123  0.000142  0.000158  0.000170  0.000178   \n",
       "2     0.000076  0.000101  0.000123  0.000142  0.000158  0.000170  0.000179   \n",
       "3     0.000077  0.000102  0.000124  0.000143  0.000159  0.000171  0.000180   \n",
       "4     0.000075  0.000100  0.000121  0.000140  0.000156  0.000168  0.000177   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "9095  0.000079  0.000103  0.000124  0.000142  0.000157  0.000169  0.000178   \n",
       "9096  0.000079  0.000102  0.000123  0.000141  0.000156  0.000167  0.000176   \n",
       "9097  0.000080  0.000103  0.000123  0.000141  0.000156  0.000168  0.000176   \n",
       "9098  0.000080  0.000103  0.000124  0.000142  0.000157  0.000169  0.000177   \n",
       "9099  0.000079  0.000102  0.000123  0.000141  0.000156  0.000168  0.000177   \n",
       "\n",
       "        2227.0    2228.0    2229.0    2230.0    2231.0    2232.0    2233.0  \\\n",
       "0     0.000182  0.000184  0.000181  0.000176  0.000166  0.000154  0.000139   \n",
       "1     0.000183  0.000185  0.000182  0.000176  0.000167  0.000154  0.000139   \n",
       "2     0.000184  0.000185  0.000183  0.000177  0.000168  0.000156  0.000140   \n",
       "3     0.000185  0.000186  0.000184  0.000178  0.000168  0.000155  0.000140   \n",
       "4     0.000182  0.000184  0.000182  0.000176  0.000167  0.000155  0.000140   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "9095  0.000183  0.000185  0.000183  0.000178  0.000170  0.000159  0.000145   \n",
       "9096  0.000181  0.000182  0.000180  0.000175  0.000167  0.000155  0.000142   \n",
       "9097  0.000181  0.000182  0.000180  0.000175  0.000167  0.000156  0.000142   \n",
       "9098  0.000182  0.000184  0.000182  0.000177  0.000168  0.000157  0.000143   \n",
       "9099  0.000182  0.000184  0.000182  0.000177  0.000168  0.000157  0.000143   \n",
       "\n",
       "        2234.0    2235.0    2236.0    2237.0    2238.0    2239.0    2240.0  \\\n",
       "0     0.000122  0.000103  0.000082  0.000060  0.000037  0.000014 -0.000009   \n",
       "1     0.000121  0.000102  0.000080  0.000058  0.000035  0.000012 -0.000011   \n",
       "2     0.000123  0.000103  0.000082  0.000060  0.000037  0.000013 -0.000010   \n",
       "3     0.000122  0.000103  0.000082  0.000059  0.000036  0.000013 -0.000010   \n",
       "4     0.000122  0.000103  0.000082  0.000060  0.000037  0.000014 -0.000009   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "9095  0.000129  0.000111  0.000091  0.000071  0.000050  0.000028  0.000007   \n",
       "9096  0.000126  0.000108  0.000089  0.000069  0.000048  0.000027  0.000007   \n",
       "9097  0.000126  0.000109  0.000090  0.000070  0.000049  0.000028  0.000007   \n",
       "9098  0.000127  0.000109  0.000090  0.000070  0.000049  0.000028  0.000008   \n",
       "9099  0.000127  0.000109  0.000090  0.000070  0.000049  0.000027  0.000006   \n",
       "\n",
       "        2241.0    2242.0    2243.0    2244.0    2245.0    2246.0    2247.0  \\\n",
       "0    -0.000032 -0.000053 -0.000073 -0.000091 -0.000107 -0.000120 -0.000131   \n",
       "1    -0.000033 -0.000054 -0.000074 -0.000092 -0.000107 -0.000121 -0.000132   \n",
       "2    -0.000032 -0.000054 -0.000073 -0.000091 -0.000107 -0.000121 -0.000132   \n",
       "3    -0.000033 -0.000054 -0.000074 -0.000092 -0.000108 -0.000121 -0.000132   \n",
       "4    -0.000032 -0.000053 -0.000073 -0.000091 -0.000107 -0.000120 -0.000132   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "9095 -0.000013 -0.000033 -0.000051 -0.000067 -0.000081 -0.000093 -0.000103   \n",
       "9096 -0.000013 -0.000032 -0.000049 -0.000065 -0.000079 -0.000091 -0.000100   \n",
       "9097 -0.000013 -0.000032 -0.000049 -0.000065 -0.000079 -0.000090 -0.000100   \n",
       "9098 -0.000013 -0.000032 -0.000049 -0.000065 -0.000079 -0.000091 -0.000101   \n",
       "9099 -0.000014 -0.000033 -0.000051 -0.000066 -0.000080 -0.000092 -0.000101   \n",
       "\n",
       "        2248.0    2249.0    2250.0    2251.0    2252.0    2253.0    2254.0  \\\n",
       "0    -0.000140 -0.000146 -0.000149 -0.000149 -0.000146 -0.000141 -0.000133   \n",
       "1    -0.000140 -0.000146 -0.000148 -0.000148 -0.000146 -0.000140 -0.000133   \n",
       "2    -0.000140 -0.000145 -0.000148 -0.000148 -0.000145 -0.000140 -0.000132   \n",
       "3    -0.000141 -0.000146 -0.000149 -0.000149 -0.000147 -0.000141 -0.000133   \n",
       "4    -0.000140 -0.000146 -0.000149 -0.000149 -0.000146 -0.000141 -0.000133   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "9095 -0.000110 -0.000115 -0.000117 -0.000116 -0.000114 -0.000108 -0.000101   \n",
       "9096 -0.000107 -0.000112 -0.000114 -0.000113 -0.000110 -0.000106 -0.000098   \n",
       "9097 -0.000107 -0.000111 -0.000113 -0.000113 -0.000110 -0.000105 -0.000098   \n",
       "9098 -0.000108 -0.000113 -0.000115 -0.000115 -0.000112 -0.000107 -0.000100   \n",
       "9099 -0.000108 -0.000113 -0.000115 -0.000114 -0.000111 -0.000106 -0.000099   \n",
       "\n",
       "        2255.0    2256.0    2257.0    2258.0    2259.0    2260.0    2261.0  \\\n",
       "0    -0.000123 -0.000111 -0.000098 -0.000083 -0.000067 -0.000050 -0.000033   \n",
       "1    -0.000122 -0.000110 -0.000097 -0.000082 -0.000066 -0.000049 -0.000032   \n",
       "2    -0.000122 -0.000110 -0.000097 -0.000082 -0.000066 -0.000049 -0.000032   \n",
       "3    -0.000123 -0.000111 -0.000098 -0.000083 -0.000067 -0.000050 -0.000033   \n",
       "4    -0.000123 -0.000111 -0.000097 -0.000082 -0.000066 -0.000049 -0.000032   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "9095 -0.000092 -0.000081 -0.000069 -0.000056 -0.000042 -0.000028 -0.000013   \n",
       "9096 -0.000090 -0.000079 -0.000067 -0.000054 -0.000041 -0.000027 -0.000013   \n",
       "9097 -0.000090 -0.000079 -0.000067 -0.000055 -0.000041 -0.000028 -0.000014   \n",
       "9098 -0.000092 -0.000081 -0.000069 -0.000057 -0.000043 -0.000029 -0.000015   \n",
       "9099 -0.000090 -0.000079 -0.000067 -0.000054 -0.000040 -0.000026 -0.000012   \n",
       "\n",
       "            2262.0    2263.0    2264.0    2265.0    2266.0    2267.0  \\\n",
       "0    -1.568699e-05  0.000001  0.000018  0.000034  0.000050  0.000064   \n",
       "1    -1.410418e-05  0.000003  0.000020  0.000036  0.000052  0.000066   \n",
       "2    -1.514253e-05  0.000002  0.000018  0.000034  0.000049  0.000063   \n",
       "3    -1.570117e-05  0.000001  0.000018  0.000034  0.000049  0.000062   \n",
       "4    -1.468232e-05  0.000002  0.000019  0.000035  0.000050  0.000064   \n",
       "...            ...       ...       ...       ...       ...       ...   \n",
       "9095  6.117171e-07  0.000014  0.000027  0.000040  0.000051  0.000061   \n",
       "9096  1.197801e-06  0.000015  0.000028  0.000040  0.000051  0.000061   \n",
       "9097  1.368512e-07  0.000014  0.000026  0.000039  0.000050  0.000060   \n",
       "9098 -9.353716e-07  0.000013  0.000026  0.000039  0.000051  0.000061   \n",
       "9099  1.579923e-06  0.000015  0.000028  0.000040  0.000052  0.000062   \n",
       "\n",
       "        2268.0    2269.0    2270.0    2271.0    2272.0    2273.0    2274.0  \\\n",
       "0     0.000076  0.000087  0.000097  0.000105  0.000112  0.000116  0.000119   \n",
       "1     0.000078  0.000089  0.000099  0.000106  0.000112  0.000117  0.000119   \n",
       "2     0.000075  0.000086  0.000096  0.000104  0.000110  0.000114  0.000117   \n",
       "3     0.000075  0.000086  0.000095  0.000103  0.000110  0.000115  0.000118   \n",
       "4     0.000077  0.000088  0.000097  0.000105  0.000111  0.000115  0.000118   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "9095  0.000070  0.000078  0.000085  0.000090  0.000094  0.000096  0.000098   \n",
       "9096  0.000070  0.000078  0.000084  0.000089  0.000093  0.000095  0.000096   \n",
       "9097  0.000069  0.000076  0.000083  0.000088  0.000091  0.000093  0.000095   \n",
       "9098  0.000071  0.000079  0.000085  0.000091  0.000095  0.000097  0.000098   \n",
       "9099  0.000070  0.000078  0.000084  0.000089  0.000092  0.000094  0.000095   \n",
       "\n",
       "        2275.0    2276.0    2277.0    2278.0    2279.0    2280.0    2281.0  \\\n",
       "0     0.000121  0.000121  0.000120  0.000117  0.000114  0.000110  0.000105   \n",
       "1     0.000120  0.000120  0.000118  0.000115  0.000111  0.000107  0.000101   \n",
       "2     0.000119  0.000119  0.000118  0.000116  0.000113  0.000109  0.000104   \n",
       "3     0.000119  0.000120  0.000119  0.000116  0.000113  0.000109  0.000104   \n",
       "4     0.000119  0.000119  0.000117  0.000115  0.000111  0.000107  0.000101   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "9095  0.000098  0.000097  0.000095  0.000092  0.000088  0.000084  0.000079   \n",
       "9096  0.000096  0.000094  0.000092  0.000089  0.000086  0.000082  0.000077   \n",
       "9097  0.000094  0.000093  0.000091  0.000089  0.000085  0.000081  0.000077   \n",
       "9098  0.000098  0.000097  0.000095  0.000092  0.000089  0.000084  0.000079   \n",
       "9099  0.000094  0.000093  0.000091  0.000088  0.000084  0.000081  0.000076   \n",
       "\n",
       "        2282.0    2283.0    2284.0    2285.0    2286.0    2287.0    2288.0  \\\n",
       "0     0.000099  0.000092  0.000085  0.000078  0.000071  0.000063  0.000056   \n",
       "1     0.000095  0.000088  0.000082  0.000074  0.000067  0.000060  0.000054   \n",
       "2     0.000098  0.000092  0.000085  0.000078  0.000071  0.000064  0.000057   \n",
       "3     0.000098  0.000092  0.000085  0.000079  0.000072  0.000065  0.000058   \n",
       "4     0.000095  0.000089  0.000082  0.000075  0.000068  0.000061  0.000054   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "9095  0.000074  0.000068  0.000063  0.000057  0.000052  0.000047  0.000043   \n",
       "9096  0.000073  0.000068  0.000063  0.000058  0.000053  0.000048  0.000044   \n",
       "9097  0.000072  0.000068  0.000063  0.000058  0.000053  0.000049  0.000045   \n",
       "9098  0.000074  0.000069  0.000063  0.000058  0.000053  0.000048  0.000043   \n",
       "9099  0.000072  0.000067  0.000063  0.000058  0.000054  0.000050  0.000046   \n",
       "\n",
       "        2289.0    2290.0    2291.0    2292.0    2293.0    2294.0    2295.0  \\\n",
       "0     0.000049  0.000042  0.000036  0.000030  0.000025  0.000021  0.000017   \n",
       "1     0.000047  0.000041  0.000036  0.000031  0.000026  0.000023  0.000020   \n",
       "2     0.000050  0.000043  0.000037  0.000031  0.000026  0.000022  0.000018   \n",
       "3     0.000052  0.000046  0.000040  0.000035  0.000030  0.000026  0.000022   \n",
       "4     0.000048  0.000042  0.000037  0.000032  0.000028  0.000024  0.000021   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "9095  0.000040  0.000037  0.000034  0.000033  0.000032  0.000031  0.000032   \n",
       "9096  0.000041  0.000038  0.000035  0.000033  0.000032  0.000032  0.000032   \n",
       "9097  0.000041  0.000038  0.000035  0.000033  0.000032  0.000032  0.000032   \n",
       "9098  0.000040  0.000036  0.000034  0.000032  0.000031  0.000030  0.000030   \n",
       "9099  0.000043  0.000040  0.000038  0.000036  0.000035  0.000035  0.000035   \n",
       "\n",
       "        2296.0    2297.0    2298.0    2299.0    2300.0    2301.0    2302.0  \\\n",
       "0     0.000014  0.000013  0.000012  0.000012  0.000013  0.000016  0.000019   \n",
       "1     0.000018  0.000017  0.000016  0.000017  0.000018  0.000020  0.000023   \n",
       "2     0.000015  0.000014  0.000013  0.000013  0.000014  0.000016  0.000019   \n",
       "3     0.000020  0.000018  0.000017  0.000016  0.000017  0.000018  0.000021   \n",
       "4     0.000019  0.000017  0.000017  0.000017  0.000018  0.000020  0.000022   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "9095  0.000033  0.000035  0.000037  0.000041  0.000044  0.000049  0.000053   \n",
       "9096  0.000033  0.000035  0.000038  0.000042  0.000046  0.000050  0.000056   \n",
       "9097  0.000033  0.000035  0.000037  0.000041  0.000044  0.000049  0.000054   \n",
       "9098  0.000032  0.000033  0.000036  0.000039  0.000043  0.000048  0.000053   \n",
       "9099  0.000036  0.000037  0.000039  0.000042  0.000045  0.000049  0.000053   \n",
       "\n",
       "        2303.0    2304.0    2305.0    2306.0    2307.0    2308.0    2309.0  \\\n",
       "0     0.000023  0.000028  0.000033  0.000040  0.000047  0.000055  0.000064   \n",
       "1     0.000026  0.000030  0.000035  0.000041  0.000048  0.000055  0.000064   \n",
       "2     0.000024  0.000029  0.000034  0.000041  0.000049  0.000057  0.000066   \n",
       "3     0.000024  0.000028  0.000033  0.000039  0.000046  0.000054  0.000062   \n",
       "4     0.000026  0.000030  0.000035  0.000041  0.000048  0.000055  0.000064   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "9095  0.000059  0.000064  0.000070  0.000077  0.000083  0.000090  0.000097   \n",
       "9096  0.000061  0.000067  0.000073  0.000079  0.000086  0.000092  0.000098   \n",
       "9097  0.000060  0.000066  0.000072  0.000078  0.000085  0.000092  0.000099   \n",
       "9098  0.000058  0.000064  0.000070  0.000076  0.000082  0.000089  0.000095   \n",
       "9099  0.000058  0.000063  0.000068  0.000074  0.000080  0.000087  0.000093   \n",
       "\n",
       "        2310.0    2311.0    2312.0    2313.0    2314.0    2315.0    2316.0  \\\n",
       "0     0.000073  0.000082  0.000092  0.000103  0.000113  0.000123  0.000134   \n",
       "1     0.000072  0.000081  0.000091  0.000101  0.000111  0.000122  0.000132   \n",
       "2     0.000076  0.000086  0.000096  0.000107  0.000117  0.000128  0.000139   \n",
       "3     0.000071  0.000081  0.000091  0.000101  0.000112  0.000122  0.000132   \n",
       "4     0.000073  0.000082  0.000092  0.000102  0.000112  0.000122  0.000133   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "9095  0.000103  0.000110  0.000117  0.000123  0.000129  0.000135  0.000141   \n",
       "9096  0.000105  0.000111  0.000117  0.000122  0.000128  0.000133  0.000138   \n",
       "9097  0.000105  0.000112  0.000119  0.000125  0.000131  0.000138  0.000143   \n",
       "9098  0.000102  0.000108  0.000114  0.000120  0.000126  0.000132  0.000138   \n",
       "9099  0.000100  0.000107  0.000114  0.000121  0.000127  0.000134  0.000140   \n",
       "\n",
       "        2317.0    2318.0    2319.0    2320.0    2321.0    2322.0    2323.0  \\\n",
       "0     0.000144  0.000155  0.000164  0.000173  0.000181  0.000188  0.000194   \n",
       "1     0.000142  0.000152  0.000162  0.000171  0.000178  0.000185  0.000191   \n",
       "2     0.000149  0.000159  0.000168  0.000177  0.000185  0.000191  0.000196   \n",
       "3     0.000143  0.000152  0.000162  0.000170  0.000178  0.000184  0.000190   \n",
       "4     0.000143  0.000153  0.000162  0.000171  0.000179  0.000185  0.000191   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "9095  0.000147  0.000152  0.000157  0.000161  0.000165  0.000168  0.000171   \n",
       "9096  0.000143  0.000148  0.000152  0.000155  0.000158  0.000161  0.000163   \n",
       "9097  0.000149  0.000154  0.000159  0.000164  0.000167  0.000170  0.000172   \n",
       "9098  0.000144  0.000149  0.000154  0.000159  0.000162  0.000166  0.000168   \n",
       "9099  0.000146  0.000152  0.000157  0.000161  0.000165  0.000168  0.000171   \n",
       "\n",
       "        2324.0    2325.0    2326.0    2327.0    2328.0    2329.0    2330.0  \\\n",
       "0     0.000198  0.000201  0.000203  0.000203  0.000201  0.000198  0.000194   \n",
       "1     0.000196  0.000199  0.000201  0.000201  0.000200  0.000198  0.000194   \n",
       "2     0.000200  0.000203  0.000204  0.000203  0.000201  0.000197  0.000192   \n",
       "3     0.000194  0.000197  0.000199  0.000200  0.000199  0.000197  0.000193   \n",
       "4     0.000196  0.000199  0.000201  0.000202  0.000201  0.000198  0.000194   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "9095  0.000172  0.000173  0.000173  0.000171  0.000169  0.000166  0.000162   \n",
       "9096  0.000164  0.000164  0.000164  0.000163  0.000161  0.000159  0.000155   \n",
       "9097  0.000173  0.000173  0.000172  0.000170  0.000167  0.000164  0.000159   \n",
       "9098  0.000170  0.000170  0.000170  0.000169  0.000168  0.000165  0.000161   \n",
       "9099  0.000172  0.000172  0.000172  0.000170  0.000168  0.000164  0.000160   \n",
       "\n",
       "        2331.0    2332.0    2333.0    2334.0    2335.0    2336.0    2337.0  \\\n",
       "0     0.000188  0.000181  0.000173  0.000163  0.000152  0.000140  0.000128   \n",
       "1     0.000189  0.000182  0.000173  0.000164  0.000152  0.000140  0.000127   \n",
       "2     0.000185  0.000177  0.000168  0.000158  0.000146  0.000133  0.000120   \n",
       "3     0.000188  0.000181  0.000173  0.000163  0.000152  0.000140  0.000127   \n",
       "4     0.000189  0.000182  0.000173  0.000163  0.000151  0.000139  0.000125   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "9095  0.000158  0.000152  0.000146  0.000139  0.000132  0.000124  0.000116   \n",
       "9096  0.000152  0.000147  0.000142  0.000137  0.000131  0.000125  0.000118   \n",
       "9097  0.000154  0.000149  0.000143  0.000136  0.000129  0.000122  0.000114   \n",
       "9098  0.000157  0.000153  0.000147  0.000141  0.000135  0.000128  0.000122   \n",
       "9099  0.000156  0.000150  0.000144  0.000138  0.000131  0.000124  0.000117   \n",
       "\n",
       "        2338.0    2339.0    2340.0    2341.0    2342.0    2343.0    2344.0  \\\n",
       "0     0.000115  0.000101  0.000086  0.000072  0.000057  0.000043  0.000029   \n",
       "1     0.000113  0.000099  0.000084  0.000070  0.000056  0.000042  0.000028   \n",
       "2     0.000106  0.000091  0.000077  0.000062  0.000048  0.000034  0.000021   \n",
       "3     0.000113  0.000099  0.000084  0.000069  0.000054  0.000040  0.000025   \n",
       "4     0.000111  0.000096  0.000081  0.000066  0.000051  0.000037  0.000023   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "9095  0.000107  0.000099  0.000090  0.000082  0.000074  0.000067  0.000060   \n",
       "9096  0.000111  0.000104  0.000096  0.000089  0.000082  0.000074  0.000067   \n",
       "9097  0.000106  0.000098  0.000090  0.000082  0.000075  0.000067  0.000060   \n",
       "9098  0.000114  0.000107  0.000099  0.000092  0.000084  0.000076  0.000068   \n",
       "9099  0.000110  0.000102  0.000095  0.000087  0.000080  0.000073  0.000066   \n",
       "\n",
       "        2345.0    2346.0    2347.0    2348.0    2349.0    2350.0    2351.0  \\\n",
       "0     0.000015  0.000002 -0.000011 -0.000022 -0.000032 -0.000040 -0.000045   \n",
       "1     0.000015  0.000003 -0.000008 -0.000018 -0.000027 -0.000034 -0.000039   \n",
       "2     0.000008 -0.000004 -0.000015 -0.000024 -0.000032 -0.000039 -0.000043   \n",
       "3     0.000011 -0.000003 -0.000016 -0.000028 -0.000038 -0.000046 -0.000053   \n",
       "4     0.000010 -0.000002 -0.000014 -0.000024 -0.000032 -0.000039 -0.000043   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "9095  0.000054  0.000049  0.000044  0.000040  0.000037  0.000035  0.000034   \n",
       "9096  0.000060  0.000053  0.000047  0.000041  0.000037  0.000033  0.000030   \n",
       "9097  0.000054  0.000048  0.000043  0.000039  0.000035  0.000032  0.000031   \n",
       "9098  0.000061  0.000053  0.000047  0.000040  0.000035  0.000030  0.000027   \n",
       "9099  0.000060  0.000053  0.000047  0.000042  0.000037  0.000033  0.000030   \n",
       "\n",
       "        2352.0    2353.0    2354.0    2355.0    2356.0    2357.0    2358.0  \\\n",
       "0    -0.000049 -0.000051 -0.000050 -0.000046 -0.000041 -0.000034 -0.000024   \n",
       "1    -0.000042 -0.000043 -0.000042 -0.000039 -0.000034 -0.000026 -0.000017   \n",
       "2    -0.000045 -0.000045 -0.000043 -0.000039 -0.000033 -0.000026 -0.000016   \n",
       "3    -0.000057 -0.000058 -0.000057 -0.000053 -0.000046 -0.000037 -0.000026   \n",
       "4    -0.000046 -0.000046 -0.000044 -0.000040 -0.000033 -0.000025 -0.000015   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "9095  0.000034  0.000034  0.000036  0.000038  0.000040  0.000043  0.000047   \n",
       "9096  0.000028  0.000028  0.000028  0.000030  0.000033  0.000036  0.000041   \n",
       "9097  0.000030  0.000030  0.000031  0.000032  0.000035  0.000038  0.000041   \n",
       "9098  0.000025  0.000024  0.000025  0.000027  0.000030  0.000034  0.000039   \n",
       "9099  0.000028  0.000027  0.000028  0.000029  0.000032  0.000035  0.000040   \n",
       "\n",
       "        2359.0    2360.0    2361.0    2362.0    2363.0    2364.0    2365.0  \\\n",
       "0    -0.000012  0.000001  0.000016  0.000034  0.000053  0.000074  0.000097   \n",
       "1    -0.000006  0.000007  0.000022  0.000038  0.000057  0.000076  0.000097   \n",
       "2    -0.000005  0.000008  0.000023  0.000039  0.000058  0.000077  0.000098   \n",
       "3    -0.000012  0.000003  0.000021  0.000041  0.000062  0.000085  0.000109   \n",
       "4    -0.000002  0.000011  0.000027  0.000044  0.000063  0.000083  0.000103   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "9095  0.000051  0.000056  0.000062  0.000068  0.000074  0.000082  0.000090   \n",
       "9096  0.000047  0.000053  0.000061  0.000069  0.000078  0.000089  0.000100   \n",
       "9097  0.000046  0.000051  0.000056  0.000063  0.000070  0.000078  0.000086   \n",
       "9098  0.000045  0.000053  0.000060  0.000069  0.000079  0.000090  0.000101   \n",
       "9099  0.000045  0.000051  0.000058  0.000065  0.000074  0.000083  0.000092   \n",
       "\n",
       "        2366.0    2367.0    2368.0    2369.0    2370.0    2371.0    2372.0  \\\n",
       "0     0.000120  0.000144  0.000168  0.000191  0.000213  0.000234  0.000252   \n",
       "1     0.000119  0.000141  0.000162  0.000184  0.000204  0.000222  0.000239   \n",
       "2     0.000120  0.000142  0.000164  0.000185  0.000206  0.000226  0.000245   \n",
       "3     0.000134  0.000159  0.000184  0.000208  0.000231  0.000252  0.000272   \n",
       "4     0.000125  0.000147  0.000168  0.000189  0.000208  0.000226  0.000243   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "9095  0.000098  0.000107  0.000117  0.000127  0.000138  0.000148  0.000159   \n",
       "9096  0.000112  0.000124  0.000137  0.000149  0.000162  0.000174  0.000185   \n",
       "9097  0.000096  0.000105  0.000116  0.000126  0.000138  0.000149  0.000161   \n",
       "9098  0.000113  0.000126  0.000138  0.000151  0.000163  0.000174  0.000184   \n",
       "9099  0.000103  0.000113  0.000124  0.000135  0.000147  0.000158  0.000169   \n",
       "\n",
       "        2373.0    2374.0    2375.0    2376.0    2377.0    2378.0    2379.0  \\\n",
       "0     0.000268  0.000282  0.000293  0.000301  0.000306  0.000306  0.000303   \n",
       "1     0.000255  0.000269  0.000280  0.000289  0.000295  0.000298  0.000297   \n",
       "2     0.000262  0.000277  0.000291  0.000302  0.000310  0.000315  0.000315   \n",
       "3     0.000290  0.000305  0.000319  0.000329  0.000336  0.000338  0.000337   \n",
       "4     0.000258  0.000271  0.000283  0.000291  0.000297  0.000299  0.000298   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "9095  0.000171  0.000183  0.000195  0.000207  0.000218  0.000228  0.000237   \n",
       "9096  0.000196  0.000206  0.000215  0.000223  0.000229  0.000234  0.000238   \n",
       "9097  0.000172  0.000184  0.000196  0.000208  0.000219  0.000229  0.000237   \n",
       "9098  0.000193  0.000201  0.000208  0.000213  0.000217  0.000219  0.000219   \n",
       "9099  0.000180  0.000191  0.000201  0.000210  0.000219  0.000226  0.000232   \n",
       "\n",
       "        2380.0    2381.0    2382.0    2383.0    2384.0    2385.0    2386.0  \\\n",
       "0     0.000295  0.000283  0.000266  0.000245  0.000219  0.000190  0.000158   \n",
       "1     0.000292  0.000283  0.000269  0.000252  0.000230  0.000204  0.000176   \n",
       "2     0.000311  0.000302  0.000287  0.000267  0.000242  0.000211  0.000177   \n",
       "3     0.000330  0.000317  0.000299  0.000275  0.000245  0.000211  0.000173   \n",
       "4     0.000292  0.000281  0.000265  0.000245  0.000219  0.000189  0.000156   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "9095  0.000244  0.000249  0.000251  0.000250  0.000247  0.000241  0.000232   \n",
       "9096  0.000239  0.000239  0.000236  0.000232  0.000225  0.000217  0.000208   \n",
       "9097  0.000244  0.000248  0.000250  0.000250  0.000247  0.000242  0.000234   \n",
       "9098  0.000218  0.000215  0.000211  0.000206  0.000200  0.000193  0.000186   \n",
       "9099  0.000236  0.000239  0.000239  0.000239  0.000236  0.000233  0.000227   \n",
       "\n",
       "        2387.0    2388.0    2389.0    2390.0    2391.0    2392.0    2393.0  \\\n",
       "0     0.000123  0.000086  0.000048  0.000009 -0.000031 -0.000071 -0.000110   \n",
       "1     0.000144  0.000110  0.000074  0.000037 -0.000002 -0.000042 -0.000081   \n",
       "2     0.000139  0.000099  0.000057  0.000014 -0.000030 -0.000073 -0.000116   \n",
       "3     0.000131  0.000087  0.000042 -0.000005 -0.000051 -0.000097 -0.000142   \n",
       "4     0.000119  0.000081  0.000041  0.000001 -0.000039 -0.000079 -0.000117   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "9095  0.000220  0.000207  0.000191  0.000173  0.000154  0.000134  0.000112   \n",
       "9096  0.000196  0.000184  0.000169  0.000154  0.000137  0.000119  0.000099   \n",
       "9097  0.000224  0.000212  0.000198  0.000182  0.000164  0.000144  0.000124   \n",
       "9098  0.000177  0.000168  0.000159  0.000148  0.000136  0.000123  0.000109   \n",
       "9099  0.000221  0.000213  0.000203  0.000192  0.000179  0.000163  0.000146   \n",
       "\n",
       "        2394.0    2395.0    2396.0    2397.0    2398.0    2399.0    2400.0  \\\n",
       "0    -0.000148 -0.000184 -0.000218 -0.000248 -0.000274 -0.000294 -0.000309   \n",
       "1    -0.000121 -0.000160 -0.000197 -0.000231 -0.000262 -0.000287 -0.000308   \n",
       "2    -0.000157 -0.000196 -0.000231 -0.000263 -0.000290 -0.000311 -0.000327   \n",
       "3    -0.000186 -0.000228 -0.000266 -0.000300 -0.000329 -0.000352 -0.000368   \n",
       "4    -0.000154 -0.000188 -0.000219 -0.000247 -0.000270 -0.000288 -0.000301   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "9095  0.000090  0.000068  0.000047  0.000026  0.000007 -0.000011 -0.000027   \n",
       "9096  0.000079  0.000058  0.000037  0.000016 -0.000003 -0.000020 -0.000035   \n",
       "9097  0.000102  0.000080  0.000058  0.000037  0.000016 -0.000002 -0.000019   \n",
       "9098  0.000094  0.000078  0.000062  0.000047  0.000032  0.000018  0.000007   \n",
       "9099  0.000128  0.000107  0.000086  0.000065  0.000043  0.000023  0.000004   \n",
       "\n",
       "        2401.0    2402.0    2403.0    2404.0    2405.0    2406.0    2407.0  \\\n",
       "0    -0.000317 -0.000319 -0.000314 -0.000303 -0.000286 -0.000264 -0.000235   \n",
       "1    -0.000322 -0.000331 -0.000333 -0.000328 -0.000318 -0.000301 -0.000278   \n",
       "2    -0.000336 -0.000338 -0.000333 -0.000322 -0.000305 -0.000282 -0.000253   \n",
       "3    -0.000377 -0.000378 -0.000372 -0.000357 -0.000336 -0.000307 -0.000271   \n",
       "4    -0.000308 -0.000310 -0.000306 -0.000296 -0.000281 -0.000261 -0.000236   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "9095 -0.000040 -0.000050 -0.000057 -0.000062 -0.000062 -0.000060 -0.000054   \n",
       "9096 -0.000047 -0.000056 -0.000062 -0.000064 -0.000062 -0.000057 -0.000049   \n",
       "9097 -0.000034 -0.000046 -0.000055 -0.000061 -0.000063 -0.000062 -0.000058   \n",
       "9098 -0.000003 -0.000010 -0.000014 -0.000016 -0.000014 -0.000010 -0.000003   \n",
       "9099 -0.000013 -0.000028 -0.000040 -0.000048 -0.000054 -0.000056 -0.000055   \n",
       "\n",
       "        2408.0    2409.0    2410.0    2411.0    2412.0    2413.0    2414.0  \\\n",
       "0    -0.000202 -0.000164 -0.000122 -0.000074 -0.000022  0.000033  0.000092   \n",
       "1    -0.000249 -0.000214 -0.000173 -0.000125 -0.000071 -0.000012  0.000053   \n",
       "2    -0.000218 -0.000178 -0.000134 -0.000084 -0.000030  0.000027  0.000086   \n",
       "3    -0.000230 -0.000182 -0.000130 -0.000072 -0.000010  0.000056  0.000124   \n",
       "4    -0.000206 -0.000170 -0.000130 -0.000085 -0.000035  0.000018  0.000075   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "9095 -0.000045 -0.000032 -0.000017  0.000002  0.000023  0.000046  0.000070   \n",
       "9096 -0.000038 -0.000024 -0.000008  0.000010  0.000029  0.000050  0.000072   \n",
       "9097 -0.000050 -0.000039 -0.000024 -0.000006  0.000015  0.000039  0.000065   \n",
       "9098  0.000007  0.000018  0.000032  0.000047  0.000065  0.000084  0.000106   \n",
       "9099 -0.000050 -0.000041 -0.000028 -0.000011  0.000010  0.000035  0.000064   \n",
       "\n",
       "        2415.0    2416.0    2417.0    2418.0    2419.0    2420.0    2421.0  \\\n",
       "0     0.000153  0.000215  0.000276  0.000336  0.000394  0.000450  0.000502   \n",
       "1     0.000122  0.000193  0.000264  0.000335  0.000404  0.000470  0.000531   \n",
       "2     0.000146  0.000206  0.000266  0.000323  0.000378  0.000431  0.000482   \n",
       "3     0.000193  0.000261  0.000328  0.000391  0.000451  0.000507  0.000558   \n",
       "4     0.000134  0.000193  0.000252  0.000309  0.000365  0.000419  0.000471   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "9095  0.000096  0.000122  0.000148  0.000176  0.000206  0.000238  0.000272   \n",
       "9096  0.000093  0.000116  0.000140  0.000165  0.000193  0.000225  0.000260   \n",
       "9097  0.000093  0.000123  0.000153  0.000186  0.000220  0.000256  0.000295   \n",
       "9098  0.000129  0.000154  0.000179  0.000207  0.000236  0.000267  0.000300   \n",
       "9099  0.000096  0.000131  0.000169  0.000207  0.000247  0.000287  0.000327   \n",
       "\n",
       "        2422.0    2423.0    2424.0    2425.0    2426.0    2427.0    2428.0  \\\n",
       "0     0.000551  0.000596  0.000639  0.000676  0.000706  0.000727  0.000734   \n",
       "1     0.000589  0.000641  0.000689  0.000729  0.000760  0.000779  0.000785   \n",
       "2     0.000531  0.000579  0.000626  0.000671  0.000712  0.000745  0.000767   \n",
       "3     0.000604  0.000646  0.000684  0.000717  0.000742  0.000758  0.000761   \n",
       "4     0.000521  0.000570  0.000618  0.000663  0.000704  0.000737  0.000759   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "9095  0.000310  0.000354  0.000403  0.000457  0.000515  0.000575  0.000633   \n",
       "9096  0.000302  0.000350  0.000406  0.000469  0.000537  0.000607  0.000676   \n",
       "9097  0.000338  0.000384  0.000436  0.000491  0.000549  0.000607  0.000663   \n",
       "9098  0.000335  0.000373  0.000412  0.000453  0.000494  0.000534  0.000570   \n",
       "9099  0.000366  0.000404  0.000441  0.000475  0.000506  0.000534  0.000557   \n",
       "\n",
       "        2429.0    2430.0    2431.0    2432.0    2433.0    2434.0    2435.0  \\\n",
       "0     0.000726  0.000697  0.000645  0.000568  0.000465  0.000337  0.000186   \n",
       "1     0.000773  0.000739  0.000682  0.000599  0.000490  0.000357  0.000201   \n",
       "2     0.000774  0.000760  0.000721  0.000656  0.000562  0.000442  0.000296   \n",
       "3     0.000749  0.000718  0.000665  0.000587  0.000484  0.000359  0.000212   \n",
       "4     0.000766  0.000752  0.000715  0.000652  0.000561  0.000443  0.000300   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "9095  0.000685  0.000729  0.000761  0.000778  0.000777  0.000757  0.000717   \n",
       "9096  0.000740  0.000794  0.000835  0.000859  0.000863  0.000847  0.000809   \n",
       "9097  0.000713  0.000754  0.000782  0.000796  0.000792  0.000770  0.000729   \n",
       "9098  0.000602  0.000626  0.000642  0.000646  0.000638  0.000617  0.000584   \n",
       "9099  0.000575  0.000588  0.000593  0.000589  0.000578  0.000560  0.000535   \n",
       "\n",
       "        2436.0    2437.0    2438.0    2439.0    2440.0    2441.0    2442.0  \\\n",
       "0     0.000018 -0.000164 -0.000352 -0.000541 -0.000725 -0.000899 -0.001058   \n",
       "1     0.000027 -0.000160 -0.000353 -0.000546 -0.000733 -0.000910 -0.001072   \n",
       "2     0.000128 -0.000056 -0.000251 -0.000449 -0.000646 -0.000837 -0.001017   \n",
       "3     0.000048 -0.000128 -0.000313 -0.000499 -0.000682 -0.000857 -0.001022   \n",
       "4     0.000136 -0.000045 -0.000236 -0.000431 -0.000624 -0.000810 -0.000984   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "9095  0.000657  0.000578  0.000483  0.000372  0.000249  0.000114 -0.000031   \n",
       "9096  0.000749  0.000669  0.000570  0.000454  0.000324  0.000180  0.000024   \n",
       "9097  0.000668  0.000589  0.000494  0.000383  0.000259  0.000123 -0.000024   \n",
       "9098  0.000538  0.000480  0.000408  0.000324  0.000228  0.000119 -0.000001   \n",
       "9099  0.000502  0.000460  0.000408  0.000345  0.000269  0.000180  0.000076   \n",
       "\n",
       "        2443.0    2444.0    2445.0    2446.0    2447.0    2448.0    2449.0  \\\n",
       "0    -0.001200 -0.001321 -0.001419 -0.001494 -0.001545 -0.001570 -0.001570   \n",
       "1    -0.001214 -0.001335 -0.001431 -0.001502 -0.001548 -0.001567 -0.001561   \n",
       "2    -0.001182 -0.001330 -0.001458 -0.001565 -0.001649 -0.001708 -0.001740   \n",
       "3    -0.001172 -0.001305 -0.001420 -0.001515 -0.001588 -0.001637 -0.001660   \n",
       "4    -0.001143 -0.001284 -0.001404 -0.001502 -0.001576 -0.001625 -0.001647   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "9095 -0.000184 -0.000343 -0.000504 -0.000666 -0.000825 -0.000978 -0.001121   \n",
       "9096 -0.000142 -0.000316 -0.000495 -0.000677 -0.000858 -0.001033 -0.001199   \n",
       "9097 -0.000180 -0.000342 -0.000509 -0.000677 -0.000842 -0.001002 -0.001152   \n",
       "9098 -0.000132 -0.000274 -0.000423 -0.000578 -0.000734 -0.000888 -0.001035   \n",
       "9099 -0.000041 -0.000172 -0.000314 -0.000465 -0.000622 -0.000779 -0.000931   \n",
       "\n",
       "        2450.0    2451.0    2452.0    2453.0    2454.0    2455.0    2456.0  \\\n",
       "0    -0.001544 -0.001492 -0.001416 -0.001316 -0.001195 -0.001054 -0.000897   \n",
       "1    -0.001529 -0.001473 -0.001393 -0.001291 -0.001170 -0.001034 -0.000885   \n",
       "2    -0.001743 -0.001718 -0.001663 -0.001579 -0.001467 -0.001330 -0.001169   \n",
       "3    -0.001657 -0.001626 -0.001566 -0.001480 -0.001368 -0.001231 -0.001074   \n",
       "4    -0.001642 -0.001610 -0.001549 -0.001461 -0.001348 -0.001211 -0.001053   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "9095 -0.001252 -0.001366 -0.001462 -0.001536 -0.001587 -0.001614 -0.001615   \n",
       "9096 -0.001349 -0.001481 -0.001591 -0.001675 -0.001730 -0.001756 -0.001750   \n",
       "9097 -0.001290 -0.001410 -0.001512 -0.001592 -0.001648 -0.001681 -0.001688   \n",
       "9098 -0.001170 -0.001289 -0.001389 -0.001467 -0.001521 -0.001549 -0.001551   \n",
       "9099 -0.001073 -0.001201 -0.001312 -0.001403 -0.001472 -0.001520 -0.001546   \n",
       "\n",
       "        2457.0    2458.0    2459.0    2460.0    2461.0    2462.0    2463.0  \\\n",
       "0    -0.000725 -0.000544 -0.000357 -0.000168  0.000017  0.000195  0.000360   \n",
       "1    -0.000725 -0.000558 -0.000386 -0.000213 -0.000043  0.000121  0.000274   \n",
       "2    -0.000987 -0.000789 -0.000578 -0.000360 -0.000141  0.000076  0.000283   \n",
       "3    -0.000898 -0.000708 -0.000510 -0.000307 -0.000105  0.000091  0.000275   \n",
       "4    -0.000876 -0.000686 -0.000486 -0.000282 -0.000079  0.000117  0.000301   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "9095 -0.001592 -0.001543 -0.001469 -0.001372 -0.001253 -0.001115 -0.000961   \n",
       "9096 -0.001712 -0.001643 -0.001546 -0.001421 -0.001272 -0.001103 -0.000917   \n",
       "9097 -0.001670 -0.001625 -0.001556 -0.001461 -0.001344 -0.001205 -0.001049   \n",
       "9098 -0.001527 -0.001478 -0.001405 -0.001310 -0.001193 -0.001059 -0.000909   \n",
       "9099 -0.001548 -0.001527 -0.001483 -0.001417 -0.001329 -0.001219 -0.001089   \n",
       "\n",
       "        2464.0    2465.0    2466.0    2467.0    2468.0    2469.0    2470.0  \\\n",
       "0     0.000506  0.000631  0.000729  0.000800  0.000842  0.000854  0.000838   \n",
       "1     0.000413  0.000534  0.000634  0.000712  0.000765  0.000791  0.000791   \n",
       "2     0.000475  0.000646  0.000793  0.000911  0.000999  0.001054  0.001078   \n",
       "3     0.000442  0.000587  0.000707  0.000799  0.000862  0.000894  0.000898   \n",
       "4     0.000467  0.000609  0.000724  0.000808  0.000860  0.000880  0.000869   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "9095 -0.000796 -0.000625 -0.000455 -0.000289 -0.000131  0.000014  0.000144   \n",
       "9096 -0.000721 -0.000520 -0.000321 -0.000130  0.000049  0.000209  0.000346   \n",
       "9097 -0.000879 -0.000700 -0.000516 -0.000334 -0.000158  0.000006  0.000155   \n",
       "9098 -0.000748 -0.000578 -0.000403 -0.000229 -0.000060  0.000099  0.000243   \n",
       "9099 -0.000940 -0.000773 -0.000590 -0.000396 -0.000196  0.000003  0.000193   \n",
       "\n",
       "        2471.0    2472.0    2473.0    2474.0    2475.0    2476.0    2477.0  \\\n",
       "0     0.000796  0.000733  0.000654  0.000564  0.000467  0.000369  0.000271   \n",
       "1     0.000767  0.000721  0.000656  0.000578  0.000491  0.000398  0.000303   \n",
       "2     0.001072  0.001039  0.000983  0.000910  0.000823  0.000726  0.000623   \n",
       "3     0.000876  0.000831  0.000768  0.000692  0.000606  0.000515  0.000421   \n",
       "4     0.000830  0.000767  0.000685  0.000589  0.000484  0.000376  0.000267   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "9095  0.000256  0.000350  0.000427  0.000488  0.000535  0.000567  0.000587   \n",
       "9096  0.000456  0.000538  0.000589  0.000610  0.000600  0.000564  0.000502   \n",
       "9097  0.000283  0.000389  0.000470  0.000526  0.000557  0.000564  0.000548   \n",
       "9098  0.000367  0.000468  0.000543  0.000589  0.000605  0.000591  0.000549   \n",
       "9099  0.000366  0.000516  0.000638  0.000726  0.000775  0.000783  0.000752   \n",
       "\n",
       "        2478.0    2479.0    2480.0    2481.0    2482.0    2483.0    2484.0  \\\n",
       "0     0.000178  0.000089  0.000006 -0.000072 -0.000144 -0.000210 -0.000271   \n",
       "1     0.000208  0.000116  0.000029 -0.000052 -0.000127 -0.000195 -0.000258   \n",
       "2     0.000516  0.000408  0.000298  0.000189  0.000081 -0.000024 -0.000125   \n",
       "3     0.000326  0.000233  0.000141  0.000051 -0.000037 -0.000122 -0.000201   \n",
       "4     0.000161  0.000059 -0.000037 -0.000126 -0.000208 -0.000282 -0.000349   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "9095  0.000594  0.000587  0.000567  0.000526  0.000468  0.000397  0.000316   \n",
       "9096  0.000420  0.000320  0.000206  0.000087 -0.000035 -0.000156 -0.000272   \n",
       "9097  0.000511  0.000456  0.000385  0.000303  0.000213  0.000117  0.000019   \n",
       "9098  0.000481  0.000391  0.000282  0.000164  0.000039 -0.000088 -0.000213   \n",
       "9099  0.000683  0.000581  0.000453  0.000312  0.000161  0.000006 -0.000148   \n",
       "\n",
       "        2485.0    2486.0    2487.0    2488.0    2489.0    2490.0    2491.0  \\\n",
       "0    -0.000326 -0.000374 -0.000415 -0.000447 -0.000471 -0.000487 -0.000494   \n",
       "1    -0.000315 -0.000366 -0.000411 -0.000449 -0.000480 -0.000503 -0.000519   \n",
       "2    -0.000220 -0.000307 -0.000385 -0.000453 -0.000508 -0.000551 -0.000582   \n",
       "3    -0.000274 -0.000338 -0.000393 -0.000436 -0.000468 -0.000489 -0.000498   \n",
       "4    -0.000406 -0.000454 -0.000492 -0.000518 -0.000533 -0.000536 -0.000529   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "9095  0.000227  0.000135  0.000041 -0.000053 -0.000143 -0.000228 -0.000308   \n",
       "9096 -0.000382 -0.000482 -0.000569 -0.000642 -0.000699 -0.000740 -0.000765   \n",
       "9097 -0.000079 -0.000174 -0.000263 -0.000345 -0.000418 -0.000481 -0.000534   \n",
       "9098 -0.000333 -0.000444 -0.000542 -0.000626 -0.000693 -0.000744 -0.000779   \n",
       "9099 -0.000298 -0.000438 -0.000565 -0.000675 -0.000767 -0.000841 -0.000896   \n",
       "\n",
       "        2492.0    2493.0    2494.0    2495.0    2496.0    2497.0    2498.0  \\\n",
       "0    -0.000493 -0.000485 -0.000472 -0.000455 -0.000434 -0.000412 -0.000388   \n",
       "1    -0.000527 -0.000528 -0.000523 -0.000512 -0.000495 -0.000474 -0.000447   \n",
       "2    -0.000603 -0.000613 -0.000614 -0.000608 -0.000596 -0.000578 -0.000556   \n",
       "3    -0.000498 -0.000490 -0.000475 -0.000455 -0.000433 -0.000409 -0.000385   \n",
       "4    -0.000513 -0.000489 -0.000459 -0.000424 -0.000388 -0.000351 -0.000315   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "9095 -0.000381 -0.000447 -0.000506 -0.000556 -0.000598 -0.000630 -0.000652   \n",
       "9096 -0.000775 -0.000771 -0.000754 -0.000726 -0.000687 -0.000639 -0.000583   \n",
       "9097 -0.000577 -0.000608 -0.000629 -0.000640 -0.000639 -0.000626 -0.000603   \n",
       "9098 -0.000798 -0.000803 -0.000794 -0.000772 -0.000740 -0.000697 -0.000645   \n",
       "9099 -0.000934 -0.000954 -0.000957 -0.000944 -0.000915 -0.000871 -0.000811   \n",
       "\n",
       "        2499.0    2500.0  \n",
       "0    -0.000364 -0.000339  \n",
       "1    -0.000416 -0.000381  \n",
       "2    -0.000531 -0.000502  \n",
       "3    -0.000362 -0.000340  \n",
       "4    -0.000281 -0.000249  \n",
       "...        ...       ...  \n",
       "9095 -0.000664 -0.000665  \n",
       "9096 -0.000520 -0.000451  \n",
       "9097 -0.000569 -0.000525  \n",
       "9098 -0.000584 -0.000517  \n",
       "9099 -0.000738 -0.000652  \n",
       "\n",
       "[9100 rows x 1151 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "X_preprocessed2 = pd.DataFrame(data=X_filtered, index=X.index, columns=X.columns)\n",
    "X_preprocessed2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "lactate_p2 = pd.concat([l, X_preprocessed2], axis=1)\n",
    "\n",
    "urea_p2 = pd.concat([u, X_preprocessed2], axis=1)\n",
    "\n",
    "glucose_p2 = pd.concat([g, X_preprocessed2], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lactate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed0ad77105a54db9826e11167e6b89f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  1 — train RMSE 105.972152 - train R2 -0.284605 — val RMSE 89.899938 - val R2 0.075505\n",
      "Epoch  2 — train RMSE 78.823093 - train R2 0.289290 — val RMSE 52.672361 - val R2 0.682641\n",
      "Epoch  3 — train RMSE 62.123901 - train R2 0.558528 — val RMSE 38.160685 - val R2 0.833422\n",
      "Epoch  4 — train RMSE 53.707413 - train R2 0.670045 — val RMSE 32.672950 - val R2 0.877887\n",
      "Epoch  5 — train RMSE 55.510691 - train R2 0.647516 — val RMSE 32.754151 - val R2 0.877279\n",
      "Epoch  6 — train RMSE 51.092537 - train R2 0.701392 — val RMSE 30.519489 - val R2 0.893453\n",
      "Epoch  7 — train RMSE 50.082986 - train R2 0.713077 — val RMSE 28.641379 - val R2 0.906163\n",
      "Epoch  8 — train RMSE 48.585265 - train R2 0.729981 — val RMSE 24.354581 - val R2 0.932150\n",
      "Epoch  9 — train RMSE 46.142823 - train R2 0.756447 — val RMSE 24.196562 - val R2 0.933028\n",
      "Epoch 10 — train RMSE 49.784915 - train R2 0.716481 — val RMSE 23.344547 - val R2 0.937661\n",
      "Epoch 11 — train RMSE 48.660848 - train R2 0.729140 — val RMSE 23.646053 - val R2 0.936041\n",
      "Epoch 12 — train RMSE 50.719993 - train R2 0.705732 — val RMSE 22.055335 - val R2 0.944357\n",
      "Epoch 13 — train RMSE 49.859338 - train R2 0.715633 — val RMSE 21.703100 - val R2 0.946120\n",
      "Epoch 14 — train RMSE 48.793232 - train R2 0.727664 — val RMSE 32.295196 - val R2 0.880694\n",
      "Epoch 15 — train RMSE 46.971425 - train R2 0.747621 — val RMSE 21.544342 - val R2 0.946905\n",
      "Epoch 16 — train RMSE 49.217092 - train R2 0.722912 — val RMSE 20.184618 - val R2 0.953396\n",
      "Epoch 17 — train RMSE 47.314110 - train R2 0.743925 — val RMSE 21.450124 - val R2 0.947369\n",
      "Epoch 18 — train RMSE 42.464508 - train R2 0.793729 — val RMSE 17.801932 - val R2 0.963749\n",
      "Epoch 19 — train RMSE 50.267490 - train R2 0.710958 — val RMSE 18.789104 - val R2 0.959617\n",
      "Epoch 20 — train RMSE 47.194749 - train R2 0.745216 — val RMSE 19.254193 - val R2 0.957593\n",
      "Epoch 21 — train RMSE 46.546752 - train R2 0.752164 — val RMSE 17.633052 - val R2 0.964434\n",
      "Epoch 22 — train RMSE 46.966093 - train R2 0.747678 — val RMSE 16.949211 - val R2 0.967139\n",
      "Epoch 23 — train RMSE 42.954312 - train R2 0.788943 — val RMSE 18.162283 - val R2 0.962267\n",
      "Epoch 24 — train RMSE 48.362518 - train R2 0.732451 — val RMSE 21.167149 - val R2 0.948748\n",
      "Epoch 25 — train RMSE 48.709133 - train R2 0.728602 — val RMSE 18.626943 - val R2 0.960311\n",
      "Epoch 26 — train RMSE 45.162414 - train R2 0.766686 — val RMSE 22.408976 - val R2 0.942558\n",
      "Epoch 27 — train RMSE 47.318452 - train R2 0.743878 — val RMSE 16.659351 - val R2 0.968253\n",
      "Epoch 28 — train RMSE 47.571483 - train R2 0.741131 — val RMSE 20.039524 - val R2 0.954063\n",
      "Epoch 29 — train RMSE 45.209609 - train R2 0.766199 — val RMSE 17.947313 - val R2 0.963154\n",
      "Epoch 30 — train RMSE 46.286155 - train R2 0.754931 — val RMSE 15.247741 - val R2 0.973405\n",
      "Epoch 31 — train RMSE 47.180176 - train R2 0.745373 — val RMSE 18.481308 - val R2 0.960929\n",
      "Epoch 32 — train RMSE 44.902633 - train R2 0.769363 — val RMSE 17.436903 - val R2 0.965220\n",
      "Epoch 33 — train RMSE 47.734726 - train R2 0.739352 — val RMSE 19.713826 - val R2 0.955544\n",
      "Epoch 34 — train RMSE 44.236699 - train R2 0.776153 — val RMSE 19.304763 - val R2 0.957370\n",
      "Epoch 35 — train RMSE 44.637666 - train R2 0.772077 — val RMSE 14.282545 - val R2 0.976666\n",
      "Epoch 36 — train RMSE 44.929526 - train R2 0.769086 — val RMSE 17.604898 - val R2 0.964547\n",
      "Epoch 37 — train RMSE 43.870121 - train R2 0.779848 — val RMSE 19.047753 - val R2 0.958498\n",
      "Epoch 38 — train RMSE 43.953938 - train R2 0.779006 — val RMSE 17.606337 - val R2 0.964541\n",
      "Epoch 39 — train RMSE 45.897021 - train R2 0.759035 — val RMSE 18.916003 - val R2 0.959070\n",
      "Epoch 40 — train RMSE 45.057999 - train R2 0.767764 — val RMSE 18.193602 - val R2 0.962136\n",
      "Epoch 41 — train RMSE 48.706866 - train R2 0.728627 — val RMSE 17.928377 - val R2 0.963232\n",
      "Epoch 42 — train RMSE 47.615323 - train R2 0.740654 — val RMSE 16.165645 - val R2 0.970107\n",
      "Epoch 43 — train RMSE 45.967860 - train R2 0.758290 — val RMSE 18.544259 - val R2 0.960663\n",
      "Epoch 44 — train RMSE 46.083465 - train R2 0.757073 — val RMSE 17.848796 - val R2 0.963558\n",
      "Epoch 45 — train RMSE 44.114444 - train R2 0.777388 — val RMSE 20.647225 - val R2 0.951235\n",
      "Early stopping triggered; Best val RMSE : 14.282545\n"
     ]
    }
   ],
   "source": [
    "configs = Details(\n",
    "    pred_len = 1,      \n",
    "    seq_len  = 1151,\n",
    "    d_ff = 32,\n",
    "    patch_len= 16, \n",
    "    stride = 8, \n",
    "    llm_layers = 4,\n",
    "    description = \"This dataset contains near-infrared (NIR) spectroscopy measurements of lactate concentration in a \" \\\n",
    "                  \"phosphate-buffered saline (PBS) solution. Each spectrum includes absorbance values across \" \\\n",
    "                  \"different wavelengths. The goal is to predict the concentration given the different wavelengths \" ,\n",
    "    dropout = 0.1,\n",
    "    n_heads = 2,\n",
    "    d_model = 16,\n",
    "    enc_in = 1,\n",
    "    d_llm = 4096,\n",
    "    llm_model = 'LLAMA',\n",
    "    lr = 0.0001\n",
    ")\n",
    "\n",
    "train, test = model_selection.train_test_split(lactate_p2, test_size=0.60, random_state=1, stratify= lactate_p2['Lactate'])\n",
    "\n",
    "train, validation = model_selection.train_test_split(train, test_size=0.1, random_state=1, stratify= train['Lactate'])\n",
    "\n",
    "model ,train_rmse, train_r2, val_rmse, val_r2, stand  = train_model(configs, train, validation, 100, 8, True, True)\n",
    "\n",
    "#190 min"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delete wavelengths instead of making them zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "r1 = X_preprocessed2.iloc[:, 30:151].columns\n",
    "r2 = X_preprocessed2.iloc[:, 500:701].columns\n",
    "cols_del = r1.union(r2)\n",
    "\n",
    "X_preprocessed3 = X_preprocessed2.drop(columns=cols_del)\n",
    "\n",
    "lactate_p = pd.concat([l, X_preprocessed3], axis=1)\n",
    "\n",
    "urea_p = pd.concat([u, X_preprocessed3], axis=1)\n",
    "\n",
    "glucose_p = pd.concat([g, X_preprocessed3], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9100, 830)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lactate_p.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c26874c095c4135b77d36405f2d0595",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  1 — train RMSE 105.342959 - train R2 -0.269396 — val RMSE 82.362508 - val R2 0.224030\n",
      "Epoch  2 — train RMSE 71.060860 - train R2 0.422374 — val RMSE 47.857305 - val R2 0.738012\n",
      "Epoch  3 — train RMSE 60.188034 - train R2 0.585613 — val RMSE 34.240073 - val R2 0.865892\n",
      "Epoch  4 — train RMSE 56.929743 - train R2 0.629265 — val RMSE 34.268764 - val R2 0.865667\n",
      "Epoch  5 — train RMSE 51.119463 - train R2 0.701078 — val RMSE 28.876328 - val R2 0.904617\n",
      "Epoch  6 — train RMSE 49.175660 - train R2 0.723378 — val RMSE 26.754363 - val R2 0.918121\n",
      "Epoch  7 — train RMSE 48.154339 - train R2 0.734749 — val RMSE 24.352355 - val R2 0.932163\n",
      "Epoch  8 — train RMSE 47.765049 - train R2 0.739021 — val RMSE 27.233359 - val R2 0.915162\n",
      "Epoch  9 — train RMSE 46.787481 - train R2 0.749594 — val RMSE 25.985484 - val R2 0.922759\n",
      "Epoch 10 — train RMSE 48.927752 - train R2 0.726160 — val RMSE 20.615042 - val R2 0.951387\n",
      "Epoch 11 — train RMSE 49.339633 - train R2 0.721531 — val RMSE 20.691348 - val R2 0.951026\n",
      "Epoch 12 — train RMSE 47.769005 - train R2 0.738977 — val RMSE 19.439664 - val R2 0.956772\n",
      "Epoch 13 — train RMSE 43.318812 - train R2 0.785346 — val RMSE 21.441925 - val R2 0.947409\n",
      "Epoch 14 — train RMSE 48.591320 - train R2 0.729914 — val RMSE 20.587532 - val R2 0.951516\n",
      "Epoch 15 — train RMSE 44.067268 - train R2 0.777864 — val RMSE 17.783040 - val R2 0.963826\n",
      "Epoch 16 — train RMSE 42.605812 - train R2 0.792354 — val RMSE 17.196509 - val R2 0.966173\n",
      "Epoch 17 — train RMSE 44.396712 - train R2 0.774531 — val RMSE 16.950350 - val R2 0.967134\n",
      "Epoch 18 — train RMSE 48.727564 - train R2 0.728397 — val RMSE 18.058878 - val R2 0.962695\n",
      "Epoch 19 — train RMSE 46.802866 - train R2 0.749429 — val RMSE 18.467974 - val R2 0.960986\n",
      "Epoch 20 — train RMSE 47.921207 - train R2 0.737311 — val RMSE 17.984794 - val R2 0.963000\n",
      "Epoch 21 — train RMSE 45.361081 - train R2 0.764629 — val RMSE 18.628461 - val R2 0.960305\n",
      "Epoch 22 — train RMSE 49.356157 - train R2 0.721344 — val RMSE 16.306995 - val R2 0.969582\n",
      "Epoch 23 — train RMSE 48.370872 - train R2 0.732358 — val RMSE 16.024350 - val R2 0.970627\n",
      "Epoch 24 — train RMSE 45.211729 - train R2 0.766177 — val RMSE 17.538254 - val R2 0.964815\n",
      "Epoch 25 — train RMSE 45.220007 - train R2 0.766091 — val RMSE 18.005950 - val R2 0.962913\n",
      "Epoch 26 — train RMSE 44.694370 - train R2 0.771497 — val RMSE 13.927980 - val R2 0.977810\n",
      "Epoch 27 — train RMSE 48.429472 - train R2 0.731710 — val RMSE 20.105986 - val R2 0.953758\n",
      "Epoch 28 — train RMSE 48.259887 - train R2 0.733585 — val RMSE 14.412535 - val R2 0.976239\n",
      "Epoch 29 — train RMSE 44.303134 - train R2 0.775480 — val RMSE 15.129001 - val R2 0.973818\n",
      "Epoch 30 — train RMSE 42.295387 - train R2 0.795369 — val RMSE 16.967831 - val R2 0.967066\n",
      "Epoch 31 — train RMSE 42.483054 - train R2 0.793549 — val RMSE 17.550315 - val R2 0.964767\n",
      "Epoch 32 — train RMSE 44.540268 - train R2 0.773070 — val RMSE 15.803582 - val R2 0.971431\n",
      "Epoch 33 — train RMSE 46.830278 - train R2 0.749135 — val RMSE 15.528798 - val R2 0.972416\n",
      "Epoch 34 — train RMSE 43.927795 - train R2 0.779269 — val RMSE 15.548475 - val R2 0.972346\n",
      "Epoch 35 — train RMSE 45.835732 - train R2 0.759678 — val RMSE 15.756317 - val R2 0.971602\n",
      "Epoch 36 — train RMSE 43.775462 - train R2 0.780797 — val RMSE 15.043319 - val R2 0.974114\n",
      "Early stopping triggered; Best val RMSE : 13.927980\n"
     ]
    }
   ],
   "source": [
    "configs = Details(\n",
    "    pred_len = 1,      \n",
    "    seq_len  = 830,\n",
    "    d_ff = 32,\n",
    "    patch_len= 16, \n",
    "    stride = 8, \n",
    "    llm_layers = 4,\n",
    "    description = \"This dataset contains near-infrared (NIR) spectroscopy measurements of lactate concentration in a \" \\\n",
    "                  \"phosphate-buffered saline (PBS) solution. Each spectrum includes absorbance values across \" \\\n",
    "                  \"different wavelengths. The goal is to predict the concentration given the different wavelengths \" ,\n",
    "    dropout = 0.1,\n",
    "    n_heads = 2,\n",
    "    d_model = 16,\n",
    "    enc_in = 1,\n",
    "    d_llm = 4096,\n",
    "    llm_model = 'LLAMA',\n",
    "    lr = 0.0001\n",
    ")\n",
    "\n",
    "train, test = model_selection.train_test_split(lactate_p, test_size=0.60, random_state=1, stratify= lactate_p['Lactate'])\n",
    "\n",
    "train, validation = model_selection.train_test_split(train, test_size=0.1, random_state=1, stratify= train['Lactate'])\n",
    "\n",
    "model ,train_rmse, train_r2, val_rmse, val_r2, stand  = train_model(configs, train, validation, 100, 8, True, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = model_selection.train_test_split(lactate_p, test_size=0.6, random_state=1, stratify= lactate_p['Lactate'])\n",
    "\n",
    "train, validation = model_selection.train_test_split(train, test_size=0.1, random_state=1, stratify= train['Lactate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestRegressor()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestRegressor</label><div class=\"sk-toggleable__content\"><pre>RandomForestRegressor()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomForestRegressor()"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf = RandomForestRegressor()\n",
    "st = StandardScaler()\n",
    "\n",
    "X_train = train.iloc[:,1:]\n",
    "y_train = train.iloc[:,0]\n",
    "X_train = st.fit_transform(X_train)\n",
    "X_train = pd.DataFrame(X_train, columns = train.iloc[:,1:].columns)\n",
    "\n",
    "rf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1350.0</th>\n",
       "      <th>1351.0</th>\n",
       "      <th>1352.0</th>\n",
       "      <th>1353.0</th>\n",
       "      <th>1354.0</th>\n",
       "      <th>1355.0</th>\n",
       "      <th>1356.0</th>\n",
       "      <th>1357.0</th>\n",
       "      <th>1358.0</th>\n",
       "      <th>1359.0</th>\n",
       "      <th>...</th>\n",
       "      <th>2491.0</th>\n",
       "      <th>2492.0</th>\n",
       "      <th>2493.0</th>\n",
       "      <th>2494.0</th>\n",
       "      <th>2495.0</th>\n",
       "      <th>2496.0</th>\n",
       "      <th>2497.0</th>\n",
       "      <th>2498.0</th>\n",
       "      <th>2499.0</th>\n",
       "      <th>2500.0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.209012</td>\n",
       "      <td>-1.079222</td>\n",
       "      <td>-0.953228</td>\n",
       "      <td>-0.870608</td>\n",
       "      <td>-0.819831</td>\n",
       "      <td>-0.790929</td>\n",
       "      <td>-0.776600</td>\n",
       "      <td>-0.771931</td>\n",
       "      <td>-0.773975</td>\n",
       "      <td>-0.780291</td>\n",
       "      <td>...</td>\n",
       "      <td>0.867802</td>\n",
       "      <td>0.838237</td>\n",
       "      <td>0.783499</td>\n",
       "      <td>0.705404</td>\n",
       "      <td>0.612544</td>\n",
       "      <td>0.515496</td>\n",
       "      <td>0.422431</td>\n",
       "      <td>0.337701</td>\n",
       "      <td>0.262292</td>\n",
       "      <td>0.194785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.606375</td>\n",
       "      <td>-0.774900</td>\n",
       "      <td>-0.842126</td>\n",
       "      <td>-0.878068</td>\n",
       "      <td>-0.898370</td>\n",
       "      <td>-0.907052</td>\n",
       "      <td>-0.906601</td>\n",
       "      <td>-0.898716</td>\n",
       "      <td>-0.884178</td>\n",
       "      <td>-0.864348</td>\n",
       "      <td>...</td>\n",
       "      <td>0.030344</td>\n",
       "      <td>0.016658</td>\n",
       "      <td>-0.003561</td>\n",
       "      <td>-0.028934</td>\n",
       "      <td>-0.057036</td>\n",
       "      <td>-0.085844</td>\n",
       "      <td>-0.114410</td>\n",
       "      <td>-0.142818</td>\n",
       "      <td>-0.171711</td>\n",
       "      <td>-0.201887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.583447</td>\n",
       "      <td>-0.766848</td>\n",
       "      <td>-0.842565</td>\n",
       "      <td>-0.883223</td>\n",
       "      <td>-0.905896</td>\n",
       "      <td>-0.915511</td>\n",
       "      <td>-0.915358</td>\n",
       "      <td>-0.907637</td>\n",
       "      <td>-0.893400</td>\n",
       "      <td>-0.874087</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.191189</td>\n",
       "      <td>-0.178756</td>\n",
       "      <td>-0.168305</td>\n",
       "      <td>-0.159192</td>\n",
       "      <td>-0.150820</td>\n",
       "      <td>-0.143167</td>\n",
       "      <td>-0.136598</td>\n",
       "      <td>-0.131633</td>\n",
       "      <td>-0.128746</td>\n",
       "      <td>-0.128362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.140686</td>\n",
       "      <td>0.636872</td>\n",
       "      <td>0.867337</td>\n",
       "      <td>0.987430</td>\n",
       "      <td>1.055310</td>\n",
       "      <td>1.093264</td>\n",
       "      <td>1.112110</td>\n",
       "      <td>1.117528</td>\n",
       "      <td>1.112497</td>\n",
       "      <td>1.099599</td>\n",
       "      <td>...</td>\n",
       "      <td>0.576967</td>\n",
       "      <td>0.893227</td>\n",
       "      <td>1.187225</td>\n",
       "      <td>1.441355</td>\n",
       "      <td>1.640342</td>\n",
       "      <td>1.780804</td>\n",
       "      <td>1.869638</td>\n",
       "      <td>1.917792</td>\n",
       "      <td>1.934897</td>\n",
       "      <td>1.926563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.360103</td>\n",
       "      <td>0.844039</td>\n",
       "      <td>1.025133</td>\n",
       "      <td>1.100748</td>\n",
       "      <td>1.137138</td>\n",
       "      <td>1.156701</td>\n",
       "      <td>1.168480</td>\n",
       "      <td>1.176715</td>\n",
       "      <td>1.184129</td>\n",
       "      <td>1.191780</td>\n",
       "      <td>...</td>\n",
       "      <td>0.829834</td>\n",
       "      <td>0.841471</td>\n",
       "      <td>0.823261</td>\n",
       "      <td>0.774554</td>\n",
       "      <td>0.699328</td>\n",
       "      <td>0.605181</td>\n",
       "      <td>0.499507</td>\n",
       "      <td>0.387017</td>\n",
       "      <td>0.269400</td>\n",
       "      <td>0.146099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3271</th>\n",
       "      <td>0.531304</td>\n",
       "      <td>0.802649</td>\n",
       "      <td>0.883244</td>\n",
       "      <td>0.908668</td>\n",
       "      <td>0.916810</td>\n",
       "      <td>0.919721</td>\n",
       "      <td>0.921680</td>\n",
       "      <td>0.924275</td>\n",
       "      <td>0.928156</td>\n",
       "      <td>0.933288</td>\n",
       "      <td>...</td>\n",
       "      <td>0.920277</td>\n",
       "      <td>0.884533</td>\n",
       "      <td>0.818860</td>\n",
       "      <td>0.727033</td>\n",
       "      <td>0.619208</td>\n",
       "      <td>0.507498</td>\n",
       "      <td>0.401360</td>\n",
       "      <td>0.305901</td>\n",
       "      <td>0.222384</td>\n",
       "      <td>0.149307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3272</th>\n",
       "      <td>-0.616579</td>\n",
       "      <td>-0.781031</td>\n",
       "      <td>-0.844748</td>\n",
       "      <td>-0.878007</td>\n",
       "      <td>-0.896358</td>\n",
       "      <td>-0.903843</td>\n",
       "      <td>-0.902987</td>\n",
       "      <td>-0.895513</td>\n",
       "      <td>-0.882084</td>\n",
       "      <td>-0.864132</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.212806</td>\n",
       "      <td>-0.168885</td>\n",
       "      <td>-0.127552</td>\n",
       "      <td>-0.090103</td>\n",
       "      <td>-0.059056</td>\n",
       "      <td>-0.036509</td>\n",
       "      <td>-0.023534</td>\n",
       "      <td>-0.020522</td>\n",
       "      <td>-0.027590</td>\n",
       "      <td>-0.044927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3273</th>\n",
       "      <td>0.342754</td>\n",
       "      <td>0.845276</td>\n",
       "      <td>1.035925</td>\n",
       "      <td>1.116850</td>\n",
       "      <td>1.156240</td>\n",
       "      <td>1.177457</td>\n",
       "      <td>1.190037</td>\n",
       "      <td>1.198409</td>\n",
       "      <td>1.205292</td>\n",
       "      <td>1.211840</td>\n",
       "      <td>...</td>\n",
       "      <td>0.550230</td>\n",
       "      <td>0.598083</td>\n",
       "      <td>0.628484</td>\n",
       "      <td>0.637875</td>\n",
       "      <td>0.625506</td>\n",
       "      <td>0.594216</td>\n",
       "      <td>0.548229</td>\n",
       "      <td>0.491036</td>\n",
       "      <td>0.424633</td>\n",
       "      <td>0.349749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3274</th>\n",
       "      <td>-0.396637</td>\n",
       "      <td>-0.857650</td>\n",
       "      <td>-1.026016</td>\n",
       "      <td>-1.093228</td>\n",
       "      <td>-1.122256</td>\n",
       "      <td>-1.135016</td>\n",
       "      <td>-1.140380</td>\n",
       "      <td>-1.142342</td>\n",
       "      <td>-1.142713</td>\n",
       "      <td>-1.142334</td>\n",
       "      <td>...</td>\n",
       "      <td>0.500964</td>\n",
       "      <td>0.291516</td>\n",
       "      <td>0.067870</td>\n",
       "      <td>-0.158596</td>\n",
       "      <td>-0.373674</td>\n",
       "      <td>-0.567170</td>\n",
       "      <td>-0.735556</td>\n",
       "      <td>-0.880513</td>\n",
       "      <td>-1.005929</td>\n",
       "      <td>-1.115478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3275</th>\n",
       "      <td>-1.261789</td>\n",
       "      <td>-1.094805</td>\n",
       "      <td>-0.954549</td>\n",
       "      <td>-0.867699</td>\n",
       "      <td>-0.816307</td>\n",
       "      <td>-0.788014</td>\n",
       "      <td>-0.774890</td>\n",
       "      <td>-0.771803</td>\n",
       "      <td>-0.775647</td>\n",
       "      <td>-0.783924</td>\n",
       "      <td>...</td>\n",
       "      <td>0.371750</td>\n",
       "      <td>0.767790</td>\n",
       "      <td>1.160523</td>\n",
       "      <td>1.524798</td>\n",
       "      <td>1.834781</td>\n",
       "      <td>2.078134</td>\n",
       "      <td>2.256598</td>\n",
       "      <td>2.379531</td>\n",
       "      <td>2.457367</td>\n",
       "      <td>2.498032</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3276 rows × 1151 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        1350.0    1351.0    1352.0    1353.0    1354.0    1355.0    1356.0  \\\n",
       "0    -1.209012 -1.079222 -0.953228 -0.870608 -0.819831 -0.790929 -0.776600   \n",
       "1    -0.606375 -0.774900 -0.842126 -0.878068 -0.898370 -0.907052 -0.906601   \n",
       "2    -0.583447 -0.766848 -0.842565 -0.883223 -0.905896 -0.915511 -0.915358   \n",
       "3     0.140686  0.636872  0.867337  0.987430  1.055310  1.093264  1.112110   \n",
       "4     0.360103  0.844039  1.025133  1.100748  1.137138  1.156701  1.168480   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "3271  0.531304  0.802649  0.883244  0.908668  0.916810  0.919721  0.921680   \n",
       "3272 -0.616579 -0.781031 -0.844748 -0.878007 -0.896358 -0.903843 -0.902987   \n",
       "3273  0.342754  0.845276  1.035925  1.116850  1.156240  1.177457  1.190037   \n",
       "3274 -0.396637 -0.857650 -1.026016 -1.093228 -1.122256 -1.135016 -1.140380   \n",
       "3275 -1.261789 -1.094805 -0.954549 -0.867699 -0.816307 -0.788014 -0.774890   \n",
       "\n",
       "        1357.0    1358.0    1359.0  ...    2491.0    2492.0    2493.0  \\\n",
       "0    -0.771931 -0.773975 -0.780291  ...  0.867802  0.838237  0.783499   \n",
       "1    -0.898716 -0.884178 -0.864348  ...  0.030344  0.016658 -0.003561   \n",
       "2    -0.907637 -0.893400 -0.874087  ... -0.191189 -0.178756 -0.168305   \n",
       "3     1.117528  1.112497  1.099599  ...  0.576967  0.893227  1.187225   \n",
       "4     1.176715  1.184129  1.191780  ...  0.829834  0.841471  0.823261   \n",
       "...        ...       ...       ...  ...       ...       ...       ...   \n",
       "3271  0.924275  0.928156  0.933288  ...  0.920277  0.884533  0.818860   \n",
       "3272 -0.895513 -0.882084 -0.864132  ... -0.212806 -0.168885 -0.127552   \n",
       "3273  1.198409  1.205292  1.211840  ...  0.550230  0.598083  0.628484   \n",
       "3274 -1.142342 -1.142713 -1.142334  ...  0.500964  0.291516  0.067870   \n",
       "3275 -0.771803 -0.775647 -0.783924  ...  0.371750  0.767790  1.160523   \n",
       "\n",
       "        2494.0    2495.0    2496.0    2497.0    2498.0    2499.0    2500.0  \n",
       "0     0.705404  0.612544  0.515496  0.422431  0.337701  0.262292  0.194785  \n",
       "1    -0.028934 -0.057036 -0.085844 -0.114410 -0.142818 -0.171711 -0.201887  \n",
       "2    -0.159192 -0.150820 -0.143167 -0.136598 -0.131633 -0.128746 -0.128362  \n",
       "3     1.441355  1.640342  1.780804  1.869638  1.917792  1.934897  1.926563  \n",
       "4     0.774554  0.699328  0.605181  0.499507  0.387017  0.269400  0.146099  \n",
       "...        ...       ...       ...       ...       ...       ...       ...  \n",
       "3271  0.727033  0.619208  0.507498  0.401360  0.305901  0.222384  0.149307  \n",
       "3272 -0.090103 -0.059056 -0.036509 -0.023534 -0.020522 -0.027590 -0.044927  \n",
       "3273  0.637875  0.625506  0.594216  0.548229  0.491036  0.424633  0.349749  \n",
       "3274 -0.158596 -0.373674 -0.567170 -0.735556 -0.880513 -1.005929 -1.115478  \n",
       "3275  1.524798  1.834781  2.078134  2.256598  2.379531  2.457367  2.498032  \n",
       "\n",
       "[3276 rows x 1151 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results — train RMSE 1.625549 - train R2 0.999698 — val RMSE 0.281407 - val R2 0.999991\n"
     ]
    }
   ],
   "source": [
    "train_r2 = rf.score(X_train, y_train)\n",
    "\n",
    "X_test = validation.iloc[:,1:]\n",
    "y_test = validation.iloc[:,0]\n",
    "X_test = st.transform(X_test)\n",
    "X_test = pd.DataFrame(X_test, columns = train.iloc[:,1:].columns)\n",
    "val_r2 = rf.score(X_test, y_test)\n",
    "train_rmse = np.sqrt(mean_squared_error(y_train, rf.predict(X_train)))\n",
    "val_rmse = np.sqrt(mean_squared_error(y_test, rf.predict(X_test)))\n",
    "print(f\"Results — train RMSE {train_rmse:.6f} - train R2 {train_r2:.6f} — val RMSE {val_rmse:.6f} - val R2 {val_r2:.6f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = model_selection.train_test_split(data, test_size=0.1, random_state=1, stratify= data['Concentration'])\n",
    "\n",
    "X_train = train.iloc[:,1:]\n",
    "y_train = train.iloc[:,0]\n",
    "\n",
    "cv_scores = model_selection.cross_val_score(rf.set_params(**best_rf.best_params_), X_train, y_train, cv = 5, scoring= 'neg_root_mean_squared_error')\n",
    "print(cv_scores)\n",
    "print(cv_scores.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>outcome</th>\n",
       "      <th>399.196173</th>\n",
       "      <th>401.124639</th>\n",
       "      <th>403.053106</th>\n",
       "      <th>404.981573</th>\n",
       "      <th>406.91004</th>\n",
       "      <th>408.838507</th>\n",
       "      <th>410.766973</th>\n",
       "      <th>412.69544</th>\n",
       "      <th>414.623907</th>\n",
       "      <th>...</th>\n",
       "      <th>1783.83533</th>\n",
       "      <th>1785.7638</th>\n",
       "      <th>1787.69227</th>\n",
       "      <th>1789.62073</th>\n",
       "      <th>1791.5492</th>\n",
       "      <th>1793.47767</th>\n",
       "      <th>1795.40613</th>\n",
       "      <th>1797.3346</th>\n",
       "      <th>1799.26307</th>\n",
       "      <th>1801.19153</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>505.515961</td>\n",
       "      <td>429.792633</td>\n",
       "      <td>442.056671</td>\n",
       "      <td>503.923248</td>\n",
       "      <td>561.752869</td>\n",
       "      <td>569.737793</td>\n",
       "      <td>518.432251</td>\n",
       "      <td>522.847656</td>\n",
       "      <td>549.391357</td>\n",
       "      <td>...</td>\n",
       "      <td>239.627380</td>\n",
       "      <td>213.689026</td>\n",
       "      <td>197.979660</td>\n",
       "      <td>181.284073</td>\n",
       "      <td>266.008698</td>\n",
       "      <td>243.553192</td>\n",
       "      <td>183.262085</td>\n",
       "      <td>182.584747</td>\n",
       "      <td>201.362274</td>\n",
       "      <td>167.707855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>862.735352</td>\n",
       "      <td>1257.182860</td>\n",
       "      <td>1079.357910</td>\n",
       "      <td>1334.693730</td>\n",
       "      <td>1643.421630</td>\n",
       "      <td>1639.412110</td>\n",
       "      <td>1437.808590</td>\n",
       "      <td>1631.328370</td>\n",
       "      <td>1635.983150</td>\n",
       "      <td>...</td>\n",
       "      <td>819.722778</td>\n",
       "      <td>745.088806</td>\n",
       "      <td>715.143005</td>\n",
       "      <td>969.721924</td>\n",
       "      <td>394.866150</td>\n",
       "      <td>669.154175</td>\n",
       "      <td>550.076294</td>\n",
       "      <td>681.312378</td>\n",
       "      <td>639.139282</td>\n",
       "      <td>291.460938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>-150.525482</td>\n",
       "      <td>-167.931763</td>\n",
       "      <td>-110.162025</td>\n",
       "      <td>-92.607994</td>\n",
       "      <td>-116.657005</td>\n",
       "      <td>-109.733025</td>\n",
       "      <td>3.959946</td>\n",
       "      <td>-39.254051</td>\n",
       "      <td>-36.040154</td>\n",
       "      <td>...</td>\n",
       "      <td>-120.388206</td>\n",
       "      <td>-108.129372</td>\n",
       "      <td>-119.814186</td>\n",
       "      <td>-204.408356</td>\n",
       "      <td>-63.430839</td>\n",
       "      <td>-12.132538</td>\n",
       "      <td>3.998680</td>\n",
       "      <td>1.615150</td>\n",
       "      <td>174.106812</td>\n",
       "      <td>26.561195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>213.028732</td>\n",
       "      <td>213.095520</td>\n",
       "      <td>224.707001</td>\n",
       "      <td>223.211304</td>\n",
       "      <td>233.210297</td>\n",
       "      <td>271.803040</td>\n",
       "      <td>326.114319</td>\n",
       "      <td>340.944275</td>\n",
       "      <td>306.199188</td>\n",
       "      <td>...</td>\n",
       "      <td>112.615479</td>\n",
       "      <td>113.849823</td>\n",
       "      <td>100.284653</td>\n",
       "      <td>81.723885</td>\n",
       "      <td>137.862061</td>\n",
       "      <td>117.423050</td>\n",
       "      <td>83.368675</td>\n",
       "      <td>31.846382</td>\n",
       "      <td>105.064209</td>\n",
       "      <td>139.783569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>1183.171390</td>\n",
       "      <td>1121.977050</td>\n",
       "      <td>1159.223390</td>\n",
       "      <td>1265.475830</td>\n",
       "      <td>1319.099730</td>\n",
       "      <td>1344.597050</td>\n",
       "      <td>1488.174190</td>\n",
       "      <td>1338.122560</td>\n",
       "      <td>1384.080080</td>\n",
       "      <td>...</td>\n",
       "      <td>188.364594</td>\n",
       "      <td>202.664902</td>\n",
       "      <td>185.910187</td>\n",
       "      <td>187.258728</td>\n",
       "      <td>202.907410</td>\n",
       "      <td>180.061676</td>\n",
       "      <td>204.917358</td>\n",
       "      <td>162.099548</td>\n",
       "      <td>160.202332</td>\n",
       "      <td>194.598831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>264</th>\n",
       "      <td>0</td>\n",
       "      <td>116.517418</td>\n",
       "      <td>89.226669</td>\n",
       "      <td>144.149017</td>\n",
       "      <td>212.174286</td>\n",
       "      <td>203.939697</td>\n",
       "      <td>156.757690</td>\n",
       "      <td>212.084579</td>\n",
       "      <td>157.982330</td>\n",
       "      <td>175.348022</td>\n",
       "      <td>...</td>\n",
       "      <td>-44.818634</td>\n",
       "      <td>34.807320</td>\n",
       "      <td>-34.518677</td>\n",
       "      <td>-115.108688</td>\n",
       "      <td>-89.549851</td>\n",
       "      <td>-26.099594</td>\n",
       "      <td>-0.635860</td>\n",
       "      <td>-231.556229</td>\n",
       "      <td>-120.877533</td>\n",
       "      <td>-79.159164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>265</th>\n",
       "      <td>2</td>\n",
       "      <td>1001.496770</td>\n",
       "      <td>923.082031</td>\n",
       "      <td>1222.555910</td>\n",
       "      <td>1269.627440</td>\n",
       "      <td>1468.851070</td>\n",
       "      <td>1744.580810</td>\n",
       "      <td>1411.702150</td>\n",
       "      <td>1462.815670</td>\n",
       "      <td>1431.325930</td>\n",
       "      <td>...</td>\n",
       "      <td>717.431519</td>\n",
       "      <td>829.128601</td>\n",
       "      <td>739.284668</td>\n",
       "      <td>806.189026</td>\n",
       "      <td>951.298401</td>\n",
       "      <td>750.525940</td>\n",
       "      <td>820.183838</td>\n",
       "      <td>766.859131</td>\n",
       "      <td>1096.494260</td>\n",
       "      <td>726.897644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>266</th>\n",
       "      <td>0</td>\n",
       "      <td>708.721436</td>\n",
       "      <td>657.105225</td>\n",
       "      <td>703.090759</td>\n",
       "      <td>833.595886</td>\n",
       "      <td>896.817444</td>\n",
       "      <td>868.490662</td>\n",
       "      <td>893.127991</td>\n",
       "      <td>958.369385</td>\n",
       "      <td>997.086792</td>\n",
       "      <td>...</td>\n",
       "      <td>590.589233</td>\n",
       "      <td>626.296387</td>\n",
       "      <td>610.929077</td>\n",
       "      <td>558.473511</td>\n",
       "      <td>571.249695</td>\n",
       "      <td>506.676880</td>\n",
       "      <td>593.622742</td>\n",
       "      <td>581.466553</td>\n",
       "      <td>531.453369</td>\n",
       "      <td>592.837463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>267</th>\n",
       "      <td>3</td>\n",
       "      <td>246.487213</td>\n",
       "      <td>244.675598</td>\n",
       "      <td>244.788330</td>\n",
       "      <td>310.309296</td>\n",
       "      <td>350.590332</td>\n",
       "      <td>341.414337</td>\n",
       "      <td>358.449921</td>\n",
       "      <td>310.393860</td>\n",
       "      <td>305.850067</td>\n",
       "      <td>...</td>\n",
       "      <td>65.794098</td>\n",
       "      <td>61.649521</td>\n",
       "      <td>83.493576</td>\n",
       "      <td>134.336029</td>\n",
       "      <td>135.778336</td>\n",
       "      <td>141.321121</td>\n",
       "      <td>95.968979</td>\n",
       "      <td>125.428558</td>\n",
       "      <td>71.864761</td>\n",
       "      <td>117.960907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>268</th>\n",
       "      <td>3</td>\n",
       "      <td>556.670776</td>\n",
       "      <td>528.942322</td>\n",
       "      <td>543.529419</td>\n",
       "      <td>539.082886</td>\n",
       "      <td>540.013916</td>\n",
       "      <td>575.687012</td>\n",
       "      <td>643.916138</td>\n",
       "      <td>646.243530</td>\n",
       "      <td>669.503418</td>\n",
       "      <td>...</td>\n",
       "      <td>192.310211</td>\n",
       "      <td>206.695358</td>\n",
       "      <td>191.348480</td>\n",
       "      <td>198.764801</td>\n",
       "      <td>203.074478</td>\n",
       "      <td>176.973740</td>\n",
       "      <td>110.635422</td>\n",
       "      <td>148.076462</td>\n",
       "      <td>152.184952</td>\n",
       "      <td>110.318359</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>269 rows × 729 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     outcome   399.196173   401.124639   403.053106   404.981573    406.91004  \\\n",
       "0          3   505.515961   429.792633   442.056671   503.923248   561.752869   \n",
       "1          2   862.735352  1257.182860  1079.357910  1334.693730  1643.421630   \n",
       "2          0  -150.525482  -167.931763  -110.162025   -92.607994  -116.657005   \n",
       "3          2   213.028732   213.095520   224.707001   223.211304   233.210297   \n",
       "4          3  1183.171390  1121.977050  1159.223390  1265.475830  1319.099730   \n",
       "..       ...          ...          ...          ...          ...          ...   \n",
       "264        0   116.517418    89.226669   144.149017   212.174286   203.939697   \n",
       "265        2  1001.496770   923.082031  1222.555910  1269.627440  1468.851070   \n",
       "266        0   708.721436   657.105225   703.090759   833.595886   896.817444   \n",
       "267        3   246.487213   244.675598   244.788330   310.309296   350.590332   \n",
       "268        3   556.670776   528.942322   543.529419   539.082886   540.013916   \n",
       "\n",
       "      408.838507   410.766973    412.69544   414.623907  ...  1783.83533  \\\n",
       "0     569.737793   518.432251   522.847656   549.391357  ...  239.627380   \n",
       "1    1639.412110  1437.808590  1631.328370  1635.983150  ...  819.722778   \n",
       "2    -109.733025     3.959946   -39.254051   -36.040154  ... -120.388206   \n",
       "3     271.803040   326.114319   340.944275   306.199188  ...  112.615479   \n",
       "4    1344.597050  1488.174190  1338.122560  1384.080080  ...  188.364594   \n",
       "..           ...          ...          ...          ...  ...         ...   \n",
       "264   156.757690   212.084579   157.982330   175.348022  ...  -44.818634   \n",
       "265  1744.580810  1411.702150  1462.815670  1431.325930  ...  717.431519   \n",
       "266   868.490662   893.127991   958.369385   997.086792  ...  590.589233   \n",
       "267   341.414337   358.449921   310.393860   305.850067  ...   65.794098   \n",
       "268   575.687012   643.916138   646.243530   669.503418  ...  192.310211   \n",
       "\n",
       "      1785.7638  1787.69227  1789.62073   1791.5492  1793.47767  1795.40613  \\\n",
       "0    213.689026  197.979660  181.284073  266.008698  243.553192  183.262085   \n",
       "1    745.088806  715.143005  969.721924  394.866150  669.154175  550.076294   \n",
       "2   -108.129372 -119.814186 -204.408356  -63.430839  -12.132538    3.998680   \n",
       "3    113.849823  100.284653   81.723885  137.862061  117.423050   83.368675   \n",
       "4    202.664902  185.910187  187.258728  202.907410  180.061676  204.917358   \n",
       "..          ...         ...         ...         ...         ...         ...   \n",
       "264   34.807320  -34.518677 -115.108688  -89.549851  -26.099594   -0.635860   \n",
       "265  829.128601  739.284668  806.189026  951.298401  750.525940  820.183838   \n",
       "266  626.296387  610.929077  558.473511  571.249695  506.676880  593.622742   \n",
       "267   61.649521   83.493576  134.336029  135.778336  141.321121   95.968979   \n",
       "268  206.695358  191.348480  198.764801  203.074478  176.973740  110.635422   \n",
       "\n",
       "      1797.3346   1799.26307  1801.19153  \n",
       "0    182.584747   201.362274  167.707855  \n",
       "1    681.312378   639.139282  291.460938  \n",
       "2      1.615150   174.106812   26.561195  \n",
       "3     31.846382   105.064209  139.783569  \n",
       "4    162.099548   160.202332  194.598831  \n",
       "..          ...          ...         ...  \n",
       "264 -231.556229  -120.877533  -79.159164  \n",
       "265  766.859131  1096.494260  726.897644  \n",
       "266  581.466553   531.453369  592.837463  \n",
       "267  125.428558    71.864761  117.960907  \n",
       "268  148.076462   152.184952  110.318359  \n",
       "\n",
       "[269 rows x 729 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cancer = pd.read_csv('Training_data_Italian.csv')\n",
    "del cancer['Unnamed: 0']\n",
    "cancer = cancer[['outcome'] + list(cancer.columns[:-1])]\n",
    "le = LabelEncoder()\n",
    "cancer['outcome'] = le.fit_transform(cancer['outcome'])\n",
    "cancer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(cancer.iloc[0,1:])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3    0.275093\n",
       "1    0.249071\n",
       "2    0.237918\n",
       "0    0.237918\n",
       "Name: outcome, dtype: float64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cancer['outcome'].value_counts()/269"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7101efabdbf54ab782214ad3213c9d3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  1 — train Accuracy : 0.256198 - train F1 0.250711 — val Accuracy 0.222222 - val F1 0.080808\n",
      "Epoch  2 — train Accuracy : 0.305785 - train F1 0.302505 — val Accuracy 0.370370 - val F1 0.283001\n",
      "Epoch  3 — train Accuracy : 0.400826 - train F1 0.391764 — val Accuracy 0.666667 - val F1 0.652585\n",
      "Epoch  4 — train Accuracy : 0.673554 - train F1 0.673459 — val Accuracy 0.703704 - val F1 0.694754\n",
      "Epoch  5 — train Accuracy : 0.702479 - train F1 0.701811 — val Accuracy 0.888889 - val F1 0.888889\n",
      "Epoch  6 — train Accuracy : 0.776860 - train F1 0.775606 — val Accuracy 0.814815 - val F1 0.808495\n",
      "Epoch  7 — train Accuracy : 0.776860 - train F1 0.776866 — val Accuracy 0.888889 - val F1 0.887883\n",
      "Epoch  8 — train Accuracy : 0.834711 - train F1 0.834603 — val Accuracy 0.851852 - val F1 0.851852\n",
      "Epoch  9 — train Accuracy : 0.871901 - train F1 0.871850 — val Accuracy 0.888889 - val F1 0.888889\n",
      "Epoch 10 — train Accuracy : 0.867769 - train F1 0.867822 — val Accuracy 1.000000 - val F1 1.000000\n",
      "Epoch 11 — train Accuracy : 0.863636 - train F1 0.864097 — val Accuracy 0.925926 - val F1 0.925265\n",
      "Epoch 12 — train Accuracy : 0.913223 - train F1 0.912818 — val Accuracy 0.925926 - val F1 0.925265\n",
      "Epoch 13 — train Accuracy : 0.929752 - train F1 0.929306 — val Accuracy 0.925926 - val F1 0.923868\n",
      "Epoch 14 — train Accuracy : 0.884298 - train F1 0.883765 — val Accuracy 1.000000 - val F1 1.000000\n",
      "Epoch 15 — train Accuracy : 0.917355 - train F1 0.917667 — val Accuracy 1.000000 - val F1 1.000000\n",
      "Epoch 16 — train Accuracy : 0.942149 - train F1 0.942064 — val Accuracy 1.000000 - val F1 1.000000\n",
      "Epoch 17 — train Accuracy : 0.917355 - train F1 0.916585 — val Accuracy 1.000000 - val F1 1.000000\n",
      "Epoch 18 — train Accuracy : 0.929752 - train F1 0.929397 — val Accuracy 0.962963 - val F1 0.962963\n",
      "Epoch 19 — train Accuracy : 0.929752 - train F1 0.929944 — val Accuracy 0.925926 - val F1 0.923868\n",
      "Epoch 20 — train Accuracy : 0.909091 - train F1 0.908364 — val Accuracy 0.962963 - val F1 0.962628\n",
      "Epoch 21 — train Accuracy : 0.929752 - train F1 0.928956 — val Accuracy 1.000000 - val F1 1.000000\n",
      "Epoch 22 — train Accuracy : 0.933884 - train F1 0.933977 — val Accuracy 0.962963 - val F1 0.962963\n",
      "Epoch 23 — train Accuracy : 0.925620 - train F1 0.925796 — val Accuracy 1.000000 - val F1 1.000000\n",
      "Epoch 24 — train Accuracy : 0.938017 - train F1 0.937755 — val Accuracy 0.962963 - val F1 0.962963\n",
      "Epoch 25 — train Accuracy : 0.938017 - train F1 0.938001 — val Accuracy 1.000000 - val F1 1.000000\n",
      "Epoch 26 — train Accuracy : 0.921488 - train F1 0.921265 — val Accuracy 1.000000 - val F1 1.000000\n",
      "Epoch 27 — train Accuracy : 0.929752 - train F1 0.929404 — val Accuracy 1.000000 - val F1 1.000000\n",
      "Epoch 28 — train Accuracy : 0.938017 - train F1 0.938328 — val Accuracy 1.000000 - val F1 1.000000\n",
      "Epoch 29 — train Accuracy : 0.958678 - train F1 0.958568 — val Accuracy 0.962963 - val F1 0.962628\n",
      "Epoch 30 — train Accuracy : 0.929752 - train F1 0.929766 — val Accuracy 1.000000 - val F1 1.000000\n",
      "Epoch 31 — train Accuracy : 0.925620 - train F1 0.925703 — val Accuracy 1.000000 - val F1 1.000000\n",
      "Epoch 32 — train Accuracy : 0.975207 - train F1 0.975088 — val Accuracy 1.000000 - val F1 1.000000\n",
      "Epoch 33 — train Accuracy : 0.958678 - train F1 0.958928 — val Accuracy 1.000000 - val F1 1.000000\n",
      "Epoch 34 — train Accuracy : 0.954545 - train F1 0.954499 — val Accuracy 1.000000 - val F1 1.000000\n",
      "Epoch 35 — train Accuracy : 0.925620 - train F1 0.925701 — val Accuracy 1.000000 - val F1 1.000000\n",
      "Epoch 36 — train Accuracy : 0.925620 - train F1 0.925516 — val Accuracy 0.962963 - val F1 0.962628\n",
      "Epoch 37 — train Accuracy : 0.942149 - train F1 0.942189 — val Accuracy 1.000000 - val F1 1.000000\n",
      "Epoch 38 — train Accuracy : 0.929752 - train F1 0.929364 — val Accuracy 1.000000 - val F1 1.000000\n",
      "Epoch 39 — train Accuracy : 0.958678 - train F1 0.958672 — val Accuracy 1.000000 - val F1 1.000000\n",
      "Epoch 40 — train Accuracy : 0.950413 - train F1 0.950014 — val Accuracy 1.000000 - val F1 1.000000\n",
      "Epoch 41 — train Accuracy : 0.929752 - train F1 0.929921 — val Accuracy 1.000000 - val F1 1.000000\n",
      "Epoch 42 — train Accuracy : 0.942149 - train F1 0.942366 — val Accuracy 1.000000 - val F1 1.000000\n",
      "Epoch 43 — train Accuracy : 0.942149 - train F1 0.941505 — val Accuracy 1.000000 - val F1 1.000000\n",
      "Epoch 44 — train Accuracy : 0.950413 - train F1 0.950560 — val Accuracy 1.000000 - val F1 1.000000\n",
      "Epoch 45 — train Accuracy : 0.958678 - train F1 0.958566 — val Accuracy 1.000000 - val F1 1.000000\n",
      "Epoch 46 — train Accuracy : 0.929752 - train F1 0.929051 — val Accuracy 1.000000 - val F1 1.000000\n",
      "Epoch 47 — train Accuracy : 0.958678 - train F1 0.958574 — val Accuracy 1.000000 - val F1 1.000000\n",
      "Epoch 48 — train Accuracy : 0.904959 - train F1 0.904829 — val Accuracy 1.000000 - val F1 1.000000\n",
      "Epoch 49 — train Accuracy : 0.954545 - train F1 0.954532 — val Accuracy 1.000000 - val F1 1.000000\n",
      "Epoch 50 — train Accuracy : 0.938017 - train F1 0.937858 — val Accuracy 1.000000 - val F1 1.000000\n",
      "Epoch 51 — train Accuracy : 0.938017 - train F1 0.938127 — val Accuracy 1.000000 - val F1 1.000000\n",
      "Epoch 52 — train Accuracy : 0.971074 - train F1 0.971217 — val Accuracy 1.000000 - val F1 1.000000\n",
      "Epoch 53 — train Accuracy : 0.904959 - train F1 0.904696 — val Accuracy 1.000000 - val F1 1.000000\n",
      "Epoch 54 — train Accuracy : 0.917355 - train F1 0.916737 — val Accuracy 1.000000 - val F1 1.000000\n",
      "Epoch 55 — train Accuracy : 0.913223 - train F1 0.913528 — val Accuracy 1.000000 - val F1 1.000000\n",
      "Epoch 56 — train Accuracy : 0.950413 - train F1 0.950498 — val Accuracy 1.000000 - val F1 1.000000\n",
      "Epoch 57 — train Accuracy : 0.933884 - train F1 0.934371 — val Accuracy 1.000000 - val F1 1.000000\n",
      "Epoch 58 — train Accuracy : 0.958678 - train F1 0.958714 — val Accuracy 1.000000 - val F1 1.000000\n",
      "Epoch 59 — train Accuracy : 0.938017 - train F1 0.938718 — val Accuracy 1.000000 - val F1 1.000000\n",
      "Epoch 60 — train Accuracy : 0.925620 - train F1 0.926401 — val Accuracy 1.000000 - val F1 1.000000\n",
      "Epoch 61 — train Accuracy : 0.962810 - train F1 0.962805 — val Accuracy 1.000000 - val F1 1.000000\n",
      "Epoch 62 — train Accuracy : 0.929752 - train F1 0.929402 — val Accuracy 1.000000 - val F1 1.000000\n",
      "Epoch 63 — train Accuracy : 0.954545 - train F1 0.954929 — val Accuracy 1.000000 - val F1 1.000000\n",
      "Epoch 64 — train Accuracy : 0.950413 - train F1 0.950212 — val Accuracy 1.000000 - val F1 1.000000\n",
      "Epoch 65 — train Accuracy : 0.950413 - train F1 0.950095 — val Accuracy 1.000000 - val F1 1.000000\n",
      "Epoch 66 — train Accuracy : 0.950413 - train F1 0.950926 — val Accuracy 1.000000 - val F1 1.000000\n",
      "Epoch 67 — train Accuracy : 0.962810 - train F1 0.962821 — val Accuracy 1.000000 - val F1 1.000000\n",
      "Epoch 68 — train Accuracy : 0.975207 - train F1 0.975446 — val Accuracy 1.000000 - val F1 1.000000\n",
      "Epoch 69 — train Accuracy : 0.942149 - train F1 0.941154 — val Accuracy 1.000000 - val F1 1.000000\n",
      "Epoch 70 — train Accuracy : 0.933884 - train F1 0.934328 — val Accuracy 1.000000 - val F1 1.000000\n",
      "Epoch 71 — train Accuracy : 0.962810 - train F1 0.962624 — val Accuracy 1.000000 - val F1 1.000000\n",
      "Epoch 72 — train Accuracy : 0.954545 - train F1 0.954998 — val Accuracy 1.000000 - val F1 1.000000\n",
      "Epoch 73 — train Accuracy : 0.942149 - train F1 0.942771 — val Accuracy 1.000000 - val F1 1.000000\n",
      "Epoch 74 — train Accuracy : 0.938017 - train F1 0.938063 — val Accuracy 1.000000 - val F1 1.000000\n",
      "Epoch 75 — train Accuracy : 0.909091 - train F1 0.909300 — val Accuracy 1.000000 - val F1 1.000000\n",
      "Epoch 76 — train Accuracy : 0.929752 - train F1 0.929418 — val Accuracy 1.000000 - val F1 1.000000\n",
      "Epoch 77 — train Accuracy : 0.942149 - train F1 0.941842 — val Accuracy 1.000000 - val F1 1.000000\n",
      "Epoch 78 — train Accuracy : 0.962810 - train F1 0.962628 — val Accuracy 1.000000 - val F1 1.000000\n",
      "Epoch 79 — train Accuracy : 0.954545 - train F1 0.954666 — val Accuracy 1.000000 - val F1 1.000000\n",
      "Epoch 80 — train Accuracy : 0.954545 - train F1 0.954653 — val Accuracy 1.000000 - val F1 1.000000\n",
      "Epoch 81 — train Accuracy : 0.925620 - train F1 0.925173 — val Accuracy 1.000000 - val F1 1.000000\n",
      "Epoch 82 — train Accuracy : 0.966942 - train F1 0.967049 — val Accuracy 1.000000 - val F1 1.000000\n",
      "Epoch 83 — train Accuracy : 0.975207 - train F1 0.975346 — val Accuracy 1.000000 - val F1 1.000000\n",
      "Epoch 84 — train Accuracy : 0.942149 - train F1 0.942549 — val Accuracy 1.000000 - val F1 1.000000\n",
      "Epoch 85 — train Accuracy : 0.917355 - train F1 0.917948 — val Accuracy 1.000000 - val F1 1.000000\n",
      "Epoch 86 — train Accuracy : 0.950413 - train F1 0.950491 — val Accuracy 1.000000 - val F1 1.000000\n",
      "Epoch 87 — train Accuracy : 0.942149 - train F1 0.941629 — val Accuracy 1.000000 - val F1 1.000000\n",
      "Epoch 88 — train Accuracy : 0.933884 - train F1 0.934042 — val Accuracy 1.000000 - val F1 1.000000\n",
      "Epoch 89 — train Accuracy : 0.946281 - train F1 0.946153 — val Accuracy 1.000000 - val F1 1.000000\n",
      "Epoch 90 — train Accuracy : 0.966942 - train F1 0.967189 — val Accuracy 1.000000 - val F1 1.000000\n",
      "Epoch 91 — train Accuracy : 0.966942 - train F1 0.966659 — val Accuracy 1.000000 - val F1 1.000000\n",
      "Epoch 92 — train Accuracy : 0.950413 - train F1 0.950278 — val Accuracy 1.000000 - val F1 1.000000\n",
      "Epoch 93 — train Accuracy : 0.938017 - train F1 0.937747 — val Accuracy 1.000000 - val F1 1.000000\n",
      "Epoch 94 — train Accuracy : 0.954545 - train F1 0.954870 — val Accuracy 1.000000 - val F1 1.000000\n",
      "Epoch 95 — train Accuracy : 0.938017 - train F1 0.937839 — val Accuracy 1.000000 - val F1 1.000000\n",
      "Epoch 96 — train Accuracy : 0.954545 - train F1 0.954992 — val Accuracy 1.000000 - val F1 1.000000\n",
      "Epoch 97 — train Accuracy : 0.938017 - train F1 0.938619 — val Accuracy 1.000000 - val F1 1.000000\n",
      "Epoch 98 — train Accuracy : 0.958678 - train F1 0.958529 — val Accuracy 1.000000 - val F1 1.000000\n",
      "Epoch 99 — train Accuracy : 0.958678 - train F1 0.958806 — val Accuracy 1.000000 - val F1 1.000000\n",
      "Epoch 100 — train Accuracy : 0.925620 - train F1 0.925390 — val Accuracy 0.962963 - val F1 0.962963\n"
     ]
    }
   ],
   "source": [
    "configs = Details(\n",
    "    task = 'classification',\n",
    "    pred_len = 4,      \n",
    "    seq_len  = 268,\n",
    "    d_ff = 32,\n",
    "    patch_len= 16, \n",
    "    stride = 8, \n",
    "    llm_layers = 8, \n",
    "    description = \"This dataset contains near-infrared (NIR) spectroscopy measurements in a \" \\\n",
    "                  \"phosphate-buffered saline (PBS) solution. Each spectrum includes absorbance values across \" \\\n",
    "                  \"different wavelengths. The goal is to predict the classify each spectra given the different wavelengths \" ,\n",
    "    dropout = 0.2, \n",
    "    n_heads = 4, \n",
    "    d_model = 16,\n",
    "    enc_in = 1,\n",
    "    d_llm = 4096,\n",
    "    llm_model = 'LLAMA',\n",
    "    lr = 0.0001\n",
    ")\n",
    "\n",
    "train, validation = model_selection.train_test_split(cancer, test_size=0.1, random_state=1, stratify= cancer['outcome'])\n",
    "\n",
    "model ,train_rmse, train_r2, val_rmse, val_r2, stand  = train_classification_model(configs, train, validation, 100, 8, True, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'class_model_weights2.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68875c6783e749b3a65a3ed592a643e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "configs = Details(\n",
    "    task = 'classification',\n",
    "    pred_len = 4,      \n",
    "    seq_len  = 268,\n",
    "    d_ff = 32,\n",
    "    patch_len= 16, \n",
    "    stride = 8, \n",
    "    llm_layers = 8, \n",
    "    description = \"This dataset contains near-infrared (NIR) spectroscopy measurements in a \" \\\n",
    "                  \"phosphate-buffered saline (PBS) solution. Each spectrum includes absorbance values across \" \\\n",
    "                  \"different wavelengths. The goal is to predict the classify each spectra given the different wavelengths \" ,\n",
    "    dropout = 0.2, \n",
    "    n_heads = 4, \n",
    "    d_model = 16,\n",
    "    enc_in = 1,\n",
    "    d_llm = 4096,\n",
    "    llm_model = 'LLAMA',\n",
    "    lr = 0.0001\n",
    ")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class_mod  = Model(configs).to(device)\n",
    "\n",
    "class_mod.load_state_dict(torch.load(\"class_model_weights2.pth\", map_location=device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test in new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('Testing_data_Italian.csv')\n",
    "del test['Unnamed: 0']\n",
    "test = test[['outcome'] + list(test.columns[:-1])]\n",
    "le = LabelEncoder()\n",
    "test['outcome'] = le.fit_transform(test['outcome'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 886.00 MiB. GPU 0 has a total capacity of 23.68 GiB of which 390.69 MiB is free. Including non-PyTorch memory, this process has 23.10 GiB memory in use. Of the allocated memory 18.97 GiB is allocated by PyTorch, and 2.86 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 22\u001b[0m\n\u001b[1;32m     18\u001b[0m y_test \u001b[38;5;241m=\u001b[39m y_test\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 22\u001b[0m     y_pred \u001b[38;5;241m=\u001b[39m \u001b[43mclass_mod\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m criterion \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss()\n\u001b[1;32m     27\u001b[0m test_acc \u001b[38;5;241m=\u001b[39m accuracy_score(y_test\u001b[38;5;241m.\u001b[39msqueeze()\u001b[38;5;241m.\u001b[39mcpu(), torch\u001b[38;5;241m.\u001b[39margmax(y_pred, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39msqueeze()\u001b[38;5;241m.\u001b[39mcpu())\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[4], line 166\u001b[0m, in \u001b[0;36mModel.forward\u001b[0;34m(self, x_enc, x_mark_enc, x_dec, x_mark_dec, mask)\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x_enc, x_mark_enc, x_dec, x_mark_dec, mask\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    165\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtask \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclassification\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m--> 166\u001b[0m         f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclassify\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_enc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_mark_enc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_dec\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_mark_dec\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    167\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    168\u001b[0m         dec_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforecast(x_enc, x_mark_enc, x_dec, x_mark_dec)\n",
      "Cell \u001b[0;32mIn[4], line 287\u001b[0m, in \u001b[0;36mModel.classify\u001b[0;34m(self, x_enc, x_mark_enc, x_dec, x_mark_dec)\u001b[0m\n\u001b[1;32m    284\u001b[0m enc_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreprogramming_layer(enc_out, source_embeddings, source_embeddings)\n\u001b[1;32m    286\u001b[0m llama_enc_out \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([prompt_embeddings, enc_out], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m--> 287\u001b[0m dec_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mllm_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mllama_enc_out\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mlast_hidden_state\n\u001b[1;32m    289\u001b[0m dec_out \u001b[38;5;241m=\u001b[39m dec_out[:, :, :\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39md_ff]\n\u001b[1;32m    291\u001b[0m dec_out \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mreshape(dec_out, (\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, n_vars, dec_out\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m], dec_out\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]))\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/utils/generic.py:969\u001b[0m, in \u001b[0;36mcan_return_tuple.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    966\u001b[0m     set_attribute_for_modules(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_is_top_level_module\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    968\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 969\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    970\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_requested_to_return_tuple \u001b[38;5;129;01mor\u001b[39;00m (is_configured_to_return_tuple \u001b[38;5;129;01mand\u001b[39;00m is_top_level_module):\n\u001b[1;32m    971\u001b[0m         output \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mto_tuple()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:453\u001b[0m, in \u001b[0;36mLlamaModel.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, cache_position, **flash_attn_kwargs)\u001b[0m\n\u001b[1;32m    450\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states:\n\u001b[1;32m    451\u001b[0m     all_hidden_states \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (hidden_states,)\n\u001b[0;32m--> 453\u001b[0m layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    454\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    455\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    456\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    457\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    458\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    459\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    460\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    461\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    462\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mflash_attn_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    463\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    465\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    467\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/modeling_layers.py:48\u001b[0m, in \u001b[0;36mGradientCheckpointingLayer.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgradient_checkpointing \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining:\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(partial(\u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs), \u001b[38;5;241m*\u001b[39margs)\n\u001b[0;32m---> 48\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:324\u001b[0m, in \u001b[0;36mLlamaDecoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, position_embeddings, **kwargs)\u001b[0m\n\u001b[1;32m    322\u001b[0m residual \u001b[38;5;241m=\u001b[39m hidden_states\n\u001b[1;32m    323\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpost_attention_layernorm(hidden_states)\n\u001b[0;32m--> 324\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmlp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    325\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n\u001b[1;32m    327\u001b[0m outputs \u001b[38;5;241m=\u001b[39m (hidden_states,)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:162\u001b[0m, in \u001b[0;36mLlamaMLP.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m--> 162\u001b[0m     down_proj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdown_proj(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mact_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgate_proj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mup_proj(x))\n\u001b[1;32m    163\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m down_proj\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/activation.py:432\u001b[0m, in \u001b[0;36mSiLU.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    431\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 432\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msilu\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/functional.py:2380\u001b[0m, in \u001b[0;36msilu\u001b[0;34m(input, inplace)\u001b[0m\n\u001b[1;32m   2378\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inplace:\n\u001b[1;32m   2379\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_nn\u001b[38;5;241m.\u001b[39msilu_(\u001b[38;5;28minput\u001b[39m)\n\u001b[0;32m-> 2380\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msilu\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 886.00 MiB. GPU 0 has a total capacity of 23.68 GiB of which 390.69 MiB is free. Including non-PyTorch memory, this process has 23.10 GiB memory in use. Of the allocated memory 18.97 GiB is allocated by PyTorch, and 2.86 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "class_mod.eval()\n",
    "\n",
    "#Normalize test data\n",
    "test.columns = test.columns.astype(str)\n",
    "scaler = StandardScaler()\n",
    "normalized = scaler.fit_transform(test.iloc[:,1:].T).T\n",
    "X_test_n = pd.DataFrame(normalized, columns = test.iloc[:,1:].columns)\n",
    "\n",
    "stand = StandardScaler()#Data leack\n",
    "scaled = stand.fit_transform(X_test_n.iloc[:,1:])\n",
    "X_test_n = pd.DataFrame(scaled, columns = X_test_n.iloc[:,1:].columns)\n",
    "\n",
    "X_test = torch.from_numpy(X_test_n.to_numpy().astype('float32')).unsqueeze(-1) \n",
    "y_test = torch.from_numpy(test.iloc[:,0].to_numpy().astype('int64')).unsqueeze(1).unsqueeze(1)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "X_test = X_test.to(device)\n",
    "y_test = y_test.to(device)\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    y_pred = class_mod(X_test, None, None, None)\n",
    "\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "test_acc = accuracy_score(y_test.squeeze().cpu(), torch.argmax(y_pred, dim=1).squeeze().cpu())\n",
    "test_f1 = f1_score(y_test.squeeze().cpu(), torch.argmax(y_pred, dim=1).squeeze().cpu(), average='weighted')\n",
    "\n",
    "\n",
    "print(f\"Test Accuracy {test_acc:.6f} — Test F1 {test_f1:.6f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### New config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "configs = Details(\n",
    "    task = 'classification',\n",
    "    pred_len = 4,      \n",
    "    seq_len  = 268,\n",
    "    d_ff = 32,\n",
    "    patch_len= 16, \n",
    "    stride = 8, \n",
    "    llm_layers = 8, #Increased \n",
    "    description = \"This dataset contains near-infrared (NIR) spectroscopy measurements in a \" \\\n",
    "                  \"phosphate-buffered saline (PBS) solution. Each spectrum includes absorbance values across \" \\\n",
    "                  \"different wavelengths. The goal is to predict the classify each spectra given the different wavelengths \" ,\n",
    "    dropout = 0.2, #Increased\n",
    "    n_heads = 4, #Increased\n",
    "    d_model = 16,\n",
    "    enc_in = 1,\n",
    "    d_llm = 4096,\n",
    "    llm_model = 'LLAMA',\n",
    "    lr = 0.0001\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b98fcf3f5a15459e9524fd598f9f02e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  1 — train Accuracy : 0.210744 - train F1 0.155635 — val Accuracy 0.185185 - val F1 0.071685\n",
      "Epoch  2 — train Accuracy : 0.252066 - train F1 0.248919 — val Accuracy 0.222222 - val F1 0.141975\n",
      "Epoch  3 — train Accuracy : 0.289256 - train F1 0.260408 — val Accuracy 0.296296 - val F1 0.135450\n",
      "Epoch  4 — train Accuracy : 0.227273 - train F1 0.210033 — val Accuracy 0.296296 - val F1 0.135450\n",
      "Epoch  5 — train Accuracy : 0.239669 - train F1 0.172658 — val Accuracy 0.296296 - val F1 0.135450\n",
      "Epoch  6 — train Accuracy : 0.272727 - train F1 0.164719 — val Accuracy 0.296296 - val F1 0.135450\n",
      "Epoch  7 — train Accuracy : 0.264463 - train F1 0.154594 — val Accuracy 0.296296 - val F1 0.135450\n",
      "Epoch  8 — train Accuracy : 0.260331 - train F1 0.146043 — val Accuracy 0.296296 - val F1 0.135450\n",
      "Epoch  9 — train Accuracy : 0.231405 - train F1 0.202223 — val Accuracy 0.296296 - val F1 0.181481\n",
      "Epoch 10 — train Accuracy : 0.260331 - train F1 0.170002 — val Accuracy 0.296296 - val F1 0.135450\n",
      "Epoch 11 — train Accuracy : 0.305785 - train F1 0.189117 — val Accuracy 0.296296 - val F1 0.135450\n",
      "Epoch 12 — train Accuracy : 0.264463 - train F1 0.171271 — val Accuracy 0.296296 - val F1 0.135450\n",
      "Epoch 13 — train Accuracy : 0.260331 - train F1 0.153892 — val Accuracy 0.296296 - val F1 0.135450\n",
      "Epoch 14 — train Accuracy : 0.243802 - train F1 0.144773 — val Accuracy 0.296296 - val F1 0.135450\n",
      "Epoch 15 — train Accuracy : 0.293388 - train F1 0.173277 — val Accuracy 0.296296 - val F1 0.135450\n",
      "Epoch 16 — train Accuracy : 0.376033 - train F1 0.369496 — val Accuracy 0.629630 - val F1 0.625687\n",
      "Epoch 17 — train Accuracy : 0.603306 - train F1 0.603143 — val Accuracy 0.703704 - val F1 0.684437\n",
      "Epoch 18 — train Accuracy : 0.644628 - train F1 0.644105 — val Accuracy 0.814815 - val F1 0.811966\n",
      "Epoch 19 — train Accuracy : 0.714876 - train F1 0.712527 — val Accuracy 0.851852 - val F1 0.850587\n",
      "Epoch 20 — train Accuracy : 0.714876 - train F1 0.711369 — val Accuracy 0.814815 - val F1 0.802442\n",
      "Epoch 21 — train Accuracy : 0.785124 - train F1 0.782297 — val Accuracy 0.814815 - val F1 0.811966\n",
      "Epoch 22 — train Accuracy : 0.780992 - train F1 0.780087 — val Accuracy 0.814815 - val F1 0.811966\n",
      "Epoch 23 — train Accuracy : 0.801653 - train F1 0.801600 — val Accuracy 0.814815 - val F1 0.813809\n",
      "Epoch 24 — train Accuracy : 0.747934 - train F1 0.746170 — val Accuracy 0.740741 - val F1 0.740209\n",
      "Epoch 25 — train Accuracy : 0.809917 - train F1 0.810417 — val Accuracy 0.851852 - val F1 0.849003\n",
      "Epoch 26 — train Accuracy : 0.826446 - train F1 0.825154 — val Accuracy 0.814815 - val F1 0.809659\n",
      "Epoch 27 — train Accuracy : 0.801653 - train F1 0.800961 — val Accuracy 0.888889 - val F1 0.887883\n",
      "Epoch 28 — train Accuracy : 0.822314 - train F1 0.820972 — val Accuracy 0.888889 - val F1 0.887883\n",
      "Epoch 29 — train Accuracy : 0.838843 - train F1 0.838355 — val Accuracy 0.925926 - val F1 0.925926\n",
      "Epoch 30 — train Accuracy : 0.842975 - train F1 0.842614 — val Accuracy 0.888889 - val F1 0.888889\n",
      "Epoch 31 — train Accuracy : 0.805785 - train F1 0.802592 — val Accuracy 0.851852 - val F1 0.850587\n",
      "Epoch 32 — train Accuracy : 0.851240 - train F1 0.850904 — val Accuracy 0.851852 - val F1 0.850587\n",
      "Epoch 33 — train Accuracy : 0.822314 - train F1 0.819527 — val Accuracy 0.888889 - val F1 0.886572\n",
      "Epoch 34 — train Accuracy : 0.851240 - train F1 0.851233 — val Accuracy 0.851852 - val F1 0.850587\n",
      "Epoch 35 — train Accuracy : 0.838843 - train F1 0.837796 — val Accuracy 0.851852 - val F1 0.850587\n",
      "Epoch 36 — train Accuracy : 0.859504 - train F1 0.859548 — val Accuracy 0.888889 - val F1 0.887969\n",
      "Epoch 37 — train Accuracy : 0.871901 - train F1 0.871181 — val Accuracy 0.925926 - val F1 0.925926\n",
      "Epoch 38 — train Accuracy : 0.888430 - train F1 0.888235 — val Accuracy 0.888889 - val F1 0.887883\n",
      "Epoch 39 — train Accuracy : 0.851240 - train F1 0.851159 — val Accuracy 0.888889 - val F1 0.888630\n",
      "Epoch 40 — train Accuracy : 0.814050 - train F1 0.812557 — val Accuracy 0.962963 - val F1 0.962963\n",
      "Epoch 41 — train Accuracy : 0.855372 - train F1 0.854331 — val Accuracy 0.925926 - val F1 0.925926\n",
      "Epoch 42 — train Accuracy : 0.855372 - train F1 0.854229 — val Accuracy 0.851852 - val F1 0.849794\n",
      "Epoch 43 — train Accuracy : 0.876033 - train F1 0.875717 — val Accuracy 0.851852 - val F1 0.850587\n",
      "Epoch 44 — train Accuracy : 0.855372 - train F1 0.855575 — val Accuracy 0.925926 - val F1 0.925667\n",
      "Epoch 45 — train Accuracy : 0.871901 - train F1 0.871593 — val Accuracy 0.888889 - val F1 0.886572\n",
      "Epoch 46 — train Accuracy : 0.896694 - train F1 0.894969 — val Accuracy 0.888889 - val F1 0.888630\n",
      "Epoch 47 — train Accuracy : 0.871901 - train F1 0.871574 — val Accuracy 0.925926 - val F1 0.925667\n",
      "Epoch 48 — train Accuracy : 0.871901 - train F1 0.870902 — val Accuracy 0.888889 - val F1 0.888630\n",
      "Epoch 49 — train Accuracy : 0.867769 - train F1 0.867623 — val Accuracy 0.925926 - val F1 0.925926\n",
      "Epoch 50 — train Accuracy : 0.880165 - train F1 0.879099 — val Accuracy 0.962963 - val F1 0.962628\n",
      "Epoch 51 — train Accuracy : 0.876033 - train F1 0.874250 — val Accuracy 0.888889 - val F1 0.890473\n",
      "Epoch 52 — train Accuracy : 0.880165 - train F1 0.880113 — val Accuracy 0.925926 - val F1 0.925926\n",
      "Epoch 53 — train Accuracy : 0.884298 - train F1 0.883971 — val Accuracy 0.925926 - val F1 0.923868\n",
      "Epoch 54 — train Accuracy : 0.851240 - train F1 0.849129 — val Accuracy 0.962963 - val F1 0.962963\n",
      "Epoch 55 — train Accuracy : 0.896694 - train F1 0.896479 — val Accuracy 1.000000 - val F1 1.000000\n",
      "Epoch 56 — train Accuracy : 0.900826 - train F1 0.900487 — val Accuracy 0.962963 - val F1 0.962628\n",
      "Epoch 57 — train Accuracy : 0.892562 - train F1 0.891949 — val Accuracy 0.962963 - val F1 0.962963\n",
      "Epoch 58 — train Accuracy : 0.938017 - train F1 0.937632 — val Accuracy 0.888889 - val F1 0.884141\n",
      "Epoch 59 — train Accuracy : 0.933884 - train F1 0.933731 — val Accuracy 0.925926 - val F1 0.925667\n",
      "Epoch 60 — train Accuracy : 0.958678 - train F1 0.958505 — val Accuracy 0.888889 - val F1 0.886572\n",
      "Epoch 61 — train Accuracy : 0.913223 - train F1 0.913113 — val Accuracy 0.925926 - val F1 0.925667\n",
      "Epoch 62 — train Accuracy : 0.904959 - train F1 0.904836 — val Accuracy 0.925926 - val F1 0.925667\n",
      "Epoch 63 — train Accuracy : 0.933884 - train F1 0.933678 — val Accuracy 0.962963 - val F1 0.962963\n",
      "Epoch 64 — train Accuracy : 0.917355 - train F1 0.917016 — val Accuracy 1.000000 - val F1 1.000000\n",
      "Epoch 65 — train Accuracy : 0.929752 - train F1 0.928707 — val Accuracy 0.925926 - val F1 0.925332\n",
      "Epoch 66 — train Accuracy : 0.938017 - train F1 0.937840 — val Accuracy 0.962963 - val F1 0.962963\n",
      "Epoch 67 — train Accuracy : 0.904959 - train F1 0.904536 — val Accuracy 0.962963 - val F1 0.962963\n",
      "Epoch 68 — train Accuracy : 0.938017 - train F1 0.937527 — val Accuracy 1.000000 - val F1 1.000000\n",
      "Epoch 69 — train Accuracy : 0.929752 - train F1 0.929702 — val Accuracy 0.925926 - val F1 0.925332\n",
      "Epoch 70 — train Accuracy : 0.925620 - train F1 0.925256 — val Accuracy 1.000000 - val F1 1.000000\n",
      "Epoch 71 — train Accuracy : 0.933884 - train F1 0.933917 — val Accuracy 0.962963 - val F1 0.962963\n",
      "Epoch 72 — train Accuracy : 0.942149 - train F1 0.942394 — val Accuracy 1.000000 - val F1 1.000000\n",
      "Epoch 73 — train Accuracy : 0.946281 - train F1 0.946183 — val Accuracy 1.000000 - val F1 1.000000\n",
      "Epoch 74 — train Accuracy : 0.909091 - train F1 0.909067 — val Accuracy 1.000000 - val F1 1.000000\n",
      "Epoch 75 — train Accuracy : 0.933884 - train F1 0.933091 — val Accuracy 1.000000 - val F1 1.000000\n",
      "Epoch 76 — train Accuracy : 0.938017 - train F1 0.937362 — val Accuracy 1.000000 - val F1 1.000000\n",
      "Epoch 77 — train Accuracy : 0.921488 - train F1 0.919934 — val Accuracy 1.000000 - val F1 1.000000\n",
      "Epoch 78 — train Accuracy : 0.942149 - train F1 0.942114 — val Accuracy 1.000000 - val F1 1.000000\n",
      "Epoch 79 — train Accuracy : 0.942149 - train F1 0.942201 — val Accuracy 1.000000 - val F1 1.000000\n",
      "Epoch 80 — train Accuracy : 0.971074 - train F1 0.970959 — val Accuracy 1.000000 - val F1 1.000000\n",
      "Epoch 81 — train Accuracy : 0.954545 - train F1 0.954949 — val Accuracy 1.000000 - val F1 1.000000\n",
      "Epoch 82 — train Accuracy : 0.921488 - train F1 0.920499 — val Accuracy 1.000000 - val F1 1.000000\n",
      "Epoch 83 — train Accuracy : 0.921488 - train F1 0.921886 — val Accuracy 1.000000 - val F1 1.000000\n",
      "Epoch 84 — train Accuracy : 0.950413 - train F1 0.950527 — val Accuracy 1.000000 - val F1 1.000000\n",
      "Epoch 85 — train Accuracy : 0.950413 - train F1 0.950526 — val Accuracy 1.000000 - val F1 1.000000\n",
      "Epoch 86 — train Accuracy : 0.971074 - train F1 0.971205 — val Accuracy 1.000000 - val F1 1.000000\n",
      "Epoch 87 — train Accuracy : 0.942149 - train F1 0.943045 — val Accuracy 1.000000 - val F1 1.000000\n",
      "Epoch 88 — train Accuracy : 0.946281 - train F1 0.946271 — val Accuracy 1.000000 - val F1 1.000000\n",
      "Epoch 89 — train Accuracy : 0.929752 - train F1 0.929911 — val Accuracy 1.000000 - val F1 1.000000\n",
      "Epoch 90 — train Accuracy : 0.938017 - train F1 0.938313 — val Accuracy 1.000000 - val F1 1.000000\n",
      "Epoch 91 — train Accuracy : 0.913223 - train F1 0.912657 — val Accuracy 0.962963 - val F1 0.962628\n",
      "Epoch 92 — train Accuracy : 0.896694 - train F1 0.896250 — val Accuracy 1.000000 - val F1 1.000000\n",
      "Epoch 93 — train Accuracy : 0.909091 - train F1 0.909689 — val Accuracy 1.000000 - val F1 1.000000\n",
      "Epoch 94 — train Accuracy : 0.954545 - train F1 0.954962 — val Accuracy 1.000000 - val F1 1.000000\n",
      "Epoch 95 — train Accuracy : 0.929752 - train F1 0.929519 — val Accuracy 1.000000 - val F1 1.000000\n",
      "Epoch 96 — train Accuracy : 0.950413 - train F1 0.950327 — val Accuracy 0.962963 - val F1 0.962963\n",
      "Epoch 97 — train Accuracy : 0.909091 - train F1 0.908559 — val Accuracy 1.000000 - val F1 1.000000\n",
      "Epoch 98 — train Accuracy : 0.925620 - train F1 0.926118 — val Accuracy 0.925926 - val F1 0.923810\n",
      "Epoch 99 — train Accuracy : 0.925620 - train F1 0.926429 — val Accuracy 1.000000 - val F1 1.000000\n",
      "Epoch 100 — train Accuracy : 0.933884 - train F1 0.933227 — val Accuracy 1.000000 - val F1 1.000000\n"
     ]
    }
   ],
   "source": [
    "train, validation = model_selection.train_test_split(cancer, test_size=0.1, random_state=1, stratify= cancer['outcome'])\n",
    "\n",
    "model ,train_rmse, train_r2, val_rmse, val_r2, stand  = train_classification_model(configs, train, validation, 100, 8, True, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### New config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "configs = Details(\n",
    "    task = 'classification',\n",
    "    pred_len = 4,      \n",
    "    seq_len  = 268,\n",
    "    d_ff = 32,\n",
    "    patch_len= 16, \n",
    "    stride = 8, \n",
    "    llm_layers = 10, #Increased \n",
    "    description = \"This dataset contains near-infrared (NIR) spectroscopy measurements in a \" \\\n",
    "                  \"phosphate-buffered saline (PBS) solution. Each spectrum includes absorbance values across \" \\\n",
    "                  \"different wavelengths. The goal is to predict the classify each spectra given the different wavelengths \" ,\n",
    "    dropout = 0.4, #Increased\n",
    "    n_heads = 6, #Increased\n",
    "    d_model = 16,\n",
    "    enc_in = 1,\n",
    "    d_llm = 4096,\n",
    "    llm_model = 'LLAMA',\n",
    "    lr = 0.00001 # decreased\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2618ebc7df8e44518fe754e29e164673",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  1 — train Accuracy : 0.231405 - train F1 0.209960 — val Accuracy 0.259259 - val F1 0.106754\n",
      "Epoch  2 — train Accuracy : 0.264463 - train F1 0.245976 — val Accuracy 0.407407 - val F1 0.294149\n",
      "Epoch  3 — train Accuracy : 0.247934 - train F1 0.232271 — val Accuracy 0.259259 - val F1 0.125701\n",
      "Epoch  4 — train Accuracy : 0.272727 - train F1 0.240145 — val Accuracy 0.370370 - val F1 0.315751\n",
      "Epoch  5 — train Accuracy : 0.243802 - train F1 0.234338 — val Accuracy 0.333333 - val F1 0.216524\n",
      "Epoch  6 — train Accuracy : 0.305785 - train F1 0.285988 — val Accuracy 0.518519 - val F1 0.454145\n",
      "Epoch  7 — train Accuracy : 0.297521 - train F1 0.275464 — val Accuracy 0.518519 - val F1 0.430870\n",
      "Epoch  8 — train Accuracy : 0.421488 - train F1 0.411456 — val Accuracy 0.666667 - val F1 0.630645\n",
      "Epoch  9 — train Accuracy : 0.454545 - train F1 0.448612 — val Accuracy 0.703704 - val F1 0.694723\n",
      "Epoch 10 — train Accuracy : 0.516529 - train F1 0.522087 — val Accuracy 0.740741 - val F1 0.737596\n",
      "Epoch 11 — train Accuracy : 0.553719 - train F1 0.556768 — val Accuracy 0.814815 - val F1 0.809318\n",
      "Epoch 12 — train Accuracy : 0.516529 - train F1 0.516498 — val Accuracy 0.814815 - val F1 0.809318\n",
      "Epoch 13 — train Accuracy : 0.566116 - train F1 0.564789 — val Accuracy 0.851852 - val F1 0.846355\n",
      "Epoch 14 — train Accuracy : 0.599174 - train F1 0.600670 — val Accuracy 0.851852 - val F1 0.846355\n",
      "Epoch 15 — train Accuracy : 0.570248 - train F1 0.574315 — val Accuracy 0.851852 - val F1 0.846355\n",
      "Epoch 16 — train Accuracy : 0.586777 - train F1 0.581294 — val Accuracy 0.851852 - val F1 0.850970\n",
      "Epoch 17 — train Accuracy : 0.533058 - train F1 0.531041 — val Accuracy 0.851852 - val F1 0.846355\n",
      "Epoch 18 — train Accuracy : 0.607438 - train F1 0.601171 — val Accuracy 0.888889 - val F1 0.887409\n",
      "Epoch 19 — train Accuracy : 0.595041 - train F1 0.594113 — val Accuracy 0.851852 - val F1 0.850970\n",
      "Epoch 20 — train Accuracy : 0.611570 - train F1 0.608939 — val Accuracy 0.851852 - val F1 0.850970\n",
      "Epoch 21 — train Accuracy : 0.648760 - train F1 0.646259 — val Accuracy 0.851852 - val F1 0.850970\n",
      "Epoch 22 — train Accuracy : 0.661157 - train F1 0.654768 — val Accuracy 0.851852 - val F1 0.850970\n",
      "Epoch 23 — train Accuracy : 0.669421 - train F1 0.668861 — val Accuracy 0.888889 - val F1 0.886610\n",
      "Epoch 24 — train Accuracy : 0.623967 - train F1 0.619860 — val Accuracy 0.851852 - val F1 0.846355\n",
      "Epoch 25 — train Accuracy : 0.685950 - train F1 0.684457 — val Accuracy 0.814815 - val F1 0.810401\n",
      "Epoch 26 — train Accuracy : 0.677686 - train F1 0.677275 — val Accuracy 0.888889 - val F1 0.882509\n",
      "Epoch 27 — train Accuracy : 0.669421 - train F1 0.666987 — val Accuracy 0.851852 - val F1 0.846355\n",
      "Epoch 28 — train Accuracy : 0.677686 - train F1 0.676575 — val Accuracy 0.925926 - val F1 0.923868\n",
      "Epoch 29 — train Accuracy : 0.698347 - train F1 0.695668 — val Accuracy 0.888889 - val F1 0.882509\n",
      "Epoch 30 — train Accuracy : 0.719008 - train F1 0.715296 — val Accuracy 0.888889 - val F1 0.882509\n",
      "Epoch 31 — train Accuracy : 0.652893 - train F1 0.650059 — val Accuracy 0.925926 - val F1 0.923868\n",
      "Epoch 32 — train Accuracy : 0.698347 - train F1 0.693800 — val Accuracy 0.925926 - val F1 0.923868\n",
      "Epoch 33 — train Accuracy : 0.756198 - train F1 0.756134 — val Accuracy 0.888889 - val F1 0.882509\n",
      "Epoch 34 — train Accuracy : 0.723140 - train F1 0.719538 — val Accuracy 0.888889 - val F1 0.887883\n",
      "Epoch 35 — train Accuracy : 0.673554 - train F1 0.668256 — val Accuracy 0.888889 - val F1 0.882509\n",
      "Epoch 36 — train Accuracy : 0.772727 - train F1 0.773704 — val Accuracy 0.888889 - val F1 0.882509\n",
      "Epoch 37 — train Accuracy : 0.793388 - train F1 0.792824 — val Accuracy 0.851852 - val F1 0.846355\n",
      "Epoch 38 — train Accuracy : 0.685950 - train F1 0.682016 — val Accuracy 0.925926 - val F1 0.925265\n",
      "Epoch 39 — train Accuracy : 0.690083 - train F1 0.682155 — val Accuracy 0.888889 - val F1 0.887883\n",
      "Epoch 40 — train Accuracy : 0.780992 - train F1 0.779387 — val Accuracy 0.925926 - val F1 0.923868\n",
      "Epoch 41 — train Accuracy : 0.702479 - train F1 0.698890 — val Accuracy 0.925926 - val F1 0.925926\n",
      "Epoch 42 — train Accuracy : 0.731405 - train F1 0.729600 — val Accuracy 0.888889 - val F1 0.887883\n",
      "Epoch 43 — train Accuracy : 0.735537 - train F1 0.732283 — val Accuracy 0.925926 - val F1 0.925926\n",
      "Epoch 44 — train Accuracy : 0.797521 - train F1 0.796480 — val Accuracy 0.888889 - val F1 0.888889\n",
      "Epoch 45 — train Accuracy : 0.797521 - train F1 0.796042 — val Accuracy 0.888889 - val F1 0.882509\n",
      "Epoch 46 — train Accuracy : 0.760331 - train F1 0.755819 — val Accuracy 0.925926 - val F1 0.923868\n",
      "Epoch 47 — train Accuracy : 0.706612 - train F1 0.705888 — val Accuracy 0.888889 - val F1 0.882509\n",
      "Epoch 48 — train Accuracy : 0.764463 - train F1 0.761080 — val Accuracy 0.925926 - val F1 0.925265\n",
      "Epoch 49 — train Accuracy : 0.805785 - train F1 0.804610 — val Accuracy 0.888889 - val F1 0.887883\n",
      "Epoch 50 — train Accuracy : 0.739669 - train F1 0.736681 — val Accuracy 0.888889 - val F1 0.882509\n",
      "Epoch 51 — train Accuracy : 0.714876 - train F1 0.703349 — val Accuracy 0.888889 - val F1 0.887883\n",
      "Epoch 52 — train Accuracy : 0.727273 - train F1 0.723348 — val Accuracy 0.925926 - val F1 0.925926\n",
      "Epoch 53 — train Accuracy : 0.752066 - train F1 0.750987 — val Accuracy 0.888889 - val F1 0.887883\n",
      "Epoch 54 — train Accuracy : 0.768595 - train F1 0.767229 — val Accuracy 0.888889 - val F1 0.887883\n",
      "Epoch 55 — train Accuracy : 0.768595 - train F1 0.768014 — val Accuracy 0.888889 - val F1 0.887409\n",
      "Epoch 56 — train Accuracy : 0.780992 - train F1 0.780521 — val Accuracy 0.888889 - val F1 0.887883\n",
      "Epoch 57 — train Accuracy : 0.785124 - train F1 0.783770 — val Accuracy 0.925926 - val F1 0.923868\n",
      "Epoch 58 — train Accuracy : 0.797521 - train F1 0.796602 — val Accuracy 0.888889 - val F1 0.887883\n",
      "Epoch 59 — train Accuracy : 0.776860 - train F1 0.774547 — val Accuracy 0.925926 - val F1 0.923868\n",
      "Epoch 60 — train Accuracy : 0.764463 - train F1 0.763074 — val Accuracy 0.925926 - val F1 0.923868\n",
      "Epoch 61 — train Accuracy : 0.768595 - train F1 0.765331 — val Accuracy 1.000000 - val F1 1.000000\n",
      "Epoch 62 — train Accuracy : 0.752066 - train F1 0.748083 — val Accuracy 0.888889 - val F1 0.887883\n",
      "Epoch 63 — train Accuracy : 0.793388 - train F1 0.790613 — val Accuracy 0.962963 - val F1 0.962628\n",
      "Epoch 64 — train Accuracy : 0.785124 - train F1 0.783617 — val Accuracy 0.888889 - val F1 0.887883\n",
      "Epoch 65 — train Accuracy : 0.785124 - train F1 0.785867 — val Accuracy 0.888889 - val F1 0.888889\n",
      "Epoch 66 — train Accuracy : 0.797521 - train F1 0.797840 — val Accuracy 0.925926 - val F1 0.923868\n",
      "Epoch 67 — train Accuracy : 0.768595 - train F1 0.764452 — val Accuracy 0.925926 - val F1 0.923868\n",
      "Epoch 68 — train Accuracy : 0.801653 - train F1 0.799651 — val Accuracy 0.925926 - val F1 0.923868\n",
      "Epoch 69 — train Accuracy : 0.768595 - train F1 0.770599 — val Accuracy 0.925926 - val F1 0.923868\n",
      "Epoch 70 — train Accuracy : 0.768595 - train F1 0.765607 — val Accuracy 0.925926 - val F1 0.925926\n",
      "Epoch 71 — train Accuracy : 0.826446 - train F1 0.827387 — val Accuracy 0.925926 - val F1 0.925926\n",
      "Epoch 72 — train Accuracy : 0.789256 - train F1 0.787172 — val Accuracy 0.925926 - val F1 0.923868\n",
      "Epoch 73 — train Accuracy : 0.814050 - train F1 0.815351 — val Accuracy 0.925926 - val F1 0.923868\n",
      "Epoch 74 — train Accuracy : 0.793388 - train F1 0.793426 — val Accuracy 0.962963 - val F1 0.962628\n",
      "Epoch 75 — train Accuracy : 0.805785 - train F1 0.803575 — val Accuracy 0.925926 - val F1 0.923868\n",
      "Epoch 76 — train Accuracy : 0.826446 - train F1 0.827058 — val Accuracy 1.000000 - val F1 1.000000\n",
      "Epoch 77 — train Accuracy : 0.814050 - train F1 0.814482 — val Accuracy 0.925926 - val F1 0.925926\n",
      "Epoch 78 — train Accuracy : 0.760331 - train F1 0.761599 — val Accuracy 0.925926 - val F1 0.925926\n",
      "Epoch 79 — train Accuracy : 0.752066 - train F1 0.750698 — val Accuracy 0.925926 - val F1 0.923868\n",
      "Epoch 80 — train Accuracy : 0.785124 - train F1 0.784376 — val Accuracy 0.925926 - val F1 0.925926\n",
      "Epoch 81 — train Accuracy : 0.793388 - train F1 0.795465 — val Accuracy 0.962963 - val F1 0.962963\n",
      "Epoch 82 — train Accuracy : 0.793388 - train F1 0.792544 — val Accuracy 0.962963 - val F1 0.962963\n",
      "Epoch 83 — train Accuracy : 0.780992 - train F1 0.781145 — val Accuracy 0.962963 - val F1 0.962963\n",
      "Epoch 84 — train Accuracy : 0.814050 - train F1 0.815046 — val Accuracy 0.925926 - val F1 0.923868\n",
      "Epoch 85 — train Accuracy : 0.838843 - train F1 0.837247 — val Accuracy 0.925926 - val F1 0.923868\n",
      "Epoch 86 — train Accuracy : 0.809917 - train F1 0.807833 — val Accuracy 0.962963 - val F1 0.962963\n",
      "Epoch 87 — train Accuracy : 0.822314 - train F1 0.820590 — val Accuracy 0.925926 - val F1 0.923868\n",
      "Epoch 88 — train Accuracy : 0.805785 - train F1 0.806013 — val Accuracy 0.925926 - val F1 0.923868\n",
      "Epoch 89 — train Accuracy : 0.772727 - train F1 0.770253 — val Accuracy 0.925926 - val F1 0.923868\n",
      "Epoch 90 — train Accuracy : 0.780992 - train F1 0.777104 — val Accuracy 1.000000 - val F1 1.000000\n",
      "Epoch 91 — train Accuracy : 0.780992 - train F1 0.781197 — val Accuracy 0.962963 - val F1 0.962628\n",
      "Epoch 92 — train Accuracy : 0.760331 - train F1 0.758815 — val Accuracy 0.962963 - val F1 0.962628\n",
      "Epoch 93 — train Accuracy : 0.768595 - train F1 0.767439 — val Accuracy 0.925926 - val F1 0.925926\n",
      "Epoch 94 — train Accuracy : 0.785124 - train F1 0.779662 — val Accuracy 1.000000 - val F1 1.000000\n",
      "Epoch 95 — train Accuracy : 0.764463 - train F1 0.761472 — val Accuracy 0.962963 - val F1 0.962628\n",
      "Epoch 96 — train Accuracy : 0.797521 - train F1 0.795476 — val Accuracy 1.000000 - val F1 1.000000\n",
      "Epoch 97 — train Accuracy : 0.768595 - train F1 0.767561 — val Accuracy 0.962963 - val F1 0.962963\n",
      "Epoch 98 — train Accuracy : 0.780992 - train F1 0.779238 — val Accuracy 0.925926 - val F1 0.923868\n",
      "Epoch 99 — train Accuracy : 0.805785 - train F1 0.805598 — val Accuracy 1.000000 - val F1 1.000000\n",
      "Epoch 100 — train Accuracy : 0.785124 - train F1 0.782375 — val Accuracy 0.925926 - val F1 0.925926\n"
     ]
    }
   ],
   "source": [
    "train, validation = model_selection.train_test_split(cancer, test_size=0.1, random_state=1, stratify= cancer['outcome'])\n",
    "\n",
    "model ,train_rmse, train_r2, val_rmse, val_r2, stand  = train_classification_model(configs, train, validation, 100, 8, True, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### New config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "configs = Details(\n",
    "    task = 'classification',\n",
    "    pred_len = 4,      \n",
    "    seq_len  = 268,\n",
    "    d_ff = 32,\n",
    "    patch_len= 16, \n",
    "    stride = 8, \n",
    "    llm_layers = 10, \n",
    "    description = \"This dataset contains near-infrared (NIR) spectroscopy measurements in a \" \\\n",
    "                  \"phosphate-buffered saline (PBS) solution. Each spectrum includes absorbance values across \" \\\n",
    "                  \"different wavelengths. The goal is to predict the classify each spectra given the different wavelengths \" ,\n",
    "    dropout = 0.2, \n",
    "    n_heads = 6, \n",
    "    d_model = 16,\n",
    "    enc_in = 1,\n",
    "    d_llm = 4096,\n",
    "    llm_model = 'LLAMA',\n",
    "    lr = 0.001 # Increased\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf2027288d4f4d84b3302b5e18082985",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  1 — train Accuracy : 0.347107 - train F1 0.346976 — val Accuracy 0.555556 - val F1 0.518695\n",
      "Epoch  2 — train Accuracy : 0.665289 - train F1 0.665575 — val Accuracy 0.777778 - val F1 0.779836\n",
      "Epoch  3 — train Accuracy : 0.752066 - train F1 0.751024 — val Accuracy 0.777778 - val F1 0.766888\n",
      "Epoch  4 — train Accuracy : 0.768595 - train F1 0.766275 — val Accuracy 0.814815 - val F1 0.808495\n",
      "Epoch  5 — train Accuracy : 0.756198 - train F1 0.755588 — val Accuracy 0.814815 - val F1 0.809361\n",
      "Epoch  6 — train Accuracy : 0.801653 - train F1 0.798607 — val Accuracy 0.777778 - val F1 0.770621\n",
      "Epoch  7 — train Accuracy : 0.822314 - train F1 0.822577 — val Accuracy 0.777778 - val F1 0.777246\n",
      "Epoch  8 — train Accuracy : 0.793388 - train F1 0.790280 — val Accuracy 0.851852 - val F1 0.852465\n",
      "Epoch  9 — train Accuracy : 0.818182 - train F1 0.818070 — val Accuracy 0.740741 - val F1 0.734595\n",
      "Epoch 10 — train Accuracy : 0.805785 - train F1 0.804656 — val Accuracy 0.888889 - val F1 0.886572\n",
      "Epoch 11 — train Accuracy : 0.855372 - train F1 0.855073 — val Accuracy 0.888889 - val F1 0.886257\n",
      "Epoch 12 — train Accuracy : 0.859504 - train F1 0.857961 — val Accuracy 0.814815 - val F1 0.818070\n",
      "Epoch 13 — train Accuracy : 0.842975 - train F1 0.842250 — val Accuracy 0.851852 - val F1 0.852767\n",
      "Epoch 14 — train Accuracy : 0.842975 - train F1 0.842269 — val Accuracy 0.777778 - val F1 0.767593\n",
      "Epoch 15 — train Accuracy : 0.851240 - train F1 0.851764 — val Accuracy 0.851852 - val F1 0.847268\n",
      "Epoch 16 — train Accuracy : 0.855372 - train F1 0.854112 — val Accuracy 0.851852 - val F1 0.853872\n",
      "Epoch 17 — train Accuracy : 0.830579 - train F1 0.829161 — val Accuracy 0.851852 - val F1 0.849003\n",
      "Epoch 18 — train Accuracy : 0.863636 - train F1 0.864148 — val Accuracy 0.851852 - val F1 0.845085\n",
      "Epoch 19 — train Accuracy : 0.876033 - train F1 0.875638 — val Accuracy 0.851852 - val F1 0.849573\n",
      "Epoch 20 — train Accuracy : 0.826446 - train F1 0.825335 — val Accuracy 0.814815 - val F1 0.812087\n",
      "Epoch 21 — train Accuracy : 0.822314 - train F1 0.821291 — val Accuracy 0.888889 - val F1 0.882509\n",
      "Epoch 22 — train Accuracy : 0.847107 - train F1 0.848141 — val Accuracy 0.851852 - val F1 0.850371\n",
      "Epoch 23 — train Accuracy : 0.867769 - train F1 0.868490 — val Accuracy 0.851852 - val F1 0.850371\n",
      "Epoch 24 — train Accuracy : 0.847107 - train F1 0.846792 — val Accuracy 0.888889 - val F1 0.886572\n",
      "Epoch 25 — train Accuracy : 0.847107 - train F1 0.846736 — val Accuracy 0.814815 - val F1 0.809340\n",
      "Epoch 26 — train Accuracy : 0.809917 - train F1 0.808314 — val Accuracy 0.851852 - val F1 0.849832\n",
      "Epoch 27 — train Accuracy : 0.814050 - train F1 0.814353 — val Accuracy 0.814815 - val F1 0.810401\n",
      "Epoch 28 — train Accuracy : 0.809917 - train F1 0.809762 — val Accuracy 0.814815 - val F1 0.804855\n",
      "Epoch 29 — train Accuracy : 0.793388 - train F1 0.791708 — val Accuracy 0.851852 - val F1 0.851852\n",
      "Epoch 30 — train Accuracy : 0.822314 - train F1 0.821098 — val Accuracy 0.814815 - val F1 0.810066\n",
      "Epoch 31 — train Accuracy : 0.847107 - train F1 0.846620 — val Accuracy 0.814815 - val F1 0.816835\n",
      "Epoch 32 — train Accuracy : 0.851240 - train F1 0.849472 — val Accuracy 0.851852 - val F1 0.853872\n",
      "Epoch 33 — train Accuracy : 0.818182 - train F1 0.817898 — val Accuracy 0.777778 - val F1 0.776431\n",
      "Epoch 34 — train Accuracy : 0.834711 - train F1 0.833910 — val Accuracy 0.777778 - val F1 0.777241\n",
      "Epoch 35 — train Accuracy : 0.793388 - train F1 0.791922 — val Accuracy 0.814815 - val F1 0.812789\n",
      "Epoch 36 — train Accuracy : 0.859504 - train F1 0.858926 — val Accuracy 0.888889 - val F1 0.888131\n",
      "Epoch 37 — train Accuracy : 0.847107 - train F1 0.847328 — val Accuracy 0.888889 - val F1 0.889804\n",
      "Epoch 38 — train Accuracy : 0.822314 - train F1 0.821575 — val Accuracy 0.814815 - val F1 0.818578\n",
      "Epoch 39 — train Accuracy : 0.834711 - train F1 0.834410 — val Accuracy 0.851852 - val F1 0.853086\n",
      "Epoch 40 — train Accuracy : 0.859504 - train F1 0.858696 — val Accuracy 0.740741 - val F1 0.742208\n",
      "Epoch 41 — train Accuracy : 0.855372 - train F1 0.854018 — val Accuracy 0.777778 - val F1 0.775130\n",
      "Epoch 42 — train Accuracy : 0.867769 - train F1 0.866813 — val Accuracy 0.925926 - val F1 0.926933\n",
      "Epoch 43 — train Accuracy : 0.859504 - train F1 0.859028 — val Accuracy 0.851852 - val F1 0.854125\n",
      "Epoch 44 — train Accuracy : 0.814050 - train F1 0.811339 — val Accuracy 0.851852 - val F1 0.854125\n",
      "Epoch 45 — train Accuracy : 0.793388 - train F1 0.793358 — val Accuracy 0.814815 - val F1 0.818578\n",
      "Epoch 46 — train Accuracy : 0.859504 - train F1 0.857368 — val Accuracy 0.851852 - val F1 0.852767\n",
      "Epoch 47 — train Accuracy : 0.838843 - train F1 0.838570 — val Accuracy 0.814815 - val F1 0.815513\n",
      "Epoch 48 — train Accuracy : 0.822314 - train F1 0.820299 — val Accuracy 0.777778 - val F1 0.777938\n",
      "Epoch 49 — train Accuracy : 0.871901 - train F1 0.871365 — val Accuracy 0.851852 - val F1 0.849832\n",
      "Epoch 50 — train Accuracy : 0.834711 - train F1 0.832243 — val Accuracy 0.814815 - val F1 0.812346\n",
      "Epoch 51 — train Accuracy : 0.859504 - train F1 0.856957 — val Accuracy 0.814815 - val F1 0.814302\n",
      "Epoch 52 — train Accuracy : 0.851240 - train F1 0.851858 — val Accuracy 0.777778 - val F1 0.785950\n",
      "Epoch 53 — train Accuracy : 0.888430 - train F1 0.887725 — val Accuracy 0.777778 - val F1 0.774675\n",
      "Epoch 54 — train Accuracy : 0.826446 - train F1 0.825021 — val Accuracy 0.814815 - val F1 0.817754\n",
      "Epoch 55 — train Accuracy : 0.826446 - train F1 0.824461 — val Accuracy 0.851852 - val F1 0.853909\n",
      "Epoch 56 — train Accuracy : 0.859504 - train F1 0.859420 — val Accuracy 0.851852 - val F1 0.845757\n",
      "Epoch 57 — train Accuracy : 0.842975 - train F1 0.842069 — val Accuracy 0.814815 - val F1 0.813246\n",
      "Epoch 58 — train Accuracy : 0.842975 - train F1 0.843026 — val Accuracy 0.740741 - val F1 0.751126\n",
      "Epoch 59 — train Accuracy : 0.855372 - train F1 0.855311 — val Accuracy 0.851852 - val F1 0.856974\n",
      "Epoch 60 — train Accuracy : 0.855372 - train F1 0.854831 — val Accuracy 0.777778 - val F1 0.780717\n",
      "Epoch 61 — train Accuracy : 0.884298 - train F1 0.884076 — val Accuracy 0.814815 - val F1 0.816723\n",
      "Epoch 62 — train Accuracy : 0.867769 - train F1 0.867602 — val Accuracy 0.814815 - val F1 0.812363\n",
      "Epoch 63 — train Accuracy : 0.855372 - train F1 0.855262 — val Accuracy 0.814815 - val F1 0.821597\n",
      "Epoch 64 — train Accuracy : 0.834711 - train F1 0.835073 — val Accuracy 0.851852 - val F1 0.852554\n",
      "Epoch 65 — train Accuracy : 0.842975 - train F1 0.840727 — val Accuracy 0.851852 - val F1 0.850371\n",
      "Epoch 66 — train Accuracy : 0.756198 - train F1 0.756802 — val Accuracy 0.629630 - val F1 0.593587\n",
      "Epoch 67 — train Accuracy : 0.756198 - train F1 0.756007 — val Accuracy 0.740741 - val F1 0.734188\n",
      "Epoch 68 — train Accuracy : 0.818182 - train F1 0.817488 — val Accuracy 0.851852 - val F1 0.852554\n",
      "Epoch 69 — train Accuracy : 0.826446 - train F1 0.826178 — val Accuracy 0.777778 - val F1 0.773364\n",
      "Epoch 70 — train Accuracy : 0.797521 - train F1 0.797317 — val Accuracy 0.888889 - val F1 0.882123\n",
      "Epoch 71 — train Accuracy : 0.822314 - train F1 0.821424 — val Accuracy 0.777778 - val F1 0.776834\n",
      "Epoch 72 — train Accuracy : 0.826446 - train F1 0.826063 — val Accuracy 0.703704 - val F1 0.712494\n",
      "Epoch 73 — train Accuracy : 0.851240 - train F1 0.849506 — val Accuracy 0.777778 - val F1 0.776759\n",
      "Epoch 74 — train Accuracy : 0.830579 - train F1 0.830389 — val Accuracy 0.740741 - val F1 0.735585\n",
      "Epoch 75 — train Accuracy : 0.830579 - train F1 0.831209 — val Accuracy 0.851852 - val F1 0.853650\n",
      "Epoch 76 — train Accuracy : 0.847107 - train F1 0.846999 — val Accuracy 0.851852 - val F1 0.849003\n",
      "Epoch 77 — train Accuracy : 0.851240 - train F1 0.850855 — val Accuracy 0.814815 - val F1 0.811501\n",
      "Epoch 78 — train Accuracy : 0.822314 - train F1 0.821952 — val Accuracy 0.851852 - val F1 0.852554\n",
      "Epoch 79 — train Accuracy : 0.809917 - train F1 0.809370 — val Accuracy 0.777778 - val F1 0.784541\n",
      "Epoch 80 — train Accuracy : 0.838843 - train F1 0.838783 — val Accuracy 0.814815 - val F1 0.815385\n",
      "Epoch 81 — train Accuracy : 0.805785 - train F1 0.804431 — val Accuracy 0.851852 - val F1 0.850371\n",
      "Epoch 82 — train Accuracy : 0.847107 - train F1 0.846615 — val Accuracy 0.814815 - val F1 0.819462\n",
      "Epoch 83 — train Accuracy : 0.855372 - train F1 0.855967 — val Accuracy 0.814815 - val F1 0.817405\n",
      "Epoch 84 — train Accuracy : 0.797521 - train F1 0.798085 — val Accuracy 0.740741 - val F1 0.741914\n",
      "Epoch 85 — train Accuracy : 0.822314 - train F1 0.821857 — val Accuracy 0.814815 - val F1 0.810401\n",
      "Epoch 86 — train Accuracy : 0.867769 - train F1 0.866395 — val Accuracy 0.888889 - val F1 0.888889\n",
      "Epoch 87 — train Accuracy : 0.838843 - train F1 0.838198 — val Accuracy 0.851852 - val F1 0.851756\n",
      "Epoch 88 — train Accuracy : 0.822314 - train F1 0.819297 — val Accuracy 0.851852 - val F1 0.853650\n",
      "Epoch 89 — train Accuracy : 0.888430 - train F1 0.889179 — val Accuracy 0.851852 - val F1 0.853909\n",
      "Epoch 90 — train Accuracy : 0.892562 - train F1 0.891930 — val Accuracy 0.888889 - val F1 0.887409\n",
      "Epoch 91 — train Accuracy : 0.847107 - train F1 0.846132 — val Accuracy 0.814815 - val F1 0.811966\n",
      "Epoch 92 — train Accuracy : 0.822314 - train F1 0.819013 — val Accuracy 0.777778 - val F1 0.775995\n",
      "Epoch 93 — train Accuracy : 0.900826 - train F1 0.899840 — val Accuracy 0.814815 - val F1 0.814556\n",
      "Epoch 94 — train Accuracy : 0.880165 - train F1 0.879885 — val Accuracy 0.777778 - val F1 0.775130\n",
      "Epoch 95 — train Accuracy : 0.838843 - train F1 0.838365 — val Accuracy 0.777778 - val F1 0.773029\n",
      "Epoch 96 — train Accuracy : 0.785124 - train F1 0.785117 — val Accuracy 0.703704 - val F1 0.702357\n",
      "Epoch 97 — train Accuracy : 0.880165 - train F1 0.879067 — val Accuracy 0.777778 - val F1 0.781390\n",
      "Epoch 98 — train Accuracy : 0.855372 - train F1 0.853764 — val Accuracy 0.777778 - val F1 0.775212\n",
      "Epoch 99 — train Accuracy : 0.851240 - train F1 0.848885 — val Accuracy 0.740741 - val F1 0.738841\n",
      "Epoch 100 — train Accuracy : 0.900826 - train F1 0.899298 — val Accuracy 0.740741 - val F1 0.742761\n"
     ]
    }
   ],
   "source": [
    "train, validation = model_selection.train_test_split(cancer, test_size=0.1, random_state=1, stratify= cancer['outcome'])\n",
    "\n",
    "model ,train_rmse, train_r2, val_rmse, val_r2, stand  = train_classification_model(configs, train, validation, 100, 8, True, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### New config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d773920eb4041bfa75e9641157f5a1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  1 — train Accuracy : 0.227273 - train F1 0.222359 — val Accuracy 0.296296 - val F1 0.196633\n",
      "Epoch  2 — train Accuracy : 0.293388 - train F1 0.278673 — val Accuracy 0.296296 - val F1 0.196633\n",
      "Epoch  3 — train Accuracy : 0.264463 - train F1 0.257428 — val Accuracy 0.259259 - val F1 0.193603\n",
      "Epoch  4 — train Accuracy : 0.429752 - train F1 0.424383 — val Accuracy 0.703704 - val F1 0.700486\n",
      "Epoch  5 — train Accuracy : 0.615702 - train F1 0.611543 — val Accuracy 0.703704 - val F1 0.695971\n",
      "Epoch  6 — train Accuracy : 0.632231 - train F1 0.629555 — val Accuracy 0.814815 - val F1 0.809318\n",
      "Epoch  7 — train Accuracy : 0.727273 - train F1 0.726641 — val Accuracy 0.777778 - val F1 0.772487\n",
      "Epoch  8 — train Accuracy : 0.760331 - train F1 0.758163 — val Accuracy 0.851852 - val F1 0.850371\n",
      "Epoch  9 — train Accuracy : 0.789256 - train F1 0.787477 — val Accuracy 0.851852 - val F1 0.846398\n",
      "Epoch 10 — train Accuracy : 0.818182 - train F1 0.815731 — val Accuracy 0.851852 - val F1 0.847678\n",
      "Epoch 11 — train Accuracy : 0.838843 - train F1 0.837976 — val Accuracy 0.777778 - val F1 0.773745\n",
      "Epoch 12 — train Accuracy : 0.838843 - train F1 0.836642 — val Accuracy 0.814815 - val F1 0.811352\n",
      "Epoch 13 — train Accuracy : 0.876033 - train F1 0.873485 — val Accuracy 0.814815 - val F1 0.813032\n",
      "Epoch 14 — train Accuracy : 0.863636 - train F1 0.862951 — val Accuracy 0.888889 - val F1 0.888630\n",
      "Epoch 15 — train Accuracy : 0.884298 - train F1 0.883564 — val Accuracy 0.888889 - val F1 0.886572\n",
      "Epoch 16 — train Accuracy : 0.913223 - train F1 0.913115 — val Accuracy 0.851852 - val F1 0.848528\n",
      "Epoch 17 — train Accuracy : 0.925620 - train F1 0.925580 — val Accuracy 0.851852 - val F1 0.851852\n",
      "Epoch 18 — train Accuracy : 0.888430 - train F1 0.887602 — val Accuracy 0.851852 - val F1 0.854701\n",
      "Epoch 19 — train Accuracy : 0.909091 - train F1 0.908874 — val Accuracy 0.888889 - val F1 0.888889\n",
      "Epoch 20 — train Accuracy : 0.909091 - train F1 0.908900 — val Accuracy 0.851852 - val F1 0.850668\n",
      "Epoch 21 — train Accuracy : 0.842975 - train F1 0.837647 — val Accuracy 0.925926 - val F1 0.925332\n",
      "Epoch 22 — train Accuracy : 0.917355 - train F1 0.916881 — val Accuracy 0.888889 - val F1 0.891072\n",
      "Epoch 23 — train Accuracy : 0.938017 - train F1 0.938022 — val Accuracy 0.888889 - val F1 0.886437\n",
      "Epoch 24 — train Accuracy : 0.900826 - train F1 0.899460 — val Accuracy 0.851852 - val F1 0.847678\n",
      "Epoch 25 — train Accuracy : 0.933884 - train F1 0.933395 — val Accuracy 0.851852 - val F1 0.848468\n",
      "Epoch 26 — train Accuracy : 0.913223 - train F1 0.912654 — val Accuracy 0.962963 - val F1 0.962628\n",
      "Epoch 27 — train Accuracy : 0.789256 - train F1 0.789410 — val Accuracy 0.888889 - val F1 0.891072\n",
      "Epoch 28 — train Accuracy : 0.917355 - train F1 0.916654 — val Accuracy 0.925926 - val F1 0.925332\n",
      "Epoch 29 — train Accuracy : 0.938017 - train F1 0.938368 — val Accuracy 0.888889 - val F1 0.886572\n",
      "Epoch 30 — train Accuracy : 0.909091 - train F1 0.909359 — val Accuracy 0.888889 - val F1 0.889804\n",
      "Epoch 31 — train Accuracy : 0.938017 - train F1 0.938133 — val Accuracy 0.888889 - val F1 0.886572\n",
      "Epoch 32 — train Accuracy : 0.909091 - train F1 0.908624 — val Accuracy 0.888889 - val F1 0.886572\n",
      "Epoch 33 — train Accuracy : 0.958678 - train F1 0.958531 — val Accuracy 0.962963 - val F1 0.962704\n",
      "Epoch 34 — train Accuracy : 0.946281 - train F1 0.946260 — val Accuracy 0.888889 - val F1 0.886572\n",
      "Epoch 35 — train Accuracy : 0.938017 - train F1 0.938657 — val Accuracy 0.925926 - val F1 0.925332\n",
      "Epoch 36 — train Accuracy : 0.971074 - train F1 0.970821 — val Accuracy 0.925926 - val F1 0.925332\n",
      "Epoch 37 — train Accuracy : 0.946281 - train F1 0.945682 — val Accuracy 0.962963 - val F1 0.962628\n",
      "Epoch 38 — train Accuracy : 0.900826 - train F1 0.900806 — val Accuracy 0.925926 - val F1 0.925332\n",
      "Epoch 39 — train Accuracy : 0.946281 - train F1 0.946672 — val Accuracy 0.925926 - val F1 0.925332\n",
      "Epoch 40 — train Accuracy : 0.954545 - train F1 0.954702 — val Accuracy 0.925926 - val F1 0.925332\n",
      "Epoch 41 — train Accuracy : 0.933884 - train F1 0.934160 — val Accuracy 0.962963 - val F1 0.962704\n",
      "Epoch 42 — train Accuracy : 0.954545 - train F1 0.954490 — val Accuracy 0.962963 - val F1 0.962704\n",
      "Epoch 43 — train Accuracy : 0.954545 - train F1 0.954784 — val Accuracy 0.925926 - val F1 0.925332\n",
      "Epoch 44 — train Accuracy : 0.958678 - train F1 0.958572 — val Accuracy 0.962963 - val F1 0.962628\n",
      "Epoch 45 — train Accuracy : 0.938017 - train F1 0.938447 — val Accuracy 1.000000 - val F1 1.000000\n",
      "Epoch 46 — train Accuracy : 0.925620 - train F1 0.925375 — val Accuracy 0.925926 - val F1 0.925332\n",
      "Epoch 47 — train Accuracy : 0.954545 - train F1 0.954665 — val Accuracy 0.962963 - val F1 0.962704\n",
      "Epoch 48 — train Accuracy : 0.925620 - train F1 0.925956 — val Accuracy 0.962963 - val F1 0.962628\n",
      "Epoch 49 — train Accuracy : 0.921488 - train F1 0.921232 — val Accuracy 0.925926 - val F1 0.925332\n",
      "Epoch 50 — train Accuracy : 0.909091 - train F1 0.908863 — val Accuracy 1.000000 - val F1 1.000000\n",
      "Epoch 51 — train Accuracy : 0.954545 - train F1 0.954532 — val Accuracy 1.000000 - val F1 1.000000\n",
      "Epoch 52 — train Accuracy : 0.950413 - train F1 0.950298 — val Accuracy 0.925926 - val F1 0.925332\n",
      "Epoch 53 — train Accuracy : 0.938017 - train F1 0.937743 — val Accuracy 1.000000 - val F1 1.000000\n",
      "Epoch 54 — train Accuracy : 0.946281 - train F1 0.945950 — val Accuracy 1.000000 - val F1 1.000000\n",
      "Epoch 55 — train Accuracy : 0.913223 - train F1 0.913022 — val Accuracy 1.000000 - val F1 1.000000\n",
      "Epoch 56 — train Accuracy : 0.929752 - train F1 0.929699 — val Accuracy 1.000000 - val F1 1.000000\n",
      "Epoch 57 — train Accuracy : 0.929752 - train F1 0.929329 — val Accuracy 1.000000 - val F1 1.000000\n",
      "Epoch 58 — train Accuracy : 0.958678 - train F1 0.958369 — val Accuracy 0.888889 - val F1 0.886437\n",
      "Epoch 59 — train Accuracy : 0.950413 - train F1 0.949978 — val Accuracy 0.962963 - val F1 0.962369\n",
      "Epoch 60 — train Accuracy : 0.950413 - train F1 0.950890 — val Accuracy 1.000000 - val F1 1.000000\n",
      "Epoch 61 — train Accuracy : 0.958678 - train F1 0.959022 — val Accuracy 0.962963 - val F1 0.962704\n",
      "Epoch 62 — train Accuracy : 0.950413 - train F1 0.950526 — val Accuracy 1.000000 - val F1 1.000000\n",
      "Epoch 63 — train Accuracy : 0.942149 - train F1 0.941164 — val Accuracy 0.962963 - val F1 0.962704\n",
      "Epoch 64 — train Accuracy : 0.946281 - train F1 0.946444 — val Accuracy 0.962963 - val F1 0.962704\n",
      "Epoch 65 — train Accuracy : 0.925620 - train F1 0.926085 — val Accuracy 0.925926 - val F1 0.925332\n",
      "Epoch 66 — train Accuracy : 0.950413 - train F1 0.950294 — val Accuracy 0.962963 - val F1 0.962704\n",
      "Epoch 67 — train Accuracy : 0.896694 - train F1 0.896908 — val Accuracy 0.888889 - val F1 0.886437\n",
      "Epoch 68 — train Accuracy : 0.909091 - train F1 0.909455 — val Accuracy 0.962963 - val F1 0.962704\n",
      "Epoch 69 — train Accuracy : 0.950413 - train F1 0.950543 — val Accuracy 0.925926 - val F1 0.925332\n",
      "Epoch 70 — train Accuracy : 0.946281 - train F1 0.946158 — val Accuracy 1.000000 - val F1 1.000000\n",
      "Epoch 71 — train Accuracy : 0.938017 - train F1 0.938027 — val Accuracy 0.925926 - val F1 0.925332\n",
      "Epoch 72 — train Accuracy : 0.933884 - train F1 0.934268 — val Accuracy 1.000000 - val F1 1.000000\n",
      "Epoch 73 — train Accuracy : 0.942149 - train F1 0.941895 — val Accuracy 1.000000 - val F1 1.000000\n",
      "Epoch 74 — train Accuracy : 0.958678 - train F1 0.958791 — val Accuracy 1.000000 - val F1 1.000000\n",
      "Epoch 75 — train Accuracy : 0.913223 - train F1 0.913283 — val Accuracy 1.000000 - val F1 1.000000\n",
      "Epoch 76 — train Accuracy : 0.950413 - train F1 0.950550 — val Accuracy 1.000000 - val F1 1.000000\n",
      "Epoch 77 — train Accuracy : 0.950413 - train F1 0.950801 — val Accuracy 1.000000 - val F1 1.000000\n",
      "Epoch 78 — train Accuracy : 0.933884 - train F1 0.934077 — val Accuracy 1.000000 - val F1 1.000000\n",
      "Epoch 79 — train Accuracy : 0.950413 - train F1 0.950301 — val Accuracy 1.000000 - val F1 1.000000\n",
      "Epoch 80 — train Accuracy : 0.950413 - train F1 0.950111 — val Accuracy 1.000000 - val F1 1.000000\n",
      "Epoch 81 — train Accuracy : 0.938017 - train F1 0.938187 — val Accuracy 1.000000 - val F1 1.000000\n",
      "Epoch 82 — train Accuracy : 0.925620 - train F1 0.926155 — val Accuracy 0.962963 - val F1 0.962704\n",
      "Epoch 83 — train Accuracy : 0.942149 - train F1 0.941962 — val Accuracy 0.962963 - val F1 0.962704\n",
      "Epoch 84 — train Accuracy : 0.946281 - train F1 0.945642 — val Accuracy 0.962963 - val F1 0.962704\n",
      "Epoch 85 — train Accuracy : 0.950413 - train F1 0.950612 — val Accuracy 0.962963 - val F1 0.962704\n",
      "Epoch 86 — train Accuracy : 0.950413 - train F1 0.949879 — val Accuracy 1.000000 - val F1 1.000000\n",
      "Epoch 87 — train Accuracy : 0.946281 - train F1 0.946447 — val Accuracy 1.000000 - val F1 1.000000\n",
      "Epoch 88 — train Accuracy : 0.938017 - train F1 0.938341 — val Accuracy 1.000000 - val F1 1.000000\n",
      "Epoch 89 — train Accuracy : 0.946281 - train F1 0.946582 — val Accuracy 1.000000 - val F1 1.000000\n",
      "Epoch 90 — train Accuracy : 0.954545 - train F1 0.954543 — val Accuracy 1.000000 - val F1 1.000000\n",
      "Epoch 91 — train Accuracy : 0.929752 - train F1 0.929647 — val Accuracy 0.962963 - val F1 0.962628\n",
      "Epoch 92 — train Accuracy : 0.946281 - train F1 0.947083 — val Accuracy 0.925926 - val F1 0.923810\n",
      "Epoch 93 — train Accuracy : 0.925620 - train F1 0.926033 — val Accuracy 0.962963 - val F1 0.962628\n",
      "Epoch 94 — train Accuracy : 0.933884 - train F1 0.933186 — val Accuracy 1.000000 - val F1 1.000000\n",
      "Epoch 95 — train Accuracy : 0.958678 - train F1 0.958264 — val Accuracy 0.962963 - val F1 0.962369\n",
      "Epoch 96 — train Accuracy : 0.950413 - train F1 0.950870 — val Accuracy 1.000000 - val F1 1.000000\n",
      "Epoch 97 — train Accuracy : 0.942149 - train F1 0.941624 — val Accuracy 1.000000 - val F1 1.000000\n",
      "Epoch 98 — train Accuracy : 0.933884 - train F1 0.934241 — val Accuracy 1.000000 - val F1 1.000000\n",
      "Epoch 99 — train Accuracy : 0.975207 - train F1 0.975320 — val Accuracy 0.962963 - val F1 0.962628\n",
      "Epoch 100 — train Accuracy : 0.966942 - train F1 0.966793 — val Accuracy 1.000000 - val F1 1.000000\n"
     ]
    }
   ],
   "source": [
    "configs = Details(\n",
    "    task = 'classification',\n",
    "    pred_len = 4,      \n",
    "    seq_len  = 268,\n",
    "    d_ff = 32,\n",
    "    patch_len= 16, \n",
    "    stride = 8, \n",
    "    llm_layers = 8, \n",
    "    description = \"This dataset contains near-infrared (NIR) spectroscopy measurements in a \" \\\n",
    "                  \"phosphate-buffered saline (PBS) solution. Each spectrum includes absorbance values across \" \\\n",
    "                  \"different wavelengths. The goal is to predict the classify each spectra given the different wavelengths \" ,\n",
    "    dropout = 0.2, \n",
    "    n_heads = 8, #Increased\n",
    "    d_model = 16,\n",
    "    enc_in = 1,\n",
    "    d_llm = 4096,\n",
    "    llm_model = 'LLAMA',\n",
    "    lr = 0.0001\n",
    ")\n",
    "\n",
    "train, validation = model_selection.train_test_split(cancer, test_size=0.1, random_state=1, stratify= cancer['outcome'])\n",
    "\n",
    "model ,train_rmse, train_r2, val_rmse, val_r2, stand  = train_classification_model(configs, train, validation, 100, 8, True, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'class_model_weights3.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b4fb5744bc34a5d999e455e411752ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "configs = Details(\n",
    "    task = 'classification',\n",
    "    pred_len = 4,      \n",
    "    seq_len  = 268,\n",
    "    d_ff = 32,\n",
    "    patch_len= 16, \n",
    "    stride = 8, \n",
    "    llm_layers = 8, \n",
    "    description = \"This dataset contains near-infrared (NIR) spectroscopy measurements in a \" \\\n",
    "                  \"phosphate-buffered saline (PBS) solution. Each spectrum includes absorbance values across \" \\\n",
    "                  \"different wavelengths. The goal is to predict the classify each spectra given the different wavelengths \" ,\n",
    "    dropout = 0.2, \n",
    "    n_heads = 8, #Increased\n",
    "    d_model = 16,\n",
    "    enc_in = 1,\n",
    "    d_llm = 4096,\n",
    "    llm_model = 'LLAMA',\n",
    "    lr = 0.0001\n",
    ")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model  = Model(configs).to(device)\n",
    "\n",
    "model.load_state_dict(torch.load(\"class_model_weights3.pth\", map_location=device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test in new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('Testing_data_Italian.csv')\n",
    "del test['Unnamed: 0']\n",
    "test = test[['outcome'] + list(test.columns[:-1])]\n",
    "le = LabelEncoder()\n",
    "test['outcome'] = le.fit_transform(test['outcome'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 810.00 MiB. GPU 0 has a total capacity of 23.68 GiB of which 486.69 MiB is free. Including non-PyTorch memory, this process has 23.01 GiB memory in use. Of the allocated memory 18.21 GiB is allocated by PyTorch, and 3.53 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 33\u001b[0m\n\u001b[1;32m     29\u001b[0m y_test \u001b[38;5;241m=\u001b[39m y_test\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 33\u001b[0m     y_pred \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     36\u001b[0m criterion \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss()\n\u001b[1;32m     38\u001b[0m test_acc \u001b[38;5;241m=\u001b[39m accuracy_score(y_test\u001b[38;5;241m.\u001b[39msqueeze()\u001b[38;5;241m.\u001b[39mcpu(), torch\u001b[38;5;241m.\u001b[39margmax(y_pred, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39msqueeze()\u001b[38;5;241m.\u001b[39mcpu())\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[3], line 168\u001b[0m, in \u001b[0;36mModel.forward\u001b[0;34m(self, x_enc, x_mark_enc, x_dec, x_mark_dec, mask)\u001b[0m\n\u001b[1;32m    166\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x_enc, x_mark_enc, x_dec, x_mark_dec, mask\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    167\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtask \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclassification\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m--> 168\u001b[0m         f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclassify\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_enc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_mark_enc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_dec\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_mark_dec\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    169\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    170\u001b[0m         dec_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforecast(x_enc, x_mark_enc, x_dec, x_mark_dec)\n",
      "Cell \u001b[0;32mIn[3], line 289\u001b[0m, in \u001b[0;36mModel.classify\u001b[0;34m(self, x_enc, x_mark_enc, x_dec, x_mark_dec)\u001b[0m\n\u001b[1;32m    286\u001b[0m enc_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreprogramming_layer(enc_out, source_embeddings, source_embeddings)\n\u001b[1;32m    288\u001b[0m llama_enc_out \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([prompt_embeddings, enc_out], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m--> 289\u001b[0m dec_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mllm_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mllama_enc_out\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mlast_hidden_state\n\u001b[1;32m    291\u001b[0m dec_out \u001b[38;5;241m=\u001b[39m dec_out[:, :, :\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39md_ff]\n\u001b[1;32m    293\u001b[0m dec_out \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mreshape(dec_out, (\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, n_vars, dec_out\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m], dec_out\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]))\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/utils/generic.py:969\u001b[0m, in \u001b[0;36mcan_return_tuple.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    966\u001b[0m     set_attribute_for_modules(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_is_top_level_module\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    968\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 969\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    970\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_requested_to_return_tuple \u001b[38;5;129;01mor\u001b[39;00m (is_configured_to_return_tuple \u001b[38;5;129;01mand\u001b[39;00m is_top_level_module):\n\u001b[1;32m    971\u001b[0m         output \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mto_tuple()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:453\u001b[0m, in \u001b[0;36mLlamaModel.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, cache_position, **flash_attn_kwargs)\u001b[0m\n\u001b[1;32m    450\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states:\n\u001b[1;32m    451\u001b[0m     all_hidden_states \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (hidden_states,)\n\u001b[0;32m--> 453\u001b[0m layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    454\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    455\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    456\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    457\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    458\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    459\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    460\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    461\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    462\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mflash_attn_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    463\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    465\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    467\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/modeling_layers.py:48\u001b[0m, in \u001b[0;36mGradientCheckpointingLayer.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgradient_checkpointing \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining:\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(partial(\u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs), \u001b[38;5;241m*\u001b[39margs)\n\u001b[0;32m---> 48\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:308\u001b[0m, in \u001b[0;36mLlamaDecoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, position_embeddings, **kwargs)\u001b[0m\n\u001b[1;32m    305\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_layernorm(hidden_states)\n\u001b[1;32m    307\u001b[0m \u001b[38;5;66;03m# Self Attention\u001b[39;00m\n\u001b[0;32m--> 308\u001b[0m hidden_states, self_attn_weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    309\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    310\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    311\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    312\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    313\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    314\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    315\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    316\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    317\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    318\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    319\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n\u001b[1;32m    321\u001b[0m \u001b[38;5;66;03m# Fully Connected\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:265\u001b[0m, in \u001b[0;36mLlamaAttention.forward\u001b[0;34m(self, hidden_states, position_embeddings, attention_mask, past_key_value, cache_position, **kwargs)\u001b[0m\n\u001b[1;32m    262\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    263\u001b[0m         attention_interface \u001b[38;5;241m=\u001b[39m ALL_ATTENTION_FUNCTIONS[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39m_attn_implementation]\n\u001b[0;32m--> 265\u001b[0m attn_output, attn_weights \u001b[38;5;241m=\u001b[39m \u001b[43mattention_interface\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    266\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquery_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkey_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalue_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdropout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention_dropout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscaling\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscaling\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    276\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m attn_output\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m*\u001b[39minput_shape, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mcontiguous()\n\u001b[1;32m    277\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mo_proj(attn_output)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:191\u001b[0m, in \u001b[0;36meager_attention_forward\u001b[0;34m(module, query, key, value, attention_mask, scaling, dropout, **kwargs)\u001b[0m\n\u001b[1;32m    188\u001b[0m key_states \u001b[38;5;241m=\u001b[39m repeat_kv(key, module\u001b[38;5;241m.\u001b[39mnum_key_value_groups)\n\u001b[1;32m    189\u001b[0m value_states \u001b[38;5;241m=\u001b[39m repeat_kv(value, module\u001b[38;5;241m.\u001b[39mnum_key_value_groups)\n\u001b[0;32m--> 191\u001b[0m attn_weights \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmatmul\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey_states\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtranspose\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mscaling\u001b[49m\n\u001b[1;32m    192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attention_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    193\u001b[0m     causal_mask \u001b[38;5;241m=\u001b[39m attention_mask[:, :, :, : key_states\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m]]\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 810.00 MiB. GPU 0 has a total capacity of 23.68 GiB of which 486.69 MiB is free. Including non-PyTorch memory, this process has 23.01 GiB memory in use. Of the allocated memory 18.21 GiB is allocated by PyTorch, and 3.53 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "#Normalize test data\n",
    "test.columns = test.columns.astype(str)\n",
    "scaler = StandardScaler()\n",
    "normalized = scaler.fit_transform(test.iloc[:,1:].T).T\n",
    "X_test_n = pd.DataFrame(normalized, columns = test.iloc[:,1:].columns)\n",
    "\n",
    "train, validation = model_selection.train_test_split(cancer, test_size=0.1, random_state=1, stratify= cancer['outcome'])\n",
    "\n",
    "train.columns = train.columns.astype(str)\n",
    "scaler = StandardScaler()\n",
    "normalized = scaler.fit_transform(train.iloc[:,1:].T).T\n",
    "X_train_n = pd.DataFrame(normalized, columns = train.iloc[:,1:].columns)\n",
    "\n",
    "#Scale wavelengths\n",
    "stand = StandardScaler()\n",
    "scaled = stand.fit_transform(X_train_n)\n",
    "X_train_n = pd.DataFrame(scaled, columns = X_train_n.columns)\n",
    "\n",
    "scaled = stand.transform(X_test_n)\n",
    "X_test_n = pd.DataFrame(scaled, columns = X_test_n.columns)\n",
    "\n",
    "X_test = torch.from_numpy(X_test_n.to_numpy().astype('float32')).unsqueeze(-1) \n",
    "y_test = torch.from_numpy(test.iloc[:,0].to_numpy().astype('int64')).unsqueeze(1).unsqueeze(1)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "X_test = X_test.to(device)\n",
    "y_test = y_test.to(device)\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    y_pred = model(X_test, None, None, None)\n",
    "\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "test_acc = accuracy_score(y_test.squeeze().cpu(), torch.argmax(y_pred, dim=1).squeeze().cpu())\n",
    "test_f1 = f1_score(y_test.squeeze().cpu(), torch.argmax(y_pred, dim=1).squeeze().cpu(), average='weighted')\n",
    "\n",
    "\n",
    "print(f\"Test Accuracy {test_acc:.6f} — Test F1 {test_f1:.6f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.98148148 0.96296296 1.         1.         0.9245283 ]\n",
      "0.9737945492662474\n"
     ]
    }
   ],
   "source": [
    "X_train = cancer.iloc[:,1:]\n",
    "y_train = cancer.iloc[:,0]\n",
    "\n",
    "rfc = RandomForestClassifier()\n",
    "\n",
    "cv_scores = model_selection.cross_val_score(rfc, X_train, y_train, cv = 5, scoring= 'accuracy')\n",
    "print(cv_scores)\n",
    "print(cv_scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-6 {color: black;background-color: white;}#sk-container-id-6 pre{padding: 0;}#sk-container-id-6 div.sk-toggleable {background-color: white;}#sk-container-id-6 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-6 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-6 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-6 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-6 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-6 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-6 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-6 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-6 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-6 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-6 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-6 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-6 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-6 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-6 div.sk-item {position: relative;z-index: 1;}#sk-container-id-6 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-6 div.sk-item::before, #sk-container-id-6 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-6 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-6 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-6 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-6 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-6 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-6 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-6 div.sk-label-container {text-align: center;}#sk-container-id-6 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-6 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-6\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestClassifier()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-6\" type=\"checkbox\" checked><label for=\"sk-estimator-id-6\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomForestClassifier()"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfc.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy : 1.000000 - Test accuracy 1.000000\n"
     ]
    }
   ],
   "source": [
    "X_test = test.iloc[:,1:]\n",
    "y_test = test.iloc[:,0]\n",
    "\n",
    "train_acc = rfc.score(X_train, y_train)\n",
    "test_acc = rfc.score(X_test, y_test)\n",
    "\n",
    "print(f'Train accuracy : {train_acc:.6f} - Test accuracy {test_acc:.6f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
